{"docstore/metadata": {"6094fcc0-b443-44dd-8634-5068f3ad0090": {"doc_hash": "00d10a506eafc7a3831229cf5e062d59b437c849aa006bcb0bf468c53263ba6a"}, "60689eb1-3fe9-4624-99ba-8e46b9203b22": {"doc_hash": "dcfb1239b2bf612e7facdd74c0fd8a9f17c6099c030c92986684121022943bde"}, "899fc809-a5e2-4860-b33d-5006dd337076": {"doc_hash": "3f8c3a2036c5851d1e56b8c544fd0e8493f057943b37b7eb5aceb3f74aca42d1"}, "c60b287f-80a6-43c0-9f72-d1c55c2e13a9": {"doc_hash": "e9a95f65a1052b84946946eae225c63e0f4a3208cb9c52328abeed8ba2203a0c"}, "5810e7e5-fdf8-42b5-8543-db2a98d25510": {"doc_hash": "30988781d8e965f02d724e8c016146ccdb8570bb73bde3c6a33c5355ec374f11"}, "ea6a59f1-258d-44a5-817e-04c665a9ea93": {"doc_hash": "cda702855f29c341c990013588963afc720eeb861b0de14291182c5c3521ed1a"}, "fd9ee88d-ef27-4bad-af59-a1e83479a348": {"doc_hash": "451ed6f26ff7f92cb7cab7632585ffe9bf90998ddf049f5719e2df0a07957a5e"}, "878e7182-3c06-41c5-bb5c-8222ff1a30f8": {"doc_hash": "1d5ff12a15cb1493f73d9947b794d25265c94ab04b19c921bfa0f487e53a88d5"}, "0164c163-7026-4979-826c-001726140c3a": {"doc_hash": "db58b076050df190d1883399079c4175ba60b6a884c56fa2fe7da217975b7485"}, "7bceafa1-d21a-4ac8-aabd-19eec5b0b212": {"doc_hash": "e3804dcaceaa6245dc606c909aabca949e28ace254ef7e0f8db91edd3d0c894a"}, "0fdabdfd-7bf7-429c-95fd-2843c6327b8e": {"doc_hash": "757605b2edecd0856f24ded5f2771b0b0f1e247fc81b521ed29abd7957c1ba61"}, "9f08d760-a4dd-4617-8ca6-5d1bcaeeb8aa": {"doc_hash": "65edf9de872e1c3c6cead23fac0047dcfc8e9211acb8fd6b51174b96d32e150c"}, "d9ec69d6-ca26-4231-a9d8-cb5302aadb81": {"doc_hash": "2e68dd883055e639ded9ebc5e86e80a419ff0677d39214f62bea54a6163b0d58"}, "fa31846b-7ff6-4025-b334-5531cfd98702": {"doc_hash": "2d6db71f9c6891994b3726dd0d6c092b2494169907f7b93c0e0b78e028adcffd"}, "8a4bc4f0-d445-467c-b12f-dd07a019b214": {"doc_hash": "ba6fdbb83797d3a611711ed0597ddc26030efd8f6452d8330055cbbdfcd44984"}, "89d97c01-a8ef-4666-84e0-9e1236ab18f1": {"doc_hash": "2252d55c82f90ea24b6c6faed24546e7030abb3c968463af9c50f8bfe06fe6de"}, "3fb7e556-4484-4834-ae7d-1d06d2a19cd1": {"doc_hash": "e22f23c0501dacd2a77c6e01eebd4e87c01e06430f45a65bf9c11d308d7d1643"}, "f24aea0b-aeb1-463b-acd4-d86761bb6f62": {"doc_hash": "6066069acc8a8cddef9ebe910c700100cfae5d2c7f6395e2cde1cb23a5dec4cc"}, "33965993-5109-4044-a90e-841c29acf34a": {"doc_hash": "02ae8ff140bf4bf3815dadf8b9386c5f55ae1cc6769b914df3776aefc0ae39ae"}, "7f3d4a4c-f62a-44a4-a220-26330c1afbe4": {"doc_hash": "a2515477d90d25502a96e713e05f20cf0aeeb4349492b8282c401f58e03075b9"}, "3947ecce-c8b3-4082-ae15-cb5cdb942e1c": {"doc_hash": "1969e68c643d41b454de3c20f0da8718b4bf96a530b6ae73a643cc516c0eb9a2"}, "120b9a6a-2cb6-4c6d-a97f-74212c30e337": {"doc_hash": "03c5285d84fb619a09d1840c12fe5b0b303703050f7683734826b9c1885b5d83"}, "e916bb8a-c3a6-4624-b2f4-54763c967b89": {"doc_hash": "567d69e74a93bc18e353ab25e82d45d33b65a2bb49fe929138c939fcbf27d27e"}, "65be8e2b-575d-4634-b303-5158d365fe84": {"doc_hash": "226671591eeced618832c6fe113e38f70510d01d6c7a632fba36db58bb003598"}, "5639c14e-d399-4eb7-a1af-a4fd7e42bbbd": {"doc_hash": "4b3ed4eca3160b37c386350475894c0caefd6185282449d5d6f6f571baf99f2a"}, "74e4b992-1891-4633-ba1b-db568a01b6df": {"doc_hash": "867b51ade86b059041936231fd9eef43d5d81323432a96fad46cba7bd65931fd"}, "39263651-0b90-4dba-8f01-d1be11ceff5f": {"doc_hash": "36c1ad9148e19309e6e0f51a832c4019f7ae628cf11c6e1da2ae9b0725351330"}, "8bb33091-ca08-46bb-958a-b1fe79851bac": {"doc_hash": "7741f7f60e83685ea4cac8bfaaaf7909af838051edc4ad3676790c7f0674e239"}, "cdb06cdc-5247-405f-bbcf-b114ec1912f3": {"doc_hash": "68561c8ae84a9e5f0cd840e526fefd3125b778cab5c308458508d7e8188620ea"}, "cc4df5ae-574d-4fbb-9a35-07ae63fe45e9": {"doc_hash": "cdc6289d9edf93a4a9f9157ff0acc853b7d15bb43b2a791eedb56d385613a427"}, "6951bad1-e967-4bea-ba56-06374af12b22": {"doc_hash": "857e42a0227cd42c27d450c8d87bf48c76bfad303a6bcec7635cf6ecef30fd74"}, "84f6a62c-c321-476d-84c1-c03c49274070": {"doc_hash": "0fae1ce30082a5784aa305e2277f7823edb5286508e2125cdd7c2f17a3f0bdd5"}, "82084c07-7813-466b-ac59-f31939648ddc": {"doc_hash": "ebb1b3d3f7cdc9767f33564a7c487ffbc2541a5118f010d1b93a86a7b59ab59e"}, "1b59759f-8749-412e-9c09-aec7260d6490": {"doc_hash": "44b254a64b81aefad06a7cf0ce2d0e1bf51a83e0b106889662a43d0cd1f67fa9"}, "d1ff5289-b125-4279-ae7d-f25aa654d9c1": {"doc_hash": "5b676317ce1aff5e28001a2d99ccefa054ff3fdc02e3367b73e143aa0e7df332"}, "46430440-97d6-44cb-b6e1-30f0be686743": {"doc_hash": "4e84bc1cdc15ce6c97b15a12e7495249247ea93f92011a5d80ac914c360b48b9"}, "7e5136e0-957e-4acd-9c39-81b48e3aa2be": {"doc_hash": "e1738db25e459b714e8428411bcea74f7ccd8a1ccfc3c60d735f02775996dc15"}, "5691d812-406c-4a90-b6f4-d14b890dfed4": {"doc_hash": "2446a16eff93bddd79655330f18868afd0e459d7743f547b95dcc95dd94c7ea7"}, "0446435b-2869-43da-94ca-11e81ed20e45": {"doc_hash": "30581976b063000d64d260331607ff2828ff5542e18c66efeed37043762b0133"}, "2b346bf3-5e5e-490c-9356-87ea13d9fadd": {"doc_hash": "fd3165e2dab3bd0e1565783118e328713ce08f7eedf51c4c6b8533447487663b"}, "0c4b21e2-ab4c-42fd-ad6a-145261c6023b": {"doc_hash": "ef7b580d01cb404b41ca9ec84f24d77eea7300376101612328792aad37ea7ae6"}, "784875a5-bcad-4d00-bca6-cf90a03b8861": {"doc_hash": "371453ddecfdf3d3c3da10fd1e156fede5a622f722c8d362570b424de215b627"}, "d642a07d-31db-430c-b5ad-22309d3056c9": {"doc_hash": "28967c75293714ae9e4c1193e9cd39732d45ab63f92dc59dea3b3d0415b72914"}, "b8136704-eb51-4908-b0bb-4ea95f5bea8c": {"doc_hash": "c8daba564c781bd811fa1c23967265644d6791612a2da1702f3d4a99a3447ce5"}, "f22f944c-8fd3-4a9d-864b-72e74a5e44d1": {"doc_hash": "cf27511ea6e8517a9215091edd399ffe11eea2e966a982bb57dfe1884bad188b"}, "680f93a1-410f-4b25-8d00-9e260af20180": {"doc_hash": "b7b4e0d61f4732e6801497317c0bd410f9c777b5bb1bc8ab8311a195a93f867e"}, "fac1b167-091e-4246-88c3-519a3a708ef6": {"doc_hash": "612655915f16a22cbcb6ceb0ff2121a31c496b56fb5bfc38fed4699d096ce09b"}, "07017951-119b-4a87-baee-cb8a70ab735b": {"doc_hash": "65b49a3495849bbbf9f197906d97b5800714eb74c62c30b38fa29bf8f1848f5f"}, "006a20de-39fe-46f0-8cf9-dc6353d132d8": {"doc_hash": "8ef5427196ad40eab469ad17e4abdce0cc3e55037a3c7b87981b14b00a03a0d5"}, "b93ae5d7-c1d6-4a9e-a5e0-3e78eb88ef67": {"doc_hash": "33e33fdd9d7eebe08314094bde870cbad17bcb2a26472a69bbbb7ed2b99f0537"}, "b5a55f82-e6fa-41aa-9469-b6f50187b3b0": {"doc_hash": "7d3336647328297742ddfb76f8368a03d07957e385a1e0fdae5815de54076024"}, "9f5bb9af-4b8c-4008-92f9-9df33a2c1f79": {"doc_hash": "ed9ce6454b3adee7db07da2f1a14a22ed952be161fbc2fd52b6cb819c00ab0bf"}, "e467744e-4b41-4485-a417-77e93a794191": {"doc_hash": "df36f30ba578174c73a880b95667cc9860e23baaa3f034aa20bc91ffccbebc8c"}, "3477f662-34bc-4321-a72e-9aee677858b8": {"doc_hash": "40cfa9fa55514d9ba589c0288a542c87b8113f63e747502a99f76858d91ebaf8"}, "a309121d-5285-4fed-9729-74a1185a37f3": {"doc_hash": "338ad8d49dc681697fa6685405a660908391c1fd4f35f316bbee7f333333dbcb"}, "a5854754-25e7-4ead-bd07-5ba54221941e": {"doc_hash": "fa458e4b5be66356352fa62838880b766b69e03eb2cbe107af0590db10ae2ece"}, "9584486d-8aa8-4cfe-8281-5a59b69cd8ee": {"doc_hash": "0b8ebb72d8c916c29ceb3129887d467617d7f176beeba8636226639caaffd392"}, "8cda7db5-b1a5-40c0-b518-132c38ff3874": {"doc_hash": "438bfb132913c3e229d5299474ddae4443330a759022d801826a2c488de6e8a6"}, "7254cc94-c98c-430f-8a32-ba8d5bbadb58": {"doc_hash": "162eed5dea7e44996551b1e3b0f0fc73a91fa380e23a756f5543219efb76c3fa"}, "fe67addb-a6d7-477d-a4f3-e0da12d12eb2": {"doc_hash": "4c2383e6c978e145c1068485bc7270aa587db60040e6736111af70fda9e9182f"}, "891c5a56-9517-408f-ba99-3568d7d04bdd": {"doc_hash": "9ed17cebd8c2ba0a1792b2c32febfddb6d02464df18556e9135101d557e8b50f"}, "f737561b-3951-41b4-929c-55a9fd235238": {"doc_hash": "ee99b0e0bff28283a739511bf45d477fe9673cd2ee070307a5f5bb8a0b70a243"}, "ddf66051-632a-4c96-8c64-0243cae5af9d": {"doc_hash": "6d0c3b1ae2e1e17683d6546847957f5fdc91ef7713b380d3f934389273b63944"}, "6ed7fefb-4f6b-4b74-b434-63c25726b1be": {"doc_hash": "ceab352590e23c9b8d9aee10c4ff971a59dc77e626849545cb137759b2dcf164"}, "e6e0b8cd-6d94-4b86-bd0b-cf43bb78c998": {"doc_hash": "61d266f7b5d932fc0f9fdc7b6ef074a3aa2fa330549be4a14b098f9d29132f2c"}, "d17cc04f-2556-4be4-8492-183bde7500b5": {"doc_hash": "ccad3d8d1eec22cf6f16609e35f553ef7c4c83e66539c1bb4d4172e8bdcb96e6"}, "bc11ff53-4ff5-417e-95fe-d05fcaf270ee": {"doc_hash": "cfc6e2a3d8397e3def6e76d686e567f020493a894a7cacf8ef3d254934e4fbe5"}, "c41f80f8-2b1a-4e9a-aefa-57ac6eccb785": {"doc_hash": "a0f1e0407e8d5e001bdb8aa2aa800b6f948bc1e482453895edf10abd55d4e258"}, "38a47316-3622-4770-ae78-b51ac99dc884": {"doc_hash": "3be3f936f56723eef0249b53c755f434fc14ae0ed0fec378389a695ba8bcc3d5"}, "df0c2233-afc8-42c9-b8ac-d724076b60a7": {"doc_hash": "5b555e085167623b09379a54e6f6c4b95c465b8875d3726b2d2dd12a8b59c1e0"}, "48bb777f-5654-4d58-b6cf-b579e21e6fb9": {"doc_hash": "4c83aae0fac04c7618915b51157c9b4cd6f3a31d4a3c507e3edcf624b6977d2b"}, "c51adbaa-8270-46ba-8cf6-bbb9e291580a": {"doc_hash": "858c082c49d956d5d910346a85334649b944d3428e6c1ea500a719b998b8f540"}, "376fccce-f29f-4d2d-9746-0f3cf167b13b": {"doc_hash": "51fd6ec308c70c04dbafa6eba133460f8930221194281a314eff9100b1b57efc"}, "0938c9b3-cb06-4f3f-a9c1-9af26e07643f": {"doc_hash": "5705cf538860df495d1dc22c10638d5bf8a06267d8da898fb22c5818b385bf9f"}, "e48a3946-2f14-4abc-a2ca-4ae46205d74a": {"doc_hash": "a512438c2be611f821e7126d8843fbc7a626c50c78f4161d9f4bc4ffac95c6c0"}, "6e5a34ea-6545-4637-b648-bc3a2d1168f0": {"doc_hash": "dc57255198d29c5c33c570df8f8e5684a05062db14581055fa396dcb101e73f9"}, "9162b5d8-1bdb-4b21-8319-bb413876ea91": {"doc_hash": "5e93d625b98b7284a4d6416d54c865dca09bc5002a968e98d1b10a5ac55e2505"}, "002a56bd-7e0e-474a-8f2c-623dae5ee463": {"doc_hash": "28aeacff19b32bf06372ffb9cb2f5215cacfc46b2b892821597e66cd1bad146f"}, "fc36ea8f-7747-42f1-96cb-81e60e3248b1": {"doc_hash": "d4f3cafd4cd05c02bb84a5affb0c9925403d5448651e9715baf75f9024e8bc65"}, "28484c8b-0ff3-41f0-b500-bbdf5e37ffff": {"doc_hash": "b6d7186cda3c33d1b44903f9b757b141a585573a0080d3557197e6dc15230b46"}, "d1d84a30-f22f-4147-929a-0de8a22c465c": {"doc_hash": "c23401ca4385e64bd3f3a6385ad8dcd1ed5e682880c9cf5907833cedaaaa4f1f"}, "9a0f0f46-0926-48d0-9fce-9164f33f84c9": {"doc_hash": "ff444ea1a55beadadded4b2a24cd8df77ff443b75718d421369a5135249a49e1"}, "7e5a61a8-cec2-4a35-9420-99ba06a81934": {"doc_hash": "81750c971095aaf11bd5a5b7a879b1eb65ed7bdf6886bb56558904c022308940"}, "65f1e966-8274-4cc7-9b67-ec592f3f979a": {"doc_hash": "e6c29b27cef43878ed0f32afb322a213078b546d3f37e1c63e72aea4e95eea31"}, "3ef2ad90-5594-4ddd-8790-31bac87f157f": {"doc_hash": "2a6e54f1ec17a6847a1fdbef9a06057523f81f473bb14cedfb5dee74909c3ac4"}, "b780f399-be0b-4809-875c-1f1eee3730d6": {"doc_hash": "2a7a19c316304d43b53583b1663a4ddbc1304de30e8df63b36d52f644e0b0756"}, "350c6309-8c08-48db-a63c-c82fef335820": {"doc_hash": "9a2b1bc51866b88a0c6e6bd2df76fafbeeaaed056e8c052c7f20644b9c5a6f23"}, "485880b5-6aa5-4457-9f9f-c0345431acbc": {"doc_hash": "468fe7087160e74be6dd6a9389de32236e39434fc49c9a45f84723d0907c81cc"}, "3d8f3656-6321-42e2-9002-d86c1cd373e6": {"doc_hash": "4780a549dd00b578966ed29c53c88d55ccf8f209ae1bec9bb55482621ee3a81a"}, "38fdb2ad-2e13-4759-bdc7-84eb20045d87": {"doc_hash": "d80e60df3f0f17b38f9f0e904cde40a2f9dbda8a6029ff0f987621decb89a451"}, "9dae6518-a1e9-47e1-a93c-23fe503b873f": {"doc_hash": "af83746e5445ca550c28603adecfececb312dc9cfebd93bc23739defe2d378f3"}, "c6ebfbbf-5ae3-4e87-83ff-f82186330325": {"doc_hash": "b4e2f26339899310d5a2bf8757500a11e26b5cc0b62a8f20581843b98e8cf85d"}, "26e6a64f-61f6-466e-b9dd-bb2955c286b3": {"doc_hash": "8db009ffbe55b5fe5ba5eec022c1ccd4cbc38cbf26a5c8ac64a6c0c0fdf61913"}, "e3da4913-5c12-4be3-86d2-34abf95892dd": {"doc_hash": "e6dc680ae377a003a8d881602c1d8ea582e88c7c9548896360883f62a0440fab"}, "7eeab61e-ba51-404d-9f9a-eecc8bc5a0b5": {"doc_hash": "946536e3e68779357dc63c2846d3966c95369578b30a3cd40eed06a643674343"}, "089efd66-9a30-4768-961a-5f2d28528cb1": {"doc_hash": "83242eeda1b4cfc898716c5094142e88da011362f48c40b3b242d3c71dca768c"}, "243a720a-7a98-47b9-96fd-98eceb370ca4": {"doc_hash": "5a647d55350e630b7cedab722c13a10c4b6cbfcf55eb06158e6dca80064d558c"}, "75cf788c-6b56-47c9-91ed-d23ad70fe197": {"doc_hash": "2aa403aabb6f1a04bc32fdd487c2ded44e7c601056f682134d17207de8e6c23c"}, "73786048-9fed-4015-9ef4-f53b73509fdd": {"doc_hash": "b90f03e6df9fff77d0ea46bb1ff371216222eaeef56811f6153f78fdc46868d3"}, "c36be9fe-2134-4974-9a8a-232520aa1518": {"doc_hash": "d2c0b570d17ace054fef1bd066f03eafc2e8c4ba83c6caf994f7b4586d71d2bc"}, "cd904311-7be8-4d75-a39d-db3e6741e71b": {"doc_hash": "741339cd22ecb2e6b382086c6957f4cc7ecaa6c4df59940726221e17fd89a3b2"}, "61fdfd31-cb4d-4fe2-b1ce-7c3f052da139": {"doc_hash": "c37402c69595c09ff8f2331f033fd32e122835d39c918f04abbfa5fe3cd4e91c"}, "7a67226d-9103-4d88-946f-98a131d7c5ab": {"doc_hash": "e7bda56968c364eb1062b78d7c47732b010e9d49d20bf9e5254a403e2fadfadb"}, "a70caa47-ac2e-4bea-8807-5be978f21f00": {"doc_hash": "499693b39d77768432c6aebcc9a290cdda8cf0d76936b2b2fa154e4d44fa404b"}, "21ddfb01-65c5-4553-ae6f-8922232b66a6": {"doc_hash": "331f709f3b108fe3861e5771724ddc1e21cec612e3cff6a1449df046261f252b"}, "5f7611bf-9c7c-443b-8723-8bce5ea724d6": {"doc_hash": "b80d7bade8b1db58b76258c16c71948041b3f63846fb0904b9c7180e5206665c"}, "845ce1d0-92de-406e-9d14-f4e68eb5b081": {"doc_hash": "4ee9497e8706de92612d5c2079a32a4260266b7fd4224b3b2cb54740ed32ca31"}, "991ef4fa-8d15-46b2-94a4-a6bebb870d9c": {"doc_hash": "f4d20f0a655a99a564adaf9e0fb0850231b29548af60f076705ee16ebd412cc2"}, "f60cce34-67bf-475c-bdaf-24756070597b": {"doc_hash": "98813c9a6d3b922b7444e2101420dc9742f9b545038a636375461f142f38a377"}, "e54ab27f-2295-4789-9317-288ccaf70d3a": {"doc_hash": "4ce0544759dbfd912d5f638cb5c0fa95a7d39064f5bb0526d719af2b1e4f5375"}, "2afa67eb-ad74-41b0-9eb9-cd5e2c06b3b2": {"doc_hash": "4e22698517fd7265db9429b37c4bc2b02628e51f935f37b352117a7be60a54f4"}, "9ef4bb87-a72a-4f6b-957f-8767f3b01866": {"doc_hash": "41eaa05bd67f4ebb56bb0529408a2ccdfdd05581dfd941f2c8cefea16f78359f"}, "bbdaf581-ff54-4afb-8449-57a33cb9acb1": {"doc_hash": "22d75d58f8bafbad236aabd7c1d93f986e55f090ce432166cbf821c0fa0bbcdd"}, "138a481a-6bc6-403a-81a8-2f5c17190273": {"doc_hash": "00b93b65a5da2f2578711bed800934b929a4a60b9d8c9467b377b71242791824"}, "07f5054d-8320-4945-8af8-26da39e9bbf1": {"doc_hash": "0246eae53ce3d7efd618429b695ad61a4122a084633cfcb4a671e3515638f3a6"}, "40b9a1cc-cc28-4030-8dcd-b91b00b82793": {"doc_hash": "d1c495689ed9482fc1ab364f20e842852f8174b1c5c64b410970cb7ae7f3e8d4"}, "3f07bb6e-22f7-41c6-b8da-d3796324b6ce": {"doc_hash": "7ff1ad148ff1a45d29f33d7c2c9546ddf806aa9dbf2f781ae8834d4b89cc2041"}, "66560a34-7c1b-4fa2-ad77-acda932385e7": {"doc_hash": "a321a7a7eb4525debcf2dd148f8bd3fae65c18ae479bb0a78624e80e6337afdf"}, "df21464b-c7be-47a4-9b6e-5fd8293bc972": {"doc_hash": "818ea595fdd04af15d75c01f4742f247a2ef28465858a52c6c13f5df02a3fee6"}, "4412e606-8da1-4a92-a9dc-61b86050c16c": {"doc_hash": "3f5bbe1203e560a8778c135391dbaa3c5152216d7d3420ebba9aac7f58b512d4"}, "10ec693d-b0a2-4e34-a299-2843c6c3a32e": {"doc_hash": "1c8831810d0ffdae309e0e526f1a298700626e3c73f465822a9221693a5e9017"}, "8758f6df-d987-4bab-95f3-f15fc101735e": {"doc_hash": "a9b64049c1c3f48917ebc258f5e2f8b4a9e70c81e5027f62d103e59673907d8e"}, "f16b09c7-88d7-4b01-9dd8-fa2b76417ae3": {"doc_hash": "6af5c0526af43308df8b103dde865a353bc0f985e328c94700bc361c4bb0236d"}, "057f1af5-e93b-4dd9-a266-2e699aafde04": {"doc_hash": "3c96486ef08428dfcbbfb9760376faed350df933a47112d9c5d720dc14522930"}, "b8beef72-ce8f-46f3-87f5-7aee3b82fe18": {"doc_hash": "d9159708b41efb3fc40b58ef788dbb02c8a70ac9ba8951930a05ae197cf48f41"}, "15507f04-38db-4377-b787-5f281d10192c": {"doc_hash": "d6154bfbb15a806af808e6b331e07765d114e06f045b451d20f072012e4e87d6"}, "ebae195b-60a4-4ea0-aa2a-6e64e670fe0b": {"doc_hash": "68734720f3122bd209023dd29c6fd9578c029accf29a1aad2865600aa691c936"}, "25d02b27-b012-4ed9-94cd-d3402e3f374e": {"doc_hash": "3d215f93fa7e68c0347f62b5747ea2f45235a965f88a023642c3b3f1fdcaf3ca"}, "e2429523-b149-44ce-847c-49e8a30c4dfb": {"doc_hash": "47a9c223621963c9c3554b5dd89df251aa02f2a646202a45da65a8149cedb713"}, "a3058728-f533-4bec-ac10-f63bc0c64fac": {"doc_hash": "c6c8e4b292abef3efb7494c95f88e44f1dc3b7e2df36a6b68ddd1cdc291b6bb6"}, "7d7242ee-e0a3-4d27-8c75-32f2e7e1b40d": {"doc_hash": "8a201ff36427bffdd9cf0226dfe3e914ee102e080ea343f931c901adcf3d49bf"}, "1bc58610-56f5-4fe7-922f-7d2ad04c23b0": {"doc_hash": "81c9bc267ecfabb5152a532d2ee96508c33320df6c4791da203324921a54de5e"}, "f316e2ba-173f-4612-bdcc-2a4c050491f4": {"doc_hash": "8a8f52ad257cd4397a137e5fbc4dba9599fd7a4c1d1eccef786ccec9bffc0172"}, "4b3cf393-5386-442b-9675-dc7e93a304c6": {"doc_hash": "ba32a103bc5f26d4138c1d446be1ef1ecd77e5ffad6fbb1f8834d83596179dbe"}, "1dcdd682-3688-41f3-ba4b-1038cba79e00": {"doc_hash": "7c40b12db6912e27136ca6e35ca5a606ad85cee3b27ab547087e0905da551950"}, "2a62d218-63b5-45f7-b38a-9ea517be099f": {"doc_hash": "53931af60c59d0a37e4546a469f1635e04396db49d72815d8b3a7b53edc9e5e0"}, "4fcdd957-e322-4f9d-adcc-b1725fdac1f1": {"doc_hash": "e899810368c8508829434bf472d068e6118ff85e0b804f581d93bdfeb8ed0193"}, "aeead514-701c-4be7-aa0a-5aa2f0d97e72": {"doc_hash": "7fbd3543d4456652126fb77affd8f3bbfc45980203084404c0391a3239d5be5d"}, "2c59b9d6-36c8-47bb-b95b-c3f45b5d9ff2": {"doc_hash": "4cb2339bc44e1422ffe5553cdbd67875c8fca3d0dbeac3a9298be78a52d32bcd"}, "bc31d22d-92d8-431d-a2f7-f317658115a5": {"doc_hash": "f1142ab3a9aea955af85307dae30c543767925203a05bbfd2f77ac3d3d03e2db"}, "d14e76bc-e91c-409a-9a11-a609bb01a8dd": {"doc_hash": "411527e9f7c8f9214b978a6bc1acbb225902d382777f0530ac5e2ab20ba5cb73"}, "ae82100d-8a93-498e-845a-f1bd93600563": {"doc_hash": "2fc94d3554d1bc3faa36d0c94d75a72bb7b9a8782cd78759e1607e6a5c862e5e"}, "bd522413-28da-4ff5-8131-001f353b7a92": {"doc_hash": "bf7059093398a977e08a9f12d4ba218cdb68cf55f7200655f102f6450622a1a2"}, "5c36486e-83af-438f-bd99-fe99c67fb514": {"doc_hash": "559506f6d59e50ccffb064794640fd2f4c1165e84d61af4cf6a7603e4f470978"}, "a9b4e6fe-0ce0-45e1-b1ab-1c665d54d426": {"doc_hash": "752389530952699289832f8b23943c3ad615ad6f008b331bd4e333eb7faadace"}, "c59d9ca0-7be7-4ee7-9e65-c3d26bd8bf2c": {"doc_hash": "e4e4d60a3c005e1fb4069f7886486b0c619b4e636db48ba2c3aea05c6b22e6af"}, "72f1d33e-6577-45a6-a546-38613b8d35bb": {"doc_hash": "8cfb47f3822ca2091705fb4dbd4574e805d7db534300c40fc7bdb886a73bab2a"}, "8aef010f-d6b9-478c-8951-a1454a2b7aac": {"doc_hash": "680eb69573246d248e0e3bdfc62b9986787c7c560682a2913549e9df50a28f59"}, "8f4891f0-5e7c-4ac6-9e30-df58d1c69f90": {"doc_hash": "30fdcacf8a8ab7d0348b7d6b7405c3eac3cd6179aadc4924ca698739fbaf1476"}, "f962c606-28d6-43b5-ac10-e20210f42aa8": {"doc_hash": "79b96d454da9cf88530ecf772b37766fd34aa2dba04fbdd58b33a041153f4529"}, "adf9f41b-d5e3-4793-b35d-cfe305c0465d": {"doc_hash": "8897bcb012c22622d992b7b900f413501a806316128458a9656e65ef30fe8dbe"}, "2463e660-25cd-4e1e-afe3-78d78eafcbde": {"doc_hash": "c848b2f0562a964e3afe2a910816e4772892042e6298d827bc0c1f3ee90214a9"}, "0c36437e-2907-4549-bdaa-9f0b901f64ab": {"doc_hash": "01c1593687e4050d26d04c33f54f58670f9c313c6ff647f7d0167f10d90c1403"}, "4af66db7-e787-43fa-a725-ba6d67512629": {"doc_hash": "2fdcf452e7c21fcd20252425a05675151b83e0cda4fcf89918aae06836826fe6"}, "7f94ea95-0283-45d1-8a81-38da77061a02": {"doc_hash": "1a71d8adb1bb748bbb9aa64c47f900aeb8be7162e595eb01c54f8c228f51af1c"}, "3b49b232-5373-4b58-b0c6-4126c3dcefeb": {"doc_hash": "e1611011beaa08bdb6df662c3bb78d73df4799fe5b1031dfb51b12980f97d7eb"}, "6b1ffec2-f416-49f1-91a6-6638a3707ebc": {"doc_hash": "910a4183a9ec8a93326ab9cb0ee633ef162cd393eed676c9b2224b4ddebdbdf9"}, "c896ec5f-0a2f-4aea-8f04-6806ba251529": {"doc_hash": "13ec901a4e8868be7a12dcce10a46f22943de99049e1742dd25b97a0d47d3c33"}, "69dccb96-3af3-4f1e-b23b-96d9e2f59e6e": {"doc_hash": "3f63d46f7b8405794567a4b43e7177254245a61d16985cf57d9e70f4e797c217"}, "60c3a714-f332-45f6-937d-819ad4723549": {"doc_hash": "1be4c8ed871c932d9c8d2397cff81dfe84326ac2b801fa2da1c40ebedbfe651f"}, "180fa6d7-f31e-42ee-835f-b6c88ab890a4": {"doc_hash": "cd5c35c0ac7c7815c174b2b424a5d255895ce4e69c9b639d73c9c15a7cb2b649"}, "acaff5d2-c796-476a-8f22-40a9fcfc9ac2": {"doc_hash": "6ba2d885041bf50d4a3c0f1c66acecc99332f9bb5e1001c82564fdf304b941e3"}, "869f0e38-da66-42bf-a449-f40a48963754": {"doc_hash": "65ff3aeb497f9d259771db09fc7909e34a720f140f16404a1b0a11b6c72c3605"}, "a61ec00b-17b6-4540-8a98-e5fe77d57d50": {"doc_hash": "f04d0ad25fbb3284c36f72a5cda4b11aab210f675c3c9cccffa5dfa66176468b"}, "557d23b3-4b99-42a8-b7c5-69683ef71182": {"doc_hash": "2e8346d493b2a4740402a5704f4a90e83a4c86ac7c5708d9bb8bb8af987cee31"}, "536a7f32-9a7d-409a-b6d0-b98867339186": {"doc_hash": "5145ccad5fad4875d3c57c74eafdaca3fd3c3f25144780441384b1c51d12361d"}, "1093b147-40d4-4059-933c-184a597b0c74": {"doc_hash": "e81c7c50764de292d4a153a4d7245552c3407f7e0dd2eb65ce95386f12703b35"}, "b7ebd51d-26d0-489c-a0e9-4fb094d04361": {"doc_hash": "2d03f57d32d176713e40cfbd3b6cb05eb316f28e52d501384f824bbabf1cce64"}, "6a28a010-75c5-4014-a837-af578a5eea41": {"doc_hash": "22394b12ec9da1ada79cc36c3682da2afa32b81d151fb70784c712826b03b893"}, "8de3e4ce-4634-4794-b0fb-f57dffb74aa2": {"doc_hash": "201414339513ead40b0107d5406b359aff3782e6f97c562089d05316e6419677"}, "6f3d497e-d30f-4a9c-96d9-de7c81642a38": {"doc_hash": "37533961a624146a91736a7bbd18642b10d8bc3fa8d029253c0ef7ead340db6a"}, "b4661db2-b657-4f5d-a83f-fbb2b37f1a23": {"doc_hash": "02d31467269af868046188d0e6892fbc24ab3813f6261b6aa691dd61253e3ebd"}, "a22bb487-4bb7-4be3-ba45-7b85994080a3": {"doc_hash": "e9a0589277f94a4edfd90b0d8987b1d7467639abc5a7a7b0131fd2a4f84bcf20"}, "8bc7d0bb-11ca-4e39-8897-1aa693a5d9ec": {"doc_hash": "bf327d993b90485e423f4de61df654257dee4cc3a1c6fee8bb6ea14a9e1bb508"}, "1751a2c0-07c9-4f6a-8525-243780e2ceeb": {"doc_hash": "79e41ce2de99e682685b306b56a573d029a087235a5ab44a47f307138d0a1a99"}, "13d27492-0bd7-4cdd-8fba-c815d36bf260": {"doc_hash": "dce9fe1135d34f2cbc371fda1f86e9b5466ca279ed77c8e1615e1ee3eea92da3"}, "603b96d6-97da-459e-ba51-3ccdb83a3663": {"doc_hash": "575622994bf911f74aa6cc8e80abb238a033cf2b2b3cb018f9b44ed7d8f83262"}, "d590e653-6e6d-4ed0-80e8-d9aef585bec7": {"doc_hash": "d0ce50185644fbebc8c725ae1ba5ed844305c502453d9a80c551f4de451c5d98"}, "e7d2da3a-6b4f-4bd7-9f8d-6712343642f6": {"doc_hash": "a67efbcd9f32803158e9104d07d13640ce02098f68d1be514bcc2fbb0a77c984"}, "01908d4d-c8d2-42e1-8049-f2640eecdd99": {"doc_hash": "f27d91bb68d3bc68cb33906246ec2e74b1684924804c66a2ae4b4ae507539306"}, "ef346a62-a45b-49fd-979f-c43f84a93d6c": {"doc_hash": "768a11f46b1251db232307556c7db67e6fd258a679009d2091acddb158ae5844"}, "35489d90-8530-4200-af4a-c7cd129d5f74": {"doc_hash": "3a39b877e7c87781571fa9aa770578d7611887149cab6a68378c18fe5d07cf21"}, "8ef7f518-da59-46e5-9185-1c660a27b9ca": {"doc_hash": "77fe15c70a84f9d508eaabdb7017a1f9a44f26e267ce523100a6a381a8a22321"}, "5007ad2a-5776-4f0a-8215-66598905fb4b": {"doc_hash": "98c2f631e01be341092c558d8e3cae0d45200cc2f65156ebff9ef83889635462"}, "cf876de5-32bd-406d-9907-9f5b44863f4f": {"doc_hash": "c02c4f9b68276507d8fa5a07bcdb0ed6e0b138a8899d23b28c96d8ee05929e79"}, "22cfa8ed-f7a4-4c30-9880-cb2f7d833635": {"doc_hash": "4e7f09969d3e43d74fd75cb7ada0566f51ae6e21b44b4c7c33fc90c2594ee0a6"}, "758942b9-e579-4cf0-abf5-0911dd34133f": {"doc_hash": "ad0023fae6a09059424dd2184e3db6e848272961faab1a8f3a9467daed9d150c"}, "a96f86ba-2083-4595-88dd-be386ea2d8d0": {"doc_hash": "da1140b7dd2478ec06e773c4465473c9674cb8a68a785b40bb21f3c9e06954af"}, "504aad8e-0f9e-4734-b857-341512dc0290": {"doc_hash": "2720590e56bce3149367440ea39278d86bb7f72c701f744dd2dfc753ed37daf7"}, "3c52983a-6a98-4ef5-abc1-efd71cb6ec04": {"doc_hash": "71fe9dff07eebd0fb1b6cc8d80a4e8520b48f9477ecd7abf34b833129f043394"}, "3f98685c-888c-4bc6-99e3-c56494c5b4f5": {"doc_hash": "1d2c65bc23b767ac66920a3322731e088c55079b982c18af97beea6b7f6b3466"}, "8447d23d-fc15-49a7-9c56-d6f6ec10d158": {"doc_hash": "7315e12da253a8b46fbdea33c40c9a94f97af20126387667f17ceb8303db58bc"}, "81494ac2-ec6d-4057-8c61-9b7436e7ad26": {"doc_hash": "889ba255dde269567b32c9bfc2abb9452c6f89bee5c2a88f306500ce905730b8"}, "f906dfbb-3fc4-4f79-9960-662e8d7d420b": {"doc_hash": "ce81a59304f1e5c851ce725e489feba883df1c79be6ff576527ceaeadae6f965"}, "acaab71d-b86b-4efa-aa4f-89b766603b61": {"doc_hash": "405b436168cbd4b74a9c67d6132675504258cb4af6e2530fbcb90714f1973c1f"}, "d2d95939-b5a1-4964-ab45-6ef117f429d1": {"doc_hash": "ecab5535c1df42d2535d79a78080c20a86acf56a5ed62789a2d23caba6a21343"}, "a18084ec-e602-4289-a815-065d3a9463c8": {"doc_hash": "6cb811b4ece43596c852d37abf6e7085f2a5553f93c3c1ec71d8ca607752e25e"}, "c416c2a7-e4fb-43d6-a561-4bdfc099c90d": {"doc_hash": "a0df9975bd8f494e8fbb784a0dc4113efe01a38ae6c76e5ab7b23aadde548d80"}, "0d775c92-a4e1-48ca-98aa-a483248ffa96": {"doc_hash": "b2b18f3f4d81a7b41a0c695b2fc0350e180debd30e19bf1c178b781497710b8d"}, "ff9acfb5-0f0f-41ff-aa0a-707ad5be359c": {"doc_hash": "85590b310da3d4245f2e06ac58cc9a04e24f3eb77c878c5a78a61ae59d82237f"}, "55492c51-b24b-4279-8a54-9c01271b75f9": {"doc_hash": "f8f7fc2b118a511cae5c46d0815b05fd02fb196e027cd576ea0ed72ce440ae11"}, "bc915d77-e2df-440d-8d5f-ed10ad2a2d4d": {"doc_hash": "3195b7f9ad72aef65bb146b3d6d4b45d9a9013ac8fbf86386c9ac078eb8fad16"}, "f27ef110-576c-468b-a6ff-9f3ff0b79902": {"doc_hash": "5a94927ef526c219ebbf65e72a24d9bad21263afa37a98b1528e3711fbc3934b"}, "b1e923b2-a931-410d-9a5c-ec139dde008f": {"doc_hash": "efa6abbb9e7875b89effa1a0ae6e95166bfc14f6037a29370abc9f630f3c14b2"}, "1452b476-5ebc-4dba-918a-36fa434eb4f7": {"doc_hash": "4144c0913e30e1b622fcec5f0ad255f2c39bb7309166db2bd5e8aa0ef3ec2f2f"}, "45f89314-f1a8-4e77-b0a7-79e40b747f09": {"doc_hash": "2a2c5c01efcf103705648a67ac1a15f7aac6eded2fee38cb8b3d7c99d19592cc"}, "1b41ca5b-78df-46e3-9b66-eec35a6e5696": {"doc_hash": "fded8bcfcdd69bf922423a500b9edcd3720a4c008bfc114910df2997352781fc"}, "bcd1d024-9d52-4dae-82d9-b775c32a2ea1": {"doc_hash": "2d18f079b00f2fea9d790f21740773206837192bb9818647f74e381a57af4d47"}, "b45a8597-f74b-4a7c-b3e7-56db07efbb41": {"doc_hash": "6385f0626ce3e7e2036c315113a0ea7bf43eacd80af5449b9feebdd727d95f2e"}, "5e423553-067d-492a-9b4d-6fab9610a934": {"doc_hash": "32ae2a8994395bc9a448f9559572a02e0205dd248e1b4e6868abd2d5e0b6f7b4"}, "77ed86e7-d1c0-4679-b11c-61a720a686cb": {"doc_hash": "b2c898149e129ca3bd5875b10a3b84b2194793e9bbf685da78687709c90a0ea0"}, "b2807ba0-b90d-44c2-b9c7-3188e9e9ead7": {"doc_hash": "76e14a10e626f71aacc7d7410e25872326317127acb25865e45ff6bdb67a0ff7"}, "59ae4b91-05d4-49e7-a8de-e5c79b75f98a": {"doc_hash": "ab3338d2ed76d4110c897b4fbc3fa5cf6012dd68fd166ed87dbd98e3159b2957"}, "614df1ea-acb8-4f1f-9310-0a5351b8f3a6": {"doc_hash": "433e153f3a8828d4b80adbf4ab3f019c1fc08cd2c0d8655a84bde0cc6ad25f11"}, "30dcaae6-7c4e-46da-afaf-d0bbadf89987": {"doc_hash": "f802afe489e3b81742e0140ea1b3e76109b9022557472a81cbd02b854486fb96"}, "5dd6a315-7ca6-4496-ad7c-2eff081c209e": {"doc_hash": "8d790f21c2a9f821bcd8f4d7a14e5269951ca32c44d6173174a66486dfde433b"}, "769dee6e-4071-4a68-9c2b-ff0e351b281d": {"doc_hash": "a6793f2be1015f5a017521c0364971b0cd07dc023f2183e06cf9df1540c4772e"}, "cabbca11-7bd4-4788-8231-41180c3d2191": {"doc_hash": "9695db800ca7fe8af175fde4bd938224393d5d24e264b0da7054ab4f23e48b59"}, "42bbd4ff-4afd-4a72-aab7-22644c0498ef": {"doc_hash": "f2f1ba032bfe7d4c612fef3125e70e119490ff4052d35fd2470e74aa2116977b"}, "b420f8ab-1fa2-47dc-8b7a-3f40fc7ded0a": {"doc_hash": "af8c514cd81b0d76bb3411274a180ee0f16ec5a685337ddc71ae4b3fce33e1af"}, "d851f174-6c48-4c92-8d21-01fd0d60047f": {"doc_hash": "32b2bde04973be9a94edff5be5568b9a16b8c94143cdf2e1e30d98ffa3fbc930"}, "4e83c8be-7093-44af-a08f-9ccddcf8e5b4": {"doc_hash": "eee34c832af95424e2e0d23d67fc4a2ea59d4cad44681286dd45c8dc33804c8e"}, "97cda3fb-7eef-4871-9303-223ec37599df": {"doc_hash": "5e7c82c49db4acf7e2840807fb0890d09f307c65566324c9d72420ced333bbcf"}, "ef39870b-c6cf-48c3-9dbe-d52fabb4f8dc": {"doc_hash": "d2225bf7ba167d87fce784aefd8ea30f42cfd58711214c3ccc7e8018a0b70725"}, "13c72ea9-01bd-48f0-bae0-70e041f91303": {"doc_hash": "653cc1674747ca129f434bc733d6c54681d9019b0df0f27f078bec2f60b96795"}, "4e1c2e1a-91be-46a1-84c5-f4e2539b1bb5": {"doc_hash": "1c25d44c87f1c58629b76d85f21bb18e53690436811d7bf195a1a88ff9d910cd"}, "365ae8ec-3416-4258-a94c-262e762ac13d": {"doc_hash": "d6633ea4f8ca63bb973f078b9a8c491df58f1cf1da9800f85cd5ad3a4f0287a2"}, "67ce87c6-2b05-4724-baa2-e4465c0b1b3e": {"doc_hash": "2ec86bb5ccf359ab6545b09f75fe61d380fb585a4147b2d615c345d896f1d2f6"}, "97ebb57b-dfd3-475f-9533-61f427719cfa": {"doc_hash": "0abe6b72b01d70a00d1af8d3e93d5c336314931e1f06342895509f7ecdd1ce80"}, "d5e68fae-a5ea-4609-a1c9-eca90254087f": {"doc_hash": "c655176b836c25c2f84e4c51ac6459908516323b67336f5866aff8183c8cfe54"}, "86c1e73a-1941-45c8-ad8b-19fc1e7ba195": {"doc_hash": "7a90bfea4c05f9fafb4c57e743be766577c221eae063157da41d7ac9db2f47c3"}, "c85bb471-d654-49c6-8deb-11d545d8fa1f": {"doc_hash": "c3120eabb8ca76accdd95cf82d0e5b4ce5fa6925c5583b4658072255668d7ac6"}, "0ce6612b-71b1-4463-83b2-590fa497db15": {"doc_hash": "cc7d0c27244700ad5b720ff27fb921f9ed89df0f472135194981dcf3d337713a"}, "8ab767c0-571e-4650-aee7-c658dc20b3f7": {"doc_hash": "501ecfa11d40be0f89bc566142ca880936874498463a8edd35ed1c41241e1d29"}, "a280a15a-39a7-4362-af67-477e2cdf9175": {"doc_hash": "ac16e9371d46f5612ffffeb6cd806dcd917cf4f409f524b51dfccd77a40710c5"}, "31ad2038-8762-47b8-9cd4-f0a45f1f3ad7": {"doc_hash": "5e21b616f773b1a5510db06f80c8825f7f284fd6efe15c8cc98ecc35066513f9"}, "040cb028-0e9f-43e7-b69a-952857ba6040": {"doc_hash": "e007d978583d14edc659010ec6858090e775ad2379654a899163d4dc5521a6e1"}, "8ab952f3-e453-4e7f-a756-8e734b4d1d75": {"doc_hash": "3417777f40295b318e51e31ea52c0025cac1e201c438a823818bfa7e4f71c118"}, "003de270-69d6-448c-baa2-6b7e46152a54": {"doc_hash": "3eee3b29a20ec1140799221eee82e4193a8b4996458524d04c53cc0045fdf92d"}, "74f297da-b32f-4645-8dfa-1dfd40bc2a55": {"doc_hash": "8b6ac7de388215e02a6bddca8e4f7c960ba356eb74a934e5cb6a82f413df4e9e"}, "4e88aa98-6890-4e4e-b229-6a1c70407791": {"doc_hash": "a94f8e2a3cadb9bc3460d08fec6e71984925c9b18f0cdfb79dd5bcd43a1f684c"}, "250a65c9-cec0-4c1d-aefb-0779d5e80d4a": {"doc_hash": "397306eff04a4721264a62234cd8e7cf6dfbd467fcaa669529461b1cf7b1f55c"}, "e60f9c84-8c93-4125-a5c6-114a305d7829": {"doc_hash": "5325c3dc1d20bffdae191e8639327f5b9c64ccd10db6b295099e69759c348fb9"}, "e3e2c7cb-9915-4558-ab7c-bf8620bb8f6b": {"doc_hash": "0acdde1d3d23271b636e8d341e573bccc6bc187011c10b98d5a0ef920690835e"}, "c8dada01-b453-4cc4-921c-ecacaef9e95d": {"doc_hash": "baffdffface3f55a132eeeb92b00027f3ea507618798322d4703533d675e62fe"}, "ae626a43-3f20-47e7-8aae-eca0c0137fe8": {"doc_hash": "efabf563e8a92cb99d56bb5f727db7fae1fd58a24fa922783c37ebfb108e43c6"}, "6be54b0e-7ab1-4bca-88a2-09782cf597eb": {"doc_hash": "6d50ca4255677e1a826f78f72b0359cd81d72381000499554e384ec6be82274b"}, "0068025e-e003-4fdb-b264-48aa9b734964": {"doc_hash": "f315a2299096c8103c5d9c7746904edee1654992303e5ed6bd6940352abcf0e2"}, "2a0cfdca-432d-4333-a58f-8cf82428ea06": {"doc_hash": "03cdff778deb16b14712fcebd77d69bd9a514fffc0a3b2b6dff63fb7eeebc5dc"}, "2d135c14-20c1-4295-8581-8af2fb6aff76": {"doc_hash": "db2ebe1a69f1e4b5bdf28a949b566179c6c120f21e4afc01d91ae86c98b24654"}, "ae8ae80a-9450-4928-aed0-21944aeb8281": {"doc_hash": "05391ef34a0dddec285686bd759bf38d2a86e678f411cc0c5916a7680b43ef98"}, "74511e02-97eb-47eb-9152-384ddcb7ffc4": {"doc_hash": "1cf35912d9293510f04176aad5de26fe4de5072382611dfecb151533543ae509"}, "688377e7-6469-47cd-aba0-04ab8b5bd2af": {"doc_hash": "bfb1673ccc00224e25a99e3089f0a381e4b3361b1dc063082db06730f093e6d9"}, "b43ac4f8-4158-4435-a615-d10bbae5470a": {"doc_hash": "ff9f2c6f9fe61e81008123aca6e810b99d1081db4a2987a4e176acbcda08b052"}, "d8a6a125-77e5-412c-ac7c-e950fa6c434c": {"doc_hash": "97dda247b3e64402f49b47a074d30efb8341bd8726a297d6dd4e72528a466477"}, "bb9deeaf-0a2e-4308-a3d3-d1f84da6052a": {"doc_hash": "cadd23047b410e9d6bcccfbd2f1542296560f39870de73bde416a199e259db20"}, "1c5816d1-9597-4aa6-a87a-0b82f79a41a0": {"doc_hash": "9afc89cf9ecb9e55cd34f6495f0ddc4a12dce4c7dbd415e932196c6994e913dd"}, "effe889b-e92f-4ab8-a494-425454bae218": {"doc_hash": "72cc069e9f9f231df8feb211654184e25d602a8f9555e4a09d9c2bcfafc69948"}, "41d4b649-8cd9-425e-af6d-2e8e7e1b377a": {"doc_hash": "206842466e92e78d922cc9d3302a3b0fb4d3e06cf99ebf1b98788b934cf19131"}, "8ce0863d-1182-44b3-ba89-e9b3dd6a74ec": {"doc_hash": "4e16fa03a24f9d63860aaa76b581218892dbf9cbc660429e9d713cdd97e112bb"}, "017644d3-1264-45bb-a8cc-3707e157c066": {"doc_hash": "ce1d76e48857a99e87c4af6379266669fe42ebb1d92a2ee2e738059016adcd5a"}, "723daee9-6cc2-4035-9dff-5e8faa2c90e9": {"doc_hash": "a5d0743842373629176672902f03d3e4d51c96e658ed6fc0119bf337c233cdde"}, "46dc5006-f6e9-4704-9b13-1adab09c3c38": {"doc_hash": "a59cb997a1f5aaba9bd7044d1bfeb0a5c8dd4bb027f24bad4370276438e33694"}, "28ac059f-52d7-4cf7-89eb-25746df08244": {"doc_hash": "2e0aafc43a66e8294a7d75bf716216d1d030d0c258a1016b1ae3fa5c3dc19521"}, "009dc4cc-327b-4f69-9585-8c643b6ba683": {"doc_hash": "912764d727f606f18b43a002a0350dcb3473d4619975677334aea1629e9449e9"}, "84a2f68d-335c-4f75-9586-1b7083af95ca": {"doc_hash": "d54790e7637936d595d3ddd13db4b3565aa09287d1cc50a9789b3932238c071d"}, "17afc4d4-c3f9-450b-ad51-80711b343dbb": {"doc_hash": "75f228b78c0cd6b38bc4021efcdc8fd12799d8119c8eec47ee432217d938ad38"}, "002dca2b-5dc4-4eda-8fd7-26af71a50609": {"doc_hash": "224f1f79351c785bfa3edc2e67b284a233266f5ce92a9c3f90e615fe46956ef2"}, "1b0ba0db-1592-43fc-a473-689847f3a2d3": {"doc_hash": "919ef285d906bf996f25ff8e611d716a8fed481153837ea1c38ea43b39cc6a61"}, "890da9df-2f34-4a22-9326-1866503e1392": {"doc_hash": "6d13a97639e5bbca0190db0c0aebcc1d440d87fb794207a4457e6825218ad9c0"}, "e42f5fe4-746e-439b-8104-ee4e3a42cb75": {"doc_hash": "1fcd9bc9bbd1bb25157561ec213342e51f2387f3e2036205320e15eb82beca34"}, "790b9be9-7901-4e0a-b87a-0af38770246c": {"doc_hash": "f52cdf58ac706872bfe5be7caa0b173283b93a19c62f65e03ecf05d09a4f4de8"}, "9b46e1b9-b032-4b6a-a9ef-0a4a5c4e138b": {"doc_hash": "c9747de857881094977aa1334c96486c1251ceb6133b309be1521c81dc0568f4"}, "c02fee95-bfcb-48ef-9542-9fe18b63c0bf": {"doc_hash": "590b37dfeeaee0417151a288fef9339b6037e13ef88b8c4f0c6db9f4320ae410"}, "18587f29-849f-440c-81bf-40c704fa1caf": {"doc_hash": "ca1fe929bd0e0e502a447bb22c7b6ba4a40a4ff43d3f50eabb0db2160c453f5e"}, "6561a6c6-c5c0-46d4-83c9-1ba9a82a44ef": {"doc_hash": "7652190e0388f783e3c6829bb868970982095d18e2a335c0c3655c2306373e47"}, "f5138277-6011-4b6e-8fe9-783515b62aa6": {"doc_hash": "e32eb9bf62ee80b7a3af1c94255bb267fb0516e7bb9942ca857425b874d516cf"}, "b7d9d557-8d2a-4d96-82c9-afff8c5ade86": {"doc_hash": "c12fffd7d142b472ae3ee715dc6a7f3f57c358d675a169a36f36694870c13fab"}, "690ca45c-89b9-4488-97bf-9944b4d1cd75": {"doc_hash": "adb8f4d8fd6a74388b6aab7dd08e582de5b24a573d664b5ab6b810b5d17f1cb3"}, "d40722f0-1784-4a73-ab8c-386fb074542d": {"doc_hash": "c135561553b235a92a35e10452919b577dfc753a72002b67e8c044f7e4e06141"}, "1e0b2afb-eeae-4dec-9eab-a1103bdc0daa": {"doc_hash": "17575aa7ca77fe6a5c8748b2ee9f46b56d9276f32c16b4bca170ec4bf391b479"}, "b9668b4c-d4c3-4905-9cbe-e3a39df81b69": {"doc_hash": "2c2f727731587af1348c7dae67bb15c22e7b49a627b1fa57cf91d0f2444f431c"}, "92748345-b2ba-4f27-b35c-7146a901ae29": {"doc_hash": "81952d5922ef324702720833bbf1b24e2e9bf5c60ac16e7540114915fad2a948"}, "1c28a698-8d74-45b1-b3a7-7b1abc45f04c": {"doc_hash": "141092ab6ba7d255998f1151698be985e30885ff744e3ac5e95237847f651a83"}, "1541be20-43de-4eaa-937a-ee2dfcc7efd2": {"doc_hash": "a9ac5ff699da23116d06bf62f270d5deb8921ddd0bf63557c3f41ea089815184"}, "74d54fe9-1693-4481-bf87-bc93e2581a93": {"doc_hash": "53808ae1b406eb3a13063b6db45ad9cb67b65f1ddc99c4ed81a29fe5232dc68d"}, "acedcf90-dfa9-40b5-85a4-1524d2890e31": {"doc_hash": "0d7ed175d48eb7cd09f9b47aed59ce1ae8677d6a094bef13ff3ec0b712f2e5a2"}, "cc6ef1f9-c6d4-46f9-99d0-80611cc80901": {"doc_hash": "68788568c595556b4bdee139d464984b0adc2fc02caca3a838dc51500d344799"}, "25850888-dd2e-4bfa-80fb-ee32b3fed858": {"doc_hash": "f79c590a9b472fa760ce7e1b9224dd2dbbacb45e109cd3936e8e9c5b5ae78327"}, "c4a4b07e-b0f0-4ec8-80b0-611da8a80d1c": {"doc_hash": "012e145a4247e778fb6fea493e0b465ae60aead2213827bcf9d170561041c372"}, "08fe8ec4-19b2-4f24-ab07-9f15fb558d83": {"doc_hash": "01ae5618e50d3f56c7a397d471a94ac1b1b874d09eea3cfc5a4c271946d64e9f"}, "941a2f2a-f0e7-4985-ad28-aa84aeaf75b0": {"doc_hash": "24e43e2c127cd1e467e306eb07f2d7750c9c2e69419ee523c8e717cfa1b8b9ee"}, "c406a578-f1f4-4286-8e29-b411aca57da7": {"doc_hash": "57a8a6e06fa5ee65b2ece0709a399de8934a075a35eedea1bfb2ac0222664d2a"}, "7afe79d1-666e-4f64-8417-3c9650697574": {"doc_hash": "7a2d9ee2a4ec1cffa1c5430b49a3db3c78f0d30704bccc53f8b91c4db7706107"}, "fd8640cb-a525-4be0-9edf-43e24395e18a": {"doc_hash": "3e14692df4da3d66128c727e2d147ef8e9fcfa90e8da633755c95cd3de15e72e"}, "19de5903-10bd-40e4-b9a1-22147898db87": {"doc_hash": "8a16056c7d8d0315cb3ffd170ebd9329cc2367788da208b20e21fb1a98feeee7"}, "c791a107-09fd-48a1-a566-a69a58d48299": {"doc_hash": "a6446bbc926b4f0b79c52795105f0c2685da562ac94adf4e7ddecb17857827be"}, "c6465774-0c35-4cec-9535-5cabdce7d516": {"doc_hash": "4276d12fb43ac35313cb3892c523c417e8e02c4662fa83fb66ef5b2c239d2800"}, "4438ba88-20b1-475c-867c-4749033d1cf6": {"doc_hash": "cf91ee56caef19148b00a8daa4d3ac131bd00224e66fdb8d5c73291a36bca602"}, "76b040cf-eb4f-4490-b4e0-f0fa73bb1f05": {"doc_hash": "ef37df7187391d7808d3242a86d47cc37b24e9e43e92eb15189fed18a9944d50"}, "2740a957-5269-4e3c-b5c7-8801afe45c1f": {"doc_hash": "ad1cf99bed0a763133b62a10e7a81970d2afdf93a929a1226eed877a48169350"}, "2e3b474e-8a8d-4a13-8018-cd83473e3068": {"doc_hash": "cb2b5aeb703160a49415b42fab94946f0f70b43a793d5a57977de3f9b1fdb5bd"}, "3912a02a-d236-4640-b5f0-d8e34dc8f2d5": {"doc_hash": "0045ef63fdd0711d0adc7b525f4794b319f7a5484951a9bf709056c1d7e79776"}, "5b6331ee-447e-4cfd-a8fc-4f5b1f5bb0db": {"doc_hash": "d5b165e6b50469de3d8de5eb5471c05d5f6f665863ccc19e27465b6bac7ad1eb"}, "98b4a3bd-50a2-434f-8a86-4cd5f91be920": {"doc_hash": "c6519bef6ad99d3cef6a3e450ce0350ebf88ec5dbe1efbecc30cacfaa6e93242"}, "bed722d8-dcd6-43b2-8ca2-a83b7e40f440": {"doc_hash": "fd9c10bfa7feaf831b1c90b189d34eded9746d9289364b8555a9dcb02ae0ce81"}, "16609c01-c14f-4621-8e29-4287e0689ec9": {"doc_hash": "8de6446e4011e26d75214271980b5d19ba12837a635b89b4c64bede54eb37a18"}, "3214b439-bafd-422d-9907-feda1b4b6eef": {"doc_hash": "a6805420ca303a0b39cf602c7fcba5cb6c28ece0afb687c12e9b34d513911dcd"}, "78b430b4-7272-415b-a2a2-73912578ee34": {"doc_hash": "69f25f93b97dcc1d7911be771cad8f5c5ada22a9dfdd17e5f653ff13a19a102b"}, "7b493f58-b0b2-495b-bce8-3e145def64b5": {"doc_hash": "7338534a3700dbc334de5c0e2ef1ea634593693919fec051c9effc7e1f537240"}, "58dbf862-3565-4a7d-9930-909136367af6": {"doc_hash": "fc2ead0e111b3107e66cdb600026191cc31826f332c35dca4d652e915cdb2601"}, "f710804d-cb57-4e2d-b4cf-8d27d2aeabf3": {"doc_hash": "2cacc0e226ce9004228e8654b1ac75caffd1696a93995003465f40027d39120c"}, "fc99e1ef-a4f1-47ce-84f1-e2a55b9e12c8": {"doc_hash": "3aa2df33b7ef401fb48553cb82b277286dc7a27c8b71e75b4d1d9f008e287945"}, "37e13078-b7ed-4e2d-8c99-2518a91b0d78": {"doc_hash": "a3ab5b8830183b4d2c35f94f2e5316925638ca3d68cd2df7b5e1a932edb5339f"}, "d9580c8e-aa0a-4911-804c-d9eca0243c72": {"doc_hash": "e6a8aef56efe99fc3b2dc01be6f785e88ed0546cf95a07cb299696d15df19516"}, "dd1225bf-0b70-46fd-b107-75f893995a76": {"doc_hash": "74a039db1ff8536762bd142f13c94e54625fdc986c289945346cad7f173c5f63"}, "92ad97ae-9fdc-479e-86b6-9cb8115d9f0d": {"doc_hash": "541ad5c1830e5f06aaa02cffcddc396210af3a0ca156fee307ced5a667b8f6d5"}, "ec066805-4e13-438c-9bcb-1abf6c3bb143": {"doc_hash": "586a7c618dbfc21410a392f2205da707ee859c8367963669a49353551f56dca7"}, "97e96042-9d0b-413c-8cf1-1180199dc740": {"doc_hash": "a6cab51d9886241961e4c1133cd3e54d4d3fe2d757ab2433e7e37efa2f61cd2d"}, "30c0b150-1c03-48f8-985e-a9ff33fa29a8": {"doc_hash": "bdf160d9343fd32500d88080290e77fead409d5cc59e46209c9079f03384d4dc"}, "ed626845-4764-4090-9e6a-017499d06b10": {"doc_hash": "cf2c52d3502d0a4deeab25aa7d4383225e7b6551f6fc43a78acb887c4189b1cd"}, "0943de38-d9c3-48cf-a5cf-ff0d66b26abe": {"doc_hash": "5d85bf2ec110490647976db9cf787a8ae4ffb0ce8a4cf8d92b330b2ff1762d71"}, "64bc5256-8465-45a3-8403-07d13c4061a4": {"doc_hash": "defa79271d6e2d02081338ef5a422e35ac7456dec1561956906a726e83887224"}, "1c64bded-b9c6-4710-9782-79c552f091f5": {"doc_hash": "1491bbc7b30292c418334fcb7ab141465953e7a98438d756eb2842d84e91adb7"}, "ff3ff054-6a2f-4195-82f1-8382df810dc8": {"doc_hash": "eb75229a0c7508b061ae19fde559483c5b311ce1137f491f2b80b6fad6d43fe0"}, "e745289b-b62d-45e7-9c59-d60bca128b61": {"doc_hash": "9840e1dc79a924d145001d59fc50fcdf58fccf4d1b011732f9ea71485d4e8ea8"}, "8aee8d6b-81e5-40ea-bb4d-e8c36a37af63": {"doc_hash": "4b3938e1430c35a53b0934b2423e3e8a0b38e99b3677ad3981050ce869ec3ad7"}, "8339000a-9de5-4800-97a9-f14d097a8f98": {"doc_hash": "eee77794ff8d07666dc95f958ad88fe3b602cbb82d8862a491d9434bb45c6b2f"}, "5f57f609-5886-499b-9fe8-0ced7c3944d3": {"doc_hash": "a50ef79af80af1ed40edf45a3ddd94d9a828cfb1ca7c6e0b849ce22dcb3db1f2"}, "c33e11ef-8da9-49b3-8f98-965587aab7df": {"doc_hash": "3c93efed5e52fcb1b624b7dc1fa9faee0dd12fd9e64bb916ff638e88cc7b97eb"}, "141329cb-b9d8-42e4-a0a8-ac844cbb395b": {"doc_hash": "9edd85cd4f658628a0030ac1bbb2701e1c476c4ab6f17fb66837fb57514e0514"}, "d85b0bec-594a-4e20-8bf4-6224c2766a3e": {"doc_hash": "c561d30f69e1f578590ae631003acede8a5f7e47a0d5c18991f26eccf8de0053"}, "61ec9db0-0eaf-4462-89f5-838a802748e5": {"doc_hash": "9a0994e0a9bff1a36e69db2e873711b3370fc33e9eed35a2a71ce3e2e95956d1"}, "d995420a-1d2c-4c76-aa40-e82fbc1ccdd8": {"doc_hash": "d4cf375ea9d1af4ade77b91f3219d084f50dab912060c7b4c1f8c6beb6dbbd1e"}, "4f7d55c9-73c1-45db-a4bc-b7798681a84b": {"doc_hash": "df9e80ec54c079d27f022982106ebba54a0f0353901198318c4095478e93ddb4"}, "6c487abf-3a49-4492-9846-1d71e723168c": {"doc_hash": "0383b95be8d3a4214502b23111b27f68bf2f630fc98f64a80b6f117708891586"}, "1c30fab1-1d82-4d2f-a9c7-e79da4b3781c": {"doc_hash": "07a796a93589fa55d4d6893202aa04b1b4ad3fc2756432f249b8541a3d954f33"}, "74266f51-215b-455f-bb8c-5e0cc9aab1bd": {"doc_hash": "a918b6b4ea015067e2d55d62a80dd146605b7cc57c5992b376260409ce978696"}, "ddf6f29c-7c52-4056-9f15-e5517257bf7f": {"doc_hash": "e74364b33951cec5eeed9263be49f024a722787dcc5f10c68de6421b165c194f"}, "8a323310-010e-44d4-8069-511a61b6b033": {"doc_hash": "e3c5f0bd59df598735055813f19d2f78e8c10901d881e34c786420f8a63626db"}, "5c5e39cd-5ad3-4427-9b1c-2ca86afa6c94": {"doc_hash": "04f07ffd2f1c0a05eb0c9a9049ffebd8900973dbd6124c35b3fd580ce5228db2"}, "f95450ef-580c-4067-97f4-a2850064e928": {"doc_hash": "0636d7c6246f615a00e4dfde2257118a07723a646507c2e76776499676c39e1f"}, "6c77cf7a-6d97-41c7-8c75-22ce66eee3df": {"doc_hash": "b2eac7fbb53f9f75be42cca4e026dd672f528fb34085572661eafab34b7aa1fb"}, "1bb84dbb-5682-470a-a005-33658e4f2130": {"doc_hash": "cf3b549560c556c9754192e5fbc83ad3adb7de24a8f0604674297db379e1d218"}, "4d509d84-a327-4d09-ae43-ee5f38f3f4ed": {"doc_hash": "ebed2d90b81081b23598e11c1c5fef6601243bcc8b3831afc32a021f9a8d6652"}, "8be1d785-5b3f-470b-a382-1f254cf7fa8a": {"doc_hash": "d6a2de5c65141ee7ca7cca55cc15335833d75af73e83996f573668160496abff"}, "4cb64ad2-e81e-4211-bb56-f08c48a57143": {"doc_hash": "14a3704f0e85c7040a6dd87bdbef4880e3ae62afd98dd681c9d910e0db744dfb"}, "20195e71-1e7a-46e4-90c9-f240d4041656": {"doc_hash": "79cead9173a78d6623add5e10706eb2f8f6417dfcb83e25f389a083892b66146"}, "1177bfe4-3430-4637-97cb-2b957b9cb091": {"doc_hash": "d186d6c8ef5531dc5a2fe30d461db19bb2abb4b2c34c0339154080bd1ce18d69"}, "b18c590e-95e1-4b38-8f5c-032599242ec6": {"doc_hash": "0fa5208e3f7cd2cabe8dd1e06a8bcea5e29467e4b40b9a71e8c95f23cae93269"}, "7222784a-317a-49e9-9f16-19cd60b2bcac": {"doc_hash": "b2ccf5fca0e67b97dc9bd3e01cd27b32e043238c6de840571dca512e16f88f8d"}, "f2b073d5-2327-424e-86b3-f1e568fc00a4": {"doc_hash": "e05e3f955b4f3a6fe56b3a64d06b7fd8a471ee99f7ef8fa66f807f154e95cfe8"}, "299cb7f9-c960-4da6-ad81-914ecc56d4d5": {"doc_hash": "f37bb794a05a03a590ea874411e49aa5b7068009636a159880953b5bfd5489f4"}, "ea51f9dc-eb85-48e2-840c-bdd4d02b5b13": {"doc_hash": "0c35738ac838904fdcd265c1ce9a84fa17afc05b18f7aed3650a0618514b4cec"}, "983d58d4-f85d-4b23-ad2d-72917ec8c1f9": {"doc_hash": "634736bbb8f3442705a29105665d8aa4e3d317659689907e0daf742d755b96ba"}, "15cf80fc-ac05-4e5c-8cdc-8c92fefa1b63": {"doc_hash": "c2ddbc36282657aa9e9dbaa63350fcb83f72f5650c29d425a50fba4ada5d5bae"}, "f94225fc-5d8a-457f-a27a-94a3b0b99c85": {"doc_hash": "a4eefaa49b5af97d33486a6d7de4f02fed8e54087760b2167a2585da3bd5ddea"}, "b5d2ce3d-ef4c-4b57-a17b-5154010af782": {"doc_hash": "eb9cae54a0514122af6723b523d757ff9dcfaaff9c072085f9ec0f37629b99b2"}, "fb71f892-6744-476c-b66b-05366fa0b4e0": {"doc_hash": "f33435a3b2cd7c35e366bc503920cf7a4a38931d445b1923c4f6612fc52eae5e"}, "567a01c0-2e14-4ca9-8648-6995d6c0d780": {"doc_hash": "bda1c3497070e8a5d8d3781f100192eaeb30bf447af0c0239b9868649d50f3a2"}, "97fbf5fc-62cc-4c7a-8a1a-f4810ce0206d": {"doc_hash": "a74d6154ba2d00889440fc671a3077b7718973f827f49a9e7d6c6a78e64dfc04"}, "8b970236-bcbb-407f-9cbb-c789d878070b": {"doc_hash": "5bee0eb9efd9df23772f4a8ad3ee645a0e61c39e392f09cc15dc315d0a46bd9a"}, "1589d060-28a6-4258-9600-7954de7b610c": {"doc_hash": "187360548c6716dbce844d6f7088d6b00d2813d95b074e80860e41144835f26d"}, "b87e47d0-6eac-43dd-931e-bc7c9d093c9e": {"doc_hash": "a969da4bd7a962faecbfb1b55b79e39ed271baaab3cc0873e97eb64ec6e1739a"}, "2e1da7df-8474-4df4-aa8d-4450ab07add5": {"doc_hash": "9435be3caee4c688f983bd88f12b9ef8dd4b2d64b43f65fb6f898b1703c86370"}, "bd732d7b-1080-43b9-bb48-5f7b313d80b1": {"doc_hash": "6f8b39cea52069cd5bad327eaa68abe602169a29936be7010cc9451deef3a8c1"}, "a6d0fd9e-6acf-4eef-8e14-4fc8ea23eea7": {"doc_hash": "9c7995b265ae78cc30149a4eafd617e6e9da8b8e08c4ed10c7f654eda843ec2b"}, "528130f7-6c0b-4c99-930a-1ffeccb02d6a": {"doc_hash": "92bf53d3e40ce0c2dacc4c118d0e11ff25997fedb04d2abec9795e2eacc06048"}, "58468748-a088-4fe9-95c1-b5efb4d1f77e": {"doc_hash": "ec4290f0baf10cd148ecea739060aa89970729ab8f928f13ad3f1f3ac57c2ad0"}, "37984659-24ed-4eb8-93a1-0109b8ba3938": {"doc_hash": "f224e3e6e217cbb10b09260fb365dd65564bf44e64d7ad8eb6af109955d37491"}, "365d3f67-4e38-4bfe-b76f-b39a52fd18cc": {"doc_hash": "e54a7619b82b0230062065ecd53342ab4436a7c38fa90373472d3c567550357e"}, "ce55cd0a-ec3d-46d8-b946-8a6d0523b392": {"doc_hash": "1e8a4ba5798a4c113296e4f49e8004aa107a54f1bb4a084df98e581330eddfc6"}, "b5537a91-0cfa-4a1a-8620-0ed97ab0b8e6": {"doc_hash": "96ff6b4732ec285e168cb1c9c6eee20df81f932d3f68acdc4275c22d74d33863"}, "a313570b-8c59-4b5a-aec6-c43d9339d521": {"doc_hash": "d2891acc3e07f9988642340eff6f02909a4ba0605b597ccdaabfcd7783ade09b"}, "c683e188-5d11-4e7b-ae98-d9030bea324a": {"doc_hash": "c831bd9a7de32d79b6ffdc2dc246033aed339dea8b8c2c08f8a1658978dd6d7b"}, "4bd1b38d-e6fc-43fc-828b-aa957c90ed8f": {"doc_hash": "f67d0eff87df069200b41c8fe94962657ee8038ee6a5eb2a9e3b0d815e21d133"}, "e8858b7a-bdc4-47e0-9763-e923ab713dc5": {"doc_hash": "c894d7f803107a16f9cfccd62678c33ada78b795ee279dbe2d9bfbf94b2e3298"}, "5fe7f062-582b-4afc-ad82-a1be59b2c4cf": {"doc_hash": "5e58e27f20cdd77441ccd49dd6351d900d76b33f0645bb6e351fb78b693f03a9"}, "6c689466-5b87-4a04-85f6-903fb33cbd7f": {"doc_hash": "6d5bb226c753b628317515e46473d47ead93572e607ba50f36e6f25ef474bb69"}, "afbe1199-2c08-4d64-bac7-a6d73abbb760": {"doc_hash": "1e7bbe0816ff7ec3444b6ec189f3226bb8174b71ae5fe08d534613796c041c0d"}, "be631d02-1b61-48ae-8415-b151ff592cfd": {"doc_hash": "88de09c41589ace376587a8f10b29896079c4415de883f253e53e6e983788550"}, "c812b12d-3e92-4c63-9341-f85e32ffb5c6": {"doc_hash": "6305117841d859d673c73ff9c94e4940b56f91540287145f96ff709e7b7a19ba"}, "91782f20-756b-4344-953b-1b7d636a3fc4": {"doc_hash": "6b7e5bd8cbbb164563c4230b60667e7966dfe6242859251dab576392f6bb0cb1"}, "6b8f628c-4ca5-4e88-92e0-a9dc929dd94d": {"doc_hash": "b552ffab1a48c4922487a841b2d85079e572dec3cfb52dbc224230b17050583f"}, "39b8ae14-7e62-4fb5-bd30-05fa87b4b8b2": {"doc_hash": "638c3264a2c42e842b7f3bf5efb767d2acc97d576c3cd4aae1d6971c15c20999"}, "4035b2de-dfb7-42ef-b246-39d1e917c221": {"doc_hash": "fd34f73ed2dbb29cfa0a9f8cc14151c20e02307fc8688b76ce9fe69e5041ed4e"}, "37ab87d4-79d5-4f8e-9977-8843b13f3033": {"doc_hash": "b7f9dd6460f182391e0365ccdd3bd905ed481221024d919ce584abd840aa1aac"}, "c5800b7c-34dd-4ea4-bbb1-7288e1e8288c": {"doc_hash": "1f116457dfc13b7c51744ceb924ae4bbffb95475b50016a3f23f22a88a1bcff5"}, "5b6b4be5-1d4a-44cb-94c2-16aeaeca4a39": {"doc_hash": "2060148331a9605df6c59c2e857fd78116c9eed6344ccfaeae5df1ded4358a33"}, "b028a9ee-877f-47df-b366-303c167478c7": {"doc_hash": "946cfe88d2eb3c94ae581aba9bd88f2a0d56116d6c2fa74b4ecd0f8c3b134f33"}, "af0387a8-178c-4e13-bb4e-7ca1a107c4a6": {"doc_hash": "eff930a902cf138bad8c55acca324d96f148f190241604a04672960049bdea14"}, "4275b810-7d87-4a4f-8355-1332465e1133": {"doc_hash": "cef1ae124be0cfc6ef130928f4618be028e5d50e330a71b515fafa4953034e3c"}, "c363337f-f492-41ed-acdf-df77440b3772": {"doc_hash": "7e28952ff74d9ecc7e1d485b97f276ee6df0b30358d85b64c8a132a62932d41b"}, "b4600a08-325e-424d-8707-101e6dff251c": {"doc_hash": "c7ec3228e310b5ebdffd075884ef1ec639a781436a7ab9e6e085a98e68ddcdc4"}, "4b291cc6-1c9d-406d-92ea-48c488ba1a1d": {"doc_hash": "2898fd1222ab7d23813e41d928252407682160fc4f41fb2aba3d915759669ddd"}, "8a14ac0b-ccd1-42c2-a39a-0af461bbff27": {"doc_hash": "7818b0531fd459320111181a9af92f7347c85bc5c906c8159df8895b3eac6129"}, "6d52553b-bbcb-468c-bb2d-105943b58346": {"doc_hash": "85c2f8769d707c78590bb552c4a4ea4641d93bc0d003abfd75e0b88115ba0a98"}, "e0ecfe1c-768f-45ed-a5a7-1b656ae8c938": {"doc_hash": "111d8622ec516f5d06f119a9a640a3b3fd8e632a11fa8ed0a47630dcd31ae903"}, "0d1640d9-f170-430b-911d-a029b32ef988": {"doc_hash": "4e09177fd92fdb3e38f6d5b72130bab56337e623846199173e8bff49ba8db938"}, "2205f9be-d3e5-45fa-85be-d6dddd0be8a3": {"doc_hash": "6573aacd06434fa52511a43ebce01b1514382c568143e918796095cf45b9aa5a"}, "fb8f9468-7cdf-4fe4-8c31-88a856eac71e": {"doc_hash": "3e697afd4074c760b1238d5802a0f11a8d38e5773caeb2331c01f266319df5a2"}, "9bb7b024-485f-4f3c-b531-95ea75c81dc8": {"doc_hash": "881e798dc71dc5f1612ea30520981e904cafeaa0239bbe69473d3be580f24321"}, "0c95cbc2-14f1-44f9-8775-f2446121473d": {"doc_hash": "e735eed5aaefcca8947f3e3cd2a7de7006a8bf4e50edeb506900508a98f776c8"}, "4e5b6f02-cc0a-43d9-b8cd-9e98c06e4ec1": {"doc_hash": "d3681a32ffe2c49dec8cd65099b2bc2b14c3dd43cc1a19f3c0fda12044330568"}, "61b17e81-1c2f-45ab-ad38-403a4d89862b": {"doc_hash": "bd6673adfde94b5981ac4f74505682a3c66efc353b9be26334a32f022a5bbf0a"}, "ac648878-e6f1-48ec-9807-115c802405f7": {"doc_hash": "121892d3371c7579eaf2a1fa59f5fee52e95f3169fc0605f8be7086900541add"}, "82aa7df4-4666-46b5-9ba4-d0d6beb3dee6": {"doc_hash": "493f6a50efe2f7a58f6b797e8939eb65ddcf1b4d3b541cd822c7d496a8d45c92"}, "97d5dc5d-e376-4753-8958-3334e88cd9b2": {"doc_hash": "8708161db97e40e78a43aa4e6479a21aac78e13a8fa6ef410a5cf65234181b40"}, "7bd14eba-fc09-4bc0-89c5-2c5c2dd01e1a": {"doc_hash": "8dc3493ba5b276aefd60f5b4a36b7cf5a48c4441f95b3aa222cc3cda08bea197"}, "0c0aaf4a-1170-46f5-8932-d9c2af794a90": {"doc_hash": "15330c29587e3bb176c9bc1e8d9f839ff78c3c9af73a195841f414487f5f5cf5"}, "c9a19900-6f4c-444a-8796-5d5bfacd6b5b": {"doc_hash": "e7aa6be4f94cd482686c0a386aca68a05466c1707441791fde416022d64eb9d3"}, "c5df4b95-e582-4b35-9771-6bdbea5478d7": {"doc_hash": "78fca217b95c3edf5f71668d09af4bc37eb44feef00aa342216925c7154c15e7"}, "ab5d00d9-4931-41f8-968d-6ac14ffec0ed": {"doc_hash": "c4930ae2aa8d9dc364a9bf81bff39deb1bc806f5a021a766625fc83e5685c67e"}, "f87b5543-43c6-4971-9eb3-31906a5f6a3f": {"doc_hash": "3f3e7fad197d7bd8ab008ec1666849cda9e92241b91a1e2527b2215818b4dc13"}, "fed9a6c7-2183-4a4a-a380-44b17676cf27": {"doc_hash": "11e1547015da95c0bed4d7543225b105ab0b4eb3f97ea71547df1132e9c90ece"}, "ac00b750-e85f-4271-abef-c92778783b27": {"doc_hash": "2c61fc6d5d3326243bd3ba30496acca721ccb9120bd231aa588e86cfb14616e6"}, "40dbb883-ecc4-4f7f-ac3a-694401f56fcb": {"doc_hash": "998def6a129fe99c62e25f18906a105f901f0ab80ec73cafb3cfb3965c0aa9e2"}, "fb5c9bb7-3512-4c6e-b1c9-cb6dc5efca8d": {"doc_hash": "9830f6f06881ff5f9d613bb2ee6d6c68e44a2985dcdba9825a1c4a23a66d1a9f"}, "f3a22cd8-3fbf-4dc4-9044-72336948ecce": {"doc_hash": "5093f8d1af581fa1edd978da86a8db164abd35656ed297ce0d4bc186ab162dc6"}, "f1627189-3761-48b5-9304-b33ea5a90b52": {"doc_hash": "c4d0ba2aadb32c3c889ce85b9f1d72964f6e4b264ea411294adc1cd78d5ffc03"}, "82165b48-2776-4a83-be7b-e8ceec57907e": {"doc_hash": "e8ec25f0d204d6ca577b1a2f7ebc01018b58191e459b4079609bf5abd657897d"}, "42cfe601-5767-410a-adbc-209738a671c5": {"doc_hash": "e899dd1128f8abe9ba672084d19db9bc0041c26158d22f11274fc5d959fdee03"}, "b54f5097-cbf7-4e37-92f9-12fe10928cfa": {"doc_hash": "321725668b75dc5cfd9aebde299dea6e737cd92a4f5399d3cbd006117742f759"}, "ed604304-3672-4c9f-b030-4ef69890d838": {"doc_hash": "88801a82fbaf0ab5e036e166b49069d83da4a45d664f39f83b387f4a6c0e968e"}, "01117d63-472d-4d1d-befb-7b50753e6d14": {"doc_hash": "01134f2ae6f006a19a79fbe5cc3228a83af4376b5b32c05a1a9304143ad81612"}, "013af61f-0a2f-4a18-98ef-6982616901c6": {"doc_hash": "438798fe71d3d5fd74503e8983beaf90f1d4770f55f6c1a331e76cb83f7d02c7"}, "5310c3ff-082b-4199-a04a-13f542506f08": {"doc_hash": "67819661d8845e42a0605e4ddd5fd7b9146e56f44c0b64e3501392f56e5b25b4"}, "8ed12660-8fb3-4ebb-bbc1-16464737d75b": {"doc_hash": "bd3ffc1f3c771e21605a6e6325d2b1d71d68c5c6061f873c3bf9363b3a9227fc"}, "af8f683f-ff06-4f40-b4c4-766dd47576de": {"doc_hash": "a5f189668a3359bce62de2dd6b5a4a294cddb789e2796afcafca14c4c56170aa"}, "3e5b8c62-32ec-45ab-a485-1855e9d663cc": {"doc_hash": "5d9736cf4217fb04d901113911614984a7b23fb8710421a6bd46b7f6a106a829"}, "b932d09b-ace6-446b-9a2c-310f54c38d73": {"doc_hash": "a6d121aa3056d3c947c57cb6a604def995d1f78f1f4ea67c416fe207708a1b5f"}, "f622ab2b-aef0-488a-b325-534dfbf2d40e": {"doc_hash": "078354d58c392800e4faed2ac6e551839fd67469be5db6ffde5414defe8ce15a"}, "c19a2081-d274-41e5-865a-2788c35a50c1": {"doc_hash": "57153c8e9ceae047dab90445b1f90c75a7492332d1763fd2bfbc93cf41310259"}, "4cdb16f1-42f7-4dd3-9f29-c7150a5fe163": {"doc_hash": "1c1b222917b05a0932e8665e72b3d48e3eeb0f19480dc34d431f8442e52dbe3b"}, "06b0417f-ac73-4f91-80f6-2980f6551940": {"doc_hash": "15150e877b9e6a59fd8dbfaaf6a79a779792cd203d916b004af2e2973747fdaa"}, "0fa73be9-e013-4cfb-8b21-71ab9a522688": {"doc_hash": "ae5642aa1c880907230f7ac56abd25c0e0d87492a4b6c7bde2e515809d2259f2"}, "514eb90c-1cbe-4892-81d1-68464aa8022a": {"doc_hash": "05d6e0fac2f8b5e22b017c019ff22205c0edc126e05b3d5a3177c756e600015e"}, "a75ee081-c84c-46d1-8f9c-6fb9275c313b": {"doc_hash": "6be37f540a70f1e5e69adf7a621d265561c31165de611b635389770d0f3c617f"}, "07d3b489-6490-4da8-b93a-8591cf3ef247": {"doc_hash": "9586af4086015661f77833c2554f368b8b8d0fb74e70e68e10f11041149e5d65"}, "918dd188-3baa-4b8c-9a6b-79fc1e6b307a": {"doc_hash": "9c4ec1d70a5feeb301562ff044bf0719c0dfee3c41d247d4fab9895727f18afd"}, "91bf0ea9-93ea-4bcf-9afe-d2542d1fc4d7": {"doc_hash": "d3549affad92caa02c4548bf4fc02ad030cb133fddebaddd6e162dfb9b37d3b4"}, "1598ded2-4a38-49a2-a027-6b7c724dd361": {"doc_hash": "f35cbf7305cf917bd6fbc9ab3b5d984e9547179ac745748aece4741732d593d9"}, "e05d6d91-6042-4560-b39f-60c85b3f6f8f": {"doc_hash": "b5f6827bad9e3c258647ae2f4375e121a0eba1f9ec4e1b3f8b3c179ede1c6a7a"}, "d43c9a0c-6dea-4d6c-ab58-41537e895770": {"doc_hash": "21f0bae4bc488b102814d150cf5d4f4364c5228b9200c13d8e592401d5245547"}, "e070b5ba-5842-4e1f-888d-adebce7e46ea": {"doc_hash": "2d39dbdc97b02b1ccb2057f547df734a7446da7145425f60855e954a1d6fcb2c"}, "6bab5e62-71f0-4f1d-a0c4-6ea43aa7a949": {"doc_hash": "f9b74f587d8ff86fce2461c725cc6737d6c0e29235ac96d4a3eac4d7cb287890"}, "7b867c3e-4c02-4595-b142-bc52a060fa2f": {"doc_hash": "14fb36581132481e636141053f7cae81dfd958e3e75dec0201988ab5aaf299b0"}, "02a5fecb-4917-482e-aac8-2f164068b592": {"doc_hash": "5b5b051a0ee493d676f4907b94f0976d591d5b797c439830c6c3519ee4aa5b96"}, "fd1c4dc5-0ad7-47e8-a4b2-f2d46794f0d2": {"doc_hash": "bf3dc224e4c17d82f42208ed4189f5820c12085c4ca73381bd9b189434f43e38"}, "4e616ed4-0694-4627-a179-7d09b338ec3f": {"doc_hash": "734e10029d3cdfc9798320ef58a1c33367e21b18f86bec9f2a5b79946975176c"}, "fe0285b1-558c-4055-8026-68c1d0603ff1": {"doc_hash": "e36078321aeee38a02d65d6fe9ec32b0b94476839cb04f5608faaf0df12d9fc2"}, "b75e8a37-dbbb-4281-a233-8724b1f61791": {"doc_hash": "3a034199ba475c358291726cc3640815eefd7f4a1bbadea88547ac1f87758dec"}, "5fa421c8-c6d5-4344-8edb-d0beb5a07ff1": {"doc_hash": "fa5ba8660344b7747581e95401f73eea4a43e07d6c5441415a2293f406960aa9"}, "57cb6faf-7526-4f6c-84a6-cf0d26d8b304": {"doc_hash": "d2126fa5093087dfa78a08b9a147c70c3636ffca2a89c597fe532ebc040df579"}, "7ac5b2e6-848c-436f-8f4e-4e6c7111f0fb": {"doc_hash": "25b1dc3e267c59406c421b847786f4306dbc95331bfe6b92753b74268bce996d"}, "545407a9-a8ee-4f42-baed-db62cd6406c0": {"doc_hash": "cfbbde0c6093353b4dabef9324427363c4befa5615e78fed9e20c68a8f014b63"}, "7a23b51c-4c00-4a6f-9e18-7acbc5a17aef": {"doc_hash": "7e3dc90adb8c8ca149dfdacc80534bfc292de5fff096bcc05a3a7f23a4c6d6a3"}, "2a46676e-03eb-4e0a-8478-fd9a79f16f84": {"doc_hash": "2743363c63bc2276125dc5a801e4b8b469bbd810df19cb91e8bbf18da481845e"}, "62e45bc0-2093-4f6c-97fe-2e80ac03579c": {"doc_hash": "0fdb4b80b12d7e02b3ba5dde58ebcb74a0d43b6a3deb3df33e617caa233a8210"}, "bdce32a2-766b-45f1-9cd8-417aefbaa106": {"doc_hash": "07dbe7c20e25e9e399781f1757773d6cb2e201835c857531a4e42d7cb1c3d6c4"}, "5850738f-70fb-4cb6-806a-e520a42e91a4": {"doc_hash": "51bbd0b79e1b17fccc4ac884830a4eb78396738beb9fe3b90ec043116db9a533"}, "329909d7-18a8-4a55-9067-1f234411dee2": {"doc_hash": "b9f734eca8e529aa6a0b55800a0ddf2e6ebca81562bf95581fb35830fc5df4b4"}, "11f0b836-c8a9-4817-a40f-c14145370e6e": {"doc_hash": "37b1a7eb5d6edcdddfc9324284986bc4192cbe9126b5ebddcc78e5003d9840c3"}, "25e22f8d-1312-45cf-814e-fc06cd2fb982": {"doc_hash": "e5b6e05ae211724352a79c44a4344ca1aa0fa5544b2cdee2882b40699b6cb525"}, "6b3fb3ac-e5ec-4a71-a452-e4231825f348": {"doc_hash": "b9e8e0f3b67ef1eff7f707d05ecc21f83cf9455a77beb3bd0e45bc8acabe60dc"}, "5d854e55-f328-4526-b771-6cd53939327a": {"doc_hash": "bd3be97da01269447f4f5124034f306755ddba124524075a13a6a5a5694e19a0"}, "c79e5777-7648-4a6f-b184-baf3dd16dae7": {"doc_hash": "d03ccdb1568409f98c12c66ce6a5ae910bda9da1c3762356d281344fc1435a36"}, "b8b1022c-6c6c-45c4-8259-bf4118122e14": {"doc_hash": "b4977ccb886cbaf5a2e5319185b8c91f214ce77ad103c71067fcc2ac8c1bbd80"}, "416b4138-a42c-4040-96a9-51f2e2879dca": {"doc_hash": "c619bee52d887a0c920f4cc1ddcb9c3ceae840ec40b164cbf9ed9a2368382e97"}, "b3ce64d9-37dd-4ad8-a31b-3fbc935f5705": {"doc_hash": "4df3956694765794fe4914abf8a1203e15fd3e0c67ce697acd4ae3659d2f3393"}, "4dc5a2be-2636-4920-a92a-a8d3cccb44e5": {"doc_hash": "fdcb0d0f80a9133d54302c2ef2a4d89c928c177d326bccd99357023a55a63115"}, "d36a0a4d-5441-46da-8604-223bea69548d": {"doc_hash": "758412026abc41067bb4200e936f69eef9b470ada31c691551a8fe60ecd2d078"}, "b66d1eae-a309-4856-b8ba-7089e7bcf29d": {"doc_hash": "6a93d5ffa0ed333c06f63e28944f3cc79162f501a5ae85218bc23d78719659fd"}, "6745a5a7-0c40-413f-afe3-b36c0797dae7": {"doc_hash": "7f9b7b10e78f2506fd747aa60742bc89cb8af4cf651528c34fb415fc21f93653"}, "75cf121a-b58f-47c5-8978-0155ebe436e5": {"doc_hash": "20d8471691c4480abc811213f9d9e928c9303152b5ae98aa1d3dbd4992436f81"}, "67110e7d-6565-4200-805a-df95b4678c5a": {"doc_hash": "ba06ad7422a28f878f534a89c7bc8750836c5590fb158fd0e3f64bf0def1294a"}, "abfcf9b2-4c8f-4095-b8e2-2258c90d947f": {"doc_hash": "0f7e418910c1ae7692a29fe2ac49e7b1a5b644033e2e668e31d5cf90db5c05ac"}, "12a25404-e56d-45e4-85d7-ee8724ce8943": {"doc_hash": "69092420fec2ad284e1ad9e8e55aa847c77f0d94d394f1f505e7f122d1777260"}, "683e0d30-7a68-40e9-a1d4-5f0ca0230cc6": {"doc_hash": "505d84dc77ad33c3eba5c2c7bddcb4599dd79a5d55e74448692dadbae843279c"}, "005133bf-6381-401f-abfb-cbccead6fdd4": {"doc_hash": "aa2fe93020599275e9766bb0080f2b0ee797542f91d490d681dae27f3d93f2c8"}, "81056aa8-30d4-446e-a281-b5e7d2d7373f": {"doc_hash": "324383540a97020ec1d417b2c8a0e1558574c3f8661dcfc9602170c7606fc39c"}, "fa8f088a-df23-498a-b721-18f4b314afe9": {"doc_hash": "2263a9a4ccc7156c1ed8c43630637328cff70207eb396242453a05b53cffaa0e"}, "eec33efe-81f1-4ccd-8e9e-c2abd15b9bcb": {"doc_hash": "50d17c0be143a4d5476ef671953545c6be958d1e27c0dc30d76c92d7d07141d7"}, "d4f313f2-f6d8-414a-8570-b0d87ea84660": {"doc_hash": "0cf0c3b672f74a796524a7da9c662efcada5bda3f389bebede7c9ce5f7b1ef89"}, "daa736cc-f5d7-4dc1-bd38-be113d2b0cf3": {"doc_hash": "2f5d1543e602f6112b7398cd8d29375eabca9860c32a071524d523878f59e96b"}, "abadf77c-9f91-4456-8c2d-e07f73c0cd70": {"doc_hash": "62b74c93991ea3bfa30f6822085a15830b0efb6ff13cb4185825ae08c4477346"}, "2c86d817-b5d0-4530-8ef0-105d266f8067": {"doc_hash": "c0add4c2c042daf12b61a3d911fe111545f516c8f5b2e04ea1c933264a931341"}, "b479bc95-ece2-4c3b-a6a2-2dc65c02c4af": {"doc_hash": "aaf8879badecf13797c83cee39bb8366791d92e2d70884ff99f81deaf53951f6"}, "7ad29451-98c8-4226-a133-9c4570004c6a": {"doc_hash": "b93f23b8e1d1ea5f862523c77b358261914c2832f792d89473a578883792d5c1"}, "a8ba6e53-d2e7-4ec6-b201-f82740ffd64b": {"doc_hash": "2b4dbbc01bb5389691005705df7d64442f624ee264f1a8b3567b614bbef5649a"}, "e2138144-35e1-4f8a-a96b-911c96d8c45b": {"doc_hash": "619d071b0e954a1e220d572e1854f22091fc8b9a014fcce41893a538295210dd"}, "1a0f3163-9c73-4fd7-a425-4a4e1fdee66c": {"doc_hash": "1c411a9654e970e6f5ec196b3f0a18fdd72f6d1238876ee8f7e96388ff41e282"}, "214b6204-23d7-4e73-b275-87aa7aeba666": {"doc_hash": "4b21fe2984c95fbbb45c4d51fc60e4ba1f437b74806c89c3c11810d54412a159"}, "9af93a94-6b06-4d3f-92b0-7f083b70eba4": {"doc_hash": "cb9729d4e06447a032c808ec97db23211b2ee80755e7ed8be4f7580b5c63b265"}, "6428f0c8-9897-439a-946b-3395e511bff1": {"doc_hash": "3a25a1f7628794c2082d7a9a82b4bc078f9924fe2d97656b7e9a6aa8cdfb4ed7"}, "e9ccb713-9c28-4d0f-be76-423f1336899d": {"doc_hash": "08046afbcdc848e2cd7758405ea3805b447d388f25dba7a57649d44ca4ba98c8"}, "d4e714b7-7159-4342-9a52-768799217228": {"doc_hash": "a44a874a5c41944669bc9e75f05abff22fb9e131f093c3edf67ac63462b16ab8"}, "5719b3c3-4da1-47dd-9280-729d6cc7dfd1": {"doc_hash": "6dd7c9a6cf853ee11bfbb608b3813eaa9f093ae61507d5f8dd72725c23b909c6"}, "05d6ff72-00a3-414e-9223-c65ccc9af63e": {"doc_hash": "ca0ae72bc60d5e7cb943d534d96a1f7d0df865d4f46446f07f6f2b3beb16f897"}, "030fefb5-c3bc-4a5d-8dfe-3bf991ed6e6e": {"doc_hash": "82100a12dec8f96fa2c37fe4e8bf0835db712b9a0cd74fe690f5009c3cfd04e1"}, "44fce5fe-120c-461e-aeb2-ad10856241ab": {"doc_hash": "79a98e6ad6d7a7ebe4fdbaf9201f87b1e71da341dc7fa0e5fb4c9fa93c19160f"}, "2b732d2f-c767-42d7-a3a9-35756260baa1": {"doc_hash": "2d98b4f9b4a74e95bc0d34127dc924cc05b4ebeaa41b60f42ab7c8eb8970fd54"}, "ee120089-22d3-4dbf-a9eb-ccf775aab4b3": {"doc_hash": "d8a00e3d38999ac267212ceff0e053f4e7b87a400c28f124ab289e97bdb6c273"}, "b0f2cafb-422c-4582-bd63-9d4ed41150f5": {"doc_hash": "34a0d5c1fe1b8e8666bddb9b2c21706a23c62e247082401af2fb611ed6f2eb19"}, "103a701f-14c9-4c2d-91cf-e8a9355ed1d8": {"doc_hash": "0fe41a9d842334e1525a9b0a809992a72ee6c24ef55391b92dec2cc3653e1e42"}, "a181963f-518f-4a3b-ae2e-d20c0808acf1": {"doc_hash": "1bf689007d064afef618e13a75a28491beb8498b00fdc7c671ff5a3f7965f6c6"}, "c8a71c8e-e7f7-4fd8-879a-f2104b11f2cd": {"doc_hash": "18b78a5e7f5a6b078b45b602fdd3ea26e8131f55f50a71983426c45663363a9d"}, "dc4c8148-7e73-47fa-97bf-86749ca3234f": {"doc_hash": "a70acb8ef648b90f5d6b84f2a2227ea3fa26a7af9440861104012b449b50b9be"}, "873dc934-8f63-4168-a833-aad5b89804f2": {"doc_hash": "648879bd0331136ca389ef8ae8e18beb3aa09481cd08637792256059dda16b7e"}, "d8686e99-b64f-4c80-9ddd-3ec2a9a74adc": {"doc_hash": "fe3ee2c21c07eea8073bcdc77aea9a0c98c147f79cd082ca664e454258dbc5d0"}, "98c32d68-879c-43be-abc9-afc38d096054": {"doc_hash": "90b2d7f0430c511854266f28ccd866e52d11aa5728e093283976b900dc998b3b"}, "dc390077-cb7c-4cf2-8f9b-15aa027719f8": {"doc_hash": "09e7f745a9dd08a299a4a5965739519bebd2494f91101ea1e49be8ac3bf0fff5"}, "3d37decc-2d4c-4627-b50f-234f99aa6bc2": {"doc_hash": "3c4b177f069af07633c97eeb81d3a040d7aefd8bf34056e7a53cfd24d7131660"}, "85f6ea14-fbdb-4b68-9147-9b21663b6679": {"doc_hash": "6a3118c34251e80e21cc20885c8c04ecd233754e3a96c35048a23cac08bd34ad"}, "3da80f2b-1a8f-4655-86c4-4635063958c1": {"doc_hash": "bfbf1944f8b3e4d7fce561ce2631469751ae0371d11af4bca2f94c085312137e"}, "28a07de2-28c9-48b4-bbc1-0753d324eaae": {"doc_hash": "918e7219d18d705b3a1b4a9580cd3f0c446ed9a021bc8e649cee937fe0f1987b"}, "18e4bacd-621d-40f0-8793-a8fa107defb8": {"doc_hash": "3fe0976faf2f63dd7ae7a2713e7cbcc7c67f22375f8efb9f93ccabde2d68160a"}, "a766100b-6ed6-4a4f-a28a-cabb15ea4820": {"doc_hash": "b96e4321107434ad6fc3312130c76bbd741bcf1b09a6a65332cacee40d029b7d"}, "37725764-f5dc-464e-b305-686052c2b92a": {"doc_hash": "bf01eacee104db98553540d614f7915550d596c266de296e469a552bd1d09a30"}, "230fc89a-906b-4925-abbc-29a1e46db99a": {"doc_hash": "b0d6ec376f9b1bf1c1d1feb5b6110db291fb90e333aec8464478b222c302d47f"}, "64f551ba-4e02-41c2-b816-0a4a009496c7": {"doc_hash": "07416bcd915a8cfcba0e24ff43985dcc37a43061e56b6dd59c7ae36b568005b7"}, "ab3ed68a-8da5-4b23-8357-753ac8ff28e6": {"doc_hash": "ad225af4346d9ee5368dc8fdc3569a5679ef105eaaddf73b47fdf99626eb2c02"}, "8b2725dc-2582-4a04-add5-60fcd229c176": {"doc_hash": "7d566b1482bdd64b193e292334e0c3fbb4c69d5657f7ef058ea2ceffb88f82e4"}, "d366da98-37f9-4ca6-a7fa-329d3aaeaed5": {"doc_hash": "34c5d592001b58b83e39c17526c98e4fbdb17566ee5c0a96ae53b60092d101c7"}, "4ba84d37-a2dc-4bd3-965f-9f25d4bb7179": {"doc_hash": "fe3898e3d06e7c0e336d0ed105469d0e0c0779899d8b9acd599af8a4faf985ea"}, "08f0afe3-6c02-4ac3-96f8-6b9faf8d6693": {"doc_hash": "deb0893730c359618a01dccd2a5bb5c7ec0a79ae827879bd38b3004159bbe1a0"}, "f2914984-93d6-4064-8052-405569b03054": {"doc_hash": "54ac40088cdf2b553b8cd506ecffa4f12c4e5c0a7209ec3af6ceb3b03ad4da8c"}, "f0f3ea6e-8cd6-4549-95b4-c927e37cab2f": {"doc_hash": "e6be2b8ebba6ac222ded129c1e36d0ed2124820fd5bd9e9ee61f1780b945cc87"}, "60dfb41b-bcfa-418b-b594-7e9526eb6264": {"doc_hash": "d7fb8d0342aa53cd2bdbbeb979072f40ce4a2d2b704ff5d324d03b800a1a2af4"}, "d13ee4fe-7734-43f4-8184-57bbcf381caa": {"doc_hash": "e88956a8bc7a8182d6a9d5034cad17bdeba92d990217ee82e8f9cb03e2c5b003"}, "31aaf4d1-e581-4b33-81ee-c639c0b305af": {"doc_hash": "ce7532fb74cca9d797961e454863dd3ef2d2fa2dacc205bbf15826f74547518f"}, "9d77d323-e287-47c2-9301-d17f17c1919e": {"doc_hash": "8832bf404aef6f405f9a020b1aa2c257b5ae87cbae71dfb43ba992cc2ca1eda0"}, "fe18d9c2-64b3-48e4-ad32-d186e47115b5": {"doc_hash": "da232d9996fa47216ab3060e10ea38eec30a1d26534fd8329e34bab0c2fdbef6"}, "4f12f874-6da4-49cb-97ba-54897d0ff17f": {"doc_hash": "1c3ca53a1955ea8801bd86d9ec68c13b1c4659a7c28dc1a600e1edfaffbea139"}, "1296e427-d73e-402e-934d-cf7b8d5d995b": {"doc_hash": "8f8de0f67a62a14f1e96917dbb09de7d1d6101e5f637d8b3b8b3b8bfd973bbe8"}, "2cc3c423-fdee-4eca-a002-c3755d61dc79": {"doc_hash": "68ce1a6aad1dc6790d7ac0c73c5e146fe2bca992162ff87582c2b5897220eb8b"}, "b1a62608-225e-4b81-abaf-156355da975e": {"doc_hash": "5637aa50176f4d05c98b25ac212a3f0a2294199acf2d0f24f768fb4c517dec9b"}, "ce859bd8-19f0-46d1-b8d7-1cad32382376": {"doc_hash": "8340012d302c67a527900547fec5ab0f93514b57efab3566293f62b88ccfe189"}, "42fcd292-59de-4dcc-aa75-a73d0fefc07b": {"doc_hash": "976694cb6d3c719fb66aca87159631a7b5f11e6de3fca2f016dab38d4c511e06"}, "dd79b24e-a810-4def-8a40-d9c7f67fb015": {"doc_hash": "856ffbb4a531c8694bd78cfc29cc934fb40c59e5a6bf6474fca77d5ae67a0892"}, "9a391930-3324-4cae-bbf8-653b6acf4753": {"doc_hash": "d016dc209269e33e9d1a73256f719a0759a6cccb58278977a636ac3652512e48"}, "bd54a1b5-0b94-47ac-8f86-afe20a648369": {"doc_hash": "bf9870ad408101720a659f577ed947fdbf5e585e4de93619352ec20c8f6f4a43"}, "96b5a6d7-c83d-4479-8fe1-bf88a7a63700": {"doc_hash": "1c63168f66000bab38e980d0fd5c1fbbcf74a91572036b495472c284ee7f18af"}, "385ccbfc-d3a8-4df8-a6ef-5c3c0ccb579f": {"doc_hash": "f83911ddbfe932c077dc64a4b72e240bdb414c28662b8d4d95e96f10071e0d3f"}, "34698dc2-68d0-4af2-9f49-a0688b3dd878": {"doc_hash": "97e2e1a8afb3b3c158ee681e0faab8b2aeef22802142209abacdf26af3a793c9"}, "f3ff06ce-ac51-448e-a7ba-5e199aa99211": {"doc_hash": "a8078a582124ebe6404e73e521842cd0c8943566ae69e0534ae4631654786c2d"}, "41c78e09-a863-4f35-9f27-b23f8c164a12": {"doc_hash": "f5beb9406423fa32ad0d981228ae1f904859792d14706340a5963fc5b62b83f1"}, "e8587ffe-8f61-4299-9015-9abb78d599e1": {"doc_hash": "20832f5b5f02a9667bc1b5752f8a9ee43cff9585929481867bedbddd1cfe1d3d"}, "fa8379e5-5515-4c22-b2f7-9077fe880f19": {"doc_hash": "579b9caecae1eab39656061d875f09291d1aa3967315da91413ccedee6835aa9"}, "2c16255e-c467-436a-8722-1f3772903cbf": {"doc_hash": "838b5dd8a89d66f866c27250d978cd5d457600ad43f2f22078deb4987913efb0"}, "3e0ccae5-53b5-47f8-b89e-458c985215a9": {"doc_hash": "de29430e7417f7aff1d31f96453fc59d5dc53955a5477de33e86253bf42d8e7a"}, "61a18081-2ac9-4ed5-8015-f0616ca0e6c0": {"doc_hash": "f83f330294e2f2315aa192bf74b3f1fc0fe51ecd016c2d00b12f7909c11a2303"}, "2975c766-d4de-46f4-8217-9ec77d5e5694": {"doc_hash": "83f439b3f7736246e436e6036dcb27af8013f394ebc77ce3941385df914b329d"}, "16cfd766-7ce8-485b-bce6-4f485d6ba437": {"doc_hash": "4261d55ff0f2efffe95e4c77418ea5538f284d970cce817f92f3db613251266a"}, "c0f2746e-34f6-47ab-bf69-8c1160dd163e": {"doc_hash": "6c1b6fffea3a994dfdaa67ff4d6f00bed649a40ec0748fc289f629892d0ec673"}, "91943f55-eb5a-4882-9722-e58d971b0b11": {"doc_hash": "66b3855aa58045f8bcc57874adce4ea07dd6afc3019e41360603984574022ced"}, "7ac8c78d-33c4-4ff3-aa6c-cc529a712235": {"doc_hash": "eba09e0ce370a7ea021a4b6de2af37469d27772497595161219ca2651a365bfb"}, "37ecb6f0-0c17-46aa-ad50-59d15b0d5925": {"doc_hash": "a28e14872437e369a1fcb56ce4e71aff2bbb34d9e3be0e5d1b89dce282c35803"}, "5d6116c9-2cb9-4eb0-be46-952399e626f6": {"doc_hash": "e5677dfd2ac60e3b17f04ebdefb4a24e818061cb560e1e222016e3b294dd9d5f"}, "0195fa02-c772-48e5-b18f-ffa0963851da": {"doc_hash": "87c103c61eff1cd58079b69ec30591ac8bb0b65734d4b1bb2bf758294a0913a6"}, "4aef79b3-9b44-4fbf-93a2-49760af82b5a": {"doc_hash": "c0c66d0b175d8063fd1e55700ccac7b7612a1875f2baa9da0c0bb1ccbac04429"}, "88c043bb-73e7-43f4-882b-254312d41907": {"doc_hash": "f7002a97dd759d1b4baa62fbbf971e4138b0b4763556880b3a094537cb9d3f6e"}, "ae002d4b-d816-41c4-aa97-a67e92dd8b95": {"doc_hash": "b000481315edae965f8c4fbf96f21a8fc794c1798ff080635e0a6b20118b3c08"}, "0706fc31-0438-4e89-84ee-9540c254dc80": {"doc_hash": "3e4cddfdb035e116002eb37b49e2795e17d2efc5285e1141595363053fac1fb5"}, "85df79ec-9b2d-4146-bc90-3d3243546817": {"doc_hash": "270f32da88ab10bec499955ba0b4b852fa1f01d5380ef536860d0ab9f6ccf664"}, "a0504346-f2c4-4851-b0b0-57bcff4f9ad8": {"doc_hash": "0f11faa856ec07c527ac38bc79e9a03df26ca92a476ae1291145f534e699b37f"}, "b820238f-9868-44db-8ab2-4f1d7764aefc": {"doc_hash": "3fcb8488abc7e09f4108ae962a9288bc715d8b301def50f789162e1b5518b1ec"}, "e8646b04-b0c9-40ab-9626-fb4ac1efbce4": {"doc_hash": "656e5bc6dab042d9eea004fbce9641fce82b30307c8bd8a2a58e295ebfa983e0"}, "2512d278-3030-4650-b0cb-8e1bcf542f31": {"doc_hash": "90f529b77b1b7b48eb02720f780767e2476afc35671691318bf9ced229dad6bd"}, "c016a968-aad6-4f94-b4ce-d44df22c743a": {"doc_hash": "867a4d9ed35ac1a10128944a5533fd7807ebdae5d20c11fcf81f0dd977122f5d"}, "83d8c6bb-9ba7-462e-92ec-ead4307b3bcb": {"doc_hash": "3f40b4484c1fb5067066e164937581d82a43b65d7cd600a7633f5288f8cb3f51"}, "37752dae-6e4d-4fba-98da-5d18c7b373c2": {"doc_hash": "b6bf143b493d8071b655cdcdda1e173b7638721f54469c0e6c85a1a68df2f3ba"}, "f0346e8c-7911-4514-9234-1663b08b6311": {"doc_hash": "86cb27f8e366aa4a8e8011204fe2e758335c45b5cd74ba5312e874ff6f4782a3"}, "36f27c83-4011-435e-b9ac-bd3938e5837d": {"doc_hash": "78f81ec770fcae29279876cf96c9de3c4867b3dc8dc49cb65fdc1ebd3204eb0c"}, "494d1a79-cc74-4505-9308-0a491a6bb8b3": {"doc_hash": "9ef354907a36b9d5936b714d1eb983014eea14cbbe555fb95ac27e778a3edbf5"}, "e9f5f7c3-c675-4a3f-b33b-031873ea9dee": {"doc_hash": "1cfce42d8f43e03cd65968de6d246bf8b0b06241dec84cb4fedfd26d84106f2b"}, "24ae9e8e-a48a-4171-95c2-f354f172fba6": {"doc_hash": "14a818d60c8d8c465aed7dad74a35e9f45bdca4d2ab1079dfd0021a3914f654b"}, "65fad22e-7ffc-460e-8c4d-bb24313d80c0": {"doc_hash": "887dc552494d258c4d788fc9d5b37f91eb2967faf826effabdafe54d3426012f"}, "b2cfd4ee-de55-46fa-bf3c-fcdba6af709c": {"doc_hash": "7e3115a8f7fedf8f3d6657f81d139f3a1c20df740e3a2ec1e6c1db71e4624b41"}, "b2e1bb14-49eb-45f0-8667-04e87fc85a62": {"doc_hash": "e8880d4ca373dd97d842ef929b0cbe2f73fddc66aac26579748c55afc86c713c"}, "98c1edf5-6380-4f4f-8134-cff7519a4b8f": {"doc_hash": "5ad73669445e2d4c5e595a98fc80a5996e4e8b130c8814861af37f7c99727b80"}, "669c54db-c164-42d9-928b-2d495aa437fb": {"doc_hash": "36da18cc903fddba8391d7f343456a8cb0e927af6dfcc328f2cbe1b3a12f9926"}, "3f261dca-43a5-491d-8476-7ff12d1c63b6": {"doc_hash": "548427947862a1def6820a73ac98fd9fb1e1beb26ca4194df5b7256554c58fba"}, "10f96c88-23d0-448a-af3f-769d08394940": {"doc_hash": "b8820c5998beb826453c5f250632294b3432298d4f5008ec26baa99f060c63f3"}, "57ab765a-d484-4a1d-9aa7-65e553c5eeb3": {"doc_hash": "268a22a3c3a00eff307cff9d8ca7329372bf7570c7c96473b8cd7b3dd7b91e42"}, "8c7ddde4-4805-4527-8c8c-5c653cccca41": {"doc_hash": "90e6c5bd6d05e4c8048f134e9c90cf01d1ef9ca16ee6e356a55a215a14897fca"}, "05a6188a-f8fb-4187-9dfb-c234789f9564": {"doc_hash": "93eb63e4490a28d7ace7c20ac44c2c9bc1e431f89f30ba246a7e8521adb47bb0"}, "08def93a-0740-4196-8b6d-3559d0da9d33": {"doc_hash": "8d10ee5716d1bda8659ef54940e828aa7a8be8990c34bda10dcca31250a7f9ea"}, "0ae15f81-7174-4fe1-8c56-a7b4c93b62ab": {"doc_hash": "118f12678f6ab3895733ccd416b23027b36fe74a1db93ab3fa211dad2d39e3bf"}, "20572619-d5f2-4e6b-a1f0-0cc11ec723f8": {"doc_hash": "009f8959eb7c100870cf996071800293012bee026b62ab6ff14ff020f36fe77f"}, "54015bf9-880e-4cd0-93ac-1230cb28da6a": {"doc_hash": "f942b0884a6f06702d86f7a194ad55c84e7c1a8399a6efd86fee3614a90d6372"}, "d8915d10-a1ee-48c0-8cca-ed30acdaf313": {"doc_hash": "653da6c394cabf757255595f70df500565b216959b49d1e6fc0fbf02ffe152b5"}, "acddec9c-c458-4f07-a33e-8ea338cfbcd4": {"doc_hash": "b387ad90c61b20057d1b06e5f9524a7f11777d07fea6b19c751368efc68b0e30"}, "283339f0-9ebc-4c0e-b353-0aeb23550b71": {"doc_hash": "fdadd87e23b98b4e511ca4e3f8fcd7e4203d157a2621a3436e671b7f76f0709a"}, "d6cd164c-85e3-4923-949f-2bd6db4a671f": {"doc_hash": "9e61bbcd4663f7882ad9ea0ed85d920324a95b4ac2d5f8953e419074d39644b5"}, "19a07a60-813a-43f2-956a-e466ed8858d7": {"doc_hash": "8a898a77a1590f274e02582ce7d1c141ad7f6fe601b14de8c32cb66cbfa83ede"}, "de795d82-d754-4141-ae66-3b88ff19a6ce": {"doc_hash": "c690dd5a246bb21017fb7a604fc9b19b99f50787df4ecdcfad24f09190a2b7f0"}, "3e02a2b0-5f09-452a-be8d-7c639cdd6dd2": {"doc_hash": "06b4a22f2c1a04732a9918ef84a04983271230bff6010df46d63a24f76d8743e"}, "348691c7-171a-4395-a2d2-a0b9bd3685b7": {"doc_hash": "e00ca1414fd5cc75036416f115842dd360cfa72b9ee5e5248e137b4c4cb460ac"}, "769ae878-bd45-4a88-b877-e317e42087b9": {"doc_hash": "12acd473baef0236b445e7e5d0f65b07d1235249cc1a6e7fb11de27d97fe30b0"}, "86b4e1c4-9966-42e4-9878-2f7b4e96c0d8": {"doc_hash": "a29a34901c8207d052f2f5a6e3a7a379726f999aa77b02ff4c1cecdb4f2fe4a2"}, "1e37c8a0-d7b3-4149-bd29-98a638595110": {"doc_hash": "d21b5d4a4d26ff782e50bb4849b16ffb5c58325a20e36e5f055e3aac84275b76"}, "b5e04a0c-f2a5-4941-9ce8-96ee6694e915": {"doc_hash": "b4bb45109afcb67bc57b71d9f29cce7aaf34117bc8e54b7d425a36817d3b2c48"}, "5697515d-9a6e-4ad4-b720-f916cf9ebc9b": {"doc_hash": "4dc7027bc92a582cd0b493270c05318cfb08f930733b6a5f75177014d32fa95a"}, "7e619656-f68b-4ef8-8f07-4bda71a79a41": {"doc_hash": "7ecfe57161c147cc3a16fdf56a6348b45637f6abd739c67b210d316d6d823b0f"}, "0f34d5fa-b64d-480d-a677-3f8b015c5922": {"doc_hash": "4e79899cea3fa5993c104e6473ffc92cdbb62abb13c5bc300a7198526ea7e27d"}, "44a84d35-c13e-4ce0-baea-29dbc0944781": {"doc_hash": "46ed249b106f3fc5ea02677c40e19402bf420d353f5ee74de3ea551283bf8570"}, "f5ff7f61-8596-45b4-8c6c-16a7d43ce2c6": {"doc_hash": "d063dc2904cd01671a6de3137e288a1c8a4d8fb56e8cebf674bd61f0a9a32efb"}, "87b955d7-4288-4bdd-9df6-e51ef925cc0e": {"doc_hash": "ed6e32efb18a0d814bf1e2f1373eb7b6365bb33eff09009b4e5f557e2358f507"}, "2b97f61c-ee76-4d52-ac08-40afff2196cb": {"doc_hash": "b2d0ecbf59d283ebe5e9865725cdf6eaca62a2f43ee6085b8c27efa9dddcd18d"}, "0f2ad73d-88e7-4e3b-982e-bacdf3d1682c": {"doc_hash": "fddc624a510895d2c88e10c92d64d5833ca60323dc33a29c04ae48e214c64d85"}, "c68a7d62-dc0f-4bb0-afa9-ce7bf196307e": {"doc_hash": "13c01c35e857f03a139a7787c6475ba4fb63e4c83974be6eda0e44529d2a76b6"}, "e46784c0-9dee-4176-a042-1b1cc5afa453": {"doc_hash": "56456b7d4c18e430f8b9b9b306d61b926c4005761e49b3aff09388c0855321cd"}, "4337020f-f90f-48ee-8a3b-d0d401c4a4ed": {"doc_hash": "aa1cba43a72dcb3c0d03ae5ebb6d882834130fa6cabce5d05146b08d76f777fd"}, "c0f570d6-9e2a-4493-abd2-f0e4b74e3fa6": {"doc_hash": "05a860682164964413045e782274d46fbf01bf61f06f86aa8a8db9a1b25e0aca"}, "6182aab8-b995-44c1-90f3-6c8200b05ddd": {"doc_hash": "5054f02d12f400e1008935531db378be05d5c2c235226abc62cf57521dea2795"}, "2f2e7977-7e08-4f03-95bf-cf2711350739": {"doc_hash": "22a48123919f0c3461430a3a750e2e47a1243d0d62bcd4aa6f59a4d0352a6989"}, "1e82188b-f13a-443e-8a08-633a0f859b0c": {"doc_hash": "f7ff319879722e50a9ad105cb690857304999c72ea40919ad9793373ee932ba1"}, "651c8b2e-96e3-440a-ba36-7e88c9108d16": {"doc_hash": "fa4c1f28b9517214bc3ed9625cfd363d9474e1c62aa8922a383dee3d30d087d7"}, "7019f3a7-a74d-4a54-a068-853be1bdf7f5": {"doc_hash": "1d836e4c1e1d54eff2036ed19fbbab3574480ba093b8cf7bf24edc7b71b4d199"}, "a97ccd78-598f-47a7-8996-40ff72c0f6de": {"doc_hash": "b7d5d918920af36e19234bd9600c246666c82fd5c9a0a72a4507091e0e242e5f"}, "ddca0521-04fd-4472-aecf-03b85ecec732": {"doc_hash": "bc90ea0cfd357bb3ae1177c464192a62c7fc1c6b054f86f5ac1752a348423d22"}, "a543476b-70d5-4405-8102-903b60de501a": {"doc_hash": "249513ea3f5dc0003056a8a74845c73ac4a82bbf100867187ea576132c4e2902"}, "d81ebb34-568a-4ca6-87e1-a42a4fa68357": {"doc_hash": "45f95574570d63e69543d7dfbbd930643c69f01ce3917b8260d10994f879462e"}, "280d1ef5-8cb3-451e-abd6-731e72030028": {"doc_hash": "fc10a00fc441b370be0f2c89c8b80baa0257347936e691423f126cbe9c5ee089"}, "eb1d7f3e-5f5b-4470-a8a7-7f48b68b40d6": {"doc_hash": "747764d71ca04e23e2f55e395adac6af99b87673a702c6a1c047c3cb3d2ff68c"}, "a2fc76b1-5158-4836-8450-b224488c562a": {"doc_hash": "978dc2251de4b38c2dd26cdf8299d5ee321db1b639a590b83bf189f6efd1c33f"}, "d635c7d7-fe12-4655-b170-64a3be476383": {"doc_hash": "e5e137d37776be2f5d99137ec51c29bb6fb5bc7d993645c42b18f7939ae3c622"}, "d5d20651-adfb-4f5b-aa51-6e2e6485e9ac": {"doc_hash": "b449e9d0c32c23079d8c012f1d5b942f47b31506793a2560afcfecca65a67be4"}, "09aa6e63-55a4-4d58-80bb-5f31c5da631f": {"doc_hash": "a55f5d484f12efd65a80423e7d5fae57f74b1f26d9939eeba5b015530b69b095"}, "3bfccd3d-5e50-44b8-b589-68d25a9fcd7d": {"doc_hash": "c4b99b222909241a92e1a2b87ae4aaff9e1237429e4a28ffb77dda085d92bd2c"}, "7c1e82c4-7a78-41ad-a36a-6984f463e942": {"doc_hash": "f727d979de6b4527664501bd7e29ee4be0093a9e09a2d18757c47ba69c780b48"}, "344d98cb-3668-4b68-9244-5302150bf8a4": {"doc_hash": "f6110c66181c445861c84f606ffdc8e6f6c21be2a146522134419e7d8822d655"}, "1c8875e3-2734-4c9d-9ccb-d98e56ab3b88": {"doc_hash": "8a91a36524feb00fe10642b7a603d4f28f8b13598291c44d2553fbdc2974526a"}, "956b72c4-b94d-4bdb-a1f1-d751a76f4f27": {"doc_hash": "265af4a1efd3ffac5e6149fd02a89bd66ab6517a896638a0d386047610be33da"}, "8e86599d-7b92-4369-afb2-697c04aace39": {"doc_hash": "80a32a8d580144659c44249bcc94998b672a54afb8c7b8bb1fab35d5b8711faf"}, "1e65bf96-4437-4a91-8a4c-d595e883e6ec": {"doc_hash": "47fb7e5d94a3db20111b572443d31e1dfa7ff406af0ddff014b9c1bf38b2427f"}, "973a168c-2c19-492d-b3d6-780f1ed423fc": {"doc_hash": "574bd9dcd5b03b5cc6155403d44549ec1d2a8e9d7c9be3fd06c2942f385e3d3e"}, "4befef3e-ceed-43e4-8556-0b23b59c302d": {"doc_hash": "896a8f34736d8d4a9bb8622371fd404f2ab29cfa592bec9fd166ba92fa6a38ad"}, "5bf16086-ab9d-45f9-98d0-b62d20c20e1d": {"doc_hash": "cf7d9286accbf6b0265b1d90c594d5e8f9f9178aa889719dce4a881ca4a8e05f"}, "152dc485-c35f-4b67-8054-9485b33fbc31": {"doc_hash": "8ce7a055c0bee1e84cdc55eb0f4bdfeb0ab1b06e1f25ab8df5a557a9e5aaccda"}, "bc5dcddf-91a6-47b6-ace9-3bde0377f39a": {"doc_hash": "c92f0c6c7a63473751cc39534f85e68ee05fb79b3ea34aff99f03a7c7940d8da"}, "1866a56e-4726-4022-8cf1-dd68d0b5efe5": {"doc_hash": "5535d4d31c9e01e65a3134ec82198fc7e795399f287621d6f63826a5ec630bda"}, "81419660-eaa5-4cde-a282-a31e3e70cc4b": {"doc_hash": "6308583072760becc559b34cdf39617f2cd019d7f51531ba9dbb4f2d6aa9f2d6"}, "d0357042-73cb-4bd8-8114-fedd5c80d885": {"doc_hash": "a735bb0549c06832456fdba6092e6c433842ed43126b8bb95bb170421e3210af"}, "a4f9cfa2-8f00-4beb-9aee-9a43a342409b": {"doc_hash": "dd006272215604eca784fb5a2a800c965835dea202af77b02f93755f375400a5"}, "c855bfe5-9c80-49fc-9c96-8363897da3f8": {"doc_hash": "5153d01275b408ce1ae01b5715858a816a7b93785856b68378c8d139a7415cb4"}, "68550d54-4c97-4dfc-8216-52f790a932b7": {"doc_hash": "dc1e6b44a51934732e7bb3e632a498969beb8929966eaac707c2970213bfc49f"}, "8076c518-d4c8-459c-8000-a401e4dda760": {"doc_hash": "983da0f907b0417e89be30160b5aed91166007452017d6cf8e097ad7bfe51853"}, "9a4db0e2-2c1a-4748-a937-e53ad119f5f9": {"doc_hash": "1f66f6afbe91daba4123e7ee7b3c541f08bbec129abd292df5657168ffa2143c"}, "b80ea78d-1c10-4080-8683-a6c705f6c303": {"doc_hash": "2e8087bb502b7c102527e8b4c73c587210b8e580ce560891c5ee8d5c6d1ab349"}, "d2ae7594-0539-46a9-9879-5197d24318f6": {"doc_hash": "307682ce44ee493be8ac150b4b1d1c88223dea59ea2394692b3181fecebfdcfc"}, "683e1873-ea0b-4047-99c4-e736cbfb4df7": {"doc_hash": "6939d040ebfb1d232e98748c82b7cc75e58e110618800c4f70548debe9f2a766"}, "c40cfe6c-1291-4d7f-8568-1c924878b41b": {"doc_hash": "958cc3c5c9089453b1623c3f8686c6a1460a2897956d67b5a118f64f4d8db575"}, "9a5c1e87-f0c1-4b5a-92d2-0a9044c708f5": {"doc_hash": "6419e8339c35596b4ab430f60f2d6737593d84a6f8cbc1af37c8907b4ec334c9"}, "2185e75d-63fe-450f-b510-55fb11f1b243": {"doc_hash": "21757b177acb9fcf45bab87e0be5088f8d95fab91836146cae977a5668c98fa1"}, "663b50ef-319d-461a-92e1-ea6fa26adb12": {"doc_hash": "9570a3ec9efc5ea9c167d27d74f68f51df1f7a52586da6bbb1c27f6dd4c6fa14"}, "7b66b086-3b1b-4414-8ec7-c09b3506cecd": {"doc_hash": "b73747f18d173b1840ed0412609b18fd4eae5c67700f9a88df9fe3b1f6582af5"}, "b9a31115-dc09-4e1a-8519-c6a13be8e4ec": {"doc_hash": "f7d4b1a5594e2d50cb13befa4c6421faa7e0b2fc75bae92d1d4212e7bb2c6753"}, "efffe59f-d078-4e64-b174-dac797b53c5e": {"doc_hash": "f6f1ccca73fe95d71d31b80d5142d2980f8bb354b97f74e91b46d377a7d53ac5"}, "88ad0fd3-b84a-41aa-8528-3a85c17356fd": {"doc_hash": "ce5dbe96f60c248238843b75db18a70043fc1d448f10e2001ee00b5d4ea6ae45"}, "1d1ce352-2bf2-4df7-a62f-7cd32edac32a": {"doc_hash": "45101ab750531c438f119f8836752b7a13eebd6888dc30f8c522f45bdfe9980d"}, "d77a8f98-01c6-4092-9d4e-689fe0317269": {"doc_hash": "4d10b28859e4aea7e24136326b80e08bd02001b251bc044284a558736df9d48d"}, "4634ecb0-a0c3-43dc-8858-e31e6b1bd7d3": {"doc_hash": "de62cb2626e4778c306c67bd2f3c686c4e9e61d999df9c9bdd85062f49d62030"}, "eca65b99-5757-4ccd-88bc-c88c827b6260": {"doc_hash": "b7d141cba0e9a39e77a78372d6a4e2e2aeef36b35fcf6b77ec00618b3f6ee6b3"}, "3370b1ac-4fe6-40b2-a719-2fc3adf37b17": {"doc_hash": "454c59db16d44a0430eb954660c94f77877f0c2d7452b252f0fe9dd4ed17dac7"}, "cca66543-14d9-43cf-a912-787d24d8b983": {"doc_hash": "d6f1220fee52555e45a50243d66ce1691c881689e80f9bb37b81c429392e2138"}, "37c93f88-d09b-48a5-82cd-2ee15bbe4919": {"doc_hash": "2fc7d83f1bb2bd45d1eb297b97cd27ec1afaf3d9523f20a5ca481a7b65f71824"}, "932e586a-ea57-4faa-9b32-e98017647917": {"doc_hash": "59fd59a149ad23c2c58b05b9a7956c1e84d4b22829dbaf16199ef38a15803898"}, "9196b97d-32dd-4675-b2cb-5bd7ee365e84": {"doc_hash": "aa3bcd9b208032f1d9271420305bdbda14f9e9ff96ab6c87ea68f41b3e10264b"}, "697667af-5a60-4eb9-9ec5-833d915af0ca": {"doc_hash": "88f3eef98b97a67a454f7bbac664ebb2aed8ec591cfe90740595599178736386"}, "05033585-b194-4dd8-a521-25ad5f082f25": {"doc_hash": "a484e16c04ff2c849d58a2b6ff56790a9897715bf9716d05f1088b29ed5fa9b4"}, "2d23763b-35ba-49be-bc24-4131a38ad76f": {"doc_hash": "98c4c35ae0a42d533306ef9e469c5349595c6cebe666881acf06c2d00fad0b9b"}, "6b2fd142-77ba-4699-a5da-c500089fb5fd": {"doc_hash": "5193e3d5361b042fb2b96f54a8702bf358d32ac084464f4715ab5267a875a4b1"}, "2ad50fd9-2ab9-4cfc-a072-841d182ec51a": {"doc_hash": "caed1e003319408659aa0a3adc94f3d45d20044cb2c0a5dfa10df73616d3c637"}, "0e73c68b-c872-4f49-bea7-b611cf550d66": {"doc_hash": "6739b3afa76443ebb6fffdf79172011f382f76b011066ce11621b87cdd5385a9"}, "30dcfeb3-14cc-4762-aece-246f70900bc5": {"doc_hash": "6ab5dfd336899a1311b9b46057ad281bc1b7793a70df2b25c9d9a4d571b012b1"}, "3e05bfa8-c92a-4deb-bc29-41b02384c11c": {"doc_hash": "5840755703e9910691553a5fa830533abdd8a7b9bf27d151007123cc12d36c8f"}, "84a672db-38a0-492c-b1a5-311657511aa0": {"doc_hash": "e9101c0a709086e73fbbe69a5be8c02eae49c31216cd456c3485d12190cb09db"}, "1ed1b7d0-1dbc-4a38-b0f9-cdb2a04038e5": {"doc_hash": "fffb04d3536eb6a7979be6c36420fa08802f472a95f10e3829c7de417a1d3f6e"}, "36f7aede-a271-408c-b941-5d244187cec6": {"doc_hash": "239097d4af41f276ad149fec8b0813f829eb37966aeecfbd67978221427ffd1c"}, "fba8b88e-f446-4304-8d71-330148a57061": {"doc_hash": "6b529a9e62c6e18b3bd9ebcf18d0999162fabfa96dcdbb0bf5651b16705f6982"}, "e5e9cf33-f963-4e5b-be81-902b7d729e3e": {"doc_hash": "2ad983ebe595a2de3c945099b1d7ee75254dfb269280abf585a4ffd5d784c831"}, "7e9e1df1-be84-4958-9b3f-dfda49f6687f": {"doc_hash": "aa4d904b118b9537537385e65ef0e8252376b4df9733c1350958570a09737116"}, "d2b381fa-9355-4137-9a68-23eb1092ae4a": {"doc_hash": "c6f7124827b6fb8ac014c4cbcf154c5d89fcb8bae27e822a415fdcebad5f7b81"}, "36fe691f-205b-4047-9ce0-6ccc5212d21a": {"doc_hash": "bb4894175ead028a096a3ef2933dcb153cd92dbafeae2a314fe0e5ddf7d3b66c"}, "b3938c7b-3acf-4dfd-8488-82cea76a1fb4": {"doc_hash": "ce095c5259a172ca50d15badea6ed3f78fa370e9cf1d61587a6dc61866fa98b1"}, "498cf7b6-cd23-463d-bbee-b2c842c7a2a3": {"doc_hash": "505e0c4142076c1bfe98c0a7fba70fa665eaa017341b628dc87c651963ded0bb"}, "7321359f-4efa-4f13-b8e3-b8e7c7312892": {"doc_hash": "2c279ec6270ecbb0c78c593325d4980b32f043d0ee420ad0ca68d6017cc4892a"}, "58cb4199-06c6-4b17-8e1b-3218710060e8": {"doc_hash": "1ec2edcf6003577eace71e7c71deae866a993eede580ce27f66ee57edb1cccc1"}, "63d0467a-f106-43bf-b1b8-da5ead3d9d2b": {"doc_hash": "f9a01ac9bb5838568f3f73e9685f35ffaebcb54cb62ddafc13f585790d393550"}, "f4451f81-d9da-4015-8cfa-79ddcaf0c12a": {"doc_hash": "f25f75bf8c2a22d85c065698979e7c408e965b23f76fb9bd458a6dd7352daae6"}, "1009a73d-8a24-41b2-8483-dbdb402b1fd6": {"doc_hash": "f4c756d4c19248b915ac7f47a5abd87022fb2942360105b5958cdb3edf9cd128"}, "7ee1ca01-f1d4-470c-8c43-bb37ec4ba637": {"doc_hash": "a4484b70049fab1a8b9665d24704dc0bb9f3b7f065133fcf80accacdd70ee436"}, "93c62cb1-d685-4125-adf6-d5fc2dd88ba5": {"doc_hash": "b813ab9052fe0168e19fedecafe6af5c780a81d0a24c68928190e657f7c9283f"}, "d11a3b4e-4006-4167-a098-ae288a1e5324": {"doc_hash": "4ed372e602a5fbd54ddd65ffe730ef13f5cb1eed053d96b750258ba7bd032720"}, "f6091ce1-e72f-4bd1-a09d-4707988d049d": {"doc_hash": "2798953ad4cf2396df001a2e4e355042e26ddbad10232338b80eba8fa43ffa05"}, "2a50a332-8620-4e66-8502-287080f4d095": {"doc_hash": "52a644799077fd1aaa3137355d3b7ddfcb46988f79c84b8d0f45908448c2e66e"}, "a8d3e2bb-4d57-46c9-95cd-5faa902912b4": {"doc_hash": "42eccd0342390b7ea41705e9319d08ae73aa3bf90064024b703e6707fbb0ad60"}, "9894277f-383d-49ae-a636-5262a65b1d47": {"doc_hash": "1a9a5ce5143e3970d095806f9b93c55511a6470f0bbc959be1a7635c1f8581c8"}, "91b6425b-bf38-400e-a2a2-d299fa21384d": {"doc_hash": "ee5b1818eb7b5f091dbc12f1eeb617c440d476df21a707396dcd04620ae15e07"}, "2f8a1e58-a6cf-4c3f-8a97-93182960a306": {"doc_hash": "fa976cc34669ed01c9428fd0389742b7fb336f1b42c14eb97db0c5e67e94ddc4"}, "b3bbf62d-35ef-41ff-b428-ff566db5cf5d": {"doc_hash": "9e4aee6944c5c728c0c688ddb3a2d36853a1d5a1788f6447530d0a142e17d7be"}, "d8bbda8b-6ab4-4cc1-9229-6c71788eb0f6": {"doc_hash": "d204dedcb02225966657d546c860fbc513e7aebcbf5921e396c35665038b4453"}, "bd5c140b-fecc-401a-b29a-32c1adbaa36d": {"doc_hash": "2e75fe58fa575830f8d6fb279b80204b5b9627c32405e9d9bcd9e4e24a607ae6"}, "76ddaffa-ecee-4bad-a52a-d54a1c8d87c1": {"doc_hash": "a1a852841a8c4bdc3dca7cd2697783fc6097f206168f6492d4be3e5a5d3aae07"}, "ec2135cb-9306-4b52-bdb4-63450dc6a4bf": {"doc_hash": "322b9889c6b13fee510896a386213dbf4be33c4a4f8c5db9f4e4d47605f20c77"}, "c861cb9d-72f6-4c6c-9aee-9480abb6151a": {"doc_hash": "48f18071e144eed28222c95565ca9fe6ce6091626c612893e68947cdc463876a"}, "273e2821-351c-40ae-9c16-900fdaede65e": {"doc_hash": "38ac17c94522a551211405db69bb1ba4ce49d5f0d98a7c5b7e0b96a3e58de1f7"}, "9680772c-abe7-4643-af32-3fd6362a69a2": {"doc_hash": "c3aa148e4cebee3b864ef917d73cb72c68905921e0d92c5dd4feebd5f399a0fb"}, "d56537dc-afbf-4ee7-9f36-5e08c8fc5286": {"doc_hash": "95638f381850f9500a698466cf8e7c1e5f821ae9f3bea54c31b1531382ee7a75"}, "435c5adf-f14b-4447-9255-552d8181d7e0": {"doc_hash": "0e079ac7f9e6c7fc95033fb63755f9e6b5f516d957fc0d8e889abc6870b82edc"}, "562d087a-40e1-42f9-8e7c-d317123cebc9": {"doc_hash": "69031a3bad7086d5c4682a02cb5c89c613053f8561da056a151468b7a1d94ff1"}, "90506a4f-15be-48d3-9194-8d35024120a9": {"doc_hash": "01c81fdbd84cfd74621f0c61cad6d29596a9cde59a209f1da95af04069a692d1"}, "a3c5c037-2bac-47ed-b9cb-f4fc212d840d": {"doc_hash": "c4ed3c548269f7e52bf53919d6e8984db4f7686e20a0e54f2b13a7963a1178f1"}, "dc9781bd-bc47-4042-892a-8db01033f748": {"doc_hash": "f6268b74848185bc204ce8dbaece3e95e427968c9a753d5c2348d65e1e03fb39"}, "b5deaaca-1c55-46f5-a1c6-d481774fc841": {"doc_hash": "f60438e6b2a8ceb88f4fa94c9c9c3b6821d49df268ca9a2ddd7157bb1f7afe26"}, "b36931ea-65f1-4711-a5cc-a378faefa500": {"doc_hash": "f309ac5083945081ca7880ad8dc4bbd097ec6dc7a2ffae33482a853f8fbf5924"}, "199e26da-eb33-4fac-97e6-d1803764d2f1": {"doc_hash": "83f6797eda027060ea27b10414b420557c3aff9fa850dddd2ef0811e47ee31f1"}, "fe8e008d-ea58-480b-a286-0cdb28d9a604": {"doc_hash": "5c982e9cda0bfec625938d95231bf3397a93c9f8535147371642c821e6dcb44f"}, "a9b94e00-a888-413c-b8f4-3420afa80c5b": {"doc_hash": "b5bd926006ddddebd7112b1652b39fc9fdeee43eef5fb475085522f999ed8cb3"}, "a1870a83-c7fa-4092-9c4e-a9889f501047": {"doc_hash": "2d373464067f768814ed33430d3b1aa9ddf4f177f9e44aea6e6bc93f3f84a546"}, "dc0ad0ec-e5d6-42fc-984e-6d5691607475": {"doc_hash": "3c36892765b74c14db53c6793e0b74a3b43297b3fc11a4469e0c1ed6b2354b58"}, "febe3ae6-916a-4c37-943a-b9c05240c5b7": {"doc_hash": "a90a791bb0bd33b2281ed038dea6e432af4b2f17758ac2deafc96e1e7a583781"}, "7a136fce-67ff-4ddb-b5bf-7a7377ec75ff": {"doc_hash": "c1632ded1d34ce9e6958736029df104442cd7fb8e1f3d7a66fba490a62d48921"}, "ffb4c078-72e6-408c-ae24-76e5cdd3fd78": {"doc_hash": "8a9c3f5e59def47b94bc86871c846a6581a4d69dde8ddce33f158ceb8146b050"}, "7b67d5dc-89de-4fd2-8ecc-710be02c9855": {"doc_hash": "de85b4e10db5e451350e7a0b9c6054d3b945031ec5db561af9e8d57f2024624f"}, "eee46f57-c693-448f-9e7c-adb9abb485dd": {"doc_hash": "c85debf9d6592b52f4675749be6a01203dd5846c352bd801d2350c2ad37f845b"}, "43044aa5-2546-47c5-8e1e-f1aeb73e1cfc": {"doc_hash": "738d3e51dc3c188e7cc16e06f22c295c7254c9d1ecbce5fa5b1111614e670503"}, "1d86a1ee-65eb-45b8-afc3-9e57273f4fe8": {"doc_hash": "50d1a78c91dedd7362f2c810c7e59584e3c950cba8ee84dedf483f4e3ff7513a"}, "5d02052a-34ea-4819-891d-11a2312c1828": {"doc_hash": "2267ee2c6d2dda3c5234555eef173f2e716155501b28dcab5d0e6c6771071ef9"}, "5917bdd7-9ec3-4f32-8bcf-9ff7cc3e279d": {"doc_hash": "b2f4823684aa2e0a533e2a83151cff59486196196f94c9c5949ae44c349397fa"}, "3196a3f9-f559-4e86-b44c-228d81110463": {"doc_hash": "bed63a7ab7f23b87c6d241d3c8e3ccd450bf535fbc55f438ccd38957f0211865"}, "02f2c303-ad6b-47e5-9134-4c25a6e5464f": {"doc_hash": "67011fe05aa3025ec432bd6c09063607f859a66eec7916961fbd1e33bc9dd0ed"}, "60249dc1-fc98-4ebf-9d18-1f7d43673985": {"doc_hash": "398aeaabbc5d5dc3ee6430338a5feb422319bb78a07c37414ff05ecefcea973c"}, "5aa575d7-f034-40fc-b742-8a0b1b04e100": {"doc_hash": "8cf480a937d5f98b5cae73983af4c8b309a2899db401914d33c2bf11f01a7193"}, "d3aea701-7ec6-496a-b362-fa9fe9b299c3": {"doc_hash": "6dd73c1b1ec2dde0f567a72b6aee1fc9fc75a17f1139e9d9603cb6c9670cdc2b"}, "2d15f9ed-7204-4e0b-a1e6-2099182190d9": {"doc_hash": "a5aaa9f32756d06a0b86501ba1a6ed3103477f802b5ae7b01c3f95696f21d6be"}, "e204b58e-60cb-4908-b049-2c171e8bab72": {"doc_hash": "35a62510404d8350d0b5300fe75daf086c2c319af429ad9f592ea28a805177ca"}, "39767329-45db-4aaf-95f0-a6f2285695dd": {"doc_hash": "fc6a9fc9d8bfa261cbcfdcb8204eb885b0c86b2c69814b0edbc065a368206904"}, "31c20893-d85e-4963-a8f5-e5285b504299": {"doc_hash": "e4daa5dadc1b538f0be9e78039427c6420d420514cf3c8db41281761ff9c4b35"}, "6103f60c-7fbf-4be5-94bb-e32239114463": {"doc_hash": "0b861810d232561c4ee44f0d7f3d357a8620668d57306d2911bc1d93b5e4a146"}, "7e94187b-c817-4cd4-8acc-b437afb94869": {"doc_hash": "afd82d0bbe927396f8111e0da7f74b5ac4b60f762a51d792a8c87fb2f47286b9"}, "6b04f224-ff87-4171-b25f-b8556681d8b6": {"doc_hash": "66a33272493ac0894cfb84da522d3701dd9edc7aae9f2f169d6a7215e464be54"}, "265717ab-2465-4c4a-82a8-d340e63a3411": {"doc_hash": "dcb97e5d948ceb97afa2bf9dc9e0b32edea7b1c309579f149f1d54da0fadef7c"}, "326fa34b-ec7e-4744-ab1d-a0b04c7e2cfe": {"doc_hash": "988b38ae7713fbea7a44308eb40616db9e3dede5314e6f07fe30509f84ee6cf3"}, "9cb4f445-3389-4d7b-9fb0-b9f35e77b145": {"doc_hash": "784a52ca924e5e597acbfe19a321433bd19e8304f0cb8fc952ee2bc4250fcbf3"}, "8d976c50-a93f-4676-97d2-cf4d09bc4f77": {"doc_hash": "1d00b66c8232e96b8167736bbc8c1cca8fbe532cc9a7ddeabdd088c5d0a5b418"}, "f93b4a25-2d76-4d92-9baa-a8c705f8c976": {"doc_hash": "1a98d07e3d780863550504fe120c38bf32e094b4a656c1ab4b438d390b48f327"}, "75f8bf61-daeb-4328-9617-0e75e519b03d": {"doc_hash": "cbb4b75b24024d07b84015d5f8fc50a47475adcc9f46484b3a1c6e740245fea2"}, "4f528974-17ce-4df6-b72f-b402932b594a": {"doc_hash": "9ec8da1c5a6df162b7ef33ad532eb913144ac6b2456b7ac9a41a1d347778d55e"}, "e35e1b96-8216-480e-89d5-8dad2ecd1959": {"doc_hash": "a55dbadb0fe635f3fa2aa8e9660f604f07b01d840654d5a29970ffd64d1a551c"}, "0218830a-93cd-4ddf-9b70-1531207acffb": {"doc_hash": "84cb567851c0e186f97864b4c6dd81a5093bda9c507f9d4011bbc4eb0eb2a514"}, "3bd46432-3105-408b-b5fb-1df51633d112": {"doc_hash": "bc235c117790e36db04cf44a4a5a9ec26a575712258c48197321113c8b07a9c9"}, "c65ed8ec-dcef-46ad-81ae-e3bfab4e096a": {"doc_hash": "bca2c00e2180efd4733a3849d02b17374ba1cad3484fb6f168d673cb006dd4fc"}, "fcc414ee-c97c-44d1-ac15-83403f0816e0": {"doc_hash": "87c8c96df4a614957799b365ec7d49459766217d2819956ffa1b05f3f377cb72"}, "699d186a-2014-4281-8fa5-d46c478de3b4": {"doc_hash": "d1bf4d69a6434522892cf962ca996629ffff4c39a743d33b21d27121a157d886"}, "15f2eb40-eab0-44b4-b3d1-9ca18e860200": {"doc_hash": "e42b051e611e3a13b36f7f998345b45b8d3694fb55757f6aa96419726bece4ab"}, "b59756c7-e125-4dc5-8310-7740c56d2132": {"doc_hash": "8416bc8bcdbd929eca9b1822a9b00969c0637627ac15bd5a5c97fb91d9fc2b1d"}, "99b8fbff-ceaf-400a-8790-f450481a81c5": {"doc_hash": "043615257b281b133328d769f8615097f8be5b99e4a5f44b93c75f81eeb8a675"}, "52a8bac8-ba8e-43cd-9994-6757ed38ca84": {"doc_hash": "8a1dcb5a59c866b76b5a320dc22349cae908c986b5c28ea56aae1b804af15c48"}, "46e36a31-3fa3-42e8-8c36-6858c5d24a05": {"doc_hash": "5281291de5b17135203401069be5e9017b2c39ed290c288fdc8aea8330f6c763"}, "5b5f51d4-1b43-4a4c-a9ab-6b7694ddacf8": {"doc_hash": "5a96a67f933fb313bd7b9b7938f641093ea915052a309b7d1b287158546d1bfc"}, "84a8ec6f-e69a-4b5d-9355-ce958d5d9aff": {"doc_hash": "d43e5145013c173bfc2f8806ca33243f3ed9ef546bb6ff365a1009ba337ba23c"}, "6ce1cfde-5fa9-46ef-a905-d1d867cac1c4": {"doc_hash": "e7906ed097e3e561e09242bbf4cf873e412408742642c8e7a7f14b9cb18276f2"}, "7fe67618-9175-4136-a06f-776277c64206": {"doc_hash": "6e403b2bf328e3125e91b73048f0226a43380da856b6ff6344b426c33c5a94c9"}, "002bd546-8fb4-433c-9df6-152aa8a5ff19": {"doc_hash": "f0a0cc1b35fcdc41bd4d9f7458e9f556b123c2d95818ff16d3a9b62de24b357a"}, "c463ac14-cb93-49be-9759-3ca4874c7d4a": {"doc_hash": "5de27f240c7fa5607bd9e1ba9cc875f6670c97d713df769b79328af714fca44f"}, "a0169162-c9aa-4f25-a0d8-aa7f91f9aa5d": {"doc_hash": "02b50368ad0e6f073c4a67cc4c931a8abaf840a8fcba944879638cd3fca709f1"}, "9060b0b9-b204-4e80-a786-77048b462699": {"doc_hash": "4be976f7e3cabe596305a3988780f1948622282a5e001388799ae833fcb49f38"}, "4d06aa7c-a9f3-434c-9426-ac9b12d5e2d2": {"doc_hash": "c67fe010fe548937fd7c1b480c2c958d7bdaf7383f5be2449a2acd934ada6f84"}, "bb7f030c-667a-4363-b213-aaee39cf1873": {"doc_hash": "fa0c9295a2397d8252e15027a8adc584e9b6e0edb3587914b3157f46711eab8a"}, "ae5806c0-979f-448e-a6ed-7e428fcd265a": {"doc_hash": "1cce04b5e2b84cfcd5cce70ff692544f798f66b57c55c73b2b54e3ab241dd923"}, "42659385-bfb6-4873-b34e-7525fa958db3": {"doc_hash": "3b54a70c6a6c1813b4d777824bb2772620a47e4befb7d505cd0bbd3b2734681a"}, "0732527c-dd95-441a-b4fe-24ba2ec7b5df": {"doc_hash": "d2d9231449aaaaf1ff346efa597ac69a308775da424d4fadf45e4de81a444c99"}, "0c5b937c-d14e-4a49-b616-b9550e7661e7": {"doc_hash": "6ef70ca5c4ae7ef922dcb41ad23cbb9da090fed543e9079b21900f1f29ab77fe"}, "1fc2926b-98cc-4241-b175-fa310ee487b1": {"doc_hash": "1652eeb4c7d47fdfb02ff268e14570e0a2729024dd460d62a41ac3b83bdfe96c"}, "b560c667-42e6-4dae-a4a0-68ad5f748dc1": {"doc_hash": "510e738ca4e22adef8474678710f19c56673161c1e996968005ff074d262c9dc"}, "6ad8d613-51a5-4b6b-a904-ba28c148e829": {"doc_hash": "cebf8732aec612bf59dec7ad77020770ae953aac121f2a140223fef56c46bfdd"}, "51c0b513-82a8-4b17-963d-cbe03130de46": {"doc_hash": "ed6de85042ae1f43566b1e7e5ca86eb035d0925b151442358cd4da4542711845"}, "9dd8f58b-5aee-4377-9be3-8a887b277c9c": {"doc_hash": "c236570dd559cbb20b77e193538be9dada39b52be42ca54261d398c1d6f41e0e"}, "e00e6f10-9ba3-41ec-9218-fe0353c7302d": {"doc_hash": "0c56f7d7506fe39cb9ca69caa22e450f0deabf962f80ad5f2dc0d729f11fc6bf"}, "9a38bf55-d1f8-43db-9135-b8a5eb7234bd": {"doc_hash": "df49fd6317efa06e64324b6b93a308af0c08ccf364cbe155b372d2a4393730a2"}, "3a14a873-0aa6-4f64-93e7-700cabe07a43": {"doc_hash": "08bc28ec9c289befcc4057d097c9d0822b915147c346a1adb0ca46a8697ed898"}, "e21103d5-3542-4241-84a8-f90830408018": {"doc_hash": "134fc50080ec7957ee4b28427bcdcea08cf6897129b2fbfa9b31508caa542927"}, "c904c305-b9ff-43e2-bf65-aa00c4b32d47": {"doc_hash": "791e7106d24123bc217edaa2457dfa8b000dfe48c8f34bbd7d0e95a31c3be6f0"}, "268728d4-1915-441c-9749-728825864547": {"doc_hash": "012e60cce309414160e45e2c604d03e055aba01bf48b401d1e41ecbba7c33fef"}, "b94964ad-a356-472c-a3b6-6513e5cd1399": {"doc_hash": "f43fe5cb5cd3f494ec08e99db76f4d4b01208accfa00943decee213aee6c313a"}, "1659456d-6c48-4485-b66d-29a8e888b5e3": {"doc_hash": "18ee3033c703bc5a6ba2cc97c6eaf7fdc6d9439293da061bb2b474d3d13f2f0b"}, "ecfee4e3-1bc3-4036-998d-8d6ac11dc233": {"doc_hash": "5ee639e5229821c90c861e0b7d4895d7cf406526aee1504c0df54e34cf45d1ca"}, "d8279567-0178-450d-99fe-7663cc70a647": {"doc_hash": "9ac6b5198f8bfbb920e42c8f514ecdfd21b9575069becedd64ec99cc1c193a83"}, "b9b8f471-9334-454e-b476-81ba82380962": {"doc_hash": "b2f732cb9fe0867ca91117332a489031fa0f25863be957c7d836ead8d60debae"}, "8bd1feeb-d8b9-4dba-b07e-daaa351a87f6": {"doc_hash": "604f200b2b6c44180782e372253d4cbee10a35c6881e10a26c0011e4326732c2"}, "9a287a74-41ae-414d-9f0e-230784312d87": {"doc_hash": "cb453f4cc35b7268fc2ccc4508a1d0000226806e67b6ba0dfb8e0842afdf7cc4"}, "898ab30b-be46-499e-9e5e-e82f786c2943": {"doc_hash": "2087ea653675657d52cdc6f08349f1a646fcec97fbb848b1d0b05b87d3630f9c"}, "39696ee9-ce77-4e48-8b45-51c188dfc290": {"doc_hash": "5506dfdb9c86e28cc8f33738364c91f23245838df9f5f94822aef11c7d5216ab"}, "0aaf3074-b4ab-415d-9eee-e021be762ddc": {"doc_hash": "747bc23a89c3c98e75b9a3caa8162610fe4dceb8c21ab24cf5304190dca7ca90"}, "314e1552-438a-482f-9f47-bc3b6c06c590": {"doc_hash": "e965f362c3d26ddbdf46db39d8f8341b63d6868935216689f50a7be9ef6f923d"}, "93283ef8-2219-4af8-b8bb-e01ec47b8324": {"doc_hash": "68cd474e4d95de4939a50f777fc8ca9a9a773272f212748f95a56b95f18da1e0"}, "83647637-a00d-407e-9dee-4e415aba5bfc": {"doc_hash": "65d32f9a109f45a41a64104c07bfcf3dd02589199d0b926b0ff5f3adb6acdbea"}, "5eb4f8f5-22b1-49ca-bcc3-a3d4d93df0d0": {"doc_hash": "a09065fc1267e9ede3cf75de73544bdc93c4e1adb2725fb854f7aeb48081c183"}, "0546b4e7-9792-4b2f-bfd7-3580bddab0dc": {"doc_hash": "2bb20ed8075d1860ac02b35117630deb5178484b93350bf31d01e103cba27293"}, "6fa991ff-1293-486d-8f1a-7cde0bee63a9": {"doc_hash": "3481e1f4887c1b72b6682d44f69ff60140a1aa1e3078b9993a16dd895ba281d6"}, "56eca053-5fb4-40e5-a33a-2e8a956f63e4": {"doc_hash": "a4e394088e3d93bfae19636a914fdec49c400d53cd7b61e770ade965e3063c1c"}, "dfd9824b-4139-4220-a085-6e79fadeb1b0": {"doc_hash": "4cd0a8111bd127d91c73a7d09121e71ee72aa76ceaa5d98db9115b020ddd019f"}, "1cd8ec23-45b6-4031-a37b-b6008799a3e7": {"doc_hash": "a84622ed273e205a904ac11d324e4e210d24039cae5aa154357f45aed85779ea"}, "95b41ca9-87bf-4121-b8be-c939626b7ee6": {"doc_hash": "edd4b435d529b7ecbfbf7274d4263b453f24c72b114890ee041692648499dbe8"}, "32ca113b-746c-403d-bee1-5c64f59a2049": {"doc_hash": "e3a7d1e5db424ee605bcd7a585810edbbbbeaa7e1ab054713dc15dd71f845846"}, "1cab42b5-27ed-4a89-843a-9d2a65abacc3": {"doc_hash": "c28fc4f3bdbef3d6bf1a9a9e0dc3f989a0f489b60191468926975e2cf0b8a56d"}, "2e90e617-26c7-427d-ab7f-6840dbd88823": {"doc_hash": "8f4c874dde3fde8c19369a4102dfc70ab6d082e7699baba6cd9858f0cf1c1843"}, "e2d6d3ca-a1db-4c39-a81d-6bdb49cb8006": {"doc_hash": "bb5c921902f6de688380281124814fe58ac23bafa56afdee181a34cf5b61c985"}, "7db4c44f-e699-4b87-a87d-a71b1282bfd3": {"doc_hash": "078b83bc0bb7c0a26942ee294b3d6f3737b7800e9f919f6ffb9e0cd2a62bb06f"}, "fe71d34e-096f-44f8-bd51-d16c6d9f942b": {"doc_hash": "122b9eae2e74cf614df240e46b32791892d2c7c38a1d0ff6379db1ab2bf9767e"}, "bcaeee74-de9b-48d3-b6a5-0c39dfd2b997": {"doc_hash": "85e2fede133cda1ee04f38ffa94acf56e1db6aad5c1fffe34805e7ad30ee8832"}, "ee6f8217-0f1b-4a54-b1ce-25d66344683d": {"doc_hash": "47aab9bbe3fd0df98fcdc4c8ace4e59ecd31d7f88e4df105a9cee78b6c1033a1"}, "8845a7ca-e9f0-4e95-87c5-f97e8f16352c": {"doc_hash": "4cd0f15a684c658f1363fb44cd0d67a0da73cb807d6f274dceac184b56bb2d83"}, "5722fd4b-957c-4761-9d5d-b6796708cdb4": {"doc_hash": "d7552e151e6130bb5514cba0c31a37f760c19ef8324f2c4212815410eb0561ec"}, "8d687fc6-126f-49fa-bc37-2069d9cf8125": {"doc_hash": "b1d0e296043fc9b010bd2d634a16f0f5ebb35a767088259359f0252b0db4b646"}, "97f812bd-d27a-46d1-9106-dda1f768a5a9": {"doc_hash": "deafd50fc380a4770f183160060fcf909ae997dba09e9bc6add32e4c42bde2fe"}, "daa48086-e858-46db-9e9c-2b1318fb72ea": {"doc_hash": "c4925cc51a6ec9bc68f5bfbd303466088c3da1251f5db84ebf0396cb01ccdf27"}, "c4d7bfbe-4e35-4d2c-9bf2-3153f0973629": {"doc_hash": "83717f2e211af9e66b96471e28e8a5a84592e1492ec36f62b7499f66a8c5184f"}, "7c9955c0-b712-4e08-8630-56445c97aa45": {"doc_hash": "0310f58f08bb50fe004344f589d03eb920d58e77b542ce88600e9225fc53636c"}, "a1dfc68b-6bd4-4957-83c0-f6f4a6cfe3c3": {"doc_hash": "8db0fce9f41fde385f986145f34918530214240f52125b03578e6e8f7750e44e"}, "75c67807-ec76-42e0-8ed1-54faea0f7543": {"doc_hash": "185ca379f52f29eb054b4c5a22c69b90cc2040ef25a59a2ecebcaa718e939d95"}, "78afd476-d315-42fe-9d5d-8d6acd3df7cb": {"doc_hash": "8e2cffc8f910751ebcc643eb656285c614bf576dbaaebd0edf13c5de3cea3f01"}, "af980508-4ddc-4e09-abe7-31c2d2f42d6c": {"doc_hash": "4f125ba806b582f0125cf659ae8c2f1c69ec7477957df771f65369de4fbd518e"}, "0788d96a-90d1-40e3-9550-8e7ee829a559": {"doc_hash": "fe446228c93c6e172676fae99ceaf89af16b734ec87375df605e747ad25cfa94"}, "da33cebe-16b2-4be5-bdc3-515357f7ff68": {"doc_hash": "5c5a03673c99ad69a54c23fd4841ef61262641304c0156e5522d8f6e964d23b8"}, "85eae4b5-95a7-4951-afe4-2fdf2e96ff44": {"doc_hash": "5d35d44ba52d4303a455d08958a14d23bd0aec35dc04e6350bd260fcb6351d90"}, "eb03315b-7300-4936-8769-2f90151fc43d": {"doc_hash": "779f9f5da32c3956da1ac944089c19abd1b486304d903be0c47ac7485e957f5e"}, "36ecb6e4-ae9a-4767-902f-b4d377d79ea8": {"doc_hash": "073801b11583fa8207c90fb46694ef1fb05e2aa67fec6deb9e1ce301886685c3"}, "875e9265-07d9-4691-9e67-4827c9f9054d": {"doc_hash": "207e6ebf06aefee2ee703e50866ffd321f5de0c9282d351a1a14e8c25d55374d"}, "f4236a4f-9f76-4230-84a1-d483eb3591c8": {"doc_hash": "1e46d3c8854cc3cbbfc8029fd8cdfe1b7c63de3a61e175f11000fb44d8092bdb"}, "bf6d08df-33a8-44c7-acdc-c852b72d8a43": {"doc_hash": "f4008efc97be835c37a2aa59905ccbb92bd7184fa45b4ba4365c666245fd1ff7"}, "9ed703b9-be5d-4830-9a26-4c3f704bae39": {"doc_hash": "6822962b6a181cb2e3d76c60394ff760112fd1bd1f1e0e9bb326a032478f63ee"}, "3415773f-3081-4401-87f0-285c4ed96389": {"doc_hash": "663a6c5db22ccbc79fcc50732c149677eccb27c218a8d41dd19e8520b84ff800"}, "9b7f85d6-71a8-4224-8d8c-291b2b2abb47": {"doc_hash": "6d865e757a98042a0f193827e52793d9be6ff7ce07406b006240a5f5027b9a41"}, "30c6c8ed-deb6-455b-aee5-d17a13d86c9a": {"doc_hash": "2ab667cf1a1f43165d5be5eff812c99750834a3ce81d3bd99671bfc87bfd1090"}, "d57d3330-42e3-48ba-82b7-3cfcc8951174": {"doc_hash": "bb657a3658fc32527fba11e33989b28e1c97e48851c86bd5097faa43d97a60f0"}, "43d1eaef-fe3a-44ff-8167-656c98ce10ae": {"doc_hash": "3a240bcd3ccb9fc2f372d30315534851ab6cebc072784becc7585a6121438d02"}, "323954ca-6773-4339-8e96-c58e22f5b2c7": {"doc_hash": "1564085bda6d9aa71c823ce3617e477e02bca9b1625ed3b083eae71609acefb8"}, "f8870fa7-fdf8-40b9-a806-1278060db34c": {"doc_hash": "79a3801b856310e8cef32185ab9449a0a55ec1d913729d71396dc58808df9ad9"}, "b68c90e4-25f7-415e-9e76-a667a48e40fc": {"doc_hash": "5d96670ab75512bd6dd9159cc6ee23efd76750b86a846685aaea0842a6f4caf1"}, "81891401-d971-4807-b1f4-8214c74b3403": {"doc_hash": "7e34586a1b32832c35fdd34e192f16be01f7b56c690e6e817e5de79468529ae2"}, "b4332993-7547-4bc4-a723-ca5fcb14d801": {"doc_hash": "b6a72add03da8db148175a82e367d749574d703206681776b0804bf645cae48f"}, "5242e3f9-5a43-4f86-a065-dbbe58304f50": {"doc_hash": "05c36e8a664df1e09d2f0c4566c2c3b6d679b948d68eecf6313bcacceb751106"}, "18b72851-6d5e-48cc-b00e-1e442f2d9590": {"doc_hash": "e8b30db946848a738a55426c6ead24b7e54ffb416effb87ea76aaff3b1d4cc37"}, "63c1c3f1-e428-445e-b41a-786e757dc764": {"doc_hash": "368dca8319ba8ffe6557fbafa7c7e471d72d5547245b48a32ee40ce1c9448d2f"}, "b442e95e-8120-494a-9aec-d4bd544fd529": {"doc_hash": "77cc24b6e92ea83993b03874c696425a03ddd03a918f7f6ea9ea11f06ca4f89e"}, "6fa11236-ec5a-4812-b768-8cef0a8568f2": {"doc_hash": "993a7b62a6acaa68f989f4b90495114c071f784ec763341bfdcd36dd6e6fc641"}, "6fe96df6-7687-421a-952d-2798e7c01060": {"doc_hash": "7654972b4231bc7f7771ba84b70ef8a19ffdcbb2a73ce7654e18cc5d59da4560"}, "6f303d2f-a1d2-4424-bbe1-fb9c7d0c13a8": {"doc_hash": "6dfe51ba88791b9edffa79cb15ba561255f78df3c07a957d7a218353117fb9e0"}, "38a8820d-d0ff-46fc-a09e-5a963749ff0f": {"doc_hash": "65bc5dc86539cdd1ce8c07a51660b1dedb9a714f5a88b83dda222941bf487fe9"}, "6eea69db-d2af-4e66-8e7a-8be1d149e02a": {"doc_hash": "5f8f7665ac58721e9594405e3dfbb5a83b4ba38a152e7a2bf958ad1aadcccf3a"}, "18488222-f311-4bb6-8cde-a63c5bb357c8": {"doc_hash": "14cd8195fa83453b5de8001ae29925fed445106184b6960aaa75619aae203f7e"}, "b3a481e2-5388-4dfe-b0e2-9118b4a8abc7": {"doc_hash": "e9bbd7726b044d21b08336f326e2295b5540e1ddf69b4906f9679a948f462d83"}, "290412b5-4303-4786-81cf-b04b75e5de1e": {"doc_hash": "90ee4c72c8ccd6cce20b0a33233b879b2e7de707ba508116c615e9477ab8040d"}, "7b54a0ff-24c9-415d-ac1e-eef923aacb8a": {"doc_hash": "d86922966bc63a05e405eee3bed9b5b8970923034bd9405025944322f0cd0a85"}, "8570e97b-6fd5-488b-a397-6cc2e940e33a": {"doc_hash": "ea2fc12c46501b153241707412755c1a10d80aad5746177d1f234d0cd3bdc75d"}, "8f8655f5-0f66-4219-888b-62f33291abd6": {"doc_hash": "b334d707f99fc6e2a3b07ceed9b410b3befd1afc33cdd3e58b9b33a697d1f740"}, "0f800b55-5208-4311-9cd0-a8bc59e6f84c": {"doc_hash": "0f9cd053c1f09f7c3165f12c4344a75afbfb153092c946039a73c6325c8967ba"}, "fafd2ba6-5df4-48b3-a6ac-12ecd07c9f37": {"doc_hash": "64299c7baa116d1c743f89c1674d2d4f82dc04f296649f490a8f8192953ed1b3"}, "2808b57a-9ed3-44a0-b321-a62572cda607": {"doc_hash": "4a290893a4a4f660b7b609bbad5f794b4b845b10e912356abb297d3206e91196"}, "cb55b00e-11c1-46b0-b030-43539644fae3": {"doc_hash": "6e3f9b217b80fef59e8b1bb4fe3c94704b8e58a9fd1f1d6e96eee284b811a560"}, "36b27665-c63e-49da-b97a-420e5399eb89": {"doc_hash": "d1b248043e3c57eba19c9b8dffa8f28c71dfbeb0e8fd2c6244c2b6c517c877c1"}, "d3973197-d432-4b86-889d-0ab950f29f57": {"doc_hash": "04df6f3502c057f2a1c0f6f3f7abc4ba8872629ea7a12c261d9bd9ba115ad05c"}, "1b9091e5-a412-47ce-b495-247a75c10ef5": {"doc_hash": "4571b6bdaf746aadf1c6140f251c1911d1762270663326d22e890f565527fa64"}, "b9f36262-f69e-4e01-959e-ed420251cbcf": {"doc_hash": "965033f7e03739ab41e94bad839c8a8ac0b5b12f786b8afd95dba97dc6aa3a72"}, "e8c54f02-c166-4c95-a286-4dbf81c1f4fc": {"doc_hash": "b8991ca00c92eedf0ecfcf8a1793a9f6e5f8f6d777d503cad6885da839c6fc55"}, "fc1a7c7c-a819-4bba-84ca-39fe836c2490": {"doc_hash": "eecc8cca3a78e4d374fe1c4e856abb3a5c071cd706367aa0a20a0a346e9f0049"}, "a2cbf396-ef19-43c7-a15c-86baf1f001a0": {"doc_hash": "7ec794690d15a04f7e945edfe8e710ec407a288bfefae409b8bdeba976542ac1"}, "cd6af929-dff0-4a99-b9e0-0e460c08b7e4": {"doc_hash": "aa338aac848bfb2c5e1f09aa932f991ecbd73ea7977553c94c4b4d73c571b88a"}, "c61eae0f-d1bb-4290-829f-c7072f80d08c": {"doc_hash": "e0411d63f2b39c4577a483dfd060e55d9038f20272be6d8e5225aa62333bf076"}, "7349a39a-cfd7-4c23-bf17-ed270c770ea0": {"doc_hash": "47ce53e490279b0d1c7a122f38b859b793b761f4d14dec7ba0e960afc0d1bb56"}, "5c49fe42-d58b-4c45-a243-4d8a016ac7d8": {"doc_hash": "a948a5bce8c70faa97c2b86b4614803666a4d707c06a8c4accb03795e3bf1159"}, "f97c04f7-7741-4c83-bf74-c3fac0856c3d": {"doc_hash": "57ced77e64bd0972ae8dd24cb06394c27bd5365acab95e039e44d47c262f0204"}, "c6859032-32f1-4703-93e8-7b9e6c95453e": {"doc_hash": "77ae92344919fa8b06df16e1b12687dc649f8d06883fcdf0043290a3350a7148"}, "bae0c8b0-fd27-4c32-bc8e-d95641bc5c7d": {"doc_hash": "62aaeee7f6434cb449ae096ed1645953587a5f932423446a1f14ad390d1f6da4"}, "ea4b7b0a-1ced-4d51-b85d-de7c2cd9d3d3": {"doc_hash": "d8e7f7f58a243f24403ac940254ec1c4ee423448c93d61e9418ed9dadbf5fd34"}, "0eaff0a8-fbd9-4bf3-99bc-947e0effa6ef": {"doc_hash": "d549d0d3e40884dd4a1f4b431b0156a836b5a5ace29473a3580d793ea0a5040f"}, "a1164754-9806-42e9-a6a5-e73ee406388e": {"doc_hash": "5cc958976f79402431b1b1176818a7202cb9336f6ab3ee39cd3a3da9a11cb6c5"}, "b5216b80-5103-48f6-81af-14b94e50a4a9": {"doc_hash": "d41d2bdff1ab92c9a22929d96552a69120800c14dfe66ac7bd7e73a3540748c1"}, "f92cccab-29d5-40ba-bf6f-f614dcebd768": {"doc_hash": "5cc1e788304da6d668b043bebe51dd27504cd4428574b2ab35e309d3cf7e79ba"}, "96afeab7-28ca-494a-9bfa-b5aa49aed86d": {"doc_hash": "cab2266d7cc5d70144fb817e74920541d470421ad85fa46cb23838b9550ff44f"}, "37a87002-5d33-4cb1-89c6-0e3a9cdd3b3a": {"doc_hash": "bf09bd845e94addbb46d40372bd131dff84c99c0a7dba9ebe8c01282843ca36c"}, "4ec1ac3c-5335-442b-9c87-94a92d636327": {"doc_hash": "e506e471cadee94de696e524b8e60e19c2fb31ed0ec798ba12226def8df7f562"}, "6dedce11-ddbd-4f1e-8b8a-93aa99d7c5e1": {"doc_hash": "544e458d9c79cd93022d5809f9966e2ae11c4fadeebdf92c36a52267f5923999"}, "b52cf43b-8adc-4e65-92b5-66d57870a103": {"doc_hash": "2d9c617d1bebb356367acd25ae91dcdd815b2b9ef2c1bf82176e5df4a4357c42"}, "956b801d-f4b6-4869-b17b-2f9fb6f04d0d": {"doc_hash": "1294b644ab42ad1cfcfe6c2efb84abfc4552e9714a6840aef6a35c7cefc3b06a"}, "b5c75f33-816b-4863-b28d-51cec4d3b529": {"doc_hash": "04fc4af190f96144d4cb1b40cb0c06c4d1ba5e845ac06b620ba3b57592c23055"}, "3d390d59-9101-4607-ae69-71824268fa87": {"doc_hash": "2983bf42f867df43c5bdd7bba40a656c99271a06d7df60b29b5d2865bc89ae60"}, "cd9ad72c-0414-4e21-8086-23886873827a": {"doc_hash": "a987a062201495d6fc1dcc90f53c633d7cfb02e62d62a0ac8a6cb3a2f7043cb7"}, "b240c279-dcae-4e0e-b619-149cf2442806": {"doc_hash": "45c74e638b80bcbc7516bb0f99fed6feff3e5faf5c26244cf345797df11f0d92"}, "b2bb905c-3558-404b-9ed2-fdee69cb81ea": {"doc_hash": "ce6b974c4189021595c1eb81f1d8619de4bf876d8dcfd4126a9d7a2fc05bb34a"}, "6550ff3c-0c39-488f-a6fb-ec7fdb79e127": {"doc_hash": "b3ab2308e54b2448d1e3006cda17ecf9feae77d5feefb16cd762713e1245250e"}, "230d3219-36f4-4bde-bb69-355cda70d5f6": {"doc_hash": "f114a2fdeaad338cf28fe6776b24e50fc179b704bc1cf8a4edab751036719831"}, "efcb2069-1933-4eae-93bd-ab290586a3b2": {"doc_hash": "63fcb5ad4cb14297c9092df67d4d8ef23d445b629d7712b689eca9bf2ba950e8"}, "fbb23cff-c529-46fe-9fbe-92a9ea2b3750": {"doc_hash": "d5b2a2a375c7df665c31def89a26ac7a8513158750b990f6ade55a87439e3645"}, "97f6bb7c-60ed-4be7-89cf-2a2d820277ed": {"doc_hash": "a2c7d9584b1c61caf16cb926873a2cb769ce536a1040a80f6fb34b778369c0ec"}, "4f62c1df-e1f8-47a2-b79f-b79839306c0d": {"doc_hash": "2bfb35cd6de0e7fcb0da8cc1b9f994938871e97f3afdfe429be640eedab07436"}, "4a007f77-a9da-47a7-97ee-58ec53715948": {"doc_hash": "e17174362fd515ffcb4619e3ece328b36410fe78b75d6435bb8906a6a5811a72"}, "9bddfd4f-f097-4a4f-aaa6-7e0824ef3948": {"doc_hash": "91b65b45baf1070f23b32e942e9b867152da55fed5d31216b01d4b975a093bc5"}, "f286dde6-5e05-4582-876a-f6e6b14c2966": {"doc_hash": "ed9bbaf419b5ac9d947c19bbb309e0065ef17a1874301fd606972b9c654ab0ef"}, "6a233649-a7fb-4d92-96f8-cdd486554d17": {"doc_hash": "ac0e1db64fa621b1263b1d773d56bc045f0ca3a9d7dc05f2a24cea03fa970ea5"}, "670f214d-2fee-4eba-ae05-e4af5389371a": {"doc_hash": "1739f5c9aac38d42ebaebf3152f8c66e02ada8808685950e5ccd303de4c9b879"}, "bafb9a70-4250-43d8-9cce-d99225bc788e": {"doc_hash": "b62643e30af4c7dce51d5cb887289580ebcc757723d37e54d2da4e9bd1ce4da3"}, "4ce3996c-b193-4014-a767-c4ef44f971ff": {"doc_hash": "32a018f966b78a67a255e3c62488a3f701d7379a734d8c0b17c7b6bc8ca924bf"}, "47530729-e34b-4a50-8d0f-fd3bd80a70b0": {"doc_hash": "89e18da262c1e6ae77aba19befa637c2afc3b3d98ca85dcbc2747b1b70f78ccd"}, "450b9095-9db1-4a7d-aca0-28221ac7ba21": {"doc_hash": "53ee9eb5d2f64a6eceb10dc40e98ea3450782ac235c2626a42063cb6dc2c57c3"}, "d703b4c9-83d1-4332-a78f-4e0e8b936d38": {"doc_hash": "83760f3a3ddef84a1470528e02b436e3d6ee50393581e0b19dbc7ea1d62977e3"}, "eed11105-b8b4-4fec-8c7b-e937d9f7e54d": {"doc_hash": "15df61b6c66f812d9045bba5b140865b78771b3ca7e23e62488ada71ce5c0dcd"}, "ddca60da-d5c3-4473-b5ec-10869091ec7f": {"doc_hash": "1746da60da1632d8857b444decd3225ffb14cb97d8a5249d47534ffd2f97a2f4"}, "11c28e0f-bfcf-4f20-905b-d935c59c84aa": {"doc_hash": "7d8e75fd42cebb3c4e0d2929ef92849c26946cab7716b0b00d8ddfdc0e2c04c2"}, "206a325b-2ba2-4b1a-a37f-3395c772c50b": {"doc_hash": "7b56c9f119dc058fe5b2b1fa615f468dffa7f3f2e8323e19d9f3bbfd0b04a00c"}, "49df4625-0661-4a0c-a7e8-cc152a295e96": {"doc_hash": "a4658a096d45f01c117712f02818bb48321eb718d641c3663126acfda07adaa6"}, "1858b1e7-41bd-4c4d-ac0b-727fc158ad60": {"doc_hash": "5588e1c0f8a219fed4c8ba17e3299de921905f27f47290c8ec7491e2ca6ee3fa"}, "98932691-9658-4b72-9f32-cca87b88640d": {"doc_hash": "e3a8b15257d8217c381c429b9bafd2f1bbb8714856ae84ba4816cd45c59cae01"}, "d5b3c507-d5b4-446d-bc96-be2669b748cb": {"doc_hash": "814824b2988f978a2c2f4d96707e81c3872c3f8102b9c56a44846fb389b81d57"}, "5406a7e8-6a98-42a4-a0e3-0e0e6c15835d": {"doc_hash": "73a6e2b5667e2afd0bb451103d58f300a2cea8929f803839157f00f42b9040a0"}, "f7cb44b3-d4a6-40f6-b0e7-dd790c594e57": {"doc_hash": "d241d125bbe76d4a754ee95a5873d5e179115d160cbad7bc2dd189092d0eb058"}, "2470caf3-782b-4802-ae14-e833646fc961": {"doc_hash": "2d82341cc914def274ed255142de0ed0e7155dbfa23195a2d55cdc53d42e3e9c"}, "27c4933c-9cf8-4176-8bab-cab08935e61a": {"doc_hash": "53add4a2e9a4ea8b82077605db8b2221a9467ee7318fc2093c99e5e28780b5fb"}, "7ffc6755-54d2-4d01-a257-2d7a5191a139": {"doc_hash": "fe076498d360d1ae8bf9d63d5112dfa06cbffcf0be3ee2c914b16c3efb417386"}, "543b0ae1-2f13-48cf-a344-cb67045ef7bb": {"doc_hash": "4cbd48e919fa7b256148ea93fd025e279cc8a47ea64f169c3e1b9923ba261417"}, "52ca2beb-b4ec-4cbe-95dd-9bb2a35460bb": {"doc_hash": "4ebf1043ee448a4d831ccbebba2fd85ecc87a7aaab716263d00e7672f4447978"}, "d207774e-acd5-408d-871a-c87c3a53fe93": {"doc_hash": "4c6d5d8e8986d0b596aa3ad51b6da30ca87d9aa9d3a91c7937c01c9466bf5094"}, "02d625f2-bb2a-43d9-a4ae-aae00c226437": {"doc_hash": "c186307dd1c2d95c2a534bda57d16ed8aa917a153c9e33cf5c1dab8eb68163d2"}, "621e5e1d-65fd-4868-9e3d-575f78528742": {"doc_hash": "34a35015bfd6f2865a98a1ffe33d2fac81a1176694520a88973abc2562debe90"}, "e857529b-17d6-40b7-a5d3-dd246e1731f9": {"doc_hash": "ced605d56997c3a8466051e79c6df64b9fee11b3d26e0e0880252ade8c6e45d4"}, "652e63cd-50e9-47c5-b5c0-05069ec9a45e": {"doc_hash": "e8f0c586f0775cf62bf731f7e9baf53bbd869582b31a6873d9d299341393ea4a"}, "7377f3b8-0bd3-4cb8-b3bd-337711129bcb": {"doc_hash": "8260be8eb55c42f505c5035df4eb2242174314fd90882206646c3232bf56d440"}, "7f4af078-8631-4a7e-9b92-c9eca377e19e": {"doc_hash": "3269c9721ec351b59b53a7f9ce10d789f77c6a861d8b6721b2c2d379a91792fd"}, "ae3c8357-e8f3-4ce7-a39b-8976e9687996": {"doc_hash": "ddeb3b2066b1f2229fe388ec5986aff64e70d902c062e9484e7d47499029e6e1"}, "58f6aa3f-f410-41ce-b9bb-d2c4fb818eb0": {"doc_hash": "01fb7a1c301cc97ab77f581f4524240b9336c797f7ffd34b9966415dd46e82f1"}, "a5383756-ad7d-473b-8748-8e5ca3db9873": {"doc_hash": "b6c48b09ab2650d4ce3e46c4a83575a3b939ec13e70d4d91d53e431f625abe15"}, "32057854-cb08-4bfc-9a7d-5a8390ab5503": {"doc_hash": "41b77584de8f1074691272d119003554fc570c25a35301784a51ae9d81d8f7e7"}, "dd7371e0-94b6-4347-9e8f-e40fa9bb20b7": {"doc_hash": "6b06366a249f60ff6ec449e890927c90c8129459b2b75b0d8f847a274b296601"}, "b2f05688-c05e-4d48-97aa-dba15fa156d1": {"doc_hash": "d4b45aca86a8a1a81766b50e58c8187080baeddbf32287207be00823d1b39ab3"}, "8e7a6c66-f944-40a2-8a74-ad7a15bf7e0c": {"doc_hash": "dd497616e85ed0ff18059bc360185ed6c6834c9a3d165a83413c7fc2fc85dcb0"}, "8f8aab45-0db5-45c5-87da-2ddbec6fac83": {"doc_hash": "1d80e6728bad95f869b8db9720fa3e8805ee32c4755682f3800cae4089c496a2"}, "8b79e3c1-a36c-456c-9af4-78184a34c259": {"doc_hash": "7013244c037acd5191cacd0f917bc878ba5b84e59622780a247db4254cee813e"}, "22385d06-cfcf-4cf8-a4ad-c8782f270e7c": {"doc_hash": "6bace5e2344942cc111ab1553d44a7791d86cf2ab2cb87b7d8b9363a3c55aecd"}, "bb6a9b07-3ea9-4e54-9ca6-374759e2ab12": {"doc_hash": "5739719edf92e20784ece8b634686a7c80841e3f11b1c2e9c93effdaf4335a41"}, "4b88b751-f134-4b4f-86a5-f267d9404d34": {"doc_hash": "d07b29f4adc777938727e680939f6be5d2d7ad7e44606bf12372bd1dc754c82e"}, "6fac8246-f819-492c-8c3c-13db2df74f81": {"doc_hash": "b168121be28079ca5fb90f836446761c0b9ef5530b2ea0924d7549ed96c761cf"}, "0936e118-139e-4968-92aa-c2b13fdf6a56": {"doc_hash": "33cf5955b9edd104efbd3f9b045670e3d176064c42836ebb9538acb6398966ed"}, "36440607-c683-4dff-9733-34b32fde5961": {"doc_hash": "65f60da710e153b56c4686928213ad67026acd53387d6066e00f711e90ee8d15"}, "0395d880-e48b-4cbd-9c60-b3423d610b38": {"doc_hash": "9f181d2aad7db39756496c53728785c3e10bc458627030fca50c0905cbe01963"}, "87547b46-efa1-4f0c-b5bf-b4d3e868f54b": {"doc_hash": "b72524c8f274376bf7cce0715cb8ec9449fa1f4eb0c217a1566621e8d956bbe1"}, "d3248e59-364f-453b-93c4-3810ea964e4e": {"doc_hash": "a19c06e08d8e9944ea6c1384e84ab687edeadafff2bccddd0078f3f87ac40404"}, "2137841b-687f-44b0-900a-9f46763c4e98": {"doc_hash": "10705889e2b00ee81b12992b7f129c95d61c390874a0b6c254ed8b22c79aeec6"}, "afd89625-11d5-441b-84e9-a7a2e4372c6d": {"doc_hash": "07e1556bfa17968bb1fa2dc859fbd5ee6c7e4f53b29dcb8be0881222b6de6228"}, "4387c6f1-280d-46a5-a45b-06b5ecdfd831": {"doc_hash": "4091c84a6675ff5f8e8cdd086b7593a410eb195772e31e6d57eb45fc2dc5acd1"}, "c844f8f7-2cad-4320-959b-0a2c86c1745f": {"doc_hash": "9f957d6ccbcf72404d5891d99ac1236cddefce01c2995c63130c52f475d0e403"}, "8bac2634-4280-4e50-af81-46bf0ffd6d18": {"doc_hash": "5989d0572ebb41cedd1f92afa6b1393e74767674a2b9fb6cf274fa76954d56fb"}, "f22c021b-18a4-4c68-b7db-f919355c79f1": {"doc_hash": "2d0789647aef39fb1c0ed660ab2c413750eca1deb84b571ae1ad146ac71e36c2"}, "ba329b1f-4c71-4067-8634-8fb764e0809a": {"doc_hash": "1117073eaadfbafb6b379dd75855bca5d57a5b20316d9bd582906b1d8757b737"}, "2db6666a-2f77-4fe6-835c-6b4923f29798": {"doc_hash": "adad8ff1bea7c9a0820f37f3f285545f4324c142521f1721722617ddfdd3f5f9"}, "6162e212-cdf6-4d23-8336-f548aea9e5d4": {"doc_hash": "53d6b8886a0ac5208c77d37a504ec20237210beb786ff8b540598d2fefe1a854"}, "188ed599-0bef-4e2f-a6a8-ed897a3a33ea": {"doc_hash": "7df1a89d977562924a7ff16c3907922c163b4f71f52e0fd64763e2f58413f05b"}, "7066ee75-cd40-4dee-a9c3-12e566896e8a": {"doc_hash": "9d3636bf339da85578de3ec88a4177e0a0494b857a4c09279257dcc6774f9afd"}, "87870eaf-c124-48e4-90c7-fb047d51fe8a": {"doc_hash": "7d8ce7aef806f638154ffd49aa4c931c98e22f942f31dd553f95554d48ec7f80"}, "256b7554-b675-4320-b4b0-63d1c495fe40": {"doc_hash": "9abd848d184685512d006c96f3d594b5177ca1964e5c34807314430c62d1abf5"}, "6949dab2-4c32-43a2-a6f2-e97b28262b91": {"doc_hash": "0cf5957564e1395d8f94b6567caaa0582d0444454d45f7c3123ab9b931986b4a"}, "233713ff-9f4a-4d52-bab8-e89bae908ac0": {"doc_hash": "f6398e4055adf4bb3c6b6853d2fd66a68fa422fa347494852195daefb0ed328e"}, "dbb25a45-cab6-4951-9892-f16aefb15e8f": {"doc_hash": "3fbeb01c2b36713183c80f823819db8400c0c7f0ffda36e0c3456a56d725e675"}, "edaed966-2e8e-4488-8778-c7be3d46cd79": {"doc_hash": "a08c62623b8b9dd2f250b9d68fd466feea36c3d9051b98f230b7de2d8855eb5b"}, "769eb413-31c1-496b-a30d-c6fd5a77cc53": {"doc_hash": "b7288b5b93f3ddd0ea86350246177702128d2d6c5bb2f71551987079c00dab23"}, "ec8a90ef-0811-47a7-986f-9b493c5416ca": {"doc_hash": "aa261747b9f48bfd87f252aabb9424c6121b1375fbfad667c4329302e9e4e53e"}, "39839b0c-3cc8-4b2b-88f6-6759d9c35c5c": {"doc_hash": "ace781034ea53df160ebd24aae28e26d664e85f4be88c0035462434e740ffef3"}, "011ed87b-4187-4f74-94fc-c87a96a166b1": {"doc_hash": "e8ab1a397755e3e49ec7ee507a050c858150a5fe9bcf20fce5c938d16bf69b91"}, "b9e152ee-53fa-4018-902a-7c51d497294f": {"doc_hash": "38beec92290301a1e5ffde9e0921691de5a60b2b2ef6fdebfa6bad1d8c96c978"}, "e0b257d8-3d67-4f8a-bc49-0bd2a65d4b21": {"doc_hash": "bcaadf9bd925fef78ce6a5dd2cf28eab93b6825a524ecf67a322099af36ab3ad"}, "6ddb4227-96d3-4160-99b4-dcc7527c59be": {"doc_hash": "7650d32cc58d40d03c630a16afb5a598cdc802d59614e4f1041d6afa65434798"}, "8282f81f-04fa-4306-b0e2-158ac8560fc9": {"doc_hash": "df3242652ccdeb3c7dd3318fa6a5ca5da320d63a59435e6ea13f66370d74970b"}, "3493933d-f315-4006-9040-419979d2e68b": {"doc_hash": "9d442e534f6b20633ab710a8d1c4c254ac8c4df7fb003f9708fcc57aae92c88b"}, "779388a3-56a8-46d5-9e95-71fb414b8c25": {"doc_hash": "806b552f449119aa2b87afda2a7ea797b589e20909fe6c5aa27340d4993ce72f"}, "dbe2a69f-e4ba-4656-8ab8-169e83be275c": {"doc_hash": "734f0e1ec8f2bd8c72d95fb32213ea0f08b51c68702da64692c282eaa628e997"}, "b2e8fe1a-8646-4db8-a3e8-ddda282679ba": {"doc_hash": "0bbed2fafa85297f069d2f3a9ef5474699416f63cc8759638d7c8f6da8743978", "ref_doc_id": "6094fcc0-b443-44dd-8634-5068f3ad0090"}, "39c8a6dc-7107-43e4-92dd-297be9523764": {"doc_hash": "cb7e644cb5aa9c9be568f4ceedd6b1f9b781a6612cfd390c77bc785689ec475d", "ref_doc_id": "60689eb1-3fe9-4624-99ba-8e46b9203b22"}, "f34ea1b8-4f15-43b7-9276-4ebc06388179": {"doc_hash": "e14486b527f81a7b5a899bd3de3fb8f5119a195fa8669829fc2bde5d4777f6c9", "ref_doc_id": "899fc809-a5e2-4860-b33d-5006dd337076"}, "1f0a3d74-fb21-4b0c-9637-3fccb799e058": {"doc_hash": "f79f0eeb5b8f4c308d271d814d9e42183b099cfd14ce96475437710ae45bdde0", "ref_doc_id": "c60b287f-80a6-43c0-9f72-d1c55c2e13a9"}, "bccdb4cc-146d-4828-b0f3-b6c5e91fa930": {"doc_hash": "6e093379b0b8b57378482a67a611f55b21aefb54dffe2fab7174fa8aa6349d29", "ref_doc_id": "5810e7e5-fdf8-42b5-8543-db2a98d25510"}, "2930f7f9-4c01-4d4e-8d7e-34b3eed65ffa": {"doc_hash": "44d4e9dbb59dcef0892c2e026881487851242b5aaf44eeeb81eae1e217131b94", "ref_doc_id": "ea6a59f1-258d-44a5-817e-04c665a9ea93"}, "061f059c-8726-4e53-bb9c-7fd15c60ffd9": {"doc_hash": "89ea57922612d1c85c835991962f2fb977b6d2911cf23f9b72700e67caa9037b", "ref_doc_id": "fd9ee88d-ef27-4bad-af59-a1e83479a348"}, "f367d0ab-6788-4362-bf3c-5add6d606d3f": {"doc_hash": "bfe22d2f71948aaaaf943e459cf3883f19bfb3581dc7ee753674394d3f4fa805", "ref_doc_id": "878e7182-3c06-41c5-bb5c-8222ff1a30f8"}, "21f0534b-ed8d-41af-a628-13405fdbbf8e": {"doc_hash": "d4d0aec7af41c703faa6232ccdb364c17fe0e10c5b1327b8bd0c7ba96535d848", "ref_doc_id": "0164c163-7026-4979-826c-001726140c3a"}, "7cf5a801-dd90-4825-91d3-c211d857d9c2": {"doc_hash": "10638b2b242ec6df0a0bec03b908aef0824b9fa34359a229c61a7362da1a3347", "ref_doc_id": "7bceafa1-d21a-4ac8-aabd-19eec5b0b212"}, "ada3a148-aeeb-4866-8e6a-fd1be013c7bc": {"doc_hash": "397a20acd6c2a3ea91316bfb268a7b7e86fab33eb5ae532a979bddee8e0466d4", "ref_doc_id": "0fdabdfd-7bf7-429c-95fd-2843c6327b8e"}, "989c8e88-2806-452c-ac07-551c9740379e": {"doc_hash": "00096b3467588aa072a263a976eac8cdf746a5ce441fe1ddd71196eed0412602", "ref_doc_id": "9f08d760-a4dd-4617-8ca6-5d1bcaeeb8aa"}, "ff3eed74-6e15-4ecc-910f-1b6e8f348ec6": {"doc_hash": "3f7d2e839ab9af7570a2fbb598bc083e06b1c36e0d9273d3882e99223a7e450d", "ref_doc_id": "d9ec69d6-ca26-4231-a9d8-cb5302aadb81"}, "047b3d80-a6bf-4716-9efe-9c706661f461": {"doc_hash": "9dd5094d5f58266eede6c04f130ba2da90b337b83b9bc19a38b602be173f44fc", "ref_doc_id": "fa31846b-7ff6-4025-b334-5531cfd98702"}, "dc7a3ff6-8c5d-4cbc-82c7-e9c361311577": {"doc_hash": "13afee1202f1820736cdd8bdf68eb7f0bce592baeee0393392db38e80e601a9e", "ref_doc_id": "8a4bc4f0-d445-467c-b12f-dd07a019b214"}, "6dad2aa7-f344-48a3-9748-57e19f4e1513": {"doc_hash": "9bdcc57b32442e9997113031cce79293a4e3c5331caca90c5997e40a48e9eebf", "ref_doc_id": "89d97c01-a8ef-4666-84e0-9e1236ab18f1"}, "28954239-7003-4d07-8fed-232f158d5ac3": {"doc_hash": "d2eb94ab67189da271a0c86ab2fca7b97e82578b7abddc47c5ca70d7bed46a54", "ref_doc_id": "3fb7e556-4484-4834-ae7d-1d06d2a19cd1"}, "3947e646-5404-4758-9786-1048c046d6d4": {"doc_hash": "f63f858297972e81621c16579fc453b6a615c56c4b7fda786ca75befad4fddb4", "ref_doc_id": "f24aea0b-aeb1-463b-acd4-d86761bb6f62"}, "6ac6f13a-9319-471e-96de-ceff489389bd": {"doc_hash": "db4b61d92748f417cda54a8db0809c82244cc23fbd067f3bc830fca3834307bd", "ref_doc_id": "33965993-5109-4044-a90e-841c29acf34a"}, "6621799e-d1f5-40d6-afb3-6e9a18d60898": {"doc_hash": "94b8a8bc2bf5c1fc1830daf398829fb9edf9b21d209b0bc74afb8ed13e0b063f", "ref_doc_id": "7f3d4a4c-f62a-44a4-a220-26330c1afbe4"}, "b0de6215-749b-403e-a470-6f6b68f7ce5e": {"doc_hash": "93c454285cdddff9efadaab0a23f9aab4b87711a7d2d9d9c68015d4f517b4c26", "ref_doc_id": "3947ecce-c8b3-4082-ae15-cb5cdb942e1c"}, "bd056e1b-08f1-4ba9-9a7f-a8f6cbaeda87": {"doc_hash": "184c9ee84ff0495e82e58460279d35a09e81b25a204dea524ff768654d68108b", "ref_doc_id": "120b9a6a-2cb6-4c6d-a97f-74212c30e337"}, "5d0730f4-a764-43e9-8501-dce3ed11d4d2": {"doc_hash": "41f9e22cb976a9a02658d2beb40207c2ca6abf650956fdb851ddd13fe9874c8f", "ref_doc_id": "e916bb8a-c3a6-4624-b2f4-54763c967b89"}, "93559659-41b7-4fc8-ae7c-5f9972d6b9a2": {"doc_hash": "1b86c0d2affa24c0fb77bcd50d6b5497c05b4fbdd9a6f73b9a4ce7d67cbef74c", "ref_doc_id": "65be8e2b-575d-4634-b303-5158d365fe84"}, "8a44d185-30d1-4bc6-986e-04a7eba7e313": {"doc_hash": "4b71286c21556cbccd7880206ba6ed93353fd92c05f1ab4c88314b90c799a7f1", "ref_doc_id": "5639c14e-d399-4eb7-a1af-a4fd7e42bbbd"}, "340b2b95-5b28-4067-976a-fd1d97e84f2e": {"doc_hash": "c4b2b905423fc4746853ec64c88b199959b18be16d325f52f73385ce652bff8b", "ref_doc_id": "74e4b992-1891-4633-ba1b-db568a01b6df"}, "aa5fed15-a60b-4f14-97fa-66b3a2683635": {"doc_hash": "6e6e0f25d0801975458b72a075798e779e0c4e951827c27e2a5da5a802e894ae", "ref_doc_id": "39263651-0b90-4dba-8f01-d1be11ceff5f"}, "74c94af6-d202-4cfb-920b-f37847f73fa4": {"doc_hash": "92090798b48208e794fd6d8db60608d08a0a7d436ceef40a6514ba99e4f9acac", "ref_doc_id": "8bb33091-ca08-46bb-958a-b1fe79851bac"}, "6a389593-16ee-4d41-9df2-d6f19a7fc207": {"doc_hash": "42709521395de0a822d248a7d117b996ee3983810a330065953d3e2bc160fe0f", "ref_doc_id": "cdb06cdc-5247-405f-bbcf-b114ec1912f3"}, "4f0b2333-72a2-4e70-9176-d9261008f955": {"doc_hash": "1b41242a786cd33bf52144ad36105f09a695d262dbd88cb42ce6e641b25bd9f0", "ref_doc_id": "cc4df5ae-574d-4fbb-9a35-07ae63fe45e9"}, "09f3d72a-83dc-4328-bddd-ab2bd989623c": {"doc_hash": "2183711e145ffc98825bed90eec1548a4ba2ad5f9f336fc2526f38dfb0c6e824", "ref_doc_id": "6951bad1-e967-4bea-ba56-06374af12b22"}, "12cb3e97-6ecb-4147-b3cd-37ec6277c76e": {"doc_hash": "5c49673ac84e6239b958961f4f47419dd8360b3c6a4013241bd2527ac55e057e", "ref_doc_id": "84f6a62c-c321-476d-84c1-c03c49274070"}, "b1d2bc2e-054f-4ea9-b8f5-d768700e39ae": {"doc_hash": "51a39450629b21bf071645abd9db8621dba0956fe512f5ab6ff095e1d90653ca", "ref_doc_id": "82084c07-7813-466b-ac59-f31939648ddc"}, "b851b373-85f2-4f1b-a519-55d7e5a25478": {"doc_hash": "af5ac402437a60d1aaedfd218a08d981d230439a07be0cca0bd2cce27ec33122", "ref_doc_id": "1b59759f-8749-412e-9c09-aec7260d6490"}, "29d88efd-250d-4691-a2fc-0a50e7d0c600": {"doc_hash": "80af8379c60097aa2bfc0a799ab0c31732b79eb71fbacf8dc52302e57bd7fda9", "ref_doc_id": "d1ff5289-b125-4279-ae7d-f25aa654d9c1"}, "1402c676-c4f4-4e9e-861f-a9c638e04fcb": {"doc_hash": "f9a63f4c57f1dd941f116b0609c9f79063d902bb314a325052106530bc566d0e", "ref_doc_id": "46430440-97d6-44cb-b6e1-30f0be686743"}, "2a75f063-ea78-4686-9b6f-44cb41361b75": {"doc_hash": "b447f0ce46a37bdd53eaa45b110508f1507fe2dd023ab51f69a24dfc1cf14126", "ref_doc_id": "7e5136e0-957e-4acd-9c39-81b48e3aa2be"}, "d933a600-b9bf-4034-8f80-99dba27f274c": {"doc_hash": "9fdea6b2b9405b19ef57ee937ec5b1f08cc93e21844f90c17c274e29b88e8f03", "ref_doc_id": "5691d812-406c-4a90-b6f4-d14b890dfed4"}, "348b03b8-f316-4d09-8165-c3fb7ca0ff81": {"doc_hash": "c0174d84b1b3c6ae9a34caa4ded3af38909fcd4a9cfb8d5e5698386e6319fc68", "ref_doc_id": "0446435b-2869-43da-94ca-11e81ed20e45"}, "c6faea92-87b0-4935-ba16-e7ff1e516fb6": {"doc_hash": "a10679576ed78307056e65139392f60b420617ca812eadce858dbe12ea722682", "ref_doc_id": "2b346bf3-5e5e-490c-9356-87ea13d9fadd"}, "c08e923f-1a5e-415f-8b95-96ac2f175014": {"doc_hash": "950f88a10b7d5a4c8c43a6d0ab3223cb92ea68d68f771ae64caee0afec2d3c39", "ref_doc_id": "0c4b21e2-ab4c-42fd-ad6a-145261c6023b"}, "f5d9629d-4e4f-4a64-ad76-83a8fb9b6e21": {"doc_hash": "130757afb79b4a9929aadcac445c77c652b68d81b47bdb173cdfd34cfa6985a9", "ref_doc_id": "784875a5-bcad-4d00-bca6-cf90a03b8861"}, "ed302f67-ff74-4d01-8549-fdc1e7817f48": {"doc_hash": "d3ad7e0f1a9beb3202ef1fabfc37d55d6d63cfc7db8793ebc221837843ce79a6", "ref_doc_id": "d642a07d-31db-430c-b5ad-22309d3056c9"}, "62530c90-823e-45b5-8ae8-2dd1729084bd": {"doc_hash": "c7b03358ba40215a26e7b1e62da7026e9d036b2df89af7b63d473dbcc59153bd", "ref_doc_id": "b8136704-eb51-4908-b0bb-4ea95f5bea8c"}, "38834211-5145-4b56-9c04-9fb00f303606": {"doc_hash": "d5291ce6046858846a2b1f47d84d2737d342865b7209faf997e7e2b9f3d59f15", "ref_doc_id": "f22f944c-8fd3-4a9d-864b-72e74a5e44d1"}, "f01ded40-aa2c-432f-91f2-680b7ccc3065": {"doc_hash": "948da04173150bc04681dda46f593278f0cf41c459880b2de91ee3935ab8a555", "ref_doc_id": "680f93a1-410f-4b25-8d00-9e260af20180"}, "1f945d7c-470a-4b90-a491-b3b5652c144f": {"doc_hash": "67d7b068661c4216cbe3d963dd192927134868a284bf288510582a06632ca9c2", "ref_doc_id": "fac1b167-091e-4246-88c3-519a3a708ef6"}, "f6de8d81-a37f-474d-b9b5-3f594d72cc23": {"doc_hash": "55761cbfa51ca544e93f07989d3a808a065c51ba21a36e968b7f4eccbd512ed0", "ref_doc_id": "07017951-119b-4a87-baee-cb8a70ab735b"}, "4cef62bc-a2d1-4790-b6c5-accfea1fb134": {"doc_hash": "75ff47448a80d33a2d5f37cea40e321102c9feff70fd779ee9a26ce886cd4611", "ref_doc_id": "006a20de-39fe-46f0-8cf9-dc6353d132d8"}, "c2b9b6e7-3824-49fd-aacf-0a184780cd99": {"doc_hash": "5843e32677789e5166baa7a9a0f0a7c0ae78708ec079b335d62ef134638cadb9", "ref_doc_id": "b93ae5d7-c1d6-4a9e-a5e0-3e78eb88ef67"}, "da4195fd-c83e-4d71-84bc-b889f61c3b34": {"doc_hash": "f76ca427ecc8272f18542f6250ccfd05e07a875c6a081c5f83d16130ae2387e3", "ref_doc_id": "b5a55f82-e6fa-41aa-9469-b6f50187b3b0"}, "345c9541-9073-49c9-9e62-229afb98afef": {"doc_hash": "551b039b5771e2fba5d8d455e3483870008f270b089fe688495e3b427f76852f", "ref_doc_id": "9f5bb9af-4b8c-4008-92f9-9df33a2c1f79"}, "ee35189c-756c-4aeb-a633-40969834379c": {"doc_hash": "deb49153bbaec37ba589b3ca77861b7c857cb81edd266166d2f2b77099a39142", "ref_doc_id": "e467744e-4b41-4485-a417-77e93a794191"}, "43c46c4c-db2e-4bae-9667-6ecd323b402b": {"doc_hash": "66072e1d094fa03dbf61c6a0b57781cc88d6f3c423a62a47a9ded4ca6bcfcd60", "ref_doc_id": "3477f662-34bc-4321-a72e-9aee677858b8"}, "2b3051ae-73c5-4c54-9acb-9307537ab897": {"doc_hash": "04822f683670268c6e54960e9550d9e9ce818bb6050641d49473878373608531", "ref_doc_id": "a309121d-5285-4fed-9729-74a1185a37f3"}, "f6ed877c-aea2-4160-8b8a-73602e7f3f39": {"doc_hash": "bf32e019336cc4eeaca0c361d26d4b4f53edda073e4286ccc0c1b6d56434a70e", "ref_doc_id": "a5854754-25e7-4ead-bd07-5ba54221941e"}, "50289ea6-4d57-4d67-86f2-7387f0e2682e": {"doc_hash": "6ccf5c4f34dede9783ba155c27c2a384d5e9b7716b57d3aba392cf39c1ae6950", "ref_doc_id": "9584486d-8aa8-4cfe-8281-5a59b69cd8ee"}, "28d70e9b-a6fe-4702-ad1c-4ea4b8f517d5": {"doc_hash": "42532a4d52c64bd9123f36bca4a82fe99d47b2fc416a53d01c54e1356bcc94c8", "ref_doc_id": "8cda7db5-b1a5-40c0-b518-132c38ff3874"}, "bc21ac7a-9d62-4f42-940e-2b6642c9808c": {"doc_hash": "b2c240dd8d9350153f495fd6af838d187168c242a5ef39fe26f5dc1a1621ddb3", "ref_doc_id": "7254cc94-c98c-430f-8a32-ba8d5bbadb58"}, "da7c61ca-5061-4556-818f-d6bb4d65de52": {"doc_hash": "84d2f18d484622d5aea7af24b48fe4cc9468e25f7309c3fa14d55f3d145efc06", "ref_doc_id": "fe67addb-a6d7-477d-a4f3-e0da12d12eb2"}, "75624f8e-28cd-4399-8640-b5b5aeff2694": {"doc_hash": "ee15e0e0dd9bf6e3afee0d8e9e4afc62da14491005d943afbe2bb53301c43573", "ref_doc_id": "891c5a56-9517-408f-ba99-3568d7d04bdd"}, "6197b9a8-694c-4f78-a126-9e337536b8ef": {"doc_hash": "0a1954304a8aa1e4ac33f8215aae521e76d10ccda7ca984e4e499d7fb5556fe1", "ref_doc_id": "f737561b-3951-41b4-929c-55a9fd235238"}, "e65271b7-4772-4d81-8f3a-6317a47041b3": {"doc_hash": "016a6a7af1ec885bb3a18095d269498bdc9b47b5d0377d90ba079ef3d6352720", "ref_doc_id": "ddf66051-632a-4c96-8c64-0243cae5af9d"}, "8813068b-9fbc-41ac-963a-011a3e73bc41": {"doc_hash": "ce3ca6fbf2853a671a82e350729a0782e8af5baa6aebc3f482ce7c3d3cc4681c", "ref_doc_id": "6ed7fefb-4f6b-4b74-b434-63c25726b1be"}, "9d66076d-5939-4335-874c-5934255d43bb": {"doc_hash": "d0e8523f9e75f45786b82e19705eb668f6e02c705723e391176e60bc5b2658fe", "ref_doc_id": "e6e0b8cd-6d94-4b86-bd0b-cf43bb78c998"}, "5b51b3f0-e39e-430d-8d1e-60a3ffe60564": {"doc_hash": "3c1d3749cd1acea9e021fc0b3b71624ed8deb72b0df2ee1b32d1873222e9b43b", "ref_doc_id": "d17cc04f-2556-4be4-8492-183bde7500b5"}, "56b9c850-b312-431f-a280-8e7d937d7a2e": {"doc_hash": "0ffba6e5b655e3f5d6e8929136c8af432eac0823dbf9ce7fd16a427bcb59a008", "ref_doc_id": "bc11ff53-4ff5-417e-95fe-d05fcaf270ee"}, "fe262dcf-bbaa-4787-b585-db30d7df91d3": {"doc_hash": "fda6fed7a4c86879a62494375b0f3bd5b1a6058734b9f53fe20444567581cb4b", "ref_doc_id": "c41f80f8-2b1a-4e9a-aefa-57ac6eccb785"}, "90723a68-e2e5-4821-9340-0591384f0ba6": {"doc_hash": "a6d42747668dab1e87a70bb27dd0b5c3dabed88be59d48183a306bf2fd5a9146", "ref_doc_id": "38a47316-3622-4770-ae78-b51ac99dc884"}, "6327fd99-ca61-4c04-ab8b-794ed99e1447": {"doc_hash": "0e63aced5d4a44e4a40dd90b7dfcfe879982d19e224d5b4c21f142d8f59b870c", "ref_doc_id": "df0c2233-afc8-42c9-b8ac-d724076b60a7"}, "617f5ffe-02dd-4de8-ad20-90888f039f77": {"doc_hash": "9641c267605532f37f7f7e247b1bb66cdd58f950017307d4910456329ffc27de", "ref_doc_id": "48bb777f-5654-4d58-b6cf-b579e21e6fb9"}, "364cd22c-e52c-442c-a21a-bc076ed7abf4": {"doc_hash": "9854632612d7838102975f710b195c65bf420f034078802aedadb4b4c9573090", "ref_doc_id": "c51adbaa-8270-46ba-8cf6-bbb9e291580a"}, "108c0891-258a-4b24-b3b4-16e7421ddcaa": {"doc_hash": "08bea365aaa35cb08abd5a1689b6d532a45c133fb55beb6374a8ad1a0132d489", "ref_doc_id": "376fccce-f29f-4d2d-9746-0f3cf167b13b"}, "fbc3e0b0-beaf-4635-b221-474877d34e8b": {"doc_hash": "86417e895de8af2b70961466ec59be2f55a7f1a71e267f8e7a64cf30971e4fa5", "ref_doc_id": "0938c9b3-cb06-4f3f-a9c1-9af26e07643f"}, "c6622d93-31e0-483b-8dd2-fe7dc1071ff5": {"doc_hash": "b98d67a9bd90f8fb38fe5e5b7af5470e71f475203719543c3b6a15a93dcd72b1", "ref_doc_id": "e48a3946-2f14-4abc-a2ca-4ae46205d74a"}, "8d93b1c4-40b3-4fa0-80be-c083cbdf53b3": {"doc_hash": "182cd9ccec8c71fc6ec6018c3a29ecee26ecc79d04f8721592185b3a532ae03f", "ref_doc_id": "6e5a34ea-6545-4637-b648-bc3a2d1168f0"}, "2569e1dc-bc28-4da9-9a2f-faee0fad778a": {"doc_hash": "1f4871b60ff5359a903bec0208ced0293eef109fb3947ad1d3c8e531023c72f1", "ref_doc_id": "9162b5d8-1bdb-4b21-8319-bb413876ea91"}, "a6c5efa8-1a8d-4aa6-8fb8-9d45551e2153": {"doc_hash": "a23c09d5e70a1cb5ad6629ff207821798820eec2e81ce9bc700e45048f9d64d0", "ref_doc_id": "002a56bd-7e0e-474a-8f2c-623dae5ee463"}, "a01bfb75-7720-4e8d-ad8f-9fce2011072c": {"doc_hash": "766c7f3689832c036e258c97089e99af5d9f28b14cdb49eaaeffd67f2832b493", "ref_doc_id": "fc36ea8f-7747-42f1-96cb-81e60e3248b1"}, "1b5ff690-4513-4452-816b-1f934b6e9c87": {"doc_hash": "c73588b14235fda194169e6372d90fd98ce9c85e24e94d5d62bee528fb907fd5", "ref_doc_id": "28484c8b-0ff3-41f0-b500-bbdf5e37ffff"}, "4cbb3c39-a01e-4e92-91da-3d44683f3634": {"doc_hash": "13b8518f39cb4a2af2c435e1049fed6f1b7eda043052146d9d39048e24b1ccbe", "ref_doc_id": "d1d84a30-f22f-4147-929a-0de8a22c465c"}, "12802952-3737-44bc-a7b0-aff4ffc36ab4": {"doc_hash": "cbf2d068bc61d8e021cc651336e9fa91ad472c586c47ff59c2623f615daa23fe", "ref_doc_id": "9a0f0f46-0926-48d0-9fce-9164f33f84c9"}, "498c2406-ae08-4f53-a5da-5b7f308424a4": {"doc_hash": "195c572b9f5a4d3366c136ac751c464642767ed5855e798ea34a60c8198f9a7b", "ref_doc_id": "7e5a61a8-cec2-4a35-9420-99ba06a81934"}, "79412024-e706-43cf-9fd3-ca2cd10d7f62": {"doc_hash": "2b4869e43c6db40c19b119f3f84bdeb23574fc2594061332906070b9f47910b5", "ref_doc_id": "65f1e966-8274-4cc7-9b67-ec592f3f979a"}, "eeeae394-009d-482c-826d-2421c4d4a464": {"doc_hash": "323ea2250d75073f17cff7f92ecce40619f1396f7c508a16bbb1d95478878e67", "ref_doc_id": "3ef2ad90-5594-4ddd-8790-31bac87f157f"}, "a5375ae3-31f1-4295-b489-9dda0704b50f": {"doc_hash": "fdb27f035971fddb0f8d18ce6b055db0c2412b9283209257414860ba7c86f841", "ref_doc_id": "b780f399-be0b-4809-875c-1f1eee3730d6"}, "1d345b7c-1a86-474a-975b-cd3acad8fdc4": {"doc_hash": "25a84ec53d61ac8e2eff98f3cd8ab26dcf0639e57a25a54a839e45724166df6a", "ref_doc_id": "350c6309-8c08-48db-a63c-c82fef335820"}, "72ee43d7-7bb6-48e2-bed5-2ad4d5c5ef83": {"doc_hash": "d258a6b0be6170d6e2641c182b119af72c8a97967af4e1431db7dbd9d8b1a9b1", "ref_doc_id": "485880b5-6aa5-4457-9f9f-c0345431acbc"}, "f8dbc309-1173-440f-8104-c915eee8a157": {"doc_hash": "77199012fc4b99894c179b144be2628cbabc074c483dab93fe62ede422f2a25f", "ref_doc_id": "3d8f3656-6321-42e2-9002-d86c1cd373e6"}, "809bf121-161a-4f7e-a144-e87f55f45fc5": {"doc_hash": "30f5baf0bc85bc5e0f3b74ba6d90f2873924f36a8c85c78dd73c8c952e83a539", "ref_doc_id": "38fdb2ad-2e13-4759-bdc7-84eb20045d87"}, "13bc16e4-b716-4244-a923-8fe3778b0c66": {"doc_hash": "1d750b2a2d4009e84715da50fbe525f74ac2febd7e0b40d305ce9f1893136bc9", "ref_doc_id": "9dae6518-a1e9-47e1-a93c-23fe503b873f"}, "197e2008-a477-488f-aa83-feb1e6ce3642": {"doc_hash": "79fde1b0e851cb11c45124731413fb3e19bab2c52d77a420942f483cd63ef870", "ref_doc_id": "c6ebfbbf-5ae3-4e87-83ff-f82186330325"}, "6bdcdce4-a9a5-46f3-a328-1095937fcfb8": {"doc_hash": "f4fcfec2b0f13039efa2db8f7038f7340736d0ec7a17bbf4b4d415a1f89c7136", "ref_doc_id": "26e6a64f-61f6-466e-b9dd-bb2955c286b3"}, "e1c6ff47-7a0f-40bb-b6ab-deff14562aec": {"doc_hash": "947d1ff9f8af9e62403d978b9e25424389d763ebb76826a6ec73e678a6055ae1", "ref_doc_id": "e3da4913-5c12-4be3-86d2-34abf95892dd"}, "c22aab93-ecf4-46e5-8abc-f973ac43cf30": {"doc_hash": "3c9f33b0807e67316487f7664fc1412dea73a92dfe75effb47afafeb0f857a39", "ref_doc_id": "7eeab61e-ba51-404d-9f9a-eecc8bc5a0b5"}, "94d11640-707a-4b97-bce1-c44456ab92a7": {"doc_hash": "ff5888571cacb1dca6ad36e1825fda637c23b3bd3e0ea26d7517757e5277996e", "ref_doc_id": "089efd66-9a30-4768-961a-5f2d28528cb1"}, "3f1001b8-a37c-4607-a40f-e5c3cc471e25": {"doc_hash": "77c01eb55a8cec7d906783a52f5262576ae6b919cc61982c4ccf995de781816f", "ref_doc_id": "243a720a-7a98-47b9-96fd-98eceb370ca4"}, "c3400d92-0410-4f05-9cef-5b244be18db6": {"doc_hash": "94e11a51f5855c3db77160320fc2386a061b9f3981f89727908e07c02e520a0d", "ref_doc_id": "75cf788c-6b56-47c9-91ed-d23ad70fe197"}, "c5cd3f6a-6d93-4e8c-a908-f99df372ca20": {"doc_hash": "2cf2619ebf02b825c8603a53a17d673ca270be03cec7c9c037ea59c2d54c1291", "ref_doc_id": "73786048-9fed-4015-9ef4-f53b73509fdd"}, "7905e80d-7912-4ddb-a623-c6c0b73a1040": {"doc_hash": "dd183ce42ce46c609615865d90eb6c618df745da2f5b6db1442e01c826582b19", "ref_doc_id": "c36be9fe-2134-4974-9a8a-232520aa1518"}, "109ed287-9f70-4690-bab7-3d4b4267afbd": {"doc_hash": "9f8ac3ca1af071ac6f5ef00f5dbe9e82c01792aab34d703d1cb73e4c5bf9ba63", "ref_doc_id": "cd904311-7be8-4d75-a39d-db3e6741e71b"}, "86b07e53-90a5-45b4-9bff-b15a25733c96": {"doc_hash": "768b333f2ad06b55522d52f22f548fc6862c6c62f6650b584a363a71a64dbed1", "ref_doc_id": "61fdfd31-cb4d-4fe2-b1ce-7c3f052da139"}, "da21513f-a890-4594-adb1-c83ccdd16530": {"doc_hash": "b736b19a43a29188340db2efa38d54ccfc6ad56b658e92ab22d6ab25a2d6e045", "ref_doc_id": "7a67226d-9103-4d88-946f-98a131d7c5ab"}, "13c5249d-dfce-4c69-9ed4-25161852294e": {"doc_hash": "99453229364587125638f4bdb21f51518fa7a452244cb30b355658a7f7f650ee", "ref_doc_id": "a70caa47-ac2e-4bea-8807-5be978f21f00"}, "7e519b41-ec89-42fe-9403-890916131790": {"doc_hash": "55c0ae51ebd40338a129139d060b428639a9c70947104d13cb478ee80e2686ca", "ref_doc_id": "21ddfb01-65c5-4553-ae6f-8922232b66a6"}, "50059248-7fb1-4ca0-9336-fdc25b56d3d0": {"doc_hash": "2cfe64ef1166cd5cb78023fb6e27ec77a018a426cc1774ff0f4a7e6a78c32261", "ref_doc_id": "5f7611bf-9c7c-443b-8723-8bce5ea724d6"}, "e601a6ec-c22e-4ad0-b3d5-ff7de0d0b28f": {"doc_hash": "1fbf1807f08df263f7457ea43bfae089241a5f8ae7368cda800c2e16d550a073", "ref_doc_id": "845ce1d0-92de-406e-9d14-f4e68eb5b081"}, "5cd3fe57-80f0-469a-a76b-25659275d9f5": {"doc_hash": "0e42e31c99f49a4309127161aeb16163971bd8412ee5ce85ccfb4dcaf5bec418", "ref_doc_id": "991ef4fa-8d15-46b2-94a4-a6bebb870d9c"}, "0cce7747-d815-4d50-b465-9257b08db87f": {"doc_hash": "62f52c1908cd4bdf4aec0cdf781c150d1bcf66a767c1766f5e0cffbbc9407d2b", "ref_doc_id": "f60cce34-67bf-475c-bdaf-24756070597b"}, "414aed5c-ec53-4922-872c-c145fc79d82c": {"doc_hash": "300cd768e4536d5728ff63695900b1119c1068814786b3c2acbd4637bddb9acf", "ref_doc_id": "e54ab27f-2295-4789-9317-288ccaf70d3a"}, "8e067e52-a3f9-4a49-8118-8c6a2f8dd9da": {"doc_hash": "4a6dcb459b2b4c5914a7e53f376ac51c488a871e58c070d659dbc717c50598a2", "ref_doc_id": "2afa67eb-ad74-41b0-9eb9-cd5e2c06b3b2"}, "6bffc94a-ab80-43fb-8cca-0e93b93b3d9b": {"doc_hash": "aabd980f201b7ab4247c78ae62d95a1ce147f31d9f43c3b9250ecdacb4cce1d7", "ref_doc_id": "9ef4bb87-a72a-4f6b-957f-8767f3b01866"}, "f185b236-e9df-4afb-bc78-b2864d943ae7": {"doc_hash": "f7a244499a183c6985f7b47439c7169b60ffccfd06f70d887dd9f6eb82c53f9c", "ref_doc_id": "bbdaf581-ff54-4afb-8449-57a33cb9acb1"}, "9237f21f-d5fc-4800-83e9-81a26087d30f": {"doc_hash": "26b4422cbab9658e893e640c5f3dba6b233ab3e7e87dc5a343540dc5d0883303", "ref_doc_id": "138a481a-6bc6-403a-81a8-2f5c17190273"}, "87497604-ff97-4b4d-b3fb-a087170190ad": {"doc_hash": "9fcb17f5e45e731b6e0fe103cb2c0683188ccfe715e3e6670667ecd8dbe7226f", "ref_doc_id": "07f5054d-8320-4945-8af8-26da39e9bbf1"}, "9737deaf-25c1-49b0-afbd-67133394a021": {"doc_hash": "fee2b6f54ef021ee54050493f3a05a0806ac53500b27ead36593f555e8575dd9", "ref_doc_id": "40b9a1cc-cc28-4030-8dcd-b91b00b82793"}, "291295e5-b0f8-456b-a7d3-4e044aba2fe1": {"doc_hash": "1edae1a3427109fa03d8231b2d8692bf8469d477ae69c98c81b31168c357b624", "ref_doc_id": "40b9a1cc-cc28-4030-8dcd-b91b00b82793"}, "10396245-96c9-40e6-a013-eb2c0c13f725": {"doc_hash": "552d6b846fdf710813865eee3e5a9d42d9881b07d46982f5279633a8ba9088f8", "ref_doc_id": "3f07bb6e-22f7-41c6-b8da-d3796324b6ce"}, "7415086e-15fb-40d2-a53b-24ce9afb4acc": {"doc_hash": "f742a8d3435724035391360135b7d78eaac5ded8ef42202888a874fe45b1ec9f", "ref_doc_id": "66560a34-7c1b-4fa2-ad77-acda932385e7"}, "98f4af05-6d23-40bc-b68d-b752447acdee": {"doc_hash": "5105bce4b3a7d037ae16c93ad17876580fa43e7474b976f782124696130774b7", "ref_doc_id": "df21464b-c7be-47a4-9b6e-5fd8293bc972"}, "e354cf5e-1685-411c-9d04-65869e4ee66e": {"doc_hash": "50fe1e708f7e052761839ba5fd44f92e2b104e9d0379181dedb06eed077fc527", "ref_doc_id": "4412e606-8da1-4a92-a9dc-61b86050c16c"}, "7b563e6a-a32c-43f5-8c21-e393be6d73c1": {"doc_hash": "00df9c694432f2d9397a8c9d0a0448c5838f76a0c16f61c2536bc3bd9b43fe48", "ref_doc_id": "10ec693d-b0a2-4e34-a299-2843c6c3a32e"}, "24008349-7c43-42b2-baf6-bcf01ffc1048": {"doc_hash": "c5f68531a5cc3e2ba3e743b2c92ae45e3e707faeb9a80fd27402b45b8c89bac1", "ref_doc_id": "8758f6df-d987-4bab-95f3-f15fc101735e"}, "c9b1e539-831b-4684-949b-535526589a08": {"doc_hash": "3339700583ad8f8b52fb0ebfcba1f3b7b39921ee3388104c47fe88f736cff74e", "ref_doc_id": "f16b09c7-88d7-4b01-9dd8-fa2b76417ae3"}, "46d0b0cc-61a6-45f9-b645-e3875aabc7b7": {"doc_hash": "f516fbd51d15d0495e498b4d6aa4c5aa0582aa204acfcefff582cb45c24d4638", "ref_doc_id": "057f1af5-e93b-4dd9-a266-2e699aafde04"}, "3a984861-5316-4625-9016-10bec07df664": {"doc_hash": "e2ab05c987df9b9b2d4c3dda9aa3c3cceaee5ebdebe2d22d72f8911505a5fcaf", "ref_doc_id": "b8beef72-ce8f-46f3-87f5-7aee3b82fe18"}, "9ec96a62-c504-4652-845e-6b70c5b16c29": {"doc_hash": "518dbba3497ee44df91c1d1cd4ee7354bcc44a7778cd22831e69ab56d28ca94d", "ref_doc_id": "15507f04-38db-4377-b787-5f281d10192c"}, "41eed3e4-ee8c-45b6-8e65-25dbe683d959": {"doc_hash": "3f6e92a79d893770d39ea68a3239a8a24032f4bb28e872f095d914265294fc79", "ref_doc_id": "ebae195b-60a4-4ea0-aa2a-6e64e670fe0b"}, "f28e9cc7-1611-4606-bec7-4ee244b14b28": {"doc_hash": "bcdae84c0be841261978b7380fab15eca438c7fe9915e669ab501f352be6a70e", "ref_doc_id": "25d02b27-b012-4ed9-94cd-d3402e3f374e"}, "f5edc9e0-e56a-45a5-bb15-df95d93a907c": {"doc_hash": "86184a1fc013ca02c6e1388c394c95a4529c2a8501a30b4bd6886eea35e435a3", "ref_doc_id": "e2429523-b149-44ce-847c-49e8a30c4dfb"}, "03c01a7a-4a15-4999-818b-077abefc71a8": {"doc_hash": "1fe9f8fe1408d9c86617d0b40c1c457a3ff698903ddcefe1d09fee879253d460", "ref_doc_id": "a3058728-f533-4bec-ac10-f63bc0c64fac"}, "46ba7a57-3691-4487-b5cd-c83820fef529": {"doc_hash": "085c70bb9793f3e8221902d2ea401a8e4a7a8392b88c84c3fbffc537872bd0fd", "ref_doc_id": "7d7242ee-e0a3-4d27-8c75-32f2e7e1b40d"}, "146c09e5-7e89-48ca-8f56-513358b00b97": {"doc_hash": "af9fbf1b3787c7cc0023c965e822c459a577a3becef3ad6ec16994f59385bb43", "ref_doc_id": "1bc58610-56f5-4fe7-922f-7d2ad04c23b0"}, "2495de7a-5231-44d8-99b8-01bc460a2c66": {"doc_hash": "3d7c81fd6faa638aced544d1cf5494db2abfb58f32ca199775ac8c7bfe00d964", "ref_doc_id": "f316e2ba-173f-4612-bdcc-2a4c050491f4"}, "f6970b5b-6c43-4a7b-afd9-aa3092ee5321": {"doc_hash": "75ae0a05760dfec6417a6d38636965d66ed424c8fe2af771fb42955127991ec2", "ref_doc_id": "4b3cf393-5386-442b-9675-dc7e93a304c6"}, "4014de6b-3a7b-406a-abf2-a9cdcccd52e2": {"doc_hash": "162ad21d906cf50780b72f7acb18579fff73628936f93b4e0e73c0810e855a40", "ref_doc_id": "1dcdd682-3688-41f3-ba4b-1038cba79e00"}, "d1637837-4685-4bdd-aae8-3b0271964031": {"doc_hash": "1fad0ca3c53f382f34c5a4c40d8c5fb19f2710cb385ae15bf9a222b20c25bd5a", "ref_doc_id": "2a62d218-63b5-45f7-b38a-9ea517be099f"}, "c76e899a-a93f-4a23-833c-fdcff790fc9c": {"doc_hash": "9280bbee19eb09e04b011b417033279678335bc16102ef07c6257c1f0a6a0c0a", "ref_doc_id": "4fcdd957-e322-4f9d-adcc-b1725fdac1f1"}, "d5d1f626-49b8-4ec0-a839-04aef8c5cc80": {"doc_hash": "c59169f0b4194ce470454c1900b5d421b2468136d5efd46b7b90c60ed941e546", "ref_doc_id": "aeead514-701c-4be7-aa0a-5aa2f0d97e72"}, "2c96b25b-3f5a-4369-9ae2-100e93c32e32": {"doc_hash": "7d5223f73b34745e8678fa19e6632aeb7f19adeb799f30aaddedd8a35f72f7f4", "ref_doc_id": "2c59b9d6-36c8-47bb-b95b-c3f45b5d9ff2"}, "d4f095b2-c735-4141-a4fa-aa9798eac5f4": {"doc_hash": "fb7614595cc99e580d68fffa777b193c606d701dbf967d28dcaac697e232b9e2", "ref_doc_id": "bc31d22d-92d8-431d-a2f7-f317658115a5"}, "23546f82-a25a-4326-9d07-e05529a31672": {"doc_hash": "a6e4681c2696a4e75fe6fb0eaea453df3227ee6c629823e5c5c10bac3ad98979", "ref_doc_id": "d14e76bc-e91c-409a-9a11-a609bb01a8dd"}, "4978da61-21e6-42ed-8ef6-c9bd449f1f2e": {"doc_hash": "4860d8ea679232f87f69a7929ac57ba08289bd55069079c6ef871933586b4c1a", "ref_doc_id": "ae82100d-8a93-498e-845a-f1bd93600563"}, "fe71ffa5-dc19-43ca-a8a6-75d24526d12b": {"doc_hash": "6129a7204d30d48d82432c14e22da48bb20f0175754b23a9706139dd55110051", "ref_doc_id": "bd522413-28da-4ff5-8131-001f353b7a92"}, "81061db1-604d-4a22-bc1a-a362f6d6d8ab": {"doc_hash": "94548a274af5c9c3e1132b72b4519cb609e35c1770a3217eed6ac65318be1a6c", "ref_doc_id": "5c36486e-83af-438f-bd99-fe99c67fb514"}, "61c975e4-a365-4528-a52c-4ee4c60dda6c": {"doc_hash": "37dafc8f58669226e348c4642d88011820b240a5b7437821d3852832949c89d6", "ref_doc_id": "a9b4e6fe-0ce0-45e1-b1ab-1c665d54d426"}, "d0a5cf6f-db77-4a1e-b23f-37dd6816959f": {"doc_hash": "4b397d6d51930911521cf511680ec74f4b4b28cfe59d41881263876b7a37babc", "ref_doc_id": "c59d9ca0-7be7-4ee7-9e65-c3d26bd8bf2c"}, "813c32fd-8061-46ee-8f56-b997ac751fc6": {"doc_hash": "6c2cc028ccccd2fb759e962ca1f62eb16796309f49824b696666c7df481b4d6f", "ref_doc_id": "72f1d33e-6577-45a6-a546-38613b8d35bb"}, "4e3c21c4-4203-4c46-8278-17ed9b06c913": {"doc_hash": "9d5c18b07b8035ca40a897bc7b6bce2f3e0754c104e9c89e4e7c484dd8ee0b12", "ref_doc_id": "8aef010f-d6b9-478c-8951-a1454a2b7aac"}, "e9b793d8-48a7-4cea-86c6-9cbd901fca1f": {"doc_hash": "0994ff22208ae22fd4d90c76ae7a5a2d9721099d9fa969d02f134cba1c00212c", "ref_doc_id": "8f4891f0-5e7c-4ac6-9e30-df58d1c69f90"}, "ddb55c98-adde-40f9-a75a-ed989d620e76": {"doc_hash": "cb1a9dcda4c29862053f5ae78d0f1408ec177c555f8ec5198070b73f45ccdc67", "ref_doc_id": "f962c606-28d6-43b5-ac10-e20210f42aa8"}, "a50fa191-c7cf-402b-bc10-46c4a2f04071": {"doc_hash": "b3ab1559717ee9957128749e5a00c8db84787f9793b7a59a0a3694316b1587c6", "ref_doc_id": "adf9f41b-d5e3-4793-b35d-cfe305c0465d"}, "fe7a65fa-04aa-4e4e-b697-9b3d03a744d3": {"doc_hash": "ecf6e36847f1dae2cf56853ab0efef59f19ea116bfdca71476e96d9902f668b7", "ref_doc_id": "2463e660-25cd-4e1e-afe3-78d78eafcbde"}, "bc112e93-414c-4d61-bb88-7efab9e2277c": {"doc_hash": "f05ddba066fbd3c4fe30e10d86a6dca8b8b3b3bea7686b2b387a3a7c3a88de52", "ref_doc_id": "0c36437e-2907-4549-bdaa-9f0b901f64ab"}, "b808ee3d-6754-47c3-986c-0ca58d5d2507": {"doc_hash": "1e9325647c5b8709cf195b1c355527b71de65a5b54d57968746aabe6455faacc", "ref_doc_id": "4af66db7-e787-43fa-a725-ba6d67512629"}, "a152016d-1484-4154-9491-705f69feddd8": {"doc_hash": "f2efacc4fb4b9b7c379783d482a57c1129286a348bdcd5b150532b24b29b97eb", "ref_doc_id": "7f94ea95-0283-45d1-8a81-38da77061a02"}, "1fb27db6-3aeb-4de5-870c-2970b0315647": {"doc_hash": "210f2ce41bfade7ff90d986f5308c5db52ce123a7ae230a248ad575c12107205", "ref_doc_id": "3b49b232-5373-4b58-b0c6-4126c3dcefeb"}, "42be38a5-6c0c-48f7-9365-42259c6be171": {"doc_hash": "b704c77c4568050b56b9391245288cd877f2611e0c09578ccbd2897626b8c45b", "ref_doc_id": "6b1ffec2-f416-49f1-91a6-6638a3707ebc"}, "8bdb543e-a7b8-462d-9bf9-d3f6cd8670a9": {"doc_hash": "4e54583b5eb1bb663cc7479c46edc0747193532db52e10faf0decfdd0627992d", "ref_doc_id": "c896ec5f-0a2f-4aea-8f04-6806ba251529"}, "168b83c4-ebc5-435b-86e3-33a4368ec5b8": {"doc_hash": "e9b9369be808d9e081bddeac83820819944dc5eb8aa9c962e432545493bbc925", "ref_doc_id": "69dccb96-3af3-4f1e-b23b-96d9e2f59e6e"}, "c0bf57af-e364-4236-a1ef-153ee4563a70": {"doc_hash": "5c8e8bccd9876bc3961cf2fc654753f7e9148211507a1dde5f95b4ec8601f376", "ref_doc_id": "60c3a714-f332-45f6-937d-819ad4723549"}, "590bbb30-6772-414a-83c4-e58aa98d4753": {"doc_hash": "4c82f29c6934cdf88a956af4f505c5e6acf697eb23d4483ad32aba2bc662aac0", "ref_doc_id": "180fa6d7-f31e-42ee-835f-b6c88ab890a4"}, "c3d62f1f-5df7-4dd1-986f-e1c628481dbc": {"doc_hash": "29d67cc762d73329d73a5e6ac493fdf4128c8110f77683085ef8b0d553b4ba74", "ref_doc_id": "acaff5d2-c796-476a-8f22-40a9fcfc9ac2"}, "12e66299-4608-4caf-8592-fa617027b782": {"doc_hash": "105576135a7d0c7f0c8408d2c3c8f19903c529aba40f1337f6be629bbd799007", "ref_doc_id": "869f0e38-da66-42bf-a449-f40a48963754"}, "09ef6012-468e-4d9f-bf3b-62dcf9d41179": {"doc_hash": "4784e2690e2cc840daf2151b6db1da53d969272671a86a658c20abb1db52d966", "ref_doc_id": "a61ec00b-17b6-4540-8a98-e5fe77d57d50"}, "cc71cc61-1640-44c3-8acb-0dbba54498f8": {"doc_hash": "2c6aa36190c9347e4a090adee2f574dc480b31ee5437b35b5da7609349a0b71a", "ref_doc_id": "557d23b3-4b99-42a8-b7c5-69683ef71182"}, "e8db72d1-59b9-4e22-970f-4d6b02a1731d": {"doc_hash": "d26f9ee5e72e9981e06fc946036dab5c2c4c7e9c6b975f4d5ea41a6821b81d63", "ref_doc_id": "536a7f32-9a7d-409a-b6d0-b98867339186"}, "70b45da8-9608-4d17-9923-d63e58fa0fd2": {"doc_hash": "7ce49bfb4ba8f693137411b7af19abf8e6435eb7d19f256d40be50516c9c1d77", "ref_doc_id": "1093b147-40d4-4059-933c-184a597b0c74"}, "aa081668-3de0-431f-b7f3-4e56fd1eb9a9": {"doc_hash": "cf781394f928aa1343913743595b869a31b57050000393faa658294172070f59", "ref_doc_id": "b7ebd51d-26d0-489c-a0e9-4fb094d04361"}, "16f4e90f-fb76-4e6b-9ce1-683e850be6d3": {"doc_hash": "dc60a2701d8608cfc32a6fd21b08c469ba616d741545746de63d7b9c6d5584a4", "ref_doc_id": "6a28a010-75c5-4014-a837-af578a5eea41"}, "957f5aeb-9579-43d4-bad2-4adc476e3a49": {"doc_hash": "4b80608910cb822f78bec9373681c2a063e174d8dadc14acc42f3d7b32318455", "ref_doc_id": "8de3e4ce-4634-4794-b0fb-f57dffb74aa2"}, "30557e0e-3173-465a-a8fe-7d27a34a4401": {"doc_hash": "2738b551b590b61049daabefe3a8c19f1c118196193c58ebaad3cf03c9c4c731", "ref_doc_id": "6f3d497e-d30f-4a9c-96d9-de7c81642a38"}, "0cd5b501-ed12-4851-94b9-b0ec33ec350e": {"doc_hash": "272ff25331055a2f9ee7ee6948e0401224ac90a61e539c124a09fc78ae9f31a3", "ref_doc_id": "b4661db2-b657-4f5d-a83f-fbb2b37f1a23"}, "ee9a83e0-a25a-4ec5-9dc9-b3e4761ea42d": {"doc_hash": "2dceab8457970c4821d81e583522ddbad45af1dea1e6e07ec747f6def4121def", "ref_doc_id": "a22bb487-4bb7-4be3-ba45-7b85994080a3"}, "ebc15953-cd6b-484e-a055-d6a398350d27": {"doc_hash": "35b62d8099a06b5882107020415367ea2a0500077214e6820b50298e0d065cdf", "ref_doc_id": "8bc7d0bb-11ca-4e39-8897-1aa693a5d9ec"}, "624f3045-0292-490b-93d6-87c7104acfd5": {"doc_hash": "79a68982206f9b0158c7a35eb8a6316ec7fd1251321377c6437a7b4ddfeaf0f4", "ref_doc_id": "1751a2c0-07c9-4f6a-8525-243780e2ceeb"}, "bd860081-b3ed-4936-a7a6-4a888869bd99": {"doc_hash": "66ec362603cdded7c74ab7d43e5a68d12d3a9ce6ebcf0019c1c7fddc6e046a86", "ref_doc_id": "13d27492-0bd7-4cdd-8fba-c815d36bf260"}, "2db0f05b-01c6-40f6-b7fd-926c6609a282": {"doc_hash": "b7e8e4f6b7d0d547d55a4240c9b0cd414fdef5ef233bfc8eb694d91afd0e6ebb", "ref_doc_id": "603b96d6-97da-459e-ba51-3ccdb83a3663"}, "e7cc1021-af56-488b-881a-431b808a657d": {"doc_hash": "92002a9e36e2ea4727f26e80da57051d4ae37903e68a88e9e1e1f0baf246e9fe", "ref_doc_id": "d590e653-6e6d-4ed0-80e8-d9aef585bec7"}, "2adc12c8-f956-4ca8-8ecb-6328660d645c": {"doc_hash": "848e3f1426dcc42b281a567d17b547aad17ebee08adc4f06885215dd4e8f2104", "ref_doc_id": "e7d2da3a-6b4f-4bd7-9f8d-6712343642f6"}, "45e28d1b-2679-4b2c-b6f1-682563c936a3": {"doc_hash": "854af16871a9bc1768da78c5372ed7d581414b680ad2e6d542efbd7e6dcd2605", "ref_doc_id": "01908d4d-c8d2-42e1-8049-f2640eecdd99"}, "f3925c94-975f-4a1e-8161-ab3a08b4c9da": {"doc_hash": "ef46a7250bb035f246d7ff94f98a251ca7ace0b808de12ca56741075f46a3697", "ref_doc_id": "ef346a62-a45b-49fd-979f-c43f84a93d6c"}, "986c666b-ebb2-4bc5-bb6d-8ca33d636af7": {"doc_hash": "fb510905b1fa35526d8c04c695ff56a24cf2464854647e7757b739ae7b37db64", "ref_doc_id": "35489d90-8530-4200-af4a-c7cd129d5f74"}, "9ddbb48b-24cf-49cd-a599-e0fe678a95f2": {"doc_hash": "3aad0dfa1e9e3fd112d03e68c6c198d416b8e5c33adfc46ee749bee888d1503e", "ref_doc_id": "8ef7f518-da59-46e5-9185-1c660a27b9ca"}, "0394dec8-a084-423c-a812-26295f9af42a": {"doc_hash": "29d84ebc052e43592eef984fb93856c889fa9319e7d6b4b955e458d88fbee464", "ref_doc_id": "5007ad2a-5776-4f0a-8215-66598905fb4b"}, "d6a62002-712b-410e-b7fb-2e792d4d4260": {"doc_hash": "29a5865eb27286c000f429f690732f582c6c39e4e6fcc0441d90d5b5c606d9de", "ref_doc_id": "cf876de5-32bd-406d-9907-9f5b44863f4f"}, "55acb479-9866-4b9a-926a-6b28c59e2d4a": {"doc_hash": "539e3d8fc449a6a9dbc6e3dccf9c239f37b58a1c2dba272db5ba88bd262a0f36", "ref_doc_id": "22cfa8ed-f7a4-4c30-9880-cb2f7d833635"}, "14a637ff-0e83-4d4e-ab69-84fe36e1201d": {"doc_hash": "1eb0d71f55631957b607c9830c0a42ece1c8cec24ace9eaddef086c02b774b57", "ref_doc_id": "758942b9-e579-4cf0-abf5-0911dd34133f"}, "28a21f32-3532-47a0-b0f9-42c678e51f56": {"doc_hash": "2633a808db9a1200958f30b4d71e92c5d5d404d41b204a2a4a73dab13402bbba", "ref_doc_id": "a96f86ba-2083-4595-88dd-be386ea2d8d0"}, "f3e50ea8-4c49-4575-8c72-3a2521da661d": {"doc_hash": "82b5df5ed6755539a5cbd0bac7ea782568085cfbde47005eb247f3bf9c54f57b", "ref_doc_id": "504aad8e-0f9e-4734-b857-341512dc0290"}, "254eae49-7748-4606-b0cc-bf1abc648c73": {"doc_hash": "9d1c1d5a10fbff474b1339ab964bffcbcec91361641a8e8dfa3de97a916e3c1f", "ref_doc_id": "3c52983a-6a98-4ef5-abc1-efd71cb6ec04"}, "49657dc5-9a37-4393-8fcf-72fa9300e991": {"doc_hash": "af47d5a30c5739b1d31d1a4ae0fc483c07578bb9bd75fb3a47c2dbfec9324974", "ref_doc_id": "3f98685c-888c-4bc6-99e3-c56494c5b4f5"}, "4b61695e-1cf7-4df0-ae20-58f1736ca80a": {"doc_hash": "b0d9f9d8485141fe5ee837e9e493496c993c45d90e086cbc474a0870f270a30d", "ref_doc_id": "8447d23d-fc15-49a7-9c56-d6f6ec10d158"}, "e1611f22-9e94-4a05-bb43-d4ad915e1708": {"doc_hash": "7a28b7f6d75b175656fa89c1c036f9587f12fecd9c056e9cf6723eed073c9099", "ref_doc_id": "81494ac2-ec6d-4057-8c61-9b7436e7ad26"}, "3060fbc1-4e87-4961-ba77-3e61b3797c9e": {"doc_hash": "3eb64858765b1650d6f9d82757452625b15c5769319d801b0c1f63b438adfbff", "ref_doc_id": "f906dfbb-3fc4-4f79-9960-662e8d7d420b"}, "dceb3e93-456b-4ffe-9c05-b66ee1b4541b": {"doc_hash": "169948c5a929038271397f12c18d71351df518058cec808f3d87a907d519ce9f", "ref_doc_id": "acaab71d-b86b-4efa-aa4f-89b766603b61"}, "06fc6a2b-3fbb-4364-8cc8-3800f45b3bb1": {"doc_hash": "3f30662d777c9d76da2327970d364944c25f8d686b63968d2bd59364af51aeb9", "ref_doc_id": "d2d95939-b5a1-4964-ab45-6ef117f429d1"}, "32f9d7ad-f5b7-41e4-9d4e-98c32f33b141": {"doc_hash": "f1b010e35ec6057fa38493934f4e51f1c83e0046847d44eb25d991da5d1f8c43", "ref_doc_id": "a18084ec-e602-4289-a815-065d3a9463c8"}, "1b48f23e-eaf2-4d85-b657-cd09cb974b41": {"doc_hash": "700a78aebc27e4d4811578093acff51cfcad7a9583c33d9db17e7c3ac1a0b09e", "ref_doc_id": "c416c2a7-e4fb-43d6-a561-4bdfc099c90d"}, "7fd2281c-8330-48f1-baa1-2c29423e5511": {"doc_hash": "cb5107efd65b33867e691b8ca2d8fb76a25bb646c65f179736be2b4c460cd206", "ref_doc_id": "0d775c92-a4e1-48ca-98aa-a483248ffa96"}, "b5795ca1-c647-4be5-ab7a-ecd46fe3a38c": {"doc_hash": "fd41d2ba91a945dfa83ac70c4555dccb21f8e2a2b72f7c4d7297a8155a0edbd7", "ref_doc_id": "ff9acfb5-0f0f-41ff-aa0a-707ad5be359c"}, "c36fd0bf-5302-4023-9adc-5f0b61a53fa7": {"doc_hash": "1d9941b7f597b5cd1e707b9aeb5086ca57ec685de9dd35b0b094d8d7369d101e", "ref_doc_id": "55492c51-b24b-4279-8a54-9c01271b75f9"}, "1e0d01fd-019f-4dc2-a995-902f1048ff4c": {"doc_hash": "6781ad3083e4578b55e73cdab2f9985b49ce028507dc0a66c11269170a209e36", "ref_doc_id": "bc915d77-e2df-440d-8d5f-ed10ad2a2d4d"}, "7fa30983-9c67-4321-befd-ff90a04122b8": {"doc_hash": "cc67b22c183ae74b72d519b76077d8fa38f9860e38906bd6f4b5825dd0c331fe", "ref_doc_id": "f27ef110-576c-468b-a6ff-9f3ff0b79902"}, "80ddb8ea-84aa-447a-ae87-d87b0ed33ec0": {"doc_hash": "1e8cf46aadbe9b570db2b6e56fb68312b9a1e5563194ba9d866904476ef322f2", "ref_doc_id": "b1e923b2-a931-410d-9a5c-ec139dde008f"}, "0579e9f4-e3d2-445c-99f5-24635594b890": {"doc_hash": "cba04f529df8fae20bb7287b535e2e311ba381e746acc3f1210f0439ffeda55b", "ref_doc_id": "1452b476-5ebc-4dba-918a-36fa434eb4f7"}, "5a8cf1fc-890c-49af-affe-c8916fbfb7e6": {"doc_hash": "b0d79d53e3c703844c25306db4467263e23d9bbbfc6010cd2688dfbb22650d75", "ref_doc_id": "45f89314-f1a8-4e77-b0a7-79e40b747f09"}, "37c21c61-d9d3-45a3-8cb6-3d951eabc55d": {"doc_hash": "b991e0bb6962bfe61d9686d1fbbed7bfa7c4fd5648249d8bc66a8f75b927118f", "ref_doc_id": "1b41ca5b-78df-46e3-9b66-eec35a6e5696"}, "3ac16900-04f5-43d0-998b-c250211bb3ee": {"doc_hash": "eafb80a07bc11985be830e72c44ad29e3b22d3586a60ed9651d85f402ea361cb", "ref_doc_id": "bcd1d024-9d52-4dae-82d9-b775c32a2ea1"}, "a68e83bd-6689-40d4-94c6-f575dde9551b": {"doc_hash": "140d3558495409512ba66d66b38112093d9793360a55a24deff14d35ce313d4c", "ref_doc_id": "b45a8597-f74b-4a7c-b3e7-56db07efbb41"}, "4d845ca7-5573-4db0-b141-edf6e5d16bbe": {"doc_hash": "48b2100ffc77d13f10fb37b98937981ffb252b6bf4e29e47d6be10c61a182f9c", "ref_doc_id": "5e423553-067d-492a-9b4d-6fab9610a934"}, "29de0072-9fb1-422c-8397-0097cccd5c16": {"doc_hash": "11adf3e1b6067ac43accb42a0d3cb71893c723c9f57084bced827d000b0b0641", "ref_doc_id": "77ed86e7-d1c0-4679-b11c-61a720a686cb"}, "f52cbded-985e-4363-afd1-9073a592fcd0": {"doc_hash": "c97726d17d0757b033d9622cf3098386a81fa77f8e92166f897fc8a7ff5d7bae", "ref_doc_id": "b2807ba0-b90d-44c2-b9c7-3188e9e9ead7"}, "bb4de0c0-5ce3-400b-b5b7-26592e0b8d9a": {"doc_hash": "790be0d21c4f852aad516e40826718a0895e41e4baf938fa9573f3b4badff872", "ref_doc_id": "59ae4b91-05d4-49e7-a8de-e5c79b75f98a"}, "5f6f3059-ed46-48c7-ae2c-13f53c878683": {"doc_hash": "22aea9bb47fab93a94e1e6888071e392a96ff8239d64079c833a4f27b054d11f", "ref_doc_id": "614df1ea-acb8-4f1f-9310-0a5351b8f3a6"}, "8ab5cd7a-9be0-4bfa-bc2b-fa5852c23ea0": {"doc_hash": "b157ee1eba64fa37dc18320347cffc25c268d73c1732ed65321f74abf82ca725", "ref_doc_id": "30dcaae6-7c4e-46da-afaf-d0bbadf89987"}, "05466cc6-b42b-4ee2-a36a-09a121c43850": {"doc_hash": "1577502f6c41e7ee64276b9a6b1ac79a365f1f3395f5232eaef36bd4f4df68d2", "ref_doc_id": "5dd6a315-7ca6-4496-ad7c-2eff081c209e"}, "76a71639-4d25-4409-941f-54820b4353ba": {"doc_hash": "318eea1c6d89686205eedfbbf71479c4b9a1a07bf02494d7375be3f3f6a4877c", "ref_doc_id": "769dee6e-4071-4a68-9c2b-ff0e351b281d"}, "14236b03-0bd5-498c-8ac5-7cec66f8f19c": {"doc_hash": "5b0cc9df69cd9bc17f85fa38134c699da2aa445afee798b6ed38b9bc5c9e7359", "ref_doc_id": "cabbca11-7bd4-4788-8231-41180c3d2191"}, "11b0ab22-4c2f-4e2b-a463-9a1d2521e86b": {"doc_hash": "20379749b3206e094488351b9d9db3caf1851dd9ebbb70ecf65d3329a0d5e238", "ref_doc_id": "42bbd4ff-4afd-4a72-aab7-22644c0498ef"}, "3d469b8f-a26b-41b0-9e26-6b7e4b6a89e6": {"doc_hash": "7c1e332fbd2f68a3b609b907561182a5f09e4816777530d1b4ddb20b995d7705", "ref_doc_id": "b420f8ab-1fa2-47dc-8b7a-3f40fc7ded0a"}, "ccd51dc1-643b-4b1b-a589-88eb22bd18db": {"doc_hash": "a1345eaaa3e52ed55184b9d7cf8df68d609c5bca9cdebd222d1f40de24df0bbd", "ref_doc_id": "d851f174-6c48-4c92-8d21-01fd0d60047f"}, "bddff48e-9caf-4123-bb01-690c5c8b01ed": {"doc_hash": "d6a7acbe429f3fafb8da212288c167d6d1d805244990acf5a9f21bef440e612e", "ref_doc_id": "4e83c8be-7093-44af-a08f-9ccddcf8e5b4"}, "6064286d-8683-4b91-b499-10d4ee9271c4": {"doc_hash": "93a770c9bbb4409036b7731ba9e86f9fe4e14e722c152e14918e932b589932c6", "ref_doc_id": "97cda3fb-7eef-4871-9303-223ec37599df"}, "e689b712-e50e-4f59-9b21-324e5ee38131": {"doc_hash": "809ac7f118371723dae12fa72e10c7ba6d1bcfb87be66534e96470f4c0a237a8", "ref_doc_id": "ef39870b-c6cf-48c3-9dbe-d52fabb4f8dc"}, "65e9d45a-e732-4a1f-bdef-2a99e08616df": {"doc_hash": "3cdf359cc07a09e1b9c72919189436cc6973365d1621cba5feab73b42c37f242", "ref_doc_id": "13c72ea9-01bd-48f0-bae0-70e041f91303"}, "2addd8ea-b729-4902-9a4f-3747e8fbde68": {"doc_hash": "110fd2bdef2709705dd62698bef43ff5dc07d72ca11d007bcf1fae08f73ec790", "ref_doc_id": "4e1c2e1a-91be-46a1-84c5-f4e2539b1bb5"}, "015fa942-fcba-4baf-9034-abde3cd25834": {"doc_hash": "efcefabf0743407fafd8c124288ef7c85433a5d86539a2f05dd82159bb9126d1", "ref_doc_id": "365ae8ec-3416-4258-a94c-262e762ac13d"}, "3c44a254-aed9-43af-a7b9-8569e1f91a8b": {"doc_hash": "11fa50a014cc6036f4c7de3122a148ad4c3000fcd61d8c636fa902a94c0cc1f4", "ref_doc_id": "67ce87c6-2b05-4724-baa2-e4465c0b1b3e"}, "1b23afaf-d168-4332-a082-e8dddce986f4": {"doc_hash": "e4f59a8f6ba8ba50dffed808be31b725fbce8b91101075133066d784f76fee09", "ref_doc_id": "97ebb57b-dfd3-475f-9533-61f427719cfa"}, "7639b672-c2dc-4a9f-9bb6-2bf69e12099a": {"doc_hash": "118fe24dc64796f735bb23cf988e0101f5eeebf4da1996c3b31236cc7d0d22c8", "ref_doc_id": "d5e68fae-a5ea-4609-a1c9-eca90254087f"}, "6024d058-6e00-4a6d-9be0-36929fd94629": {"doc_hash": "c276cd56c86b7db37454eda32ff38196457991dbea10ab58e42d10a0f98fda09", "ref_doc_id": "86c1e73a-1941-45c8-ad8b-19fc1e7ba195"}, "84881dad-2ed6-4848-9f96-ddf3798175ce": {"doc_hash": "308c001a8fae3083c218652c76ea694819d0ef2535d0ca9582daa0f5ed52bd36", "ref_doc_id": "c85bb471-d654-49c6-8deb-11d545d8fa1f"}, "571ef21b-821c-4eeb-8cb0-adef7773b524": {"doc_hash": "f0b26991155231510d9d25b44f6825829d6836d3d75a18489a385686bfc81d4e", "ref_doc_id": "0ce6612b-71b1-4463-83b2-590fa497db15"}, "cee5b5c9-8bc6-40b8-9202-211f128b9280": {"doc_hash": "54cbd9283e8e6c14081eede1614b8efe84953e949568637303105f0155f39748", "ref_doc_id": "8ab767c0-571e-4650-aee7-c658dc20b3f7"}, "5f4f41e5-4b82-48b1-b78c-3e9fb16c6e67": {"doc_hash": "1b6839fdbdd76fd4f79709abdc626adf951d19bcd0fffe5011dd3bfdb3a3a183", "ref_doc_id": "a280a15a-39a7-4362-af67-477e2cdf9175"}, "a688b663-9e5c-4989-b1db-09b70d6e5893": {"doc_hash": "a5bbfd5ebefba4da760ea36751ed4b3c5b2413fb52d31ae585f3de961562bd08", "ref_doc_id": "31ad2038-8762-47b8-9cd4-f0a45f1f3ad7"}, "daedae2f-2fd7-4b3d-a6f6-1e4cc4b7c59c": {"doc_hash": "34f8f01f28a156aeacca6ece4061d94d210bf41bc4c2911b70622fcb71358ae4", "ref_doc_id": "040cb028-0e9f-43e7-b69a-952857ba6040"}, "ce583192-0484-4e10-868d-c445c8e2517b": {"doc_hash": "fcce8e4bc47ceda5bf9720254fc1b81f611e36d998653e060ea961ed861872d8", "ref_doc_id": "8ab952f3-e453-4e7f-a756-8e734b4d1d75"}, "93547004-12f5-442e-a393-2a8b02f99aaf": {"doc_hash": "ca9ceddd7ed977283f3af041ccc184606272a5a66f427c38fe83cc81162acd06", "ref_doc_id": "003de270-69d6-448c-baa2-6b7e46152a54"}, "267790ea-e84b-4f31-acfb-c44038774e76": {"doc_hash": "8a331af12e75373337d4ee506383a4b5dfce7f412a49b31cd54da31063a727bd", "ref_doc_id": "74f297da-b32f-4645-8dfa-1dfd40bc2a55"}, "ac4471b6-e7e4-4406-8dff-68e58a4e498e": {"doc_hash": "eb1269c374b7a560b185208bc30145d7268eee02551e8c42b8aa7dc8f152a9ec", "ref_doc_id": "4e88aa98-6890-4e4e-b229-6a1c70407791"}, "e3a5f1fd-539c-4d4e-9731-cece5e1cd013": {"doc_hash": "77da45229084fbd5244b467905eb8bb46cc41e4bcc85339c0c98072baedd4c64", "ref_doc_id": "250a65c9-cec0-4c1d-aefb-0779d5e80d4a"}, "a0418dd5-d840-41c2-9687-9f8c6cb020a1": {"doc_hash": "07f0cea3930446aae3bc2a681672b554c8c92a808f4c5a7907ff88538d8ea977", "ref_doc_id": "e60f9c84-8c93-4125-a5c6-114a305d7829"}, "b7b752a4-fc6d-423c-94f1-4066d5c3cbc4": {"doc_hash": "f7e0ae2140ab843427ff6989782c9d6e7b1ba5c56e954025c0e62e0ff8f0ac5e", "ref_doc_id": "e3e2c7cb-9915-4558-ab7c-bf8620bb8f6b"}, "6a8537b7-e4a8-4b8e-98b9-93b5c3ff4b86": {"doc_hash": "401e3ca3b21ce7ff3134b40f9a601a3f116fec379a857ef45241b016f415443d", "ref_doc_id": "c8dada01-b453-4cc4-921c-ecacaef9e95d"}, "20389d11-730b-4f3c-90d1-2f545a706d9b": {"doc_hash": "433c813d4688d6fc323c3e39893376fdedb6a84177718c0d0abee0d41cc15bed", "ref_doc_id": "ae626a43-3f20-47e7-8aae-eca0c0137fe8"}, "132c2436-b060-45cf-8656-a983307dac69": {"doc_hash": "914dd76b8b3999e5bc54ba609b1c009ac42dde177b3d7cff16c313c3295bf55b", "ref_doc_id": "6be54b0e-7ab1-4bca-88a2-09782cf597eb"}, "5a571a73-2a7c-4fc2-9b64-cb2f50d1c5d5": {"doc_hash": "7e051141cbe577720e4e610c77074c1c1ba391caccf12f01e776c58d65b39caa", "ref_doc_id": "0068025e-e003-4fdb-b264-48aa9b734964"}, "b7876197-3dd3-412d-b3e0-380f2c932421": {"doc_hash": "326c9c8095c3b55f00edfae05b198af158aa9903b961d12d876913e142cdb205", "ref_doc_id": "2a0cfdca-432d-4333-a58f-8cf82428ea06"}, "177fc440-88eb-4b23-9d62-5ed5680a3143": {"doc_hash": "754c43711094662aec227438e3b77fb70d37e833f98192bdd27206d680208d7a", "ref_doc_id": "2d135c14-20c1-4295-8581-8af2fb6aff76"}, "8ddb5efc-2977-4657-812a-07cfb19fdb1c": {"doc_hash": "578fb87098c8a312096cbb80e0483e217f5ce9225620126359d255e91d5ad043", "ref_doc_id": "ae8ae80a-9450-4928-aed0-21944aeb8281"}, "388556aa-902b-4591-adb1-70b268285804": {"doc_hash": "f56d54f08fb25377457203b6885ba0d568140f03474a2d365e80ba8a7e6cc243", "ref_doc_id": "74511e02-97eb-47eb-9152-384ddcb7ffc4"}, "53e93d68-a103-4e36-a0c1-f00814c889fe": {"doc_hash": "e8a9f1341738f0a93ec91b83f5413f4a3b81b3c22efda11eaa68f80acd679724", "ref_doc_id": "688377e7-6469-47cd-aba0-04ab8b5bd2af"}, "bdd2cfe3-b1fa-455f-a323-d11f777119b8": {"doc_hash": "6604b8287d49589411cf4851fb6800f63e9c12617716f446bbadd64d9a7f782c", "ref_doc_id": "b43ac4f8-4158-4435-a615-d10bbae5470a"}, "7af4a5f6-01f5-4b01-bfdb-864a1287f2f0": {"doc_hash": "0817443e53efd811ec00d3d2ef3c19f84876415d4b1fc56db4ddfe457765bff1", "ref_doc_id": "d8a6a125-77e5-412c-ac7c-e950fa6c434c"}, "8d0cf81b-7a9f-446f-9ac0-0121ab8a563c": {"doc_hash": "aed0e6f12ebd0ffd8b873642bd2ff75b21dfd25f69203c9288fe95ad70a6cd1b", "ref_doc_id": "bb9deeaf-0a2e-4308-a3d3-d1f84da6052a"}, "e9e45f97-1bcc-444d-8fbb-65a2a8f6861f": {"doc_hash": "344a6c48ebcedd8d6f7051934ea79b0b995c11eb87d980983a632c88b737803d", "ref_doc_id": "1c5816d1-9597-4aa6-a87a-0b82f79a41a0"}, "5262af32-5f69-4ca9-ab95-e8717ef7aed8": {"doc_hash": "a73268296aa243ddebddf13370430bc9ef18ff8e5c747f5cd97c2da2beb55501", "ref_doc_id": "effe889b-e92f-4ab8-a494-425454bae218"}, "3b64a0d7-82fb-403b-aef6-606b3325bb29": {"doc_hash": "ca18dd6a3a3d1bdb00f66bf175c8ef14d309677d1389742c2e7a5ee22b394591", "ref_doc_id": "41d4b649-8cd9-425e-af6d-2e8e7e1b377a"}, "1ede5beb-ab92-44c9-8c56-e70706fa9294": {"doc_hash": "781884ef96b9b59962a0fb667b028f812ca6573716b797ef171a1d1412094020", "ref_doc_id": "8ce0863d-1182-44b3-ba89-e9b3dd6a74ec"}, "144603d2-4edc-4ac6-ad47-0707cdd019e4": {"doc_hash": "3fc397a861c0d76d7cf200f99e04f7e20db737a1364f3af1cc71b92fd08b0520", "ref_doc_id": "017644d3-1264-45bb-a8cc-3707e157c066"}, "bd849597-5990-49aa-aaf9-b35a05453887": {"doc_hash": "9324f3f2815d2a52a5734a336e06bcb6e25d30355e05bd04ab18f4163ca6edc3", "ref_doc_id": "723daee9-6cc2-4035-9dff-5e8faa2c90e9"}, "b21800fb-f7d8-4b4d-baa6-b70c06a421c9": {"doc_hash": "b0e10b791fdf123782eaf712a3dd051d0f9cb7314882e203407ff66cf9c1ee8b", "ref_doc_id": "46dc5006-f6e9-4704-9b13-1adab09c3c38"}, "862835bb-a602-4b05-aca2-24810d11f42e": {"doc_hash": "976a6d6cb6f6c1a6298d6af24b7af5a3725aab9a7497e2187dff519ae2af19dd", "ref_doc_id": "28ac059f-52d7-4cf7-89eb-25746df08244"}, "3cd32ed3-23fd-46ba-a0be-c12d1c67ff6b": {"doc_hash": "2242152ffd7fe8f255cecbfb7019cc3bdecafd93fa139f9d4a154e7aed2e4abd", "ref_doc_id": "009dc4cc-327b-4f69-9585-8c643b6ba683"}, "c576d618-a7f5-4368-8cb6-9c86e5f53142": {"doc_hash": "3a9c9ea6cc0bb9343bffb49f0617ea366c4565894c23b67003a43efbab4e4b7d", "ref_doc_id": "84a2f68d-335c-4f75-9586-1b7083af95ca"}, "53dda6e0-5bd9-443f-aeab-6a79b84ce0e3": {"doc_hash": "c5e9e1f4a0a2d2c2688b1bd75d1e461f1233b9980b660f33a2a1398534315049", "ref_doc_id": "17afc4d4-c3f9-450b-ad51-80711b343dbb"}, "1f095d4b-1562-4aec-aa14-7219a4854076": {"doc_hash": "b6ffc4439f70122bbcd4e571e20773f33ea22803814c182a0d30b4d39fa42a7d", "ref_doc_id": "002dca2b-5dc4-4eda-8fd7-26af71a50609"}, "4fca823b-3065-4e03-ab9c-8622fe850f41": {"doc_hash": "ef5448efea8b55ec3b5d91205faf49547495b91b4f0b53ca6b7f8189f061a756", "ref_doc_id": "1b0ba0db-1592-43fc-a473-689847f3a2d3"}, "8fc9ca66-984a-424c-bd1b-97fd672fdefe": {"doc_hash": "9cf4b8460fd6bb1a2f9de0fb597f87f38a6cd5fad53709738c609220db8d44a9", "ref_doc_id": "890da9df-2f34-4a22-9326-1866503e1392"}, "7d1d2b34-57f9-4f05-8f4b-f687a615ce9d": {"doc_hash": "e10de8069b7d5dc5596a547cf2827d2f5af908ce07ba28ac5cae264a224c5148", "ref_doc_id": "e42f5fe4-746e-439b-8104-ee4e3a42cb75"}, "8ae1e5b1-d0a8-4152-8e8d-c0f637051206": {"doc_hash": "baee577b7df79acf5a935e657c2267ddc3e6779efe3bf8febb41c186b9a01629", "ref_doc_id": "790b9be9-7901-4e0a-b87a-0af38770246c"}, "d9ed022f-53ee-414d-8906-8544035c8f2a": {"doc_hash": "4f50663e11c148b7709d8d0574a4210c8269c134afa4150e40e6ed44cb546f5a", "ref_doc_id": "9b46e1b9-b032-4b6a-a9ef-0a4a5c4e138b"}, "aba59c68-d4eb-4938-b212-a5e7ef22ed01": {"doc_hash": "1030fbe92914dc6eb1f461e2ac94b05c233c71f4a6c77f67868d98e930e3d669", "ref_doc_id": "c02fee95-bfcb-48ef-9542-9fe18b63c0bf"}, "3f6ab1e3-d371-4d13-b768-5b8d7128a4d6": {"doc_hash": "eedefcd2a0d9be1cace81169ba63fe02f92a0716858f4594a80171725a110310", "ref_doc_id": "18587f29-849f-440c-81bf-40c704fa1caf"}, "f033922e-74ef-4000-a409-6996d1d88f06": {"doc_hash": "a7a214210f95db984b3060fd3ef6f945645970deb564db9eb16121877dcdaebd", "ref_doc_id": "6561a6c6-c5c0-46d4-83c9-1ba9a82a44ef"}, "2a49708c-915f-401e-a755-f54fb14684a0": {"doc_hash": "0170b784ffda946db8148007ce7d7dc020524decb1ecc909c13212078f846c1b", "ref_doc_id": "f5138277-6011-4b6e-8fe9-783515b62aa6"}, "5bb5fcb3-4010-45c0-9df1-7ed8d4e7e565": {"doc_hash": "5367cd0090e417e15dbfe2a73fbe41ec341b6e8280047ade8a056b94eb586611", "ref_doc_id": "b7d9d557-8d2a-4d96-82c9-afff8c5ade86"}, "d2cfe20f-de2d-4da0-ae51-d9ac9da68d30": {"doc_hash": "d8379351fc8b94e921323e94d725eb8da7079f7161ecdafdb1fa8e0e605b3c2a", "ref_doc_id": "690ca45c-89b9-4488-97bf-9944b4d1cd75"}, "f6e0e3fc-4289-4ead-a47c-278ffc9a441a": {"doc_hash": "1108cb3a835a72771866e43a380a9199752858c59fc9e912d38ef680543de4a2", "ref_doc_id": "d40722f0-1784-4a73-ab8c-386fb074542d"}, "da5dbcaf-8de6-4eff-81d2-530e7de7cbd9": {"doc_hash": "cc721d45635b2bfe51dd1802e03d628c3d65bd7942fe7bd4e2179b2a996e61fa", "ref_doc_id": "1e0b2afb-eeae-4dec-9eab-a1103bdc0daa"}, "799976c1-893b-4004-acf9-f1b644ff1780": {"doc_hash": "3e2aea4d0751fe0e3b03c24489b16215479d1cf562fb77c1d2bbd81e9179e63d", "ref_doc_id": "b9668b4c-d4c3-4905-9cbe-e3a39df81b69"}, "a0aee713-5f5d-4ad7-a51c-de8221ffe1b1": {"doc_hash": "92d462df804c0652583bd349c5984e7c94ec8e849b21c8e75fd795e41f01cbca", "ref_doc_id": "92748345-b2ba-4f27-b35c-7146a901ae29"}, "9b416f9c-869c-4d53-9324-fa4ae5f79d93": {"doc_hash": "850c403b7b2ea05aa185b8d9c23c2ae51a13062a517ae79008f12f50a2cddbd2", "ref_doc_id": "1c28a698-8d74-45b1-b3a7-7b1abc45f04c"}, "b6e25c4c-118f-491a-b493-9b125e507782": {"doc_hash": "d0a6ac58d672a206c45a5f5b4e79a89446ae7b86d1f9fe0b3b94501015f3f16f", "ref_doc_id": "1541be20-43de-4eaa-937a-ee2dfcc7efd2"}, "4f7ab63b-c073-4024-af37-a2bd3c172d9f": {"doc_hash": "040b1348a94ce021743d77b29995db6894a52b70bfc17050aa07e66c32380f32", "ref_doc_id": "74d54fe9-1693-4481-bf87-bc93e2581a93"}, "450273d4-a380-4a7c-81fd-792056ba233b": {"doc_hash": "47572d9de8bd5e5d60514a453b4a9bdbf595a423170eac4a5a0a23c50be5fc66", "ref_doc_id": "acedcf90-dfa9-40b5-85a4-1524d2890e31"}, "13ae1bdd-64b5-4e4e-9a84-5a800dc049b9": {"doc_hash": "299df9467f895659856324d1d257d3622fdbfd2e2c2e509a77a5192b138acc2a", "ref_doc_id": "cc6ef1f9-c6d4-46f9-99d0-80611cc80901"}, "e5ff1692-e5b6-4a62-89f3-ec1951e6a287": {"doc_hash": "16cff413b9410160f82fc5e1695b1fba0b3de516af822e7b2c78cf66814fa7fc", "ref_doc_id": "25850888-dd2e-4bfa-80fb-ee32b3fed858"}, "1acefc48-2267-43f2-a44d-78072640a737": {"doc_hash": "a0f48147529714f87379ca1558c3ff6531778592330a391eccb73d71cc13dc61", "ref_doc_id": "c4a4b07e-b0f0-4ec8-80b0-611da8a80d1c"}, "17f7a772-7a2d-45b1-ab44-6d8be2322e4a": {"doc_hash": "1b5251e38d4503a01cd8ad473f27c72a7259ee56b650db9411a46cab60d8b14f", "ref_doc_id": "08fe8ec4-19b2-4f24-ab07-9f15fb558d83"}, "b837b583-a44a-4016-9e0d-c826e5fa4a2d": {"doc_hash": "a3964da8b3894b56a9c8f68040aecb0e2d615e521cdf0646f7c5ee97003a9f0f", "ref_doc_id": "941a2f2a-f0e7-4985-ad28-aa84aeaf75b0"}, "fb67eeed-0054-4d4b-b6d5-f4f2bba2648f": {"doc_hash": "6ccb04514533af238f80531c7a5fff3c036f45174f77d35fbc7cce3f42199cce", "ref_doc_id": "c406a578-f1f4-4286-8e29-b411aca57da7"}, "6d33ab2a-4634-492e-be28-e94e274f158f": {"doc_hash": "0c7242a7a6caf1469414cc39f589b2bed5238d8a3f17d78000a8b56bef0c4c83", "ref_doc_id": "7afe79d1-666e-4f64-8417-3c9650697574"}, "6ff9a6e7-b8dc-46ea-ba90-6013c3fffd43": {"doc_hash": "140c753033b531c3a51bedcc11d7183430eb620352729f2cf8dad39a0ae62c48", "ref_doc_id": "fd8640cb-a525-4be0-9edf-43e24395e18a"}, "a4d36f28-bd17-4ffd-badb-b4a8b6855ac1": {"doc_hash": "d376bb47b304d10d71c0120c3e344f4dc5d0d4e582bac5a18a1d9b236d308502", "ref_doc_id": "19de5903-10bd-40e4-b9a1-22147898db87"}, "fef1d7cc-b56a-4075-8069-a3675a4a4691": {"doc_hash": "69878147588dc0664f87cac3145e75fc373a21af1fdf06f959624cb698eafb3b", "ref_doc_id": "c791a107-09fd-48a1-a566-a69a58d48299"}, "6baa806f-e2bb-4503-8d11-d600a208fea5": {"doc_hash": "7d8616ef49395c937b364211474759222f7ec9d72068b2d0b6009c301664d7fe", "ref_doc_id": "c6465774-0c35-4cec-9535-5cabdce7d516"}, "dada8533-19a6-4585-886b-f7b5be30ca2a": {"doc_hash": "bb3b22406ce3c7b190c580795e3dc036d7d6ffa49c380420e661665c8d33097d", "ref_doc_id": "4438ba88-20b1-475c-867c-4749033d1cf6"}, "1ec86bf8-9986-4fe4-8889-3068ed1aa07c": {"doc_hash": "2d767d8c73bae766807547b2b4749907a88d7e3bff15dc5ae73341e105ffcc5f", "ref_doc_id": "76b040cf-eb4f-4490-b4e0-f0fa73bb1f05"}, "c9e9b02f-d500-4da6-af12-82b3cb43bd3d": {"doc_hash": "5405877cc9cc2a0f4b597dd8fc1c22de79f511cc87bf57e0c2b834ef7bce72a2", "ref_doc_id": "2740a957-5269-4e3c-b5c7-8801afe45c1f"}, "4ba30823-0fd8-49fd-ac85-f2ea7285b84f": {"doc_hash": "bf4a634b7de409f4f0de90e6dfce2c89007895b372feaae83dea3cc8de0742fa", "ref_doc_id": "2e3b474e-8a8d-4a13-8018-cd83473e3068"}, "13f5eba0-1c19-40be-bfb0-ec855f2ca7d5": {"doc_hash": "63e0f92d6003673c88e891ca27e5d6ca8e22d479c3360be742faac560c77dc97", "ref_doc_id": "3912a02a-d236-4640-b5f0-d8e34dc8f2d5"}, "4eb85f59-4e13-41fc-8292-0972786ce823": {"doc_hash": "6b2db34c167d8c948b1bc3b2fad7211aee4deacd01e936c4d169411e4454953d", "ref_doc_id": "5b6331ee-447e-4cfd-a8fc-4f5b1f5bb0db"}, "8e936397-905a-48a7-9342-bdce3216696e": {"doc_hash": "b4d8f23ccbc637db260281f80bb950753231357666e52d077184666d0bbd3473", "ref_doc_id": "98b4a3bd-50a2-434f-8a86-4cd5f91be920"}, "ab48d009-0c7d-4643-af73-b34eb7711219": {"doc_hash": "f46d359f73614359419e413ac82f3735b1c9fc42b5bb8bf843d153d1e6920f07", "ref_doc_id": "bed722d8-dcd6-43b2-8ca2-a83b7e40f440"}, "f7c0d771-ee6a-4ab8-bb11-21d9738a3078": {"doc_hash": "4d5602fd45475290bba12ce2cc201aba74be2cc1effc0046417dfb5471a92e33", "ref_doc_id": "16609c01-c14f-4621-8e29-4287e0689ec9"}, "212c6696-5166-4a07-8824-c91c349a8c27": {"doc_hash": "f4c9f53964a0eb00591d998f51b365803a39fcc4392b0a972b0e5bd365a5b46d", "ref_doc_id": "3214b439-bafd-422d-9907-feda1b4b6eef"}, "3f641b18-ef26-4eb9-aafa-39a636923c41": {"doc_hash": "13fcf4d268a01930fa5b78852b5461e61d80ff5d95cee8a0fe1dc8e96243cdf5", "ref_doc_id": "78b430b4-7272-415b-a2a2-73912578ee34"}, "bd7c2e12-38d9-41bd-89a6-ea87737c1191": {"doc_hash": "3b0b26b3e1ae639dd6621ed8ef99615423607f772f66a7acd28cfee058d68943", "ref_doc_id": "7b493f58-b0b2-495b-bce8-3e145def64b5"}, "eacd58ec-ba79-4bb1-a19d-a0271eacb36b": {"doc_hash": "6fb8fe3ae0b4126a5ddb41b31d1f1605b60c0a37385455a10edf5804ff54f3d1", "ref_doc_id": "58dbf862-3565-4a7d-9930-909136367af6"}, "6f3726db-6965-44ad-bc4a-cd537fbafe11": {"doc_hash": "eb63b87b3a79713505efa105fd69d1e00efe2459b8c05ce86cc58ba27d246ee9", "ref_doc_id": "f710804d-cb57-4e2d-b4cf-8d27d2aeabf3"}, "808dd9c4-31fc-43fe-b604-c005c7f32ee5": {"doc_hash": "75a47a982cc5dd7aff60f1222a62c39d6e2928df3db3d9e7df49bc2cb58a0641", "ref_doc_id": "fc99e1ef-a4f1-47ce-84f1-e2a55b9e12c8"}, "989bac15-172d-4fdd-91f6-057a74c31dbb": {"doc_hash": "fc976baed90ad02d4a5a84d3d500200ced6c02b59a2e879b9d37af6781d27a3b", "ref_doc_id": "37e13078-b7ed-4e2d-8c99-2518a91b0d78"}, "70f3672b-6cb0-453f-a1fe-ac41f1e8e730": {"doc_hash": "fdffbb5764ad3bd922c0b1c6b61d26ad262d1feac0bca950ac71ba5fa1794d74", "ref_doc_id": "d9580c8e-aa0a-4911-804c-d9eca0243c72"}, "43d91298-f0ff-455e-a071-5faf90484ff1": {"doc_hash": "9de8b514cdc5b18ba47dc1876dcd659c0457ce6f0ffb2746d0d9e21652cb05a0", "ref_doc_id": "dd1225bf-0b70-46fd-b107-75f893995a76"}, "abf33d71-697a-45a9-9647-fd60a497cb42": {"doc_hash": "6302f69a6bc90d1c9ae04ac90224b641ec78befef0b07c6b5cafb83e70d53478", "ref_doc_id": "92ad97ae-9fdc-479e-86b6-9cb8115d9f0d"}, "c6b672ce-c294-4b90-9a31-92791208d2e7": {"doc_hash": "f064b682f1388e36463f786de3f56b9b6a043040dc5fdd8c6a5b92a9e3455c6c", "ref_doc_id": "ec066805-4e13-438c-9bcb-1abf6c3bb143"}, "d669b237-6cc6-489e-996c-e83047a55198": {"doc_hash": "8e857890a6ce9d196af42bc1be59174afe740803cf530ac2d4ee63c8a85cc2ff", "ref_doc_id": "97e96042-9d0b-413c-8cf1-1180199dc740"}, "90df7dc3-8109-40f3-a595-5ec6ddb3ff94": {"doc_hash": "d400a996767edf1dd4439eadaee343bd8ae7aca48f4edc0321cf55862d35a197", "ref_doc_id": "30c0b150-1c03-48f8-985e-a9ff33fa29a8"}, "8e961401-f8cf-4f63-8736-b65bf3e955b6": {"doc_hash": "fa89164ce25c4aef4d3a06dd19c2002169d04888f40a7b8103414dea5e17a016", "ref_doc_id": "ed626845-4764-4090-9e6a-017499d06b10"}, "00deeea4-9094-4672-879a-3df8be6ad6c5": {"doc_hash": "a4d50a44e71105910db0f2d2effc36f46cee7b68d1a5f89495a6f0fe425b02db", "ref_doc_id": "0943de38-d9c3-48cf-a5cf-ff0d66b26abe"}, "5ea5f4e4-0ed5-415f-aa82-10e95e3df23a": {"doc_hash": "1e6ca252895cc621b3fd579da5dcd8be9940debd2cba40e9e9f067a5d0e82af9", "ref_doc_id": "64bc5256-8465-45a3-8403-07d13c4061a4"}, "37f1706d-f151-49b8-b597-40417ecaa0da": {"doc_hash": "aa7275e5dfad43f749d1aa3b0baade855df4f77a233f0b8ee689804728630a0e", "ref_doc_id": "1c64bded-b9c6-4710-9782-79c552f091f5"}, "093291fd-6bc1-4acd-8110-c4f54a5c1057": {"doc_hash": "b28b078851141268af826449729613352ec1e65dc4316f9ee28d86313afa6952", "ref_doc_id": "ff3ff054-6a2f-4195-82f1-8382df810dc8"}, "4c336bee-aa93-41ec-8d6f-0f5c43d943f7": {"doc_hash": "c4c5350fb55df0c77609cfcfd352a0564b3fb29161aba4097ddd330b31da60ba", "ref_doc_id": "e745289b-b62d-45e7-9c59-d60bca128b61"}, "32c2d795-aac8-42b6-bd04-336a1827f21c": {"doc_hash": "baffdb2fb265beb26f9faf5d3e092f5125a7958ae3ee02285210d2c73dd12301", "ref_doc_id": "8aee8d6b-81e5-40ea-bb4d-e8c36a37af63"}, "92f6e08a-f043-45e6-b417-edc6df733674": {"doc_hash": "c4e0d3b2a595c1d4ca801906b4b12c0dd75d71bfe31c14bef386b4e5bb50ad8f", "ref_doc_id": "8339000a-9de5-4800-97a9-f14d097a8f98"}, "b79ef5c6-0c66-4911-b425-0b25fc11f737": {"doc_hash": "11da1f433fb94b67c83085c2320a1a0d8976af995d4dfc74356956aa31802954", "ref_doc_id": "5f57f609-5886-499b-9fe8-0ced7c3944d3"}, "88065c62-16f9-416a-bcf6-ba9e40768cbc": {"doc_hash": "9265929b7a80c0bba93625e892d30e5d0dd1f6162f147d56657688e57f9b6613", "ref_doc_id": "c33e11ef-8da9-49b3-8f98-965587aab7df"}, "0eb1db1a-a1e8-443c-ad3d-2ac783af26a8": {"doc_hash": "f88039a8ad00067f2732dd4d55376bf02b4da7a95f30ca1a98af886f2d994d02", "ref_doc_id": "141329cb-b9d8-42e4-a0a8-ac844cbb395b"}, "b2a1ef57-3152-4336-889c-0dba5e4f6e85": {"doc_hash": "4e2bb9f72f1fee21b71d1957e0498e0edd27d353fa4f4ab8e5206f4d95baf39d", "ref_doc_id": "d85b0bec-594a-4e20-8bf4-6224c2766a3e"}, "a2e88f3d-f5e1-4ffa-96a0-51f14c61044b": {"doc_hash": "c7704c746a4b40ac43d7ea07480263095831446f24cfc29d2b64eab01446bccd", "ref_doc_id": "61ec9db0-0eaf-4462-89f5-838a802748e5"}, "ddb45ea1-60c9-4bd9-866a-739cd4d8f0a5": {"doc_hash": "f545d9b1de5d06d06b392c73bce86e9eeb064f7f867553e17eee16efa165046d", "ref_doc_id": "d995420a-1d2c-4c76-aa40-e82fbc1ccdd8"}, "37226689-635d-4627-aa04-26206f4dab38": {"doc_hash": "1b811eb1b609384735f1978e13e83440de4f08a2e16aaaaea26e4ae7410c8456", "ref_doc_id": "4f7d55c9-73c1-45db-a4bc-b7798681a84b"}, "57b077d8-d531-4e36-bdc1-c09344ef6b28": {"doc_hash": "e8243ce2dbd5c89b0d8764aa85b7574c1feebcaa62a6a8a69ecbc91d3bfb862a", "ref_doc_id": "6c487abf-3a49-4492-9846-1d71e723168c"}, "79b47871-2683-46da-8f8b-54e02c18df01": {"doc_hash": "ac6e5ca52cb532197f648b314aba441ab98b460ee8231ca9a68c2944e10223e1", "ref_doc_id": "1c30fab1-1d82-4d2f-a9c7-e79da4b3781c"}, "1fd9f536-8556-4cf7-a119-d689740c95c1": {"doc_hash": "8c2d6c81fa61242886ca712e9541a9b8b707856d7e3985af7193c079405f53db", "ref_doc_id": "74266f51-215b-455f-bb8c-5e0cc9aab1bd"}, "2cfe2f4a-748c-4940-bc05-679328690c4b": {"doc_hash": "887f2ae88f697c35dce0d94b2e900102725824abeeeb318ff152d9d84461cb2e", "ref_doc_id": "ddf6f29c-7c52-4056-9f15-e5517257bf7f"}, "80c92746-c83b-4514-ad3e-8d938ac332cf": {"doc_hash": "b19f67e69fc9047928fa02e0e1819161e785783c6cc4ee983dc3a920777e0da5", "ref_doc_id": "8a323310-010e-44d4-8069-511a61b6b033"}, "3d6706de-e0ee-413c-911e-1eaf3ee50683": {"doc_hash": "4d7902c0eda3e38e53aa045e4777b1ccdd1f81b4a0452155a81198dbf5ebc182", "ref_doc_id": "5c5e39cd-5ad3-4427-9b1c-2ca86afa6c94"}, "38bd51fb-c307-4eb6-92e9-7a649f93afd2": {"doc_hash": "6353db1e99852e876c923102bdb952cff4a31bfe69ea4572f89644337c3dfd6b", "ref_doc_id": "f95450ef-580c-4067-97f4-a2850064e928"}, "129059a7-c96c-4e67-8b33-768de33a8395": {"doc_hash": "986aa6aa9141ee7f6e5785be08262d420f97bee4ab695ffafa53f6894f9f1263", "ref_doc_id": "6c77cf7a-6d97-41c7-8c75-22ce66eee3df"}, "892bc3c4-6965-4feb-bd37-988c1fcc6d21": {"doc_hash": "dc3ed725eaf655e752a8f13b3948230cfafa9b94d078db87e3026ec7c6b1cf06", "ref_doc_id": "1bb84dbb-5682-470a-a005-33658e4f2130"}, "1e640f64-77a3-43e8-980f-73d58fccb425": {"doc_hash": "13971a85c7d794cfc4e84e56182af2ffa22de730e8d47427c9c7b06c91050384", "ref_doc_id": "4d509d84-a327-4d09-ae43-ee5f38f3f4ed"}, "1b7c1878-8830-40d6-931f-1a9eb42328ae": {"doc_hash": "1eb84f029844b07f4ce34cc2d39d0d2826fcc64bb12b8126aafaa8c128e2f57c", "ref_doc_id": "8be1d785-5b3f-470b-a382-1f254cf7fa8a"}, "1f01178e-8188-4d03-a5bc-293808050f76": {"doc_hash": "5bc34f50c3e0e92db210097ff1dd257b37a845afa8cffbaf069b16b98440fa92", "ref_doc_id": "4cb64ad2-e81e-4211-bb56-f08c48a57143"}, "100d86cb-9423-4c7c-9bf5-7198ec31bb2b": {"doc_hash": "23bcd17eccd9cb5e0ec7dfdaee2aa27efc63227f6c15129d347e0e50834cc6da", "ref_doc_id": "20195e71-1e7a-46e4-90c9-f240d4041656"}, "9dd49142-1de6-4f62-9aae-4eefcb803c29": {"doc_hash": "be73ca2659c2f1b0b6ae16aaf5039ef9c65896507d7f5a85735a991c5e88e467", "ref_doc_id": "1177bfe4-3430-4637-97cb-2b957b9cb091"}, "98807f67-27fd-4937-a4fb-c27fea75f204": {"doc_hash": "c308457cc4fe3c5a35d9a6fcfc506c859ae806151c5a046638632d3fb0446488", "ref_doc_id": "b18c590e-95e1-4b38-8f5c-032599242ec6"}, "eecbb970-1f2f-4d2c-b934-744b9041e0e5": {"doc_hash": "aa2da5b654a0d25bbf7acbff2169db3a96ab7b559eefd67192d0f065d2f18650", "ref_doc_id": "7222784a-317a-49e9-9f16-19cd60b2bcac"}, "4f8381af-79bf-41eb-9d8d-76356459c5a6": {"doc_hash": "f615e44689c53650a1c68fc5cdb5aaed8cdc09425bab8c277fcca7e884096d1b", "ref_doc_id": "f2b073d5-2327-424e-86b3-f1e568fc00a4"}, "51ff03e0-de60-4ab5-a8da-595486e20c9d": {"doc_hash": "082ed2aa7793e9ae59d107bf67d4c4f8367b6e62149f36f0869285a7d8d1bffa", "ref_doc_id": "299cb7f9-c960-4da6-ad81-914ecc56d4d5"}, "fd7dd8b2-714a-40f6-9707-41d940436ad6": {"doc_hash": "a6c8d7c1829fd3d085aa1707683966bf56cbf3e724881019b7c24e97dc42c148", "ref_doc_id": "ea51f9dc-eb85-48e2-840c-bdd4d02b5b13"}, "0e3033d2-c5c4-4a88-b829-4f3e1630e7a6": {"doc_hash": "c613f5caee367cf21e44b2873517f9b3594d0632669356e893bc29684335e8e8", "ref_doc_id": "983d58d4-f85d-4b23-ad2d-72917ec8c1f9"}, "26a29e68-b270-42e2-811f-444ef84cc7eb": {"doc_hash": "e8835c363ba187b3a34e3b8a92b1bd98de18be9344444c27e3c8662244e46a0c", "ref_doc_id": "15cf80fc-ac05-4e5c-8cdc-8c92fefa1b63"}, "708d7042-45e8-4a19-b38f-fe4bf01289ee": {"doc_hash": "e986687a19e58ae09387dcb8037e0e6c5d58cfb9e0cbf7014b3dd1f865852e75", "ref_doc_id": "f94225fc-5d8a-457f-a27a-94a3b0b99c85"}, "0d9165bd-af64-42b2-a785-042065993498": {"doc_hash": "22dac1835f35ea1bca77d4e3f387e39dcfd5eb56e2f21e1b2b42bd3df5cab8b7", "ref_doc_id": "b5d2ce3d-ef4c-4b57-a17b-5154010af782"}, "2b0c1bee-5b53-427a-bd43-a3a1b56e058a": {"doc_hash": "6fe1db1be254124a486325ec17a975edbaafed25896f87aa4c7b5e310ebe7132", "ref_doc_id": "fb71f892-6744-476c-b66b-05366fa0b4e0"}, "7074808d-8a8b-469e-bfb8-ab52ef7d80e8": {"doc_hash": "d623c4dc24386bdca5781803563f18363db07ebb342ec73ae482835e01edde22", "ref_doc_id": "567a01c0-2e14-4ca9-8648-6995d6c0d780"}, "2f71a0a1-ae8d-4b8b-b164-341bf70908d4": {"doc_hash": "0496f2388b42d3b74f44e5df86d7a6e168c2484d1c3aa987ab4e05fd907d7a51", "ref_doc_id": "97fbf5fc-62cc-4c7a-8a1a-f4810ce0206d"}, "770f5be5-07b4-4c47-ac95-f40129feb453": {"doc_hash": "6a1e04b324d2798d14a342360a4e2589668beb40ffd1b337e9d430afda58cca7", "ref_doc_id": "8b970236-bcbb-407f-9cbb-c789d878070b"}, "05d27cd7-c9c1-4a2f-b587-02a40923f143": {"doc_hash": "cceb2f0d3630aa5cf577cfe0a9c72102a4cfe4318df2692d46d120a5cf57fc31", "ref_doc_id": "1589d060-28a6-4258-9600-7954de7b610c"}, "fc006b9c-9740-4c95-9919-d8afbfecf107": {"doc_hash": "450e6ffb76c011ad9951bd3994dd9be6cd32bec902a0d207c866a6da635fb2a4", "ref_doc_id": "b87e47d0-6eac-43dd-931e-bc7c9d093c9e"}, "9265280e-6404-437d-a8f4-1dad38ed0224": {"doc_hash": "ec619c1a536661657a7fa3af7934c8432e80404ef9e527047efb546126541f91", "ref_doc_id": "2e1da7df-8474-4df4-aa8d-4450ab07add5"}, "f070ccd3-58aa-4066-a1bc-d5679e129d3a": {"doc_hash": "83fcff8681fb08319028a1f19ed4ee737dd2bbcc5d764e5a6a301ff96dce6077", "ref_doc_id": "bd732d7b-1080-43b9-bb48-5f7b313d80b1"}, "c22699ae-0f17-4c3d-abd9-92a665bd27d5": {"doc_hash": "e695ca6c906061679b12441fcfe5028095f1f33693827df3e138da158b0cb037", "ref_doc_id": "a6d0fd9e-6acf-4eef-8e14-4fc8ea23eea7"}, "095e96e7-91c5-4737-a391-2fd5080fd894": {"doc_hash": "8b39f72029ca3452abc47bd52bf1dd64aecb7df90ba2f7f6a3fd69a6537c212e", "ref_doc_id": "528130f7-6c0b-4c99-930a-1ffeccb02d6a"}, "e74843d6-e310-4e39-b2a2-12c35c0947b0": {"doc_hash": "a2b43a502aa4a922c311bf58e843c837ceb4d41007822d3635ddb852a6664066", "ref_doc_id": "58468748-a088-4fe9-95c1-b5efb4d1f77e"}, "23f99809-d2e4-411d-9e65-60bed9dd9c39": {"doc_hash": "3959bd38c467fdb91e425d3da535619cc20beb4cf2de5df76cc341b2962161d6", "ref_doc_id": "37984659-24ed-4eb8-93a1-0109b8ba3938"}, "c6493ac8-de7a-4885-a45a-9b52f593fb05": {"doc_hash": "25c3b88ac1c71cc7dee5c7379bc2407320472dbbfcc400b2120de0a6b72af59d", "ref_doc_id": "365d3f67-4e38-4bfe-b76f-b39a52fd18cc"}, "c2e5a885-85b8-4add-aba5-2be88ab105c7": {"doc_hash": "b669f7b1a2a6cdf5a147a78c8aabfba559871b8fd2e95fce9a8a37f15f2f5fc6", "ref_doc_id": "ce55cd0a-ec3d-46d8-b946-8a6d0523b392"}, "5f63ca01-dd7a-460c-8fb2-245aaf9147f2": {"doc_hash": "cb847b89053e1a3318e97c94c22802b832fe3a332f0cec8610857b613633cdb4", "ref_doc_id": "b5537a91-0cfa-4a1a-8620-0ed97ab0b8e6"}, "bd151ad5-0ffb-4732-a139-71cbd9e211d2": {"doc_hash": "093b74f065f188eb6aa890e55b9d7ba3ffbf3b6aa5dd93a8e3db8fe196dd6245", "ref_doc_id": "a313570b-8c59-4b5a-aec6-c43d9339d521"}, "3ffd2c78-51a6-49ab-8990-81758a7e79f3": {"doc_hash": "410515626e2805ac968e51530f32ff636c26bb73a7e3675ad4bf0fd99d283568", "ref_doc_id": "c683e188-5d11-4e7b-ae98-d9030bea324a"}, "846728f7-be78-4fdb-861f-bd48ae7288dc": {"doc_hash": "9b39446a760ad0d03a0772002a46b383b4e6a8645334c6639ad5947d21d9300a", "ref_doc_id": "4bd1b38d-e6fc-43fc-828b-aa957c90ed8f"}, "9b224c32-0261-4188-8e83-73ffe87dc31d": {"doc_hash": "ee97006bbeda0ae77a76677027b996fb06f6176445cf4a9082840a6f3493d535", "ref_doc_id": "e8858b7a-bdc4-47e0-9763-e923ab713dc5"}, "13fe5629-634b-42d1-92eb-e036eabf8b07": {"doc_hash": "c5956cbff818fe7a144939ad9c12a9d92c08268db1a0a6ca2343eda11d570f5e", "ref_doc_id": "5fe7f062-582b-4afc-ad82-a1be59b2c4cf"}, "3b18f951-c536-4fe5-b000-c7cea8aebd3c": {"doc_hash": "e0bf294abf64ea37e46047bfc5751ae9fecc6b7f760b4175f8cbd1944b109bec", "ref_doc_id": "6c689466-5b87-4a04-85f6-903fb33cbd7f"}, "d66b0cc2-30da-4000-bcdd-d729e6fe5b50": {"doc_hash": "bf448c46e55dace9b11eb817d3809d6ad9da540d45a03187c9d9489fd2e21ae0", "ref_doc_id": "afbe1199-2c08-4d64-bac7-a6d73abbb760"}, "0277977d-8f9d-4402-bc5a-3ebc3ca924db": {"doc_hash": "61eb4a60f2eabcda57589875603967aa147a992a101e3b3b80ae59d7b93c4bec", "ref_doc_id": "be631d02-1b61-48ae-8415-b151ff592cfd"}, "ee2a87f6-72b3-4a97-8946-01a9ab88235b": {"doc_hash": "b1b54ad420a331fc7d62861eeeaed66c79006c0c84d2ca839f2aba158d7a5326", "ref_doc_id": "c812b12d-3e92-4c63-9341-f85e32ffb5c6"}, "ba487b7a-6877-4e84-9d54-8b239aaf133d": {"doc_hash": "0bdadc03ab0ea6c898586ee4bd93c6c85ec2478822300a740af1bfd4c099aa7f", "ref_doc_id": "91782f20-756b-4344-953b-1b7d636a3fc4"}, "250eac4b-19f8-4afa-88ad-f1dfcdead992": {"doc_hash": "4512ca4008d065e8efba44762376e331450d21b10e4b36d76522a054ab2d2a7a", "ref_doc_id": "6b8f628c-4ca5-4e88-92e0-a9dc929dd94d"}, "ffaaffc8-87e8-44d1-a9c7-346e2d234ea9": {"doc_hash": "0771a2d1e878e0aca3929866a14882e4f0a2ef88ab18ab7fb468eb7a973e7217", "ref_doc_id": "39b8ae14-7e62-4fb5-bd30-05fa87b4b8b2"}, "450e4647-d842-44fb-80d0-1c8cf35a50ae": {"doc_hash": "baccb1c6be711d1328aa93e32c27ca2e9e4a1b4272c7abd0db820d0455eb426d", "ref_doc_id": "4035b2de-dfb7-42ef-b246-39d1e917c221"}, "5801212d-6f81-44a0-8c36-169c28c4b61a": {"doc_hash": "fd524e6bd7b870d2d92e42d99d104a5c4401f4b895bd0bfbdd0e8762b7c86622", "ref_doc_id": "37ab87d4-79d5-4f8e-9977-8843b13f3033"}, "f2f2bc27-1c39-4282-8a7c-9b5b956f5451": {"doc_hash": "eed953d3d4dc97952136d9605fe253c2d2b0b181ccb1de23cbeb0a0b3637e939", "ref_doc_id": "c5800b7c-34dd-4ea4-bbb1-7288e1e8288c"}, "52ba9edf-10eb-49d7-9f1b-ecc2c22dc26c": {"doc_hash": "df5d5980685a2cce03d136f07c7118e9195c731b2ba36932eee2b6438727eccb", "ref_doc_id": "5b6b4be5-1d4a-44cb-94c2-16aeaeca4a39"}, "95f59716-7855-44ac-936a-27b925e9c270": {"doc_hash": "4f59c0ef82059a31d1f44fff747ef34cefce98118c86434e9e60d1c847c1d6c6", "ref_doc_id": "b028a9ee-877f-47df-b366-303c167478c7"}, "e71487d6-0795-4797-ba61-3e9813e83173": {"doc_hash": "480623a584f86461bfda7ecda2c9e873d1ba87f0de74d736be28ffd8b15d9639", "ref_doc_id": "af0387a8-178c-4e13-bb4e-7ca1a107c4a6"}, "b7efa100-207d-4fa4-96a5-f1e6c6c52986": {"doc_hash": "351410aa21343f44e9ced9e50f9973fcfe9f493aff7e501b2a6c85e297f31cfb", "ref_doc_id": "4275b810-7d87-4a4f-8355-1332465e1133"}, "a29468ce-c643-4e96-a6ec-7e1b8fa618ba": {"doc_hash": "42d9fa483b9d76cbc77f82ba71619826555bfd34bdd72471b404a26f1197441a", "ref_doc_id": "c363337f-f492-41ed-acdf-df77440b3772"}, "948b987b-3d71-4b30-804c-e689af9744a2": {"doc_hash": "097948a65b8ae356553e23129109652b9b10ee9a9c228c7ed48dbafa859bcbb3", "ref_doc_id": "b4600a08-325e-424d-8707-101e6dff251c"}, "d4d538e2-edaa-4276-8e57-488d142b6fa2": {"doc_hash": "5fbb50b0168ce4fd2602d0a35dfc007ad8db9ebd7396f3af037ff67ac9ab0c5b", "ref_doc_id": "4b291cc6-1c9d-406d-92ea-48c488ba1a1d"}, "d6d03f6b-ac42-414a-9f93-73850f1807bb": {"doc_hash": "4215624ef48a26371ca67b99cfa279da3041a304f79db16e941e608ce4f9b628", "ref_doc_id": "8a14ac0b-ccd1-42c2-a39a-0af461bbff27"}, "684649f1-f1f3-4ce4-b484-92ede919b206": {"doc_hash": "31330cea29715d4eac6f97455bfaa778a839136c4577e4e39ffefd089fdca30d", "ref_doc_id": "6d52553b-bbcb-468c-bb2d-105943b58346"}, "29587923-d2b4-4b8c-ba66-d7a2d1349fd8": {"doc_hash": "f54f80362bdbb16b6d181031eb62441687e54e9cfe03b2593f72434b8849f7c1", "ref_doc_id": "e0ecfe1c-768f-45ed-a5a7-1b656ae8c938"}, "d0e610e3-3f2e-4329-a135-6d504e106993": {"doc_hash": "fd8ad773d07badbd1eb4c073899684278bda0330e0c8fcf807150d94e7939f19", "ref_doc_id": "0d1640d9-f170-430b-911d-a029b32ef988"}, "59799741-7f51-4f77-9a07-eb26b7aaad93": {"doc_hash": "faffe230d99218e095261d4ffdbc79b51b253dba9c18912ba8c9a8a7ddb37f4f", "ref_doc_id": "2205f9be-d3e5-45fa-85be-d6dddd0be8a3"}, "0f38306d-b966-43c9-80bb-5a013b2326c9": {"doc_hash": "08b2b4e6bf0e253d919a9d6d1a490c293ca97cb06761c889088b73d4d513c87c", "ref_doc_id": "fb8f9468-7cdf-4fe4-8c31-88a856eac71e"}, "a4457f8e-1b7e-42b5-882a-b9137da39417": {"doc_hash": "384713a231327a3f0802334ecd7345e66345ff33b6a666a9348366aaf6a1f4f0", "ref_doc_id": "9bb7b024-485f-4f3c-b531-95ea75c81dc8"}, "16bb5c6b-e74d-4fd6-922e-cf158944d990": {"doc_hash": "106a24bf5f2e74aea872fbedf33d4706e672ca09744b4136b68c5b8b9e156fff", "ref_doc_id": "0c95cbc2-14f1-44f9-8775-f2446121473d"}, "daf264a2-e62f-45ab-b3ab-e519a994c823": {"doc_hash": "7bb5b7dca14611ee00caaf0afea61f475c32ca2fadc73f07f73b6a74d25473de", "ref_doc_id": "4e5b6f02-cc0a-43d9-b8cd-9e98c06e4ec1"}, "7b7f4e99-ef34-49ef-8b25-cd8c63421550": {"doc_hash": "71106fbd4fd42face0777e3d4c02c30c32aff1ba130063cd345e531928a34f4d", "ref_doc_id": "61b17e81-1c2f-45ab-ad38-403a4d89862b"}, "54c264db-7628-40a6-bd16-77144685579b": {"doc_hash": "fa629e395a0f2210b0c687f98920b0cf1502e59c8da3b14d02c7dc00126ab272", "ref_doc_id": "ac648878-e6f1-48ec-9807-115c802405f7"}, "95dac4eb-26c2-45cf-9425-9c5f3463a8c3": {"doc_hash": "dfe97de166a6f48090aaebc416824cada752e18e3a3b6e154ac06da3177b75f9", "ref_doc_id": "82aa7df4-4666-46b5-9ba4-d0d6beb3dee6"}, "441160b0-59ed-4324-aa32-ec8907752b1b": {"doc_hash": "45ad37f8c79384749b590ddab02938273e13bf21fe8da1a7a14a303776af2d7f", "ref_doc_id": "97d5dc5d-e376-4753-8958-3334e88cd9b2"}, "a143c1ac-a7a4-4579-ae74-eb38dd2782d2": {"doc_hash": "54b815125d4f6d9aa236175906b25fece8461cc3cd3a57d6c33a0091f98efca5", "ref_doc_id": "7bd14eba-fc09-4bc0-89c5-2c5c2dd01e1a"}, "c0f38dca-aada-463e-a115-10e380657dcd": {"doc_hash": "447f0a6119fbc513370a050f7610db6c08ade59184078e2b3602a03e60e4f1ce", "ref_doc_id": "0c0aaf4a-1170-46f5-8932-d9c2af794a90"}, "b3ee40de-6ff6-4c97-9d20-1bb2fd78d12f": {"doc_hash": "f4287980f7d229ee41ff64a51bf14fc29840f425bb2171a28a9d9a9c61c85f02", "ref_doc_id": "c9a19900-6f4c-444a-8796-5d5bfacd6b5b"}, "1e05c857-8084-44fa-a2bf-6f6013512da1": {"doc_hash": "041bad328eb50c86a03d3b5cdbdaa1adc77e830b743c41488e73892769c5c9cb", "ref_doc_id": "c5df4b95-e582-4b35-9771-6bdbea5478d7"}, "25b33a9a-922e-4f48-a8d4-acfbdcc44cfb": {"doc_hash": "8721c0b100b26ff9b08e34169e518334af57978fcc3ed9a73a9fe98b36dfbe98", "ref_doc_id": "ab5d00d9-4931-41f8-968d-6ac14ffec0ed"}, "279fb32f-3c93-4695-a44b-9e7fb5e9d9c4": {"doc_hash": "b21139be598f9b470c1623737e2346d2aa93856be941d76158a56806c5b4fa76", "ref_doc_id": "f87b5543-43c6-4971-9eb3-31906a5f6a3f"}, "3129273e-8317-42ee-822e-c6006bee143b": {"doc_hash": "f914f7f20960f75a44a39290fdf366eb604e61de6b611bd33ea2580be8126d3b", "ref_doc_id": "fed9a6c7-2183-4a4a-a380-44b17676cf27"}, "8319fefb-ad3e-4dc6-8e19-e48fbac048f2": {"doc_hash": "36e2ff074f982fea9877dcf782cdac8db6b326f61803a1ac893106a8db70b4f7", "ref_doc_id": "ac00b750-e85f-4271-abef-c92778783b27"}, "176a00cc-9caa-4a1c-8bd8-bc4f2411a485": {"doc_hash": "93d96c805c79298bcb5efb4d7e92b0722edb8be46600faf4465419adf8de5c89", "ref_doc_id": "40dbb883-ecc4-4f7f-ac3a-694401f56fcb"}, "171cd2ce-b2a5-409c-bff9-bc7c51011c05": {"doc_hash": "93993ae4d03ac4e983f8c29fdc83245610e33281042a4f625cd4b5b0368fed67", "ref_doc_id": "fb5c9bb7-3512-4c6e-b1c9-cb6dc5efca8d"}, "675c36cd-9a80-45ca-99b6-7d2d45c8ca2d": {"doc_hash": "46645fc21cb0f339f6f1fd5512faf7ce73d4ce0ac754bd87c961ab3876189149", "ref_doc_id": "f3a22cd8-3fbf-4dc4-9044-72336948ecce"}, "1367af17-1289-418f-8448-54066615ccd9": {"doc_hash": "6eab7992124aba94a6fbfea733e71157a6813d01f111848923538269b84a26d6", "ref_doc_id": "f1627189-3761-48b5-9304-b33ea5a90b52"}, "874a1eca-2d6a-4061-9c47-1948275bfdf6": {"doc_hash": "c52817ea7f574111fceb8e51f04c49f666a13ac27e84938638ca1a0480ab7537", "ref_doc_id": "82165b48-2776-4a83-be7b-e8ceec57907e"}, "f828bec8-92a4-455f-9e6b-df0c0023156e": {"doc_hash": "f6b5a0c7de5dac4123e2d89604641aaf5480bbfd0425e234b3460e7157272906", "ref_doc_id": "42cfe601-5767-410a-adbc-209738a671c5"}, "4392a67f-6a2e-4ece-9654-4a4777de8699": {"doc_hash": "448e673f7b36e502bcb64a65335fbb01d0ac2a1ca104cc7c129bd9b1655a6eb5", "ref_doc_id": "b54f5097-cbf7-4e37-92f9-12fe10928cfa"}, "5ed5da24-f655-4372-aa18-0327d6d9a8ee": {"doc_hash": "cdee02e2101cbdcd07d90087ab88c400b744bba5af051e94b6b2fb7249299439", "ref_doc_id": "ed604304-3672-4c9f-b030-4ef69890d838"}, "6ff136c8-a4a1-404a-9943-7732f50a9b5d": {"doc_hash": "475910a824bc2fe98cfeffc936e92c4ca7e4cc7d79a177a56c6726d66a6f304b", "ref_doc_id": "01117d63-472d-4d1d-befb-7b50753e6d14"}, "79db8bda-27f4-4a9a-aba4-95ba99bd7f2d": {"doc_hash": "a54a9556b205dacd99238274d39257740c509fe7812379ce967230b8cdedab14", "ref_doc_id": "013af61f-0a2f-4a18-98ef-6982616901c6"}, "31d569e2-2a8c-4449-9c2c-401c094901f6": {"doc_hash": "81a0e84e9a8b204d36d29342a334133bf6067c02ef692375bd429b87fecbb94d", "ref_doc_id": "5310c3ff-082b-4199-a04a-13f542506f08"}, "558427bd-522b-4862-9e34-5af7433feceb": {"doc_hash": "d10391e6701ab4cedd8a093dacb24443abcfd8351bd3da9223b0ec5957282f19", "ref_doc_id": "8ed12660-8fb3-4ebb-bbc1-16464737d75b"}, "1748f0ae-adc8-44e5-b8d2-1d66e082e4a1": {"doc_hash": "3a3bfd328da1a909877fb2f269f8a25ccc75171723b120d643d2be2c7bc973e7", "ref_doc_id": "af8f683f-ff06-4f40-b4c4-766dd47576de"}, "d8fa2dd2-7e45-4adf-a33f-6e2116642ad9": {"doc_hash": "16a209f7fcc9ee65e6ad28115b5b0c4cf5bf603a51a5cd8cff39d34d1c220609", "ref_doc_id": "3e5b8c62-32ec-45ab-a485-1855e9d663cc"}, "22febf1d-08fc-4aad-8506-4f46e17328b1": {"doc_hash": "137a7b05c786ac298367c3904ef6480c264c336e639dbb3b960838751834954d", "ref_doc_id": "b932d09b-ace6-446b-9a2c-310f54c38d73"}, "644c77ec-6250-48c7-8872-dff2e4dd8692": {"doc_hash": "5e83ac7d1bd4302bbf5827380d42520cab44546ff1e136cbaf1435891c70846f", "ref_doc_id": "f622ab2b-aef0-488a-b325-534dfbf2d40e"}, "da9bb393-983c-406c-97b0-6de958ebc17b": {"doc_hash": "44008f7794efe3b02d24f261f52ea4cf1d8183c06ae248038bc0ee80bcac5e07", "ref_doc_id": "c19a2081-d274-41e5-865a-2788c35a50c1"}, "f0047d2f-1670-4805-9119-d2265c0d9828": {"doc_hash": "0dfddff8bcf45c8403b5e753a47d1a0c984880781f4d3e12790ce17d4ce4ad76", "ref_doc_id": "4cdb16f1-42f7-4dd3-9f29-c7150a5fe163"}, "1f5590c4-ec79-4a3d-9120-7f7a41f6d3e5": {"doc_hash": "8ab8bf8f8aa0a412601666500c6c7b923880d330fb116451772b824ad0dc404a", "ref_doc_id": "06b0417f-ac73-4f91-80f6-2980f6551940"}, "5cd3f0f6-fd29-474d-bb9c-550167fac11f": {"doc_hash": "ed9d080f72b738c3d07a4cd356a7d8b9bb3ed52cd15099a104b58a3f3838f82b", "ref_doc_id": "0fa73be9-e013-4cfb-8b21-71ab9a522688"}, "cf498972-e055-4742-9c03-31f25d56e4a7": {"doc_hash": "53203ec0315829a2152946b4d6ea3d11a14fea19066a425dc73dee3617000664", "ref_doc_id": "514eb90c-1cbe-4892-81d1-68464aa8022a"}, "d7a059f1-e149-41ad-a834-40e8c4b7ca19": {"doc_hash": "8902a35a41ad56a9fdb246ce3bbb4b1c488240bafb73903b95f81476c8903753", "ref_doc_id": "a75ee081-c84c-46d1-8f9c-6fb9275c313b"}, "8853cbbc-65c8-4c0e-a552-2aaa57fcf7fe": {"doc_hash": "1810b82c89e9042f1d2c7589aa54344c68e71d9cf376456d409369742e3b08e8", "ref_doc_id": "07d3b489-6490-4da8-b93a-8591cf3ef247"}, "e2f5e2c0-454f-431a-91c6-2ecacfb20c17": {"doc_hash": "3c53c22d80658a2973f7d2f9d0193abdc5971160a5b3dc2cd9c80920ecc97d8a", "ref_doc_id": "918dd188-3baa-4b8c-9a6b-79fc1e6b307a"}, "377c6b41-04ec-41fb-ad24-ed0801ca3916": {"doc_hash": "a7ffd2c6a5e01f1fefe69b0b7d0db4945b8abf730eb7fa61f7058cabd70a1b14", "ref_doc_id": "91bf0ea9-93ea-4bcf-9afe-d2542d1fc4d7"}, "dd355122-7365-4853-b418-a469c5a34456": {"doc_hash": "fa02ee8ae4fdba44dbf09a25d1385661e1f484e82fd69f69c90403faf86dcdb0", "ref_doc_id": "1598ded2-4a38-49a2-a027-6b7c724dd361"}, "23b7d867-013f-4db8-9799-801b4004a0aa": {"doc_hash": "dff31f588a80b44fa16954e76cfdd97206fa3e310082ce48a080b72ecfebd5c0", "ref_doc_id": "e05d6d91-6042-4560-b39f-60c85b3f6f8f"}, "0f8ac128-806e-48e2-84af-b9c27981ada7": {"doc_hash": "3dc9fdf9b5e32002bd8286541d542cc460cad3c570bbea3f4ca9326d6a9014a0", "ref_doc_id": "d43c9a0c-6dea-4d6c-ab58-41537e895770"}, "6d5f1b0b-159f-44ec-9c58-2e12edc4895b": {"doc_hash": "f31e325051ac592cc51080bd6666093ac1f94f796dc3a6bc77cbffb6139e9a57", "ref_doc_id": "e070b5ba-5842-4e1f-888d-adebce7e46ea"}, "c98d1768-63c2-40e5-81bf-bc03a8063109": {"doc_hash": "518f9d07782f3fcf03e191ce19b45ceb25833528f9f3f76fae9ae7b8867a9ec5", "ref_doc_id": "6bab5e62-71f0-4f1d-a0c4-6ea43aa7a949"}, "472b41d6-f007-4ac5-9405-68134254eb19": {"doc_hash": "8c11b8ed1fffea440ca3a58a5bac8ca2c5e863b5bc46fb0fce9fba13f30e954e", "ref_doc_id": "7b867c3e-4c02-4595-b142-bc52a060fa2f"}, "1f72c991-109e-4a33-9699-13c28c83e122": {"doc_hash": "936f7ff8485e73f2bf7eb2e28d2c438582f3171a87b5afae18d30aca76900e60", "ref_doc_id": "02a5fecb-4917-482e-aac8-2f164068b592"}, "469607df-af30-40a6-85fd-3332491e200b": {"doc_hash": "14b586718adabbc117e3dfb9bb9d00327979fa60ad43e4046229eeb493b55e30", "ref_doc_id": "fd1c4dc5-0ad7-47e8-a4b2-f2d46794f0d2"}, "3ed2de99-970d-4b72-adbc-1e0a79599a6b": {"doc_hash": "72f31a0812513bdaa2413c62f80bbf7fad78732d100e49065e258b8f0241a100", "ref_doc_id": "4e616ed4-0694-4627-a179-7d09b338ec3f"}, "3475e6f5-f286-479d-8883-fba0b9c06866": {"doc_hash": "f69e87b7fed0707eebdf945a1f6b359c9af52d0e9752dbe22ed03672dfb60c73", "ref_doc_id": "fe0285b1-558c-4055-8026-68c1d0603ff1"}, "9c9cfc79-e2f0-4b10-923c-12d4fb41b896": {"doc_hash": "a4299580022bef2aaf4d9816c102b6a40d84412273ff43a8ee356bc3edef7594", "ref_doc_id": "b75e8a37-dbbb-4281-a233-8724b1f61791"}, "28c48ae1-fa9e-4885-a906-d939aa972f2a": {"doc_hash": "d2b83705ebce1c84d61919ddd90c0aa23a36db9697e4f29aab2a1a54727fd134", "ref_doc_id": "5fa421c8-c6d5-4344-8edb-d0beb5a07ff1"}, "dfd16fc0-c97b-4b64-8fc3-a53b723e2e1d": {"doc_hash": "cd70784dfcde09bcd44143c09ce430f717a5448dc3eeea0f3cea67776cd19c10", "ref_doc_id": "57cb6faf-7526-4f6c-84a6-cf0d26d8b304"}, "57ea6356-774b-45c5-8997-39626d734828": {"doc_hash": "bf3f9f82a7f1f7ec3a8c0065826e82975bb199abf47ee17dc008b90027722440", "ref_doc_id": "7ac5b2e6-848c-436f-8f4e-4e6c7111f0fb"}, "8dfb559a-c7fd-4402-ac5c-7153ca21ebf7": {"doc_hash": "c34bb4d3d0ad871071ab241c36855035032071c4611cc4eef89322ccafcb8ee6", "ref_doc_id": "545407a9-a8ee-4f42-baed-db62cd6406c0"}, "24e0b27e-b861-42f7-a043-d07221a9c371": {"doc_hash": "f77332bd326e1ce2f88f166866f2a5e0ab0f3d95bcc90052d15922d672b00b4e", "ref_doc_id": "7a23b51c-4c00-4a6f-9e18-7acbc5a17aef"}, "ee9e141f-5aa8-4ff4-afc0-d2103cd486fc": {"doc_hash": "9086fef61ffda228648b3ce726cc64dd07b7f59fe4fa64ec582f3ce03210fac3", "ref_doc_id": "2a46676e-03eb-4e0a-8478-fd9a79f16f84"}, "b05ed1d3-a11c-4187-9c68-4f828de0020a": {"doc_hash": "3b0e8131eabb1fe5e524555a8a8f8ddd3fb88cf4bd671179c3368b853125a9f8", "ref_doc_id": "62e45bc0-2093-4f6c-97fe-2e80ac03579c"}, "c6d4e588-eb4d-4985-882a-c698c70baa89": {"doc_hash": "979f3391ba9cc66fc0d505e6fa69241518956b6be98aaebe20531d0e6381e78b", "ref_doc_id": "bdce32a2-766b-45f1-9cd8-417aefbaa106"}, "5b45845e-0527-456f-bc9c-e457ca84ba5f": {"doc_hash": "ddaca03f1128e6275adfb4a9ed81d82b0dfb6e246fe0f46b4b62464f8bb47b6b", "ref_doc_id": "5850738f-70fb-4cb6-806a-e520a42e91a4"}, "5b96f6e2-87c4-4f5b-a61d-dbe474baf79d": {"doc_hash": "66ff82ed686cdbf66b339b13d03fe213b5523dfa97b8e4816679604367666eee", "ref_doc_id": "329909d7-18a8-4a55-9067-1f234411dee2"}, "6c617d4c-fc8c-4040-9917-62349c0a8339": {"doc_hash": "e2036b4ed63bf21cecb6918736f80846f0e022bcb4184de1296e75bf264b1067", "ref_doc_id": "11f0b836-c8a9-4817-a40f-c14145370e6e"}, "6d530190-db2d-439f-be0f-e5a9bbb6ef8f": {"doc_hash": "69a9ec1d61d15fcb0e57998b9d8adc12f42fc4dce560f4a1481bf45278c30af5", "ref_doc_id": "25e22f8d-1312-45cf-814e-fc06cd2fb982"}, "8f7be33e-6211-429b-a1b1-7c636c62827c": {"doc_hash": "8aa777c313402da4935bb06d3544c3a6f0cbe4e026a098703b9e99db091cd331", "ref_doc_id": "6b3fb3ac-e5ec-4a71-a452-e4231825f348"}, "00b075d3-c19b-4814-b46a-ca5297b5a448": {"doc_hash": "e8414a37ea576190539c80ddcb84011da8fc641cfa9f953c1f9589ef8999f5bb", "ref_doc_id": "5d854e55-f328-4526-b771-6cd53939327a"}, "9faad3e8-6034-4c93-aa05-c4bd6603f3cb": {"doc_hash": "a1afc70d5dca7372128232746a325790bd99285c03dbe52b5b5e6f6a43ef5caa", "ref_doc_id": "c79e5777-7648-4a6f-b184-baf3dd16dae7"}, "a8af6e7d-eb08-4d41-bff5-06b70c702284": {"doc_hash": "9825ef6d2adcaba757f72d23c5c9ef9570d07ba1c9d25196616ab775f7b2e8ce", "ref_doc_id": "b8b1022c-6c6c-45c4-8259-bf4118122e14"}, "bcee9886-8073-4903-832e-20a2f590e3b8": {"doc_hash": "63df8a206116bc64116818d288041131b66fd7a7021c4618ca9e58c119901384", "ref_doc_id": "416b4138-a42c-4040-96a9-51f2e2879dca"}, "f45a8841-bb1c-4a57-9fbc-3fa3cd9bb6e1": {"doc_hash": "5693e318bf951840211a1dc1029028ad17acebf235a21c18ae530a3ea9abc3b7", "ref_doc_id": "b3ce64d9-37dd-4ad8-a31b-3fbc935f5705"}, "0a691d3a-b760-40e5-a929-5f65bcb3f0a9": {"doc_hash": "2d7893c14b3222ffa52eb3d6189858b3c3822034bbc5947705dccb804543ca24", "ref_doc_id": "4dc5a2be-2636-4920-a92a-a8d3cccb44e5"}, "81ab8765-d87c-4cbb-8973-ae7a5a0014c2": {"doc_hash": "1be7aa3917eb4ff1fa26f583ceeb8211798b2789d5ee139f16731f68719d4954", "ref_doc_id": "d36a0a4d-5441-46da-8604-223bea69548d"}, "8012441c-4b6d-4fb0-9c6c-92c659695c8d": {"doc_hash": "47b055f4be5c9e50271739ef13dfbbd4e90d444ff6f23381ecebc1b8dfdc60aa", "ref_doc_id": "b66d1eae-a309-4856-b8ba-7089e7bcf29d"}, "e7decadf-1315-4b79-a20e-528b5e3b8160": {"doc_hash": "43f6d12b4a76b079e3b140b97d7b66b0420b25cdbeddb2862efb85a918b2cc1d", "ref_doc_id": "6745a5a7-0c40-413f-afe3-b36c0797dae7"}, "406a1db5-83c9-464a-a878-345d6556056f": {"doc_hash": "e47bc143e6295cb112d8b6fb65fc61c01ab83944d9b10489b8590693dae8f396", "ref_doc_id": "75cf121a-b58f-47c5-8978-0155ebe436e5"}, "fe5d1c15-add9-48f0-8d0b-97b4fd8b4c7a": {"doc_hash": "d296d772adc86be945c24619ef23ed64a075330ea7f5eb5d1169e73138731321", "ref_doc_id": "67110e7d-6565-4200-805a-df95b4678c5a"}, "7fd15a2f-82a1-4c1c-9dc3-8ab2c3537c99": {"doc_hash": "ced090dde679a95282ca41cf361e555a2f3e39f99d285230d50224b191ec02c9", "ref_doc_id": "abfcf9b2-4c8f-4095-b8e2-2258c90d947f"}, "6bd84bd7-7ced-4cae-b2f5-9ac95b50a33e": {"doc_hash": "c9438777a51c7c547c156539c02edc6aad98660c334dcec99787029db31d0dfb", "ref_doc_id": "12a25404-e56d-45e4-85d7-ee8724ce8943"}, "2e1bdd5b-00fe-479b-a33f-ad2371a9be4f": {"doc_hash": "261f1b78d213995fecb4ba44a812c89965614dc592c8db4ec6a95799ab01a10a", "ref_doc_id": "683e0d30-7a68-40e9-a1d4-5f0ca0230cc6"}, "322a319a-84ce-41ce-8a01-6df685f601e7": {"doc_hash": "770aee758f124841531c6a6734bc245264b82e7f1c15be1b1c364436cc7af104", "ref_doc_id": "005133bf-6381-401f-abfb-cbccead6fdd4"}, "93e2b73d-8b0f-4c5f-8ef2-6ce049e42325": {"doc_hash": "4b3ba32bd192d83bf59c101b0ae2bf1f6330eb5d09075d8caa7a1e9c7bc10ae0", "ref_doc_id": "81056aa8-30d4-446e-a281-b5e7d2d7373f"}, "9486f255-38da-4734-90ba-34209394dffb": {"doc_hash": "b8c286a05831878c921704b2cec23baee85eeaaf01ec507ff0538ba0ce4ad937", "ref_doc_id": "fa8f088a-df23-498a-b721-18f4b314afe9"}, "cb235361-ba69-409e-8d79-29a042146674": {"doc_hash": "2fd38d1db18b7331fdc786baca0a5dba56f94530585ae881e443993fa08e076f", "ref_doc_id": "eec33efe-81f1-4ccd-8e9e-c2abd15b9bcb"}, "021b96dc-f6d3-45d6-84f3-01554c196bd7": {"doc_hash": "4d71990b39fa2327af1ca86493c574df8269c6c14d9d9b6c5267c67db7cc7237", "ref_doc_id": "d4f313f2-f6d8-414a-8570-b0d87ea84660"}, "985d9edc-4c2a-4105-9a58-433de98bd3c5": {"doc_hash": "f5377eb980475ba14575508ca532c6743199800b0b9eed60482000debacae18b", "ref_doc_id": "daa736cc-f5d7-4dc1-bd38-be113d2b0cf3"}, "994c67d6-052c-4463-b39c-b983ab06794e": {"doc_hash": "3833bb3c5321a460c0d95a921447827a5799627f6f77e0aab077f1dc5f6f1724", "ref_doc_id": "abadf77c-9f91-4456-8c2d-e07f73c0cd70"}, "e41b5dce-b645-456e-91d1-fa78c7754a59": {"doc_hash": "5c3c979bd94753c7f13d9cd7e7c11c242354b201441ef1fdd7c8bfbb938020f3", "ref_doc_id": "2c86d817-b5d0-4530-8ef0-105d266f8067"}, "509a2db2-3311-42dc-a583-29bfbd3ca711": {"doc_hash": "206dc5a99f593f0fbb8e1613941f850d536763ba47bdee1273cdcd30d4d5cf13", "ref_doc_id": "b479bc95-ece2-4c3b-a6a2-2dc65c02c4af"}, "b11698ce-a3c8-43c3-8949-2eeb8fe99857": {"doc_hash": "334d3a59dc7b32096186be8765b2065ea5c69982b0f43195003e15eac16aefd4", "ref_doc_id": "7ad29451-98c8-4226-a133-9c4570004c6a"}, "121aa955-d908-49b0-beb7-b844f39a8889": {"doc_hash": "2a2fa31bec8e5bdfa5832c76c11a5be2b0af24480994a54644717397bb7d2fa8", "ref_doc_id": "a8ba6e53-d2e7-4ec6-b201-f82740ffd64b"}, "35abc1c2-7d75-4d40-8b61-d0bed1127e2c": {"doc_hash": "8dc6efa5bce4b7463db8e5a5420d8b7d815cfab5e17607281d4797fb3bea10f5", "ref_doc_id": "e2138144-35e1-4f8a-a96b-911c96d8c45b"}, "d49d06af-418b-4b00-814f-e76d6f5496dd": {"doc_hash": "78eff4c76d3da121d9beef752253587920297252ff9bfcef2abbebae60d32ef8", "ref_doc_id": "1a0f3163-9c73-4fd7-a425-4a4e1fdee66c"}, "c8e13854-d4b6-46cd-81d7-9effc6891b1e": {"doc_hash": "105f330eb44d92eb0661f051d70d96dcb0c1c862bbc0dea167346df7ff83d654", "ref_doc_id": "214b6204-23d7-4e73-b275-87aa7aeba666"}, "96b4197b-6836-4dc6-ad10-dcfc4916fc62": {"doc_hash": "9ef57bf07cde2ce7e70d2735912217603c958d5acce62cf43e485c33b480dc82", "ref_doc_id": "9af93a94-6b06-4d3f-92b0-7f083b70eba4"}, "d6a80241-51aa-4fd5-8b06-4dcd76ba92e8": {"doc_hash": "b341c6472153c82e43c410287c3971f9a135eab5cb08bceaa1260b72995d857c", "ref_doc_id": "6428f0c8-9897-439a-946b-3395e511bff1"}, "cb89fe76-e678-4608-99bc-bcd4c90d3273": {"doc_hash": "40bbfbaa3cb525877f8938ab28ba9732c6c740fbd05ea4ce17b232b73dadb439", "ref_doc_id": "e9ccb713-9c28-4d0f-be76-423f1336899d"}, "95d94a5c-e569-49de-896f-a12ab8bf49a5": {"doc_hash": "9b603066b3ecbeab80b43d95371c17d0f48d68d59935c28b834f37ff5db729d9", "ref_doc_id": "d4e714b7-7159-4342-9a52-768799217228"}, "cce783bd-b444-4075-9995-5a60fa8a2f35": {"doc_hash": "a95bd0769638c3961c46ec2dee7600b89159e9a08d58333a64cab86aacf22ec5", "ref_doc_id": "5719b3c3-4da1-47dd-9280-729d6cc7dfd1"}, "64c1d2f0-b753-4a43-95c8-5282fa898a71": {"doc_hash": "12c70588beba84f09664b0265a89f09f4332de4614d397ba4e3a332f1558dbe7", "ref_doc_id": "05d6ff72-00a3-414e-9223-c65ccc9af63e"}, "301fc0ea-c54d-4927-ad95-6e35e6a407d5": {"doc_hash": "a8a59d852eb7549d606c6d28159c9793c8ce7bb6280bc2394aaa1a1c29f7d10b", "ref_doc_id": "030fefb5-c3bc-4a5d-8dfe-3bf991ed6e6e"}, "7e42b0ee-890e-4b13-aa80-ee973100cca2": {"doc_hash": "87ae0ba0b745cd7c496ee3dcb44f1c6393f43ef194bb91cb79304aa47b570c30", "ref_doc_id": "44fce5fe-120c-461e-aeb2-ad10856241ab"}, "29ad7a9c-e236-4707-ad16-f446455f31f9": {"doc_hash": "d31c89cbfb3a72f2e46778a0143dce35e5d89420ddc36a768a7028f7583438a2", "ref_doc_id": "2b732d2f-c767-42d7-a3a9-35756260baa1"}, "afbbad1b-2b65-44b6-bebc-8e118185d080": {"doc_hash": "6f437767d9a120da808b6b497d2f48fe96c537b52b6e8188424efc95df6a0293", "ref_doc_id": "ee120089-22d3-4dbf-a9eb-ccf775aab4b3"}, "0fd7690c-f313-4333-a4bf-b98b68dfbad9": {"doc_hash": "59a875aa6d07db9be7c19e83a899289cb953793a1ec823e47b421b2ae79e2380", "ref_doc_id": "b0f2cafb-422c-4582-bd63-9d4ed41150f5"}, "9b52aea1-9677-4a5f-a2c2-b4c2f150ef97": {"doc_hash": "6e21e332370749cf7b3de58eb2f44e81a82090b21acd77c684bc6e9c19a94e8c", "ref_doc_id": "103a701f-14c9-4c2d-91cf-e8a9355ed1d8"}, "681767ae-7bbf-4bf5-985d-f9aa9884dda6": {"doc_hash": "c28f3fbb89da5a0cef6bcf1b8ddcd5345b2ff1f2dfc4d6d636391e82d53bd5a4", "ref_doc_id": "a181963f-518f-4a3b-ae2e-d20c0808acf1"}, "a5a7d257-0bfd-41f9-8786-fdf8751bee82": {"doc_hash": "be50d01c61b6a430a4a43bdce51ae7097dfc154d4dc9aeac819b871bc2d54e2a", "ref_doc_id": "c8a71c8e-e7f7-4fd8-879a-f2104b11f2cd"}, "5134b113-4f47-4722-9356-af47ad709402": {"doc_hash": "f433a3fd7b5b05b44630cda91f9d0d74e4adf9500b2d46e26d88cf2db2b9318f", "ref_doc_id": "dc4c8148-7e73-47fa-97bf-86749ca3234f"}, "56b089b0-3c18-48b2-b64a-f884817618bc": {"doc_hash": "37cc2f02681735895a3f12babec46edba4976acd2525dd0b96f214a1dbf8089a", "ref_doc_id": "873dc934-8f63-4168-a833-aad5b89804f2"}, "57d4e818-a59f-47f2-a671-f7e6129386e4": {"doc_hash": "35ed1b08b34b6580c946f6705c74c6cea55731327c3343a4c28b87c0bd9f7956", "ref_doc_id": "d8686e99-b64f-4c80-9ddd-3ec2a9a74adc"}, "2f4fd679-df67-41cb-bc63-de3891d43084": {"doc_hash": "a24bc2b87ff2e4eac5fab13d8e18331d2d25f196f11e8cb01bdf4dcc50e2aecc", "ref_doc_id": "98c32d68-879c-43be-abc9-afc38d096054"}, "a05c1a32-07fc-4bb9-9eb2-2b4149eaf92f": {"doc_hash": "dea85856fc86618e8b027eb0b90fa1ef32f8e249203a36eabbe27b53037ed923", "ref_doc_id": "dc390077-cb7c-4cf2-8f9b-15aa027719f8"}, "2cc60300-0467-4fc7-af20-a8cde500438a": {"doc_hash": "0b2286612f764fad49316f070966d17dc50092efef938ef21010411c445b25eb", "ref_doc_id": "3d37decc-2d4c-4627-b50f-234f99aa6bc2"}, "b9eafb9b-4fca-4e88-9442-7d95f4abf729": {"doc_hash": "3ec1d03ea06e9445a29cde878548f83fe442092c5122200aaad9d77f4acf7b56", "ref_doc_id": "85f6ea14-fbdb-4b68-9147-9b21663b6679"}, "5a82fed2-5b0a-4323-bf81-17fd8a830ad2": {"doc_hash": "c91d0c4e5714c0cd7445a7b27e2cbc579144b56fb47faacb9dbc0faa301ffa1c", "ref_doc_id": "3da80f2b-1a8f-4655-86c4-4635063958c1"}, "96b5fad8-fb83-4ff4-8cbd-3aa041c3e20c": {"doc_hash": "63d6a358e359f51c069e4557ebe4be9edf5e58e050ef42b1cf2863f0c98e76b1", "ref_doc_id": "28a07de2-28c9-48b4-bbc1-0753d324eaae"}, "e303dc33-daba-4743-a90b-cc5d70de8589": {"doc_hash": "2dc1d8ffef44b6b9f7a21b2d6b7a3689f81b75242f56ae2d1f0eda347521791b", "ref_doc_id": "18e4bacd-621d-40f0-8793-a8fa107defb8"}, "4dfbab53-1279-4754-ac68-22842d91dfea": {"doc_hash": "604bb286731df465370eb33a6e226e40f5f130e523ee641a51288c0889f37ad1", "ref_doc_id": "a766100b-6ed6-4a4f-a28a-cabb15ea4820"}, "430663ae-fff1-4cdb-9d95-44a2ca025992": {"doc_hash": "3dd2447bcba47fe1d9e2f12d367d1eb67085b486d15d8ed326f2e6123a71a462", "ref_doc_id": "37725764-f5dc-464e-b305-686052c2b92a"}, "9cab3cef-d1b4-4fbb-9104-f9c7a9020b22": {"doc_hash": "de7aeadd0d651e69a837fecb642ce0639f416b7868114e36094a5b27e296063d", "ref_doc_id": "230fc89a-906b-4925-abbc-29a1e46db99a"}, "6864e3f1-480e-4da7-9363-39a427f32ff0": {"doc_hash": "cd591b57a14664bd375b8613dc94e9b5e06315a4d603a9342e7db88061ecb2fa", "ref_doc_id": "64f551ba-4e02-41c2-b816-0a4a009496c7"}, "44dcec57-d2e9-477b-bd3e-b1b6dba924c1": {"doc_hash": "e672b0352c0b630c5819506638fad9617e43a4e8278e97919f21c1a7d3c41117", "ref_doc_id": "ab3ed68a-8da5-4b23-8357-753ac8ff28e6"}, "12a88d13-871c-44d8-9626-d4e6cbe21f07": {"doc_hash": "7136ba3a68ba7f09bec51ec96a3404260d46409abcc95526c6daa70adf298105", "ref_doc_id": "8b2725dc-2582-4a04-add5-60fcd229c176"}, "0b83b8f6-4ef7-4fb0-91bb-208be322e234": {"doc_hash": "72afadcea55c5f807a34d77fcf315421f5dbf074e9537043ea35658517e6f8db", "ref_doc_id": "d366da98-37f9-4ca6-a7fa-329d3aaeaed5"}, "a6475ee6-c201-4b01-8076-b207ebc81dbd": {"doc_hash": "758745ea6dd419e0f361cb8f5c550b1000a065480889db9426fad13a27f5978a", "ref_doc_id": "4ba84d37-a2dc-4bd3-965f-9f25d4bb7179"}, "18d28e50-c19d-43c7-ae5c-dfb4ab5853e4": {"doc_hash": "2aba54838c0375ef40f22e73ac2459f3317252fad8c5ab1c8414ee75479d8dcb", "ref_doc_id": "08f0afe3-6c02-4ac3-96f8-6b9faf8d6693"}, "de1ee434-c695-4da5-9903-c380535a64dc": {"doc_hash": "2c107382f9d6bfd8869c5c52cf9bd55ac89a949cf3c469183210fa69e9e311c8", "ref_doc_id": "f2914984-93d6-4064-8052-405569b03054"}, "ddba2cc6-75ff-485a-991a-4b2608a6ee62": {"doc_hash": "2d9c495c2493dd8b60b31522e3e69a735b578ba16217896064c152c819929a17", "ref_doc_id": "f0f3ea6e-8cd6-4549-95b4-c927e37cab2f"}, "265dc105-c5a8-40b5-96d4-612fe0879569": {"doc_hash": "bc2745c4c8ad14391e93b5337564ed8f33df9d04ca67f3ff367c107810b0e6a8", "ref_doc_id": "60dfb41b-bcfa-418b-b594-7e9526eb6264"}, "c8daa24f-8184-456f-8e9f-942d193ac94f": {"doc_hash": "f86b222901e73603e81aeb8a40228ea60f12cd1273e1dfc9eaf84766636e5012", "ref_doc_id": "d13ee4fe-7734-43f4-8184-57bbcf381caa"}, "7707894d-c3fb-4cbd-8a35-9864c41c0e5f": {"doc_hash": "a6e825b75efaf73e1e763fbccf41617f5eccf9c21356d01873c78f1c1d4c5e51", "ref_doc_id": "31aaf4d1-e581-4b33-81ee-c639c0b305af"}, "06c7ae4e-65c1-4f81-96b1-9f578d0f5955": {"doc_hash": "3d61fa64d7fe51410ef207d1c6bbd6592d66e24b8874a57f344dacccc1232502", "ref_doc_id": "9d77d323-e287-47c2-9301-d17f17c1919e"}, "ef5ae5d8-046a-4cd3-9e62-d25d161c1d31": {"doc_hash": "699e2f627be944c58019e07553c08d88c5d05aea25f00129104acf7cd51fc8a5", "ref_doc_id": "fe18d9c2-64b3-48e4-ad32-d186e47115b5"}, "4dcffb83-de21-437f-b229-ded6fc244bad": {"doc_hash": "965f60c956afb8687d0d6f4e9983a023c8adbd908afdb73c8ad74a39a94c12aa", "ref_doc_id": "4f12f874-6da4-49cb-97ba-54897d0ff17f"}, "974fb3f9-6a18-43e4-8eb6-b33cb15d9a7d": {"doc_hash": "ac3348a85dccbd13783e443c9c706784bd0dc96e332a21f239ea965bbc6472f4", "ref_doc_id": "1296e427-d73e-402e-934d-cf7b8d5d995b"}, "c88079db-3e0d-4a2e-9b61-8a5c6437bdf8": {"doc_hash": "ba9276f7ca6e61ccd0b2c309919a922d55bcb96a7ab709fb837113620cb64ac6", "ref_doc_id": "2cc3c423-fdee-4eca-a002-c3755d61dc79"}, "ff44a9fa-a8f6-47c1-b09f-07d2638ea6f2": {"doc_hash": "e58d3e2e3b4dcef56e059989d140406597d63b21c55b0beb2191430f9431f679", "ref_doc_id": "b1a62608-225e-4b81-abaf-156355da975e"}, "ec705e36-42ca-4767-8279-8170b3875a9a": {"doc_hash": "997dc29a735b5578205c0f33db94c451b9f396cb4c5436d2584bf6b885024424", "ref_doc_id": "ce859bd8-19f0-46d1-b8d7-1cad32382376"}, "b3dc535a-6e31-41f5-bec0-d747421a859c": {"doc_hash": "dbee63c78214b9fbec21cbf608b8eafb487f6252086cf286de00ebfc2b4de3d3", "ref_doc_id": "42fcd292-59de-4dcc-aa75-a73d0fefc07b"}, "5b1048d4-aa4e-4bf8-8cc8-87501bda2585": {"doc_hash": "6d5f247f3ce8a3eb83a99772cf77469e99d3f13405e53b3120ab4c6e82779c4b", "ref_doc_id": "dd79b24e-a810-4def-8a40-d9c7f67fb015"}, "f53c02dc-2894-4d51-a8df-a0e1aa24be64": {"doc_hash": "06be74d489ab841713bd7b19b4daed58aa978471fe4518558c9508b85a58f6db", "ref_doc_id": "9a391930-3324-4cae-bbf8-653b6acf4753"}, "8171804d-5ab3-4c5a-bed2-430fbf98b126": {"doc_hash": "6232de7114da02f77feba36deddbed971e66acf874b86b35746aaac1cf8c1baa", "ref_doc_id": "bd54a1b5-0b94-47ac-8f86-afe20a648369"}, "2e260656-243f-41c3-b670-f04111a5d0d4": {"doc_hash": "67f266524d0f575b8e7c1612c8c0d3a39aa0309165a91c14a37b931bb7501bec", "ref_doc_id": "96b5a6d7-c83d-4479-8fe1-bf88a7a63700"}, "3b2d3d32-bc2f-4455-acc2-747593064965": {"doc_hash": "4d175f054dfc53604c9a09301f6f23805e8e9aaac4c3e97464c6ad9177402f59", "ref_doc_id": "385ccbfc-d3a8-4df8-a6ef-5c3c0ccb579f"}, "e72545b6-f514-44bb-966b-4f770a5be1a9": {"doc_hash": "db218a11aa3780fa0582b98879280c406c30b3193d58eb682088a0fbc249a080", "ref_doc_id": "34698dc2-68d0-4af2-9f49-a0688b3dd878"}, "3b94ba16-6c47-4cce-baef-1df4232f69f2": {"doc_hash": "3e08b6de8c3fd401451087f0e34a8d239d32451146bea19829601fef82f6ae12", "ref_doc_id": "f3ff06ce-ac51-448e-a7ba-5e199aa99211"}, "c7a18bf3-168b-4df6-acca-4a7276851237": {"doc_hash": "d945a94fa0c4d326c3e72943d092285bbbee71896ca87325cfe2132f4150a06d", "ref_doc_id": "41c78e09-a863-4f35-9f27-b23f8c164a12"}, "ed6e6547-185c-4685-9abc-2765f6ab3a83": {"doc_hash": "be544b44ce205498d440dfaf6a8d1250684b4c87f07baacff84748d5237d1113", "ref_doc_id": "e8587ffe-8f61-4299-9015-9abb78d599e1"}, "65175249-a0fb-4a8f-aa75-5ae0fffb71a3": {"doc_hash": "304e7c5deddb4d35cf57207685f02f4f4f097c4e7e04955cca2efe3c9f174942", "ref_doc_id": "fa8379e5-5515-4c22-b2f7-9077fe880f19"}, "303bf54d-4eef-49b4-87d6-490ab4fe9b29": {"doc_hash": "2c4a84c5f3753b2f6db0a81b9e23d62040ee2921d9fd9d37ab6b007de305a4ab", "ref_doc_id": "2c16255e-c467-436a-8722-1f3772903cbf"}, "7b8b244e-d99a-46d5-b853-8f09b495380d": {"doc_hash": "8853c97162baf8de359f958ea59a98b4a8c893bc88f8200c7d37a7d75e823d1e", "ref_doc_id": "3e0ccae5-53b5-47f8-b89e-458c985215a9"}, "06fb0e3e-ed03-43b2-9b68-10ad687cdcd7": {"doc_hash": "c381a2dfe9f0f06e384c1a76685ce5a3ad8708cc808cb6f355240c56350d79db", "ref_doc_id": "61a18081-2ac9-4ed5-8015-f0616ca0e6c0"}, "5f8e2533-990e-4e66-b17b-37654cbdcab3": {"doc_hash": "b27f682cd37e7c80500d0ee1b619ef47042cf903b703cfc375dc4dc3d2925d0e", "ref_doc_id": "2975c766-d4de-46f4-8217-9ec77d5e5694"}, "dc3ce791-7aa6-4a24-b3c4-6b4a48de93a3": {"doc_hash": "0d0aedccedfed6af1385171e2ab36edf2b08edd8e4c399e316e008e6b3850bba", "ref_doc_id": "16cfd766-7ce8-485b-bce6-4f485d6ba437"}, "fca15130-a494-4d2e-a92f-a7972eece9f8": {"doc_hash": "27efbd20881569ada6af8ec8f5e16aee4ce62d51256f56db7622ddeffb04990b", "ref_doc_id": "c0f2746e-34f6-47ab-bf69-8c1160dd163e"}, "3fc877f8-e3d2-4030-9ccb-4a6904c18040": {"doc_hash": "6dd778ed79ec114060710e3273dbe704cc0a3bf36c15c2a3d4e037a9b5ca48a7", "ref_doc_id": "91943f55-eb5a-4882-9722-e58d971b0b11"}, "77084eb2-36c0-4697-abaa-876f02972645": {"doc_hash": "9c57bdb8229646e85a502d57f201a2ea044c69b50ef60accf53695283b349a75", "ref_doc_id": "7ac8c78d-33c4-4ff3-aa6c-cc529a712235"}, "283a87a3-04d4-4f59-aae6-6561e0d68448": {"doc_hash": "99ee9405490c4688c7962a1a85071c2d363b3bb41d04037ac53dcbd6da6f6482", "ref_doc_id": "37ecb6f0-0c17-46aa-ad50-59d15b0d5925"}, "e6e74a44-6074-44ff-8e3b-eb1107a1a44c": {"doc_hash": "a7fc2b889bf4015cd2d69eaf939ad4dca6cfeed1832e86574f42b329789c0ef8", "ref_doc_id": "5d6116c9-2cb9-4eb0-be46-952399e626f6"}, "36f0fed9-7f45-4d15-bdd1-3c82ac2e292d": {"doc_hash": "92d70a05dae27affa40639d3e90ca174b78826b705741226bab1b9746ddd52bc", "ref_doc_id": "0195fa02-c772-48e5-b18f-ffa0963851da"}, "a598cd99-fb4e-45f8-81fb-02b83e3821f2": {"doc_hash": "9f60008b021c73b35c6dfa3d1e45af804309aeb7c13da1c6b14148d9341f700f", "ref_doc_id": "4aef79b3-9b44-4fbf-93a2-49760af82b5a"}, "a4eb0b79-f647-4bea-a572-b28e53b85800": {"doc_hash": "3f6448f390acdf51632a2046d38c587eccfd74cd5a5e719c236a1906dfb4fadf", "ref_doc_id": "88c043bb-73e7-43f4-882b-254312d41907"}, "d68c6ae0-0009-4c6f-82ba-a24ec545d851": {"doc_hash": "6d9fce702390f9a2db588d5b7dbf9ad50dc399136a94bdfb0a55a92e92ceed2c", "ref_doc_id": "ae002d4b-d816-41c4-aa97-a67e92dd8b95"}, "5a8f5b51-340e-4be4-9dbe-adb1c2bfb080": {"doc_hash": "b8a221f270faede9d2beb3bd025e3717c2a992033673165ada624203cc1db51a", "ref_doc_id": "0706fc31-0438-4e89-84ee-9540c254dc80"}, "586d0211-ab58-4df0-bd21-1f1827425de6": {"doc_hash": "bb4b1e653d0210742be06717c9c4892f786850990b7c8abb66d02fd6776f4f5d", "ref_doc_id": "85df79ec-9b2d-4146-bc90-3d3243546817"}, "7c4a3087-0c51-4ebc-a2f8-b76a2ecf7a0a": {"doc_hash": "49fe812afbb59d44284e8af40efddaa6a92e42c197f6b058b4d978ebca48cd76", "ref_doc_id": "a0504346-f2c4-4851-b0b0-57bcff4f9ad8"}, "7f0adfa2-06fb-48de-a85c-c8ac37d766e1": {"doc_hash": "297259c0909b8765cbff6f3068f0d90ddabdafbd8fa18c3e5d4f776e83c40863", "ref_doc_id": "b820238f-9868-44db-8ab2-4f1d7764aefc"}, "9d15f657-3f6a-433b-a1a7-c0e2e8f95456": {"doc_hash": "b66a22deefad7f801e317002d2832d6261664f35fecd83a5a6281fa339ec79fa", "ref_doc_id": "e8646b04-b0c9-40ab-9626-fb4ac1efbce4"}, "316aff54-5d5f-4891-986e-2c13042a2545": {"doc_hash": "2c8954c8d4e9af0c9999b95e1462518baf1ec778d8e8d2e9f16f5b866e5d63f4", "ref_doc_id": "2512d278-3030-4650-b0cb-8e1bcf542f31"}, "04bb4681-4d9e-4f9b-8d02-a438f5e321cb": {"doc_hash": "0e73c232219421f17c694f2e944305ff927f310759752ad5653822ded96bfa47", "ref_doc_id": "c016a968-aad6-4f94-b4ce-d44df22c743a"}, "92717293-a186-4a76-a8d0-c0c8069e2101": {"doc_hash": "46401d6db5d25c8ca457637a7a2e9897b1faf229625c6c9e4d92ae7c62dfe740", "ref_doc_id": "83d8c6bb-9ba7-462e-92ec-ead4307b3bcb"}, "39450837-1b63-4b44-93f5-2509da2a82be": {"doc_hash": "39937c6d8a6cdec992d1803f3351fbb52330594d607e8cb7ef11e258b67b89b6", "ref_doc_id": "37752dae-6e4d-4fba-98da-5d18c7b373c2"}, "9c36e9b5-688c-4973-9090-6d366ea6f28e": {"doc_hash": "3a8fec217dd14759b3fc60786408af37a6358f2ab7760510ef6294314e752b23", "ref_doc_id": "f0346e8c-7911-4514-9234-1663b08b6311"}, "6c2f8b1e-cd1a-4504-acce-be9274172911": {"doc_hash": "337558e4d7d9e6023117f67acfc4278d66c6677bf3472ec718e77cc6ee997e6d", "ref_doc_id": "36f27c83-4011-435e-b9ac-bd3938e5837d"}, "35c08bde-6c62-419d-b431-31432392febb": {"doc_hash": "dfee59f642fe1e9baf2f880327aa5640e59072b1a5895b50c493396c9f0876e4", "ref_doc_id": "494d1a79-cc74-4505-9308-0a491a6bb8b3"}, "aeb7fda5-aa5e-43f7-842c-70402d0edd97": {"doc_hash": "5c26adfba3c62422bd2af0e2f8692297000dcacc5fde8059a6e1251efffa37c0", "ref_doc_id": "e9f5f7c3-c675-4a3f-b33b-031873ea9dee"}, "320d9f0b-dc69-49c8-93a9-66921d93ccb4": {"doc_hash": "7e16e049dd93adb87e097ed3d44fe82d2cf314b504daca5942ec9a1527c0fb7c", "ref_doc_id": "24ae9e8e-a48a-4171-95c2-f354f172fba6"}, "f2ad224a-fd52-4853-b7ea-ed54f79081c6": {"doc_hash": "a2dc5e540c701d7dac022d1103823de6bb1c840850afe6b90bcae7d1adfe5361", "ref_doc_id": "65fad22e-7ffc-460e-8c4d-bb24313d80c0"}, "e9124981-0080-4951-b4a7-57aad50de1ca": {"doc_hash": "dfb8557835ad0d7d93efda10aaa62890fa6fe9c3caa138abe4f9e6351c5cfcde", "ref_doc_id": "b2cfd4ee-de55-46fa-bf3c-fcdba6af709c"}, "2ca6cfcd-6fa9-4410-bb72-df22d4954021": {"doc_hash": "931cfa91cc1919e46cb5f8c551cf126ef311bd012607869b7d52f3063324f461", "ref_doc_id": "b2e1bb14-49eb-45f0-8667-04e87fc85a62"}, "81abb764-db72-4a06-ad32-ca2eed3e88a0": {"doc_hash": "37471b28dac4bf6caddc8cb44e264fe203d489c6dea6dd01660687f4e363fbb3", "ref_doc_id": "98c1edf5-6380-4f4f-8134-cff7519a4b8f"}, "d172ee5d-3250-4b91-a02e-4dd76d851b97": {"doc_hash": "a1e1b0de1b1f8c0e6b6584104b878ceca7b88fb84d9daad1776c026c9b3250bf", "ref_doc_id": "669c54db-c164-42d9-928b-2d495aa437fb"}, "8d5b5769-384c-43ac-b0db-472ecbf6c8bf": {"doc_hash": "91480735180ab0aa5e5906dcc42dc9a75ddbce3a3cba50bb8c5dc82da5662acb", "ref_doc_id": "3f261dca-43a5-491d-8476-7ff12d1c63b6"}, "980b0587-0eab-4684-8eac-1abf8ee5c29e": {"doc_hash": "a7a9175e686a000eaf189d586ac4bce7b954946222dfbbaad98fc613e679bd72", "ref_doc_id": "10f96c88-23d0-448a-af3f-769d08394940"}, "565f6e2f-909d-4c64-8339-5ab5178bad59": {"doc_hash": "a2c8f25c62d1db91215653e902a489e5324ac2aa6aa2143d1b36e881e284ff6b", "ref_doc_id": "57ab765a-d484-4a1d-9aa7-65e553c5eeb3"}, "7a7f1f85-bcdf-4975-98d7-943ddff9cc2c": {"doc_hash": "5b703e30fe70ab4580a9d462301fba90ceb31842dfc7d58b34c091a7fe33c370", "ref_doc_id": "8c7ddde4-4805-4527-8c8c-5c653cccca41"}, "1e25043c-a641-4f97-8e26-9f06b5b8d9fa": {"doc_hash": "a3285fef95d0b301ea015ebefff6c4852a47092b49b24674ac5abbef844c488e", "ref_doc_id": "05a6188a-f8fb-4187-9dfb-c234789f9564"}, "4c33b4c6-d531-45d3-9fb7-934f3c7c895d": {"doc_hash": "44e95f315273d0186e6ef756cbd345a0aad46adb5ed45741ede6128907f15f27", "ref_doc_id": "08def93a-0740-4196-8b6d-3559d0da9d33"}, "0c04b51a-76c3-45d9-967d-604901b52ff3": {"doc_hash": "0fd3f7f0393f08e18bd088048e3a631acd9602236e6b237ca1fb418b49b3a255", "ref_doc_id": "0ae15f81-7174-4fe1-8c56-a7b4c93b62ab"}, "618d8338-5b0a-4602-ae38-6c166163bdd5": {"doc_hash": "f1b016c85653f96bdd86b3a904b71492e4578879c11ec9a9e576132f38b39bc9", "ref_doc_id": "20572619-d5f2-4e6b-a1f0-0cc11ec723f8"}, "8b939cfa-30d5-4075-96c5-5cee90f63843": {"doc_hash": "fdd90a4118ab3715f9c5e603fb10e3e73791be68b09a47a8cab6014b1ee33a6c", "ref_doc_id": "54015bf9-880e-4cd0-93ac-1230cb28da6a"}, "f8968ce6-e56d-4ba0-a655-d32fdd66e1d7": {"doc_hash": "5ddbcf165763a066b89bbf7a32feff60845a8db2a8ceee3ab257a8e806b723e6", "ref_doc_id": "d8915d10-a1ee-48c0-8cca-ed30acdaf313"}, "f23fe794-3ea1-4871-a851-d6bcdccfc904": {"doc_hash": "09d43f65f4a5bf3080520c666880172d6392a18e5e82f3cecf215f0caae742fd", "ref_doc_id": "acddec9c-c458-4f07-a33e-8ea338cfbcd4"}, "b69c638a-09ae-470d-9335-a9275df7f663": {"doc_hash": "54d813cc0618fb7019f41ff9c5ea2938b63f3be5ac09c73f680bcee0c1f63f28", "ref_doc_id": "283339f0-9ebc-4c0e-b353-0aeb23550b71"}, "74b3b6b6-cd87-4a47-97b2-b1509ebc0263": {"doc_hash": "8adf179e2b8876abc5cb67d6aa69497888ad20ad29f23b31b904f0f68751aa6d", "ref_doc_id": "d6cd164c-85e3-4923-949f-2bd6db4a671f"}, "75e5c486-02a0-4be2-9301-0cf8fee69db9": {"doc_hash": "25245f2ebdb017e75f3934cbd98ce52cfa2fd33b1d5c8b5bb261fdd4a7109d5a", "ref_doc_id": "19a07a60-813a-43f2-956a-e466ed8858d7"}, "41735202-8403-4ee3-82b1-fe5c0f64a350": {"doc_hash": "ba69d97eb5f128a3bd8ee97cdc7e565518a34d39b62fda9cacf1eece37da6e47", "ref_doc_id": "de795d82-d754-4141-ae66-3b88ff19a6ce"}, "179626e4-bbca-4d32-8957-5ff747102dbd": {"doc_hash": "e254d795e8f9b1027266c1608d36a5d822e09641664a1492d030ea3bf6f6e93f", "ref_doc_id": "3e02a2b0-5f09-452a-be8d-7c639cdd6dd2"}, "561d0bf1-5365-4a3d-b7f1-ff1d41334ae6": {"doc_hash": "8499cc0c245e46f46826583de19566a3fd6ace19263b5dcb869e313b055491fb", "ref_doc_id": "348691c7-171a-4395-a2d2-a0b9bd3685b7"}, "0ef7f2d6-1601-45b3-aa28-6cd0502a60cd": {"doc_hash": "1e9fd9915399e16e6f1119fdb5a2feed6d55afc870082622a6a061e1b98e75dd", "ref_doc_id": "769ae878-bd45-4a88-b877-e317e42087b9"}, "1301251c-b25d-4b01-a1c3-a80484dfe91f": {"doc_hash": "65fae120f8a5be8062fd35960fcd9ddf2595213881a1db9c98a46827b62bad02", "ref_doc_id": "86b4e1c4-9966-42e4-9878-2f7b4e96c0d8"}, "b6ba6624-0890-47c9-89d6-1bbbde8b0928": {"doc_hash": "263eed3974b27c66e2bd7f512a489988f7a722f5de7d373308f21f271cba7d24", "ref_doc_id": "1e37c8a0-d7b3-4149-bd29-98a638595110"}, "928f207d-4d47-4c97-8a29-442612f2a7d9": {"doc_hash": "90d882069e0dd561220b39141f4ba2be5d7c898f2016e98c2b6d065700b27b3b", "ref_doc_id": "b5e04a0c-f2a5-4941-9ce8-96ee6694e915"}, "ecfdf064-b916-4d25-8173-465c8db9a456": {"doc_hash": "1b19659d6fe763f1e84171a7d859155209f7e698dd634670d00f0fce66ff4039", "ref_doc_id": "5697515d-9a6e-4ad4-b720-f916cf9ebc9b"}, "3f40aca0-e07f-42e9-9230-9131904cc449": {"doc_hash": "a29d3210a62fe6b6dce6256bea1746c77dedc27ed11fb331ba85a220deb5e41e", "ref_doc_id": "7e619656-f68b-4ef8-8f07-4bda71a79a41"}, "6751dfd9-296f-4efe-9fb3-f43f60545431": {"doc_hash": "f810175ede57385ae7deb481435d0e5a026d8886aebdecf67c0db7a400e50008", "ref_doc_id": "0f34d5fa-b64d-480d-a677-3f8b015c5922"}, "3a3c6404-e4cd-48db-a8ac-4518839b8b72": {"doc_hash": "e62b5b36bcf68c181c2c0ca24a66145998378048d32a574dbc5f3d66cf3354df", "ref_doc_id": "44a84d35-c13e-4ce0-baea-29dbc0944781"}, "30d7795b-a2a7-4f91-a408-0a4d3c4b009d": {"doc_hash": "2e689566dea154f04bf738492f7541da939c59854b0fb07a244c23eeab2e040e", "ref_doc_id": "f5ff7f61-8596-45b4-8c6c-16a7d43ce2c6"}, "3dbb222c-e798-408d-bf54-8f0e23bfa877": {"doc_hash": "46f7eaf632cca50f290c69f5621e52647f54851eae1a4fb4e578e6ce03ac7edf", "ref_doc_id": "87b955d7-4288-4bdd-9df6-e51ef925cc0e"}, "db950bfe-06fe-4b76-8ebb-0ff02f627f54": {"doc_hash": "920ed4f54db6fa0d3b04120df478f8bae7e7e905b474f9137de7829923c0e983", "ref_doc_id": "2b97f61c-ee76-4d52-ac08-40afff2196cb"}, "e62e2700-b799-48e0-89cd-62af6a821370": {"doc_hash": "5a59a097ac98defc95941355db4bc35f7504b0bc8d1b91289b85fa3261294634", "ref_doc_id": "0f2ad73d-88e7-4e3b-982e-bacdf3d1682c"}, "6beb62c9-40e3-408d-ab55-c5e1381c7c58": {"doc_hash": "3bdeb91a4926fe7e6b0f4d41d77040d7bcae71cc2bfc06263bbeb80698a256d5", "ref_doc_id": "c68a7d62-dc0f-4bb0-afa9-ce7bf196307e"}, "d14c9939-e92b-46fb-92fe-0ba99f996112": {"doc_hash": "15b72fe3e1ea52476929c0df35501845d78d29a77967d6df9bd84936c51c1030", "ref_doc_id": "e46784c0-9dee-4176-a042-1b1cc5afa453"}, "e257ad70-e07a-4316-8fe3-0277f98cb971": {"doc_hash": "606ae671857b6eef7533089b38b4bc436928bf5df48ab80cc6ddfda4a6fdd363", "ref_doc_id": "4337020f-f90f-48ee-8a3b-d0d401c4a4ed"}, "9c524edf-f84e-4fbf-b6d2-c31053ab9ef7": {"doc_hash": "41982aaf1c17fd87c89d512146afcf757ae5cc4f4d310e8f837c679a2b5f4950", "ref_doc_id": "c0f570d6-9e2a-4493-abd2-f0e4b74e3fa6"}, "4f0fd80d-101c-43b8-b918-0ad8ef951c6b": {"doc_hash": "9c21a3d93fecf65c0d0ce826135d2181fa7b3220b5e3fd8ada3637dd90f9cbe2", "ref_doc_id": "6182aab8-b995-44c1-90f3-6c8200b05ddd"}, "e258e30d-8a5b-48c8-ac6b-2397cb3860e1": {"doc_hash": "1b9edf46b7872afd3d84d027074d926d69cf469ee7ed35bde68d37b94d4212d5", "ref_doc_id": "2f2e7977-7e08-4f03-95bf-cf2711350739"}, "7867fea4-e9ae-42b2-897c-b01216c83940": {"doc_hash": "23f7b17bc5aa92d197cae3eb088597cc1f45c39730c4f67ece54dc3866ca1b25", "ref_doc_id": "1e82188b-f13a-443e-8a08-633a0f859b0c"}, "8d88ba50-92a7-4408-963c-48ec1ac0c710": {"doc_hash": "4d981553e4af3770aa4f4290f956383bd1a5b11d53ea5206c654fae278de0a05", "ref_doc_id": "651c8b2e-96e3-440a-ba36-7e88c9108d16"}, "be33a365-87ba-4221-aaa2-c8db2f329d71": {"doc_hash": "da946440c7abc0ba91585f626085ab26b47725398717110e0a88ac142dbbf3f2", "ref_doc_id": "7019f3a7-a74d-4a54-a068-853be1bdf7f5"}, "7b0bd446-eea3-4f05-b715-302b14ebbb10": {"doc_hash": "558316aa411399b76e2a0a002816e14e07567997b6761be75a163e2f3ea24042", "ref_doc_id": "a97ccd78-598f-47a7-8996-40ff72c0f6de"}, "c0767d44-8e44-4dc1-9871-db339fc9901f": {"doc_hash": "ebcbe285edf17aba1e824f83c7cc48b3d5ec629714cf7a688bd6f44352110b3d", "ref_doc_id": "ddca0521-04fd-4472-aecf-03b85ecec732"}, "5182c7cd-3f74-4378-b724-a625570fcc04": {"doc_hash": "c5a79497b7dca7157df7ea3f965f1ff6e339ff9c0f7a016bb7261b86ef312f7b", "ref_doc_id": "a543476b-70d5-4405-8102-903b60de501a"}, "69cbc88c-384a-4d04-9292-de2dadb30449": {"doc_hash": "848095f415c915a70041e9ca578fb2b721a4a76ba263e3447fc5964c69ab4209", "ref_doc_id": "d81ebb34-568a-4ca6-87e1-a42a4fa68357"}, "baa80214-e8e6-4411-afa9-5a0c169d51e1": {"doc_hash": "c79f9490ec9edcc8c6f3d9bc7fcdb0bfe3e9f392b1701c8a3669fcaddc864108", "ref_doc_id": "280d1ef5-8cb3-451e-abd6-731e72030028"}, "23d087e5-0b9a-49cd-9dd2-34661c13b414": {"doc_hash": "8f8ef9efc0e78d4a14dda309d99aa5b8406526b3c962675fb82569a6e75b85f3", "ref_doc_id": "eb1d7f3e-5f5b-4470-a8a7-7f48b68b40d6"}, "0d3f3b07-a9a7-44a9-8100-74c4e3900561": {"doc_hash": "2a6eba5f5bdbe2b4156a713840d3780f63c82bb12f30b775aadcb99fdd89c56c", "ref_doc_id": "a2fc76b1-5158-4836-8450-b224488c562a"}, "da07b93c-1fa5-4e64-9f59-beee4754ff1f": {"doc_hash": "62d19d2f21f32a7f6c69e5d57802604a800e821652bee5bdb84b74e6ddf99b5b", "ref_doc_id": "d635c7d7-fe12-4655-b170-64a3be476383"}, "c69f4da4-04b7-4694-b7ab-925cc7945539": {"doc_hash": "e6928eab5fe221a8964f17688f02714dcedd77981d8329224767f059708cb4b8", "ref_doc_id": "d5d20651-adfb-4f5b-aa51-6e2e6485e9ac"}, "5b25d82f-1b58-4db6-8e58-9e69983c7eee": {"doc_hash": "3479ad080fea9f625c9cb77840734f2e1adf07e36f9e74dddf249654530c2737", "ref_doc_id": "09aa6e63-55a4-4d58-80bb-5f31c5da631f"}, "02a86635-429e-4fc4-b356-e72553995a03": {"doc_hash": "3e515778c56139ff55252f003d05f6e0c12ac9262dc0d93d0833a665b0ea2b5d", "ref_doc_id": "3bfccd3d-5e50-44b8-b589-68d25a9fcd7d"}, "eefec921-39ed-4192-9e5d-8c4777188967": {"doc_hash": "8475bc82ec51fd2ea9524b2fb8a5f680bc011fec038e2396fbda6de12f91a6ae", "ref_doc_id": "7c1e82c4-7a78-41ad-a36a-6984f463e942"}, "7775c25e-9f92-4ed9-8377-b2f4128b5f5e": {"doc_hash": "7b230c16303246fc4654d33a1bc9e45095f978a0ea3507333d4c5ccf2a459dc3", "ref_doc_id": "344d98cb-3668-4b68-9244-5302150bf8a4"}, "b651f8d9-d310-4170-9c22-2d2c83cf2015": {"doc_hash": "31205055e9eb798216d5baeba34b4b0d296699f8c01c34bb502ee588db663b80", "ref_doc_id": "1c8875e3-2734-4c9d-9ccb-d98e56ab3b88"}, "0f73c86c-1162-4372-94fc-883d11c80fa4": {"doc_hash": "372819d3d1959742169653e7469d258c5d860bdcb0d2445aaf7e816fa4a3649b", "ref_doc_id": "956b72c4-b94d-4bdb-a1f1-d751a76f4f27"}, "74a77453-e29f-4796-b16b-64cd9c07b2b9": {"doc_hash": "13e9222300ca01800151fdfc139ecffe0c2a0814108693f2d0f2ba6c071a3ce8", "ref_doc_id": "8e86599d-7b92-4369-afb2-697c04aace39"}, "c9a419a7-dcd6-46a5-9eb7-51802c7488b6": {"doc_hash": "8727b1a4812fdcc0f41127a7b68b244f8a4f029b9f86a128fafa05449ac03de0", "ref_doc_id": "1e65bf96-4437-4a91-8a4c-d595e883e6ec"}, "771b2a88-48a5-4711-8fdd-6eded2b2ab3d": {"doc_hash": "911e09a7ea38c6130655a7aec4e5c37af8ab8ee3951f9ed6bd097bd42629d0e1", "ref_doc_id": "973a168c-2c19-492d-b3d6-780f1ed423fc"}, "6935c2da-a36d-4996-8f6a-60d1f4f0a83e": {"doc_hash": "a4903f5f580ea6bdf58331538be454c1979185550e0bb6b192433eee3e3e21f7", "ref_doc_id": "4befef3e-ceed-43e4-8556-0b23b59c302d"}, "1bb718f6-ed35-4b60-9414-7f5a64bdf230": {"doc_hash": "76f8892f426bf4fbdb2708f4838de334f78f2c3cf8174e85fcaf598845e74d13", "ref_doc_id": "5bf16086-ab9d-45f9-98d0-b62d20c20e1d"}, "7f033283-2b30-4d83-ab86-659b0b67f114": {"doc_hash": "bfdc24bd4123eb5f79c6d085d5a706b65e315d199d3738fc25f104ac9495a6a9", "ref_doc_id": "152dc485-c35f-4b67-8054-9485b33fbc31"}, "ec107ed3-2444-41b8-bd4d-cb3b552f92ba": {"doc_hash": "622ee8152151468c066ab7fbcbc5de0e7f2dd3436edfe772fa986c6eaa2215cf", "ref_doc_id": "bc5dcddf-91a6-47b6-ace9-3bde0377f39a"}, "15d32515-ba1d-4e6f-b1c4-5921b610b0b4": {"doc_hash": "b8f345b6da4fb6ced372d5195b3809e615f4d013e59e8f607bd4c000f38183a7", "ref_doc_id": "1866a56e-4726-4022-8cf1-dd68d0b5efe5"}, "22cb6cc1-fb75-484e-b89b-dba4da317497": {"doc_hash": "b82c96c71c54edf86aed9844713d0e570c1943380ffe60b1eff56b19de01e72f", "ref_doc_id": "81419660-eaa5-4cde-a282-a31e3e70cc4b"}, "f15d7924-e0e2-42c6-a4a5-38e0f9fc31ef": {"doc_hash": "d2a92cc5ace9f24fae05e2bd6b3e71c88f972fad8d84249a410b0971afe4fd6f", "ref_doc_id": "d0357042-73cb-4bd8-8114-fedd5c80d885"}, "8495a69b-bd69-4b34-a2e0-2a2068d8509e": {"doc_hash": "34924b6f46abd2c6c0587d473ed2a4b32ae0c82edbccdb4be58ee78900cc6bf0", "ref_doc_id": "a4f9cfa2-8f00-4beb-9aee-9a43a342409b"}, "fc4ea02b-19a8-4f04-9ef4-fb3e6f2ea994": {"doc_hash": "84d9023561393fa18a47d1d38c9c81ae4c893123058625cf27dc45724bc8d2c7", "ref_doc_id": "c855bfe5-9c80-49fc-9c96-8363897da3f8"}, "93468089-4cef-468e-8b3b-dd841ed7dd89": {"doc_hash": "67de0b7fb604c0abdfdf1aa7c7b907b0eac8e5b403024551cf24993d5073fd68", "ref_doc_id": "68550d54-4c97-4dfc-8216-52f790a932b7"}, "496c27c2-50c3-42f0-8741-a083e23ff5f9": {"doc_hash": "bc279d86b5d0692dec0f76d4e104875f3046a015d672009c153581dae62c3354", "ref_doc_id": "8076c518-d4c8-459c-8000-a401e4dda760"}, "040f73eb-1bd0-48d3-9fc5-da6ff10960a1": {"doc_hash": "2c8f9e5682197449ccb70634be5c53ef8b13250f6e88cad229275234b87005b0", "ref_doc_id": "9a4db0e2-2c1a-4748-a937-e53ad119f5f9"}, "cfb8b5e8-8291-4e8a-ae27-7981082ac8b3": {"doc_hash": "9a97af58ea037e8501075da38eded720f38d45b8e8335dc465d9a37386a4048b", "ref_doc_id": "b80ea78d-1c10-4080-8683-a6c705f6c303"}, "cf017f8b-80a8-416f-9144-149249dbbc4f": {"doc_hash": "cd4d4b310571d1cf412caeb37abcc9b4f8f8ce3fd0bc57ec4403d6cdb09fc040", "ref_doc_id": "d2ae7594-0539-46a9-9879-5197d24318f6"}, "a0b544c1-5ed7-4e48-8552-af907b38a28d": {"doc_hash": "58ede4ac0412e264eb5bbf78f193cbb8d9f2ff96d18711ee18b5ead753a43303", "ref_doc_id": "683e1873-ea0b-4047-99c4-e736cbfb4df7"}, "ee55e73e-1a3a-4845-94ca-dc20a8496c6a": {"doc_hash": "8293e7465bf2aaf21d3517603eaba9b079e32c0bb7b61ec9997c454aa52f4cb1", "ref_doc_id": "c40cfe6c-1291-4d7f-8568-1c924878b41b"}, "abc0cec7-7e75-411f-a64f-80ab8ee8fca2": {"doc_hash": "37a2794d09e3fd4dbd461a5cd9975024131ae2cccd987e0dce9b58a1a6113275", "ref_doc_id": "9a5c1e87-f0c1-4b5a-92d2-0a9044c708f5"}, "35c12bd2-823e-4abe-a41a-89e64b429294": {"doc_hash": "bfcb6a777ce7d6648faae256bc9370508b25ed64b079ab07e1027acf78465077", "ref_doc_id": "2185e75d-63fe-450f-b510-55fb11f1b243"}, "72f23256-f74d-4625-946d-f7d38de94585": {"doc_hash": "821b62e31d96776854e53b832288d7db26b3ad41251579bae0ed652e8c717519", "ref_doc_id": "663b50ef-319d-461a-92e1-ea6fa26adb12"}, "e7b4c20c-cb8f-4c4c-84df-7d9f395b4179": {"doc_hash": "052255f3163741f26914f87d1a8f24855e61a4b278b71d745acdd464a7677ec5", "ref_doc_id": "7b66b086-3b1b-4414-8ec7-c09b3506cecd"}, "9a250d43-f9a0-4819-b66c-4fd0549a3e87": {"doc_hash": "398f7108e342e8bcf9650a41adbc18f994beee53e25f8244b51abd730fc442ce", "ref_doc_id": "b9a31115-dc09-4e1a-8519-c6a13be8e4ec"}, "bea2dca8-5e0b-4718-87e8-3eeaffd7e465": {"doc_hash": "6728075b08df911e51037262b1a6ae8304c64edae0534a056df03c3030f5d27b", "ref_doc_id": "efffe59f-d078-4e64-b174-dac797b53c5e"}, "d9b48161-3c41-4418-b68d-2fe3646d7676": {"doc_hash": "59372e87069be12a98f807884e7d533ab8f5063b11d3e9e7781b4d1ce1cb26fc", "ref_doc_id": "88ad0fd3-b84a-41aa-8528-3a85c17356fd"}, "c20c91b8-c61f-4cc5-ba7b-0b9ae371f7ef": {"doc_hash": "b75be725c5cc98d26927449821db3ec24103dbea3b57c1e4456b21ca38ab89dc", "ref_doc_id": "1d1ce352-2bf2-4df7-a62f-7cd32edac32a"}, "38b56389-b8b7-4e49-94b0-f86ef1a6c077": {"doc_hash": "a183d937c53772a026ceaf0ed60fd62e25528940ea02daeba2aa073e5251f695", "ref_doc_id": "d77a8f98-01c6-4092-9d4e-689fe0317269"}, "70acec92-85cf-4853-b0b8-c7b3378d6cd9": {"doc_hash": "f021c41b648bf9630a0532a2bcff6722d70c06ca49e41846e8c542b354b93920", "ref_doc_id": "4634ecb0-a0c3-43dc-8858-e31e6b1bd7d3"}, "14171535-18e7-439e-8fc4-196432228d13": {"doc_hash": "70fa80638c01202cd429525f84cceff20f3488ff71742103aaebb01d48ffb0f0", "ref_doc_id": "eca65b99-5757-4ccd-88bc-c88c827b6260"}, "4aceddb4-bef6-46d0-b780-b152ef7afb70": {"doc_hash": "57d969818fd62320ecd7a121d3f770ec519ecc356862c4d5650b88de43f8fc4f", "ref_doc_id": "3370b1ac-4fe6-40b2-a719-2fc3adf37b17"}, "cde1b907-4031-41fd-8390-f8937948d57c": {"doc_hash": "0f388634eea2ab12f1e7652e61517f7ba3e46ce95cd2e6f4f7f5ddce9ae41e70", "ref_doc_id": "cca66543-14d9-43cf-a912-787d24d8b983"}, "d9721c4d-68e0-4d40-b20e-cb2d16c3649b": {"doc_hash": "5cd4acc74a573518f115b9d2880d3e2056d73269f20c8e38242041faf5e93df1", "ref_doc_id": "37c93f88-d09b-48a5-82cd-2ee15bbe4919"}, "15c68cdc-8c78-4217-b15b-5fa78fc8ddac": {"doc_hash": "bdc54c9626bbe60483cdda8494dec1725bcdcd0cc8646f16c9695626a03d6286", "ref_doc_id": "932e586a-ea57-4faa-9b32-e98017647917"}, "4cab2d74-a52f-45ba-8e2f-18de11d07d1c": {"doc_hash": "02cfb6e073c197f8b6f5fb84df503c57ed8486012af05d244df6cf753e9db13e", "ref_doc_id": "9196b97d-32dd-4675-b2cb-5bd7ee365e84"}, "57b34d24-928b-4af2-8af8-e06d6f404175": {"doc_hash": "c069f8bf74b46183e775ce6da5f63af2d13470f91e4ca55571da7a2cdd403211", "ref_doc_id": "697667af-5a60-4eb9-9ec5-833d915af0ca"}, "fba38032-63d4-4215-8917-bce375464cd6": {"doc_hash": "e496ffef51e2a4e9f1d0e2c9ca515c509370ae508a3ab3848e38c5f084288d9a", "ref_doc_id": "05033585-b194-4dd8-a521-25ad5f082f25"}, "386f25b4-a00e-4d69-89f8-11bfe7537874": {"doc_hash": "679625b78188a45e5aaf908ee81bc80ee01352f46c45d44a572ab771ae60e56d", "ref_doc_id": "2d23763b-35ba-49be-bc24-4131a38ad76f"}, "c6250e35-e200-449b-a7f7-c5895e72d2d0": {"doc_hash": "237745335f6f07d081c35f945d672062a06ae2489dc43329ed435157a89c0bbd", "ref_doc_id": "6b2fd142-77ba-4699-a5da-c500089fb5fd"}, "dcce9de0-f090-4cc7-9b55-dda442826a14": {"doc_hash": "c98721d9e42554cb8a2761715357178edd8297a695108f9933571e98797d2b46", "ref_doc_id": "2ad50fd9-2ab9-4cfc-a072-841d182ec51a"}, "3a9ca515-8699-478d-8eff-8412cea9d39a": {"doc_hash": "54d3f51c1a9d3f948e5fe3614cdd19d4aa195eb30b2b5402565617bcff97342b", "ref_doc_id": "0e73c68b-c872-4f49-bea7-b611cf550d66"}, "46855935-001f-404e-a6ac-ff8719e81b9e": {"doc_hash": "5c45d1b7871764b044a88113fec40303a482d80cc4615234d81e51b1b41e1a99", "ref_doc_id": "30dcfeb3-14cc-4762-aece-246f70900bc5"}, "79964cb6-b6e3-4379-a8db-5014305132fc": {"doc_hash": "db5f135f50ee9597b24244d321919382f22b8fea765252aa7f52933694388388", "ref_doc_id": "3e05bfa8-c92a-4deb-bc29-41b02384c11c"}, "075b6f66-a8d7-4010-bb69-93c1ae78f005": {"doc_hash": "de037a9fa4213fc128980ae84a1da767429b3d43c1162a599da60b8bc5b4ed1d", "ref_doc_id": "84a672db-38a0-492c-b1a5-311657511aa0"}, "c8be58e9-5042-480d-bb7a-e08eb1878a56": {"doc_hash": "cb08e18b8c43c4fe60997e62c0755dadd239d8e06f896e0bcd992c389bc5cc6e", "ref_doc_id": "1ed1b7d0-1dbc-4a38-b0f9-cdb2a04038e5"}, "56882d47-827c-49b4-b1f4-633d482a5bb3": {"doc_hash": "3a1db929bdafbb5ea66e0b61afe240bfcf1c288c500182f1a4a2ad595ce8724c", "ref_doc_id": "36f7aede-a271-408c-b941-5d244187cec6"}, "6f5d1d11-31a1-4064-bd3b-e3650ee66979": {"doc_hash": "b153e9b3dff77d742f09cdc1c79518787c803956c63ad75e4d44afdeb782bbfc", "ref_doc_id": "fba8b88e-f446-4304-8d71-330148a57061"}, "2f152088-2abc-443a-a0e2-847f88a5ad14": {"doc_hash": "e423b96e3e3cb7f9d1d9f89f040bce4f7957508e4a40ce27eac30d6759bc3757", "ref_doc_id": "e5e9cf33-f963-4e5b-be81-902b7d729e3e"}, "598dd297-f754-4755-b85a-fca15bf4b98c": {"doc_hash": "4965d51d1bc1df2a2d8f97b437f5edaf79dff0b501318d824277bdaea75c5e2b", "ref_doc_id": "7e9e1df1-be84-4958-9b3f-dfda49f6687f"}, "69ee86f2-caa2-458c-a7be-1971f4246313": {"doc_hash": "e515a0344d9c4b0c06dd877c8de431286a9f6edf2155110a179f948a5cfea8eb", "ref_doc_id": "d2b381fa-9355-4137-9a68-23eb1092ae4a"}, "142e72cc-0e7d-4de0-84cc-6124c4ccc4ed": {"doc_hash": "dd12058df4a916c0e1ccfb120df091b69f9793e625f28c4e6da5a82e969e72e6", "ref_doc_id": "36fe691f-205b-4047-9ce0-6ccc5212d21a"}, "03d37f0b-e01b-4632-a3f4-b7d65fe406d7": {"doc_hash": "c588e0cfe334fa0ac8ac3c96feb65bc004b12826f137d87ddf2f304ddf789268", "ref_doc_id": "b3938c7b-3acf-4dfd-8488-82cea76a1fb4"}, "ae729902-9efd-4aa1-8d38-da6c5c850954": {"doc_hash": "0bd6cd77d049161c2cd700900c3c004ed180c3c860c9d8461279c6a5cc22647a", "ref_doc_id": "498cf7b6-cd23-463d-bbee-b2c842c7a2a3"}, "c2424a6c-d09c-4283-a6f4-44c09a174521": {"doc_hash": "85c27a7e3e5dedae23594bbfa0dc8b3c0631fdfcbeedb73c29f111f07ffa4a66", "ref_doc_id": "7321359f-4efa-4f13-b8e3-b8e7c7312892"}, "34f7a3cd-3829-4978-bac5-5ff9d1947e62": {"doc_hash": "cc650a3737520742d3f38b9c00c77c8b1df4a768040699145c155392caf74802", "ref_doc_id": "58cb4199-06c6-4b17-8e1b-3218710060e8"}, "9b34d375-d9f8-4fa8-a311-d05bc2885d0b": {"doc_hash": "10234f80bd30b5ba049e983486f1d65100752dcc96145a5d802a828d15a80e67", "ref_doc_id": "63d0467a-f106-43bf-b1b8-da5ead3d9d2b"}, "901b2e12-f44d-4619-8661-19ac01f05098": {"doc_hash": "4e84eadd62d17d2511ae334179b6ff723e7a59aec69a402894746308165e5af9", "ref_doc_id": "f4451f81-d9da-4015-8cfa-79ddcaf0c12a"}, "d0826cd7-cf62-45d1-93fe-68d78d5a7fba": {"doc_hash": "27d5423442d89022dc1cd529ab07ba7c579f1c29651f5eb438f7acde5a4e812f", "ref_doc_id": "1009a73d-8a24-41b2-8483-dbdb402b1fd6"}, "3f95405b-f342-4847-99fb-cb393a46d758": {"doc_hash": "a9820b0bf0eb681707279da5308e7a24473e383ec4f8abc8972634478b364d2e", "ref_doc_id": "7ee1ca01-f1d4-470c-8c43-bb37ec4ba637"}, "c2c0d875-a1d7-459a-b5b7-e71f5e71e5cc": {"doc_hash": "f0c87a95cca8e734e60497a9e3dfeeba285b9355386d1d4b8508d0f30bc85a38", "ref_doc_id": "93c62cb1-d685-4125-adf6-d5fc2dd88ba5"}, "b54039b2-32d9-4b7f-9999-13d24acef723": {"doc_hash": "66f7593a19f38a673cba3cd254ec14c2ca987ca86dac0f717c40d823bd788233", "ref_doc_id": "d11a3b4e-4006-4167-a098-ae288a1e5324"}, "bb5ecb37-aed6-48d9-afd2-918ceb6ee97f": {"doc_hash": "d11f32e475eee8ee1bd13a378fbe12935d771dae41c97df72bcd983df15ceaf0", "ref_doc_id": "f6091ce1-e72f-4bd1-a09d-4707988d049d"}, "42d9b396-1186-4859-b8bd-db60c25e78c6": {"doc_hash": "35f07f0f21887c31520afb283b258bce0c55aa0f5ebf0a837cab7cafa6b797aa", "ref_doc_id": "2a50a332-8620-4e66-8502-287080f4d095"}, "90cf82b2-ed30-41b6-acf2-eefa40cdf3fe": {"doc_hash": "f359abf763bfc1a2d012e02d8b1358162ff426921b2436bf4fc5989d90dc8135", "ref_doc_id": "a8d3e2bb-4d57-46c9-95cd-5faa902912b4"}, "3462da16-cc57-4141-9129-9dbaff173a92": {"doc_hash": "02a68836d8d91f3a1601891fae17039c131fd2b689032c4cb56321122f794e0e", "ref_doc_id": "9894277f-383d-49ae-a636-5262a65b1d47"}, "5e978f30-bafa-4e10-9e2e-1317f23d9f85": {"doc_hash": "9411ade35e69edae5dd9734e33587f79fded7447f8db297163021e202ed80bbe", "ref_doc_id": "91b6425b-bf38-400e-a2a2-d299fa21384d"}, "a75b1b2e-fd9f-46bf-a9b3-b6fdec328c43": {"doc_hash": "e531ddcea2cd5a274304dcdc64a4ebac700a69880c770e644e66e99ccc1c9552", "ref_doc_id": "2f8a1e58-a6cf-4c3f-8a97-93182960a306"}, "1aea0126-77fe-4035-8db4-9bec3434f5d0": {"doc_hash": "e35d45642ca73432a24f923f01247f1fe12e9c6dcd8aebe1679dd6e69983f625", "ref_doc_id": "b3bbf62d-35ef-41ff-b428-ff566db5cf5d"}, "ac214987-e3b6-42f4-9b3e-4419bc179794": {"doc_hash": "25d8b7c2fc7f08da2e0d06abffa800fa5a9490c96b1f2eaa91476bdbe615de4e", "ref_doc_id": "d8bbda8b-6ab4-4cc1-9229-6c71788eb0f6"}, "3441647d-a11f-4e5b-8420-861873f6c06c": {"doc_hash": "50d577c3eb37e6d68032dfae6f0504019f058a640642ebcd92cff50baaed920a", "ref_doc_id": "bd5c140b-fecc-401a-b29a-32c1adbaa36d"}, "6cce71b0-534f-4f81-aaab-4d8beb6da159": {"doc_hash": "ee194bf18bd2dc21828dd3838a0d96a67c21e5978e70751493af8ea8dba983d0", "ref_doc_id": "76ddaffa-ecee-4bad-a52a-d54a1c8d87c1"}, "175b681d-e83e-41b4-9a80-9370d27c4b06": {"doc_hash": "38ed05907e34eac7410012d134575e0f0c6cfb3ba5ece62dab404f6067bece3a", "ref_doc_id": "ec2135cb-9306-4b52-bdb4-63450dc6a4bf"}, "e7da6d52-d7ea-45b1-9773-3a294d84e23a": {"doc_hash": "5d53c5de71fbca4b1698e38e68ea72cc7151def8483f1352b5cf4bb61f43dc60", "ref_doc_id": "c861cb9d-72f6-4c6c-9aee-9480abb6151a"}, "e2c74e83-2066-469f-8259-2f152dab1136": {"doc_hash": "22759b88fd975884ab0e1430a3be93815a233989b4ef87b88deba92a70c70231", "ref_doc_id": "273e2821-351c-40ae-9c16-900fdaede65e"}, "99e40f34-a0e5-4ae2-9d29-febfd3269635": {"doc_hash": "ccf5b8b37dba28083af6b61bcd2ba360eb120a1b734f87183ffad9f6807e0ba3", "ref_doc_id": "9680772c-abe7-4643-af32-3fd6362a69a2"}, "72c44ae2-8d3a-4da8-8e95-081e69258e1c": {"doc_hash": "f1ca79cd7482963c2b6ba8ed582aa56e29ed557132a6c31629071c2ee9a18693", "ref_doc_id": "d56537dc-afbf-4ee7-9f36-5e08c8fc5286"}, "524004e3-8023-413e-b6c0-9a22d7ebfda1": {"doc_hash": "91b37c2533428261e9fba1792a3957a8ab472837ce0b583c09671cdeebdf68a8", "ref_doc_id": "435c5adf-f14b-4447-9255-552d8181d7e0"}, "961cf719-bf87-4e77-a2b2-989780ea4f48": {"doc_hash": "984f691f4ab7e0ef4c088dd59405281c567e339281102f5dd350704c4c7e5ad7", "ref_doc_id": "562d087a-40e1-42f9-8e7c-d317123cebc9"}, "8b1ca6ae-7b2d-4366-8ad8-862b25ed696c": {"doc_hash": "93f886c30dbdd66ba0df44de5ba2ec6986335566d234c3e7201a5f38f93e12b7", "ref_doc_id": "90506a4f-15be-48d3-9194-8d35024120a9"}, "4de0cf26-73d2-4753-b018-6b57e1b74a51": {"doc_hash": "7bec0beb8624bf1edd6b7dd2dfd31ad07e188363d6d5332e96b4e1ac92dff72a", "ref_doc_id": "a3c5c037-2bac-47ed-b9cb-f4fc212d840d"}, "591b8ddb-0831-4592-831e-c90e04ecac5e": {"doc_hash": "a0f61e0be3420d73d41aa1b3ae52257a906bc9aa97f321a6e844e75898a2d631", "ref_doc_id": "dc9781bd-bc47-4042-892a-8db01033f748"}, "e8ba99ed-61e2-4827-ad6f-3c4c82d721d4": {"doc_hash": "653aff3f175f6ab0cb9cd08693895a19dbf9a43d767473ecb5efee1e8ee11d98", "ref_doc_id": "b5deaaca-1c55-46f5-a1c6-d481774fc841"}, "2ea63e1d-48c0-4987-968a-a03b849d14ed": {"doc_hash": "1b510514ea43d3d38818833578ce43982f5ae74b3e7c3e388ab796b697ad2b06", "ref_doc_id": "b36931ea-65f1-4711-a5cc-a378faefa500"}, "1fce0605-88ef-406b-94fc-881d14060f6f": {"doc_hash": "8bb8b2cd3cea3750bd7d19e10017b935f4e2b8c1e091f8e996502e9e3e868258", "ref_doc_id": "199e26da-eb33-4fac-97e6-d1803764d2f1"}, "a2ed6b39-753a-49ff-9d60-cd2f7ff93e51": {"doc_hash": "b824bde56da34eb3b94beea76c9abf2dd8adb1fc584cf1ea2c4d53e7b35e2017", "ref_doc_id": "fe8e008d-ea58-480b-a286-0cdb28d9a604"}, "dc60851e-2c24-4298-94d4-f868bb890351": {"doc_hash": "d89047c0b1adfb6500d60653359d6cd0735307511919b301232453e2f21e4d91", "ref_doc_id": "a9b94e00-a888-413c-b8f4-3420afa80c5b"}, "982c29a5-f101-4f18-9203-15e1cee46049": {"doc_hash": "b8aefb76ed72c37fda4bf83887e343096a5e9ff06c6cdd6c74a1f7e0e1639a5e", "ref_doc_id": "a1870a83-c7fa-4092-9c4e-a9889f501047"}, "bd916cd9-d390-40dc-8974-8dbc0d8a84ff": {"doc_hash": "2cc62b9b4b515a2a4c92cc756008c3435634c55cd5f6af9f35e8a1afb74891a2", "ref_doc_id": "dc0ad0ec-e5d6-42fc-984e-6d5691607475"}, "8333a2ee-4513-4858-8497-ddc137a695d6": {"doc_hash": "fd6f9a6882554d4bf853352f7db9f3b8c00b8107f33f56bdd04274d1e89e9618", "ref_doc_id": "febe3ae6-916a-4c37-943a-b9c05240c5b7"}, "e5a220a6-b5fd-4c8f-93ac-7c463ea40454": {"doc_hash": "09b0de6d5410dc9fe41df66fdec93a3c10edd0bee7b528724ec9d7e58524615f", "ref_doc_id": "7a136fce-67ff-4ddb-b5bf-7a7377ec75ff"}, "73825b4e-7bd8-4e9f-9609-219503b43506": {"doc_hash": "ce459c9ff912d38e63c0f15720e9087de4c0ab1d735fe7e685e7c776083a0251", "ref_doc_id": "ffb4c078-72e6-408c-ae24-76e5cdd3fd78"}, "044daf39-a8fc-4989-8215-05d7ab27d421": {"doc_hash": "bbad8f0e2ab1f817a966fe9d9783059a46548a6b3e01868755715554c7ac5cb3", "ref_doc_id": "7b67d5dc-89de-4fd2-8ecc-710be02c9855"}, "cad3c2f8-37af-4b77-a595-1c68e809f1a3": {"doc_hash": "3185d5fa4f405c1a514e76684b0927fefc2f63d69b378759fb08181895d13fd3", "ref_doc_id": "eee46f57-c693-448f-9e7c-adb9abb485dd"}, "7f59925b-0470-4fc5-916c-c67299362a2b": {"doc_hash": "6a506fa1654e4b692d3d0691c2abf7835d225980f926acebbda034e2c4243b38", "ref_doc_id": "43044aa5-2546-47c5-8e1e-f1aeb73e1cfc"}, "75012f6a-cf08-4d1d-9fd0-333865667432": {"doc_hash": "c17951b6f28612943d9cab247753a5144203ab16d413b3159f82ac419a97ae02", "ref_doc_id": "1d86a1ee-65eb-45b8-afc3-9e57273f4fe8"}, "7bd8f7b0-ab98-41ba-9886-896f117f479e": {"doc_hash": "4ccf6aaa8b2e0a6efcb8f31666e945a7df46c5e238c017c16d0ac2535f58874c", "ref_doc_id": "5d02052a-34ea-4819-891d-11a2312c1828"}, "b3a033e2-b6c1-4305-a6e0-b5e177f68af7": {"doc_hash": "14c53a1b2116d6e7f2b457f1b0ccbdcaaddce188ecf655c67f602a7d544d69fc", "ref_doc_id": "5917bdd7-9ec3-4f32-8bcf-9ff7cc3e279d"}, "284a56ea-3c2a-4765-97e8-e97eebca5e7c": {"doc_hash": "5274727947ab1c4e01521e5b3884ec95d25b8375e29c948ab7b1dab74af4d63c", "ref_doc_id": "3196a3f9-f559-4e86-b44c-228d81110463"}, "ffe91331-33b4-4297-b13b-d924fb5f3f0d": {"doc_hash": "81beab32b26f61fa4e2136192d747824442a8ba2432cf1f93d5df082c094ed14", "ref_doc_id": "02f2c303-ad6b-47e5-9134-4c25a6e5464f"}, "b6ac1c45-e50f-422b-a938-6804d4058eb7": {"doc_hash": "c148dd8d0f38b061aef3bd7f854311315255d84d1dff6820b70902893f2c4c6d", "ref_doc_id": "60249dc1-fc98-4ebf-9d18-1f7d43673985"}, "a394bdc2-adac-475e-b215-93a6d43cf05d": {"doc_hash": "08e96b32b638dfc0e34cf13e668c576ff84e74a180732ba1c77b7db33829a0a6", "ref_doc_id": "5aa575d7-f034-40fc-b742-8a0b1b04e100"}, "e8712ab1-4cff-4e76-ad5f-e08e18190c70": {"doc_hash": "414ac4637624e5782d79ca43103035520ef5efc0e9bf301d073d08c1c21eb191", "ref_doc_id": "d3aea701-7ec6-496a-b362-fa9fe9b299c3"}, "220f2e28-39f5-4311-a3ca-14822f721676": {"doc_hash": "36de4f4b0e9a4aaaeb30c12b28501e4f1d6d2aa974c9a30caa6099832ac9c06d", "ref_doc_id": "2d15f9ed-7204-4e0b-a1e6-2099182190d9"}, "c6ac63f2-d1ca-4840-980d-20b12c9a561b": {"doc_hash": "79fa837a132da9f2efa64f906737b6fff132bf7bc453675757f8b7f5cf2c6958", "ref_doc_id": "e204b58e-60cb-4908-b049-2c171e8bab72"}, "20bd6444-cb51-4a08-b2bf-114a6a6b3900": {"doc_hash": "24de13bc14a4e980476d8314f060d5e81ed8471349d6a7cea97caecd5bf0c061", "ref_doc_id": "39767329-45db-4aaf-95f0-a6f2285695dd"}, "317d594a-7f69-4672-9d79-4054f4b9196e": {"doc_hash": "1df85f42e98dfa8471d3016b44ec3564cc1150a75616a9549458102e9419bf77", "ref_doc_id": "31c20893-d85e-4963-a8f5-e5285b504299"}, "a7bb8614-f306-4ada-b13e-8003783a4c60": {"doc_hash": "d2b761be5255d4bd9adca8b8ef0f2f8f6bfedbb2179496d4efdc596937bbe4e6", "ref_doc_id": "6103f60c-7fbf-4be5-94bb-e32239114463"}, "e632d6f2-4f98-4979-9607-9a7303bca4b9": {"doc_hash": "c2173769e7c45c30aa877a68a87a5da6ce567ab617c387d6b424695e9312b2ec", "ref_doc_id": "7e94187b-c817-4cd4-8acc-b437afb94869"}, "5e88d5c2-5677-4e43-bb6c-01839c53291b": {"doc_hash": "77b3651fdb766235b91556a9847ee56e592492cfa763d6961dea36e5c75e8f15", "ref_doc_id": "6b04f224-ff87-4171-b25f-b8556681d8b6"}, "02a6d677-7315-4066-a942-237b8532519f": {"doc_hash": "8a41a09f4482c7b696ac82d48f6ae16a925652a102e6c56d087fbfd6374aa0cb", "ref_doc_id": "265717ab-2465-4c4a-82a8-d340e63a3411"}, "67d59b96-1dbf-4199-aab5-7cec70a00263": {"doc_hash": "d7738dfdce1d3a34a6975e7879f1db82883a90a14a59b93d8f24563092d5be98", "ref_doc_id": "326fa34b-ec7e-4744-ab1d-a0b04c7e2cfe"}, "bad1b73f-a25a-424d-8910-d0d3d790bf18": {"doc_hash": "a2c3caa01f4ea38d0c167856c6da13f4f88c5056141775b0e767dc37e5a0cb90", "ref_doc_id": "9cb4f445-3389-4d7b-9fb0-b9f35e77b145"}, "8b3445e7-a970-4bc4-94c7-410510901028": {"doc_hash": "3afc58eeb10af27d6d018dd4533c305dfd576410db5d6ee2abd176fe79c03c24", "ref_doc_id": "8d976c50-a93f-4676-97d2-cf4d09bc4f77"}, "4e082aa5-5fc1-45a2-9fec-907b77c96540": {"doc_hash": "3fa07e953f4ba916cdd44b10f1b9d0287ee550adcfb43a4dedb3254bcb7027b8", "ref_doc_id": "f93b4a25-2d76-4d92-9baa-a8c705f8c976"}, "c5ca9bf6-0cb9-4c5d-a0f6-226f38d70f69": {"doc_hash": "11a9c71e99d91aaf7996883d23146ecb2eda4bcefcd9374e5096e33ac09e063a", "ref_doc_id": "75f8bf61-daeb-4328-9617-0e75e519b03d"}, "c5e7b92b-6c6d-4698-bd8d-6b90e058e1df": {"doc_hash": "9c80e7a4234b03d9b0d26e0d29e18a9f2d250dc2936a32f4387338188aa9dd9d", "ref_doc_id": "4f528974-17ce-4df6-b72f-b402932b594a"}, "7f838087-58df-4ffd-beb1-14c33816da8f": {"doc_hash": "cae86be4a056a183437df90edca2ac9db01f3ecf4daf937754ec6285a61ff269", "ref_doc_id": "e35e1b96-8216-480e-89d5-8dad2ecd1959"}, "428ede47-ac3d-43fb-8ca1-be48062d4246": {"doc_hash": "b837da936316d53998bfd59d6f1a5c92c4bfc3ca5eb814f0b93870b08226a0c2", "ref_doc_id": "0218830a-93cd-4ddf-9b70-1531207acffb"}, "c33af5cd-5c40-4c0a-bf66-a12c59d3c134": {"doc_hash": "8d73e161ce072c7b4b5154e83d51c0bfe41181490ed04f530ae430c01ee8fd9f", "ref_doc_id": "3bd46432-3105-408b-b5fb-1df51633d112"}, "84f18cf7-c9db-4866-8f9d-3cefd3e16302": {"doc_hash": "48bc3e1b806b60055dab600f959c58c52057ce332c00ad3aba6a15c6ff926032", "ref_doc_id": "c65ed8ec-dcef-46ad-81ae-e3bfab4e096a"}, "9eda0a4b-7866-4603-a7fc-256ac3c4a833": {"doc_hash": "ac6e4b8fc7da16a306573834563fd33aa29e4cc06c1c4dfb4382ea0461af022a", "ref_doc_id": "fcc414ee-c97c-44d1-ac15-83403f0816e0"}, "9361f137-c8b7-4337-a132-a6a6c03df00f": {"doc_hash": "e70e31b014f14f43a703df55954ecefe1409fc17265f963c0b126028def08ef3", "ref_doc_id": "699d186a-2014-4281-8fa5-d46c478de3b4"}, "5333567e-19ce-4535-b99d-256b75f3f906": {"doc_hash": "c107f136e0b8c0a5b36753e28ceac3b1c35f4c31f6381d8b92c95cbbf76d57ae", "ref_doc_id": "15f2eb40-eab0-44b4-b3d1-9ca18e860200"}, "ecc20ef3-fe28-492d-9ee6-795858e0a4b1": {"doc_hash": "b6cf95ac30cf2c3aeca43e7b7cc2c1c87b0470de5c8e71c15b13197a5d4ccfb5", "ref_doc_id": "b59756c7-e125-4dc5-8310-7740c56d2132"}, "257f66d5-7976-455c-b861-b226b36d6f05": {"doc_hash": "28e44efe77a640871779d83b044c83037c735459c5642608d3af5c22c75e18f8", "ref_doc_id": "99b8fbff-ceaf-400a-8790-f450481a81c5"}, "901dac7e-973f-4814-bd3a-5dfdca4ee665": {"doc_hash": "0e9aa17c45bfed37634448bfbd1a761dc03f7182dcd3908de5e978a0c6c24ae4", "ref_doc_id": "52a8bac8-ba8e-43cd-9994-6757ed38ca84"}, "ea0a0344-b7e7-4c10-bb20-d9f7fb149701": {"doc_hash": "c8329d4f3574dc698ab9352b78bc04ba376df5bde9b5a1a70d0218768d814f31", "ref_doc_id": "46e36a31-3fa3-42e8-8c36-6858c5d24a05"}, "10687a53-502d-419b-9cbf-5f4f8bac0be9": {"doc_hash": "b62502f58f2bd6188e5c2ae9411990a21c610152e23d78cc7521dd7b50595fac", "ref_doc_id": "5b5f51d4-1b43-4a4c-a9ab-6b7694ddacf8"}, "086e127e-d1a3-4b9d-9e05-090d2992fa2b": {"doc_hash": "11a96f5b91ea5104f86681616c1c29ac52ace47e8da843052eadc227b471cab0", "ref_doc_id": "84a8ec6f-e69a-4b5d-9355-ce958d5d9aff"}, "f296e3eb-36b6-4292-8e14-6b8e2ca9ece8": {"doc_hash": "28340f7adb3d6edd94b7b5dd95b0faad16cc65a562cfbcf312db9b1b494d0ae4", "ref_doc_id": "6ce1cfde-5fa9-46ef-a905-d1d867cac1c4"}, "cfeff32b-d4c7-4417-8369-09350ef5e306": {"doc_hash": "74a749bd03b8a34ab4b258cb84df84df7faa4703549c10456c5421da4dfaa6d6", "ref_doc_id": "7fe67618-9175-4136-a06f-776277c64206"}, "1d2e305e-8b22-444d-a461-f4283953d971": {"doc_hash": "5ef7635061fe4c304371b56eef457cf66f2c737d5846e1177a5643cbbeba9f84", "ref_doc_id": "002bd546-8fb4-433c-9df6-152aa8a5ff19"}, "3f732582-6365-46c3-ab8b-08477bc89495": {"doc_hash": "b3bf8910e55afb37fc1d96087c41a79d82e86731025975e0e2d12197629ee582", "ref_doc_id": "c463ac14-cb93-49be-9759-3ca4874c7d4a"}, "789440ce-5414-4224-8d3c-68243b6a2a81": {"doc_hash": "64cfb52bd4dc080d89e794a578bafb5019f7fb7da8ed4f1f10bbb26cb6596313", "ref_doc_id": "a0169162-c9aa-4f25-a0d8-aa7f91f9aa5d"}, "bd6618b0-d6ba-48e2-9e37-bfd3bc0b7092": {"doc_hash": "ab5d192d26e86ed462d1b6411b22aaf045f9675cf261f77b95dddc9d1e8f3e2b", "ref_doc_id": "9060b0b9-b204-4e80-a786-77048b462699"}, "2a37e9d8-92f9-412f-a2eb-5a99985a592b": {"doc_hash": "f6878d94fb44b8252c11d82d1701f7cd3643a224ec0c9b1886d8146cc372adab", "ref_doc_id": "4d06aa7c-a9f3-434c-9426-ac9b12d5e2d2"}, "2c969372-a9bf-4e6c-ae2a-5e35d8440a5f": {"doc_hash": "52637aa0ac918e29ca0ff0524588473037afc6816d02a87210912e1c0e280017", "ref_doc_id": "4d06aa7c-a9f3-434c-9426-ac9b12d5e2d2"}, "4c136cfd-eecb-4cec-a3cd-e95aac4e4eef": {"doc_hash": "ae8dd3547a6f8d0d0b98d389d71f40fc4437dd372d453ecc6805d6ac3c91fb73", "ref_doc_id": "bb7f030c-667a-4363-b213-aaee39cf1873"}, "74b7c3f8-124d-4411-9ab5-2ee690f93785": {"doc_hash": "e74c7069fef2d3cde6f06b069101d42bd72ede6381e5ee9a2d150a6655b4a0bd", "ref_doc_id": "ae5806c0-979f-448e-a6ed-7e428fcd265a"}, "035c3369-ba1b-4ddf-941a-1008af940660": {"doc_hash": "09aeb015d5bf48cef83f1e3157fb0b4dcef8dd5559f09883aadd3f1c22293cc5", "ref_doc_id": "42659385-bfb6-4873-b34e-7525fa958db3"}, "95a5da59-6f84-4a97-92cb-00b54a29db60": {"doc_hash": "6ac288dda8de1f2cedfa2706b6c5ea4ea9803c652668bcf1d65161f68e2eb7cc", "ref_doc_id": "0732527c-dd95-441a-b4fe-24ba2ec7b5df"}, "b65aa98e-e119-4792-8384-d198d5bf3f6f": {"doc_hash": "079537861a52b20f01401b0aeb2253e49459cd5825d467c46e018fe7167566dd", "ref_doc_id": "0732527c-dd95-441a-b4fe-24ba2ec7b5df"}, "88c1e8a8-353e-4a70-a512-a77d6be0d856": {"doc_hash": "505341c67a2e5ab8fb09626d7f79d7842e5dacd84097a8ca15af062cdfae926e", "ref_doc_id": "0c5b937c-d14e-4a49-b616-b9550e7661e7"}, "31ff4633-8026-45f0-bfed-e36f193bc65d": {"doc_hash": "835f7ed3f50008683ea3bac1e24ed4dc58d105c295d3b144863ebb3e6371654b", "ref_doc_id": "1fc2926b-98cc-4241-b175-fa310ee487b1"}, "6c3e6c17-d5d2-467c-8f5a-9b0e8f38ea0f": {"doc_hash": "9b21c86f8c51e5b7602b09ad894897c9ff4afd17b19d71891443a6dbae0e83ac", "ref_doc_id": "b560c667-42e6-4dae-a4a0-68ad5f748dc1"}, "76cc272c-e7d7-492a-af83-7806603b7990": {"doc_hash": "7ffa2dab8a8cbad155c363ee90ca04a56e1bb8b5d276657dff5ccaadb922762a", "ref_doc_id": "6ad8d613-51a5-4b6b-a904-ba28c148e829"}, "3304653f-3c48-46a4-8fb9-0cc13a887bb7": {"doc_hash": "77e744da2cc6effa31937a6fe5df4a9a9c2711c105c6f26829e87763a18db75f", "ref_doc_id": "51c0b513-82a8-4b17-963d-cbe03130de46"}, "9c7e80f6-e6c0-49ef-b443-f0687a3e93c2": {"doc_hash": "8b240f491c094c07e75e6c770b5b4e088bfa9952a7bf7c9902f58a466c0f9736", "ref_doc_id": "9dd8f58b-5aee-4377-9be3-8a887b277c9c"}, "3500af81-67ee-4043-ada8-ae193aacf7e8": {"doc_hash": "17ff478f754b3b41975247708c0c373ed7590a0ae3624e6cd75773bcdd1e50f4", "ref_doc_id": "e00e6f10-9ba3-41ec-9218-fe0353c7302d"}, "e67475c4-ab22-42b4-9e66-d2f76ecdde76": {"doc_hash": "2b6ea719383da73db0095eb15787ca12cb6a08aef4a414f7d3b8020d750f5e2f", "ref_doc_id": "9a38bf55-d1f8-43db-9135-b8a5eb7234bd"}, "63c347ae-64e6-4b5e-aea5-9f5332dbbcea": {"doc_hash": "902199a35f48d81bccde6a11ce9065c70bb79763dae43e49ada1ba6b2d75749f", "ref_doc_id": "3a14a873-0aa6-4f64-93e7-700cabe07a43"}, "d0a612c4-a28f-4543-88d1-0c52abaeb4bc": {"doc_hash": "64acc2044889d264fd7b58e73c944b589a930ea06d059bc5d3c5301fcb0973cb", "ref_doc_id": "e21103d5-3542-4241-84a8-f90830408018"}, "94c63026-b261-45ba-a2f2-c390d2423e05": {"doc_hash": "701c041a2be28c02a0116b43c62c7e89f78247330645e1b33ddf81bb41e6ac3c", "ref_doc_id": "c904c305-b9ff-43e2-bf65-aa00c4b32d47"}, "c269f3ab-72b0-4f4f-b763-25ffd47d0fa1": {"doc_hash": "6c1255351a5a366bcf27c53cf058d78003dbe00b550ae5107a814f674fabd395", "ref_doc_id": "268728d4-1915-441c-9749-728825864547"}, "3a11db6c-f849-47ae-99e3-81dd44b2072b": {"doc_hash": "3a39ed838c0fd2e4dfafcdfcb79757b005dd74dae07dd8eeaf053fb46262ecdd", "ref_doc_id": "b94964ad-a356-472c-a3b6-6513e5cd1399"}, "4fc199c9-9697-49ed-8789-9839c3ae40b4": {"doc_hash": "2cc54b536a0a1ef58e24f1fc29495e62ef1137cb3994229252248c33510bb7d0", "ref_doc_id": "1659456d-6c48-4485-b66d-29a8e888b5e3"}, "a4f4d368-31e5-4d07-8c41-16f578178710": {"doc_hash": "d0cf17827f622f2bcefed7fba5305a4936886a1d14c1b5a3528b69b89070b079", "ref_doc_id": "ecfee4e3-1bc3-4036-998d-8d6ac11dc233"}, "f0472724-0c08-406e-9987-96334d28d087": {"doc_hash": "cb434c2683220f3b778faeddc1ac4251101d9ba5bdf509a33a10a29f8ae4e979", "ref_doc_id": "d8279567-0178-450d-99fe-7663cc70a647"}, "340c6769-f912-4752-a1ca-854069b21453": {"doc_hash": "7b7852fd46a156613da9902a2607e7a3e8be15ee53e5bd31139508484176e180", "ref_doc_id": "b9b8f471-9334-454e-b476-81ba82380962"}, "d923e7d1-8a81-43b8-8f54-a8f68cb4f129": {"doc_hash": "555af74a3ba0d5ac03600a5a75418cdcf950aa0015207496eb0ae01c727e36d4", "ref_doc_id": "8bd1feeb-d8b9-4dba-b07e-daaa351a87f6"}, "bb958323-1122-4b3d-95b6-297ec99b78ad": {"doc_hash": "e5da97caee92595a2ba2c00b2913881a2e4433da888d0e216e8036f1de20cdc9", "ref_doc_id": "9a287a74-41ae-414d-9f0e-230784312d87"}, "2057b859-a0ba-4af9-993b-a8c4a499763e": {"doc_hash": "41e653bcd53e4b7a17065b6a832916f9cdaa777ca1b3b675c7b85841a82115ab", "ref_doc_id": "898ab30b-be46-499e-9e5e-e82f786c2943"}, "d0217b4d-5dcb-463b-9985-2c00fce93e25": {"doc_hash": "ecdb05bb1b5095b984205a8bce2d10638d2321657ec6401b6eca72a236fc926e", "ref_doc_id": "39696ee9-ce77-4e48-8b45-51c188dfc290"}, "c8db6d4f-7ed0-4045-b7ac-5200402e3e5f": {"doc_hash": "737469a0ebeb036f2a397ee349bcf8af7a5f24d06c8910e8c9690c9d4de52915", "ref_doc_id": "0aaf3074-b4ab-415d-9eee-e021be762ddc"}, "6d59ef69-ded6-4bae-aa74-00e064708238": {"doc_hash": "ae5232bfcaa87ad3192db2de8313c31ea158f6e24ec6121e3ed9e1f9e701b653", "ref_doc_id": "314e1552-438a-482f-9f47-bc3b6c06c590"}, "072f1939-10ae-44f5-9e13-b260f3b47148": {"doc_hash": "f64b4a5284887124cdf18c1ba5da82e7c5fac56a9c7f56322874967c614193e4", "ref_doc_id": "314e1552-438a-482f-9f47-bc3b6c06c590"}, "ff8be358-f666-4b55-99b6-7ea055e73c39": {"doc_hash": "16b620518d36d4088fe02e16add95dd8a17925560c395a6c69b36212024e2606", "ref_doc_id": "93283ef8-2219-4af8-b8bb-e01ec47b8324"}, "75f2998f-9c75-4881-b9ac-68903762e4dc": {"doc_hash": "c08d1fc5cd4acb7053ccb7c79fb095e97dc7c18dec85552a993e3269c1cc9a59", "ref_doc_id": "83647637-a00d-407e-9dee-4e415aba5bfc"}, "b289b273-67c9-4472-a5da-e6edb8270f29": {"doc_hash": "f902e485fbcac24693702a74295e3c1e5fd65b9682a74afd2c113bf5f6f90f1c", "ref_doc_id": "5eb4f8f5-22b1-49ca-bcc3-a3d4d93df0d0"}, "016379d4-5297-4ccd-a9c9-a9d572dff0f0": {"doc_hash": "65d5cffaef3707280c3af9f92dd02d99d16d67d526443c2060e179c45e81ff8f", "ref_doc_id": "0546b4e7-9792-4b2f-bfd7-3580bddab0dc"}, "965fc71d-0187-49b4-84f3-9a8cbabe1b6b": {"doc_hash": "477d539a253058de0b40c9f24b4e43c1f34f18f27718dcf2cd19ebb79167520d", "ref_doc_id": "6fa991ff-1293-486d-8f1a-7cde0bee63a9"}, "a4dd0396-5b7f-4d05-ab88-1247e00ed4d9": {"doc_hash": "3f6d6ee7063990994300214378c8465fbc9ba04d6061a9751488719bb797ac26", "ref_doc_id": "56eca053-5fb4-40e5-a33a-2e8a956f63e4"}, "5bfc91b2-9d10-4647-bf6a-787aefcde9e9": {"doc_hash": "b34ae2705aeb8d82bba4a795afea8ca48c84d9bfce155ee7470b307458623c99", "ref_doc_id": "dfd9824b-4139-4220-a085-6e79fadeb1b0"}, "27389124-f307-4325-85fa-dcc8353c863b": {"doc_hash": "965c91273b5f3a74b9afc045a3d0c060bc8ce98b08be31142b93e459aaa1f2fe", "ref_doc_id": "1cd8ec23-45b6-4031-a37b-b6008799a3e7"}, "46619238-b8bd-4861-9c09-b1471ad4d84d": {"doc_hash": "db47ba9cfad9d6bea6f134cdaafb39f1998229205dff9bfe24afba208dcc57f7", "ref_doc_id": "95b41ca9-87bf-4121-b8be-c939626b7ee6"}, "32f44fc2-63af-491e-9a64-e66979d9e29d": {"doc_hash": "4d6c81744b86753b4ac04d1932382f33ee46da494813ee8daf266546f8b2da90", "ref_doc_id": "32ca113b-746c-403d-bee1-5c64f59a2049"}, "a6bda2d8-1e04-4b7d-a422-45f122c73917": {"doc_hash": "579e750494d00c256668da6ff20054517d46fd574c2448e8f830d80972b972f3", "ref_doc_id": "1cab42b5-27ed-4a89-843a-9d2a65abacc3"}, "b99a3e6b-d4ac-4e38-bf03-1bd36b383c4c": {"doc_hash": "cb483e05efc2cf7e3ef16622f669fb0e16940f894967248c7b19425af71851cd", "ref_doc_id": "2e90e617-26c7-427d-ab7f-6840dbd88823"}, "6c0ca391-979b-41e0-b869-b945156897e0": {"doc_hash": "8080371079e19e2186efa17025915d75d2e93702abafab594fa50611b627b4d1", "ref_doc_id": "e2d6d3ca-a1db-4c39-a81d-6bdb49cb8006"}, "88c63a3f-078c-4c93-a2c2-37568d828d2b": {"doc_hash": "1106c6fa29cc63c789113667b3b12c6789bf4a046b572b2e1d01a2dd4d3b1e2f", "ref_doc_id": "7db4c44f-e699-4b87-a87d-a71b1282bfd3"}, "48ef7ccc-4fee-4c52-a9c5-5e35c1b1070a": {"doc_hash": "0861b65dfc43ac8d9fd0f111cb6ea4288c9cbde59b371620ae969e62e2b12b08", "ref_doc_id": "fe71d34e-096f-44f8-bd51-d16c6d9f942b"}, "3746a184-0d81-42f3-aa0a-901acbfa9265": {"doc_hash": "172922f2afc82d86d5082d746e572c33ea5a75175cb667741df27dfd9d33c3fc", "ref_doc_id": "bcaeee74-de9b-48d3-b6a5-0c39dfd2b997"}, "78e4731a-0e0c-445e-89d3-34e831db10a9": {"doc_hash": "e240a24faba4623aab751153eb3ec181b9221aa04a661562340767254e2bef66", "ref_doc_id": "ee6f8217-0f1b-4a54-b1ce-25d66344683d"}, "18bafec4-2c17-4c4a-873b-0b4ad3a63590": {"doc_hash": "1321ab6b561fcc63c58e9f5c175fc59f8849465aa7dcb86dcc2fe4ca5e0870dc", "ref_doc_id": "8845a7ca-e9f0-4e95-87c5-f97e8f16352c"}, "42bcfd59-5be0-41e4-b115-4756ececde8d": {"doc_hash": "e36fc43ade4cc6eea4caa018130f67b35a0c76db000bc8f9672fc325bee3d6be", "ref_doc_id": "5722fd4b-957c-4761-9d5d-b6796708cdb4"}, "83a644a9-f0ab-4173-a4b4-2b9bd51603d9": {"doc_hash": "28f350089ad33d8e7293724fff864d12994f55121844147fecbc9d1786ca575e", "ref_doc_id": "8d687fc6-126f-49fa-bc37-2069d9cf8125"}, "f58823a9-2594-44d0-aa5d-0032ad9ce8fc": {"doc_hash": "05692f08dface689f33098729c1ed6e467f461ac56de4b7376c0aaa7afb2d733", "ref_doc_id": "97f812bd-d27a-46d1-9106-dda1f768a5a9"}, "6bc3bf95-7e63-4fe7-a184-296a59bd4b0b": {"doc_hash": "e0aac10edb84a9a61d0913f4a0d9d88701bf1a9afda86d506ce3f76d52303e4f", "ref_doc_id": "daa48086-e858-46db-9e9c-2b1318fb72ea"}, "7139a2b5-8f27-4c9e-82a8-29f2537eea5e": {"doc_hash": "dd5f4bac64f282cc002ec6f22c42e605484d4a4698a54b055fd7c526ad54bbd4", "ref_doc_id": "c4d7bfbe-4e35-4d2c-9bf2-3153f0973629"}, "72ee9c1e-bc51-48b6-b09b-491ce95a2c1d": {"doc_hash": "6bcc576d647bd5494ed580bfe8dace62def8fe0e46cf1a050a5955c7e55a2796", "ref_doc_id": "7c9955c0-b712-4e08-8630-56445c97aa45"}, "6894e621-273c-4c5f-aaf0-4dd7eef21385": {"doc_hash": "3c85cee87572a81a4aa7efd3e725d1d460972e138d10ce9db546c549b1329798", "ref_doc_id": "a1dfc68b-6bd4-4957-83c0-f6f4a6cfe3c3"}, "8ae06f2b-498f-47df-8854-b9357df4a29c": {"doc_hash": "86c45871cb8730088b92389750ab07f6642bd1777f3fd95544898c1d9b93b1db", "ref_doc_id": "75c67807-ec76-42e0-8ed1-54faea0f7543"}, "b243197a-f233-49c7-8cfb-d1e641eda111": {"doc_hash": "62b30adfb6667e060dc48a949f78b3374b83304d2265d2fcdce3d090433b8f47", "ref_doc_id": "78afd476-d315-42fe-9d5d-8d6acd3df7cb"}, "b8312523-7e40-4569-8df0-c5e54eea0537": {"doc_hash": "47aecf72c9bf250bc0624da77402ddac9dea8a74d22197553d91f50e524f86f3", "ref_doc_id": "af980508-4ddc-4e09-abe7-31c2d2f42d6c"}, "05c83dd7-7006-48cb-831a-bcd8c17460ee": {"doc_hash": "35cfa05603996e533ad93cd6245835c79b9abad7829325ee0fce85bc552364bd", "ref_doc_id": "0788d96a-90d1-40e3-9550-8e7ee829a559"}, "36137c6a-a6eb-43e1-b8dd-f26f08699ec2": {"doc_hash": "566327607a25daa6506d401e193ab0ac742c119c7b4266635239c90086d029e6", "ref_doc_id": "da33cebe-16b2-4be5-bdc3-515357f7ff68"}, "7c007a53-d6d2-4dec-8423-a7f08c441b1f": {"doc_hash": "dfe879beb279ba789f26923e57dd73405efa77ba84baf13113af13680aabb7a0", "ref_doc_id": "85eae4b5-95a7-4951-afe4-2fdf2e96ff44"}, "adaf78a4-f6c7-4ac7-b1e7-33208eeab63a": {"doc_hash": "03d6d0e2d72e176d1c49080505fd4549a4ef48b356885bc7ec21d4a6a21ab6a7", "ref_doc_id": "eb03315b-7300-4936-8769-2f90151fc43d"}, "d0767f80-1684-44bc-8d62-6b746972f3bd": {"doc_hash": "b5b45b7a9034fbba09c8c51f0435d587fecc2b56471dada4260f80ec350e7c98", "ref_doc_id": "36ecb6e4-ae9a-4767-902f-b4d377d79ea8"}, "a7d02906-87fb-4945-a22c-de2bdea311db": {"doc_hash": "21f47f71f3e38f0e653696a0585598ae5af28efa421e20f59e8b82b7c22baf7e", "ref_doc_id": "875e9265-07d9-4691-9e67-4827c9f9054d"}, "b74ff94e-a03a-4e9a-b864-74f6c41af5e3": {"doc_hash": "1fd5391b64ad93daaf43a2281e46ad19cdd2a54f8482db0ab0d889e90bc1b1fc", "ref_doc_id": "f4236a4f-9f76-4230-84a1-d483eb3591c8"}, "a12c3bdc-da7b-43db-9048-2c933fd39c60": {"doc_hash": "0002a4023cd15196f91bdf0b380ded3e8e3cecef241f9e793dee19acaa1ba7ac", "ref_doc_id": "bf6d08df-33a8-44c7-acdc-c852b72d8a43"}, "268fe9f8-6b42-4fac-9597-14fb651fd7a9": {"doc_hash": "68ad1d60c919247bd0f253896a6242d30a4622a24db209fd9862c37c27cbaaa3", "ref_doc_id": "9ed703b9-be5d-4830-9a26-4c3f704bae39"}, "b08f0a1a-451f-41f0-b57f-173fe7e32276": {"doc_hash": "3273ee57c6b82f76cf7f32f392b11c2d54d83e11e1edbcefd15109d8b3082c5b", "ref_doc_id": "3415773f-3081-4401-87f0-285c4ed96389"}, "63bd1377-37bd-4b38-bd64-7e1f2a996412": {"doc_hash": "06d26a43a70d84975905e87b5b19f8c24689e3cb852c3af3b12f2ba6a525ae2d", "ref_doc_id": "9b7f85d6-71a8-4224-8d8c-291b2b2abb47"}, "74d769c4-97ed-438f-9bb5-95a9bdac71b6": {"doc_hash": "ac9e2caac8f35261b92f1a6414721e779da351c5277b581aa710c34853660c03", "ref_doc_id": "30c6c8ed-deb6-455b-aee5-d17a13d86c9a"}, "5c5e9a30-132b-49b5-902d-8d420185de62": {"doc_hash": "157a21e143607e67acae31d8e6a154bc2ec19843fa4a425f6d880f2ab2cbcf97", "ref_doc_id": "d57d3330-42e3-48ba-82b7-3cfcc8951174"}, "cd3068d0-db6f-4f83-acaf-d6123102dcf0": {"doc_hash": "af1c64cedacb90281009fe9f28635d57ef6051832db4dfe14f1e00e670916351", "ref_doc_id": "43d1eaef-fe3a-44ff-8167-656c98ce10ae"}, "db9e48c2-49d5-4737-b2e2-01b2abbf708f": {"doc_hash": "d407293e44f6bed2ffe87ac1b2ded583067dd78fabc7eed4e575bede307ad5e6", "ref_doc_id": "323954ca-6773-4339-8e96-c58e22f5b2c7"}, "1ce5b567-8733-4a12-bef2-7c6319225df0": {"doc_hash": "b6f6e5874c66b022114617039ee19acb2c4f6f044d926c84f736a309952c1bb3", "ref_doc_id": "f8870fa7-fdf8-40b9-a806-1278060db34c"}, "d33923c9-8f15-4537-a691-755c81fda63a": {"doc_hash": "4f2bb1a3c685cbd80c079ca506ea06e7ec198dde3843a653aa0ad64ff775c85d", "ref_doc_id": "b68c90e4-25f7-415e-9e76-a667a48e40fc"}, "fd5f9550-4e51-4074-8d13-fa023d7a1120": {"doc_hash": "5e36d52eab585282344dc4a4165978e104ff09de239482235c0766bcc77e9b21", "ref_doc_id": "81891401-d971-4807-b1f4-8214c74b3403"}, "b63d0326-d32b-47a1-91dc-1bf341825276": {"doc_hash": "6d917969423a5393c6d79571acc7b2dc84169557ac0548a6bef6d8cc91323a19", "ref_doc_id": "b4332993-7547-4bc4-a723-ca5fcb14d801"}, "56a1f123-95db-4e6a-8e04-03d606e0dc66": {"doc_hash": "354012e326076260e847deb6f40af6caecd25ba923fbc913b325c1a11262bd0b", "ref_doc_id": "5242e3f9-5a43-4f86-a065-dbbe58304f50"}, "d43ab2e7-4f6c-4fd7-af38-dcdf7785339b": {"doc_hash": "085aed42b1e93fd12c18869248c564cc85790a5d881710b339b4fcf32d705282", "ref_doc_id": "18b72851-6d5e-48cc-b00e-1e442f2d9590"}, "2ee0239a-ed12-43f0-b9a1-d9069665172a": {"doc_hash": "4066b07a8d0dc0d322ffcef803659ea8ec1cb6f882f4ae0addd736ce9e0bc6e7", "ref_doc_id": "63c1c3f1-e428-445e-b41a-786e757dc764"}, "d6c89e94-dd91-4d8f-b625-7e4c0a078be3": {"doc_hash": "c877ceb9eb81544deaf278c121ea940517bc37e4d76fbc48ac5946efb590badb", "ref_doc_id": "b442e95e-8120-494a-9aec-d4bd544fd529"}, "5db3f0a8-2896-48b9-9571-546c0bdf9383": {"doc_hash": "2b185d2be9402e3f5dde515db75943f8fa987b0701ae0925fb37105f42333535", "ref_doc_id": "6fa11236-ec5a-4812-b768-8cef0a8568f2"}, "70528964-8697-4085-8d1e-f1638b055679": {"doc_hash": "21a6066665a8be74ab6f72bc04e6c9818b310e73b9f3fb752a21566bcca38d8e", "ref_doc_id": "6fe96df6-7687-421a-952d-2798e7c01060"}, "788a234a-c975-4c6a-bc7c-c7cea6b490b8": {"doc_hash": "d09060dbd0e63b94e361e2846682b46988bab97ad7a274e143c3bf52933be3e7", "ref_doc_id": "6f303d2f-a1d2-4424-bbe1-fb9c7d0c13a8"}, "30d1e881-140e-4203-a4f6-a671e641eafd": {"doc_hash": "0cafae320a5a676def863fc6a3dbe3b00ac9d89014dec46d492b2ef866e62cd7", "ref_doc_id": "38a8820d-d0ff-46fc-a09e-5a963749ff0f"}, "32f086bb-6413-47fb-ace0-395e4a85461e": {"doc_hash": "231c2c4ff78506be1bdbfac74627c1d65f268e68ca3c42e419de655dc8e2593f", "ref_doc_id": "6eea69db-d2af-4e66-8e7a-8be1d149e02a"}, "54b4bdbf-27c2-41be-9054-ae9fe1f3be4a": {"doc_hash": "aebf8c889277e5348c8d43097c7f2c237c912385f40ac318ed2307077d77cc5a", "ref_doc_id": "18488222-f311-4bb6-8cde-a63c5bb357c8"}, "d1528309-abe3-4d29-bbb9-b0a569aab059": {"doc_hash": "c2d5801fc8f5cab6d5cd37f11d5004ad2d4636357c507178ad3de3ff3ae9f171", "ref_doc_id": "b3a481e2-5388-4dfe-b0e2-9118b4a8abc7"}, "e4644f69-f392-4cd4-97ba-d2a254a59b23": {"doc_hash": "2bdda25593b23dd47680cd6da7798b514de7be846fc0644a7e7a8f36cd1397df", "ref_doc_id": "290412b5-4303-4786-81cf-b04b75e5de1e"}, "af0daeb8-655d-47a9-8b19-29719cf893f9": {"doc_hash": "9fba20c0e289da6769f033ef040d38fe7894cd51183a6eed0838da033b86ac7a", "ref_doc_id": "7b54a0ff-24c9-415d-ac1e-eef923aacb8a"}, "09c8a28e-6718-4027-b375-23c4ec5fe313": {"doc_hash": "b78215fb2df6087fc702b69ff70763514de0251d6332616f43b680f123d05c96", "ref_doc_id": "8570e97b-6fd5-488b-a397-6cc2e940e33a"}, "45a5cb18-ac79-450e-b445-f43e9d00069f": {"doc_hash": "91262c94487529a68372f4f58a86625fc55f5d265593e3c1888f1924c1e42a44", "ref_doc_id": "8f8655f5-0f66-4219-888b-62f33291abd6"}, "1c2bb72d-d2b3-42d3-814a-27b4db6fb679": {"doc_hash": "cb819446461819ebee82e0f565862a61dae04577c6db9c9e196408e6cfdc80e2", "ref_doc_id": "0f800b55-5208-4311-9cd0-a8bc59e6f84c"}, "c55de6f5-1c5f-4e94-a720-b4f0033fa9a2": {"doc_hash": "9f46e2c4fa10bf667e16e297da0f5636f7ceec04f7596544ce6304107fab41d4", "ref_doc_id": "fafd2ba6-5df4-48b3-a6ac-12ecd07c9f37"}, "8317ff91-8560-45bf-a7a4-58ad987d6844": {"doc_hash": "7d528b53a84228cfaa54cc1985160d4c2e64c5c6dcab566ec6af355b8b0df900", "ref_doc_id": "2808b57a-9ed3-44a0-b321-a62572cda607"}, "81dd0e12-d9a7-4eb2-8255-fdade662ae54": {"doc_hash": "b357ae585bf485008a774d88f91bbbcc0553ee5d4bd892a8191438682fa11644", "ref_doc_id": "cb55b00e-11c1-46b0-b030-43539644fae3"}, "8e1785f2-adee-4e84-b242-0f45515b9da5": {"doc_hash": "3b16bf454e6001be48f620f3828d1353c3b27802843bc9694e180bfc2543d285", "ref_doc_id": "36b27665-c63e-49da-b97a-420e5399eb89"}, "81ec5d99-30b9-4f78-ba96-0cc531fed29b": {"doc_hash": "cc2f59424f2af7778913a1b181b55f8b422f4e824c073f25de64114203d187cc", "ref_doc_id": "d3973197-d432-4b86-889d-0ab950f29f57"}, "9feb501b-5e5d-4639-b0fd-3d14c432995a": {"doc_hash": "5aba6aebd3e33faba5b446a9f5dfb0f7f6cf311bcfb3011057dba59535a740c8", "ref_doc_id": "1b9091e5-a412-47ce-b495-247a75c10ef5"}, "9b5d7e8b-2fb7-4b57-9c99-ac8b305a1a6a": {"doc_hash": "113e3302a27c208698a07cdd96ee302ace6dc3fa7a966288c089627d3861c70a", "ref_doc_id": "b9f36262-f69e-4e01-959e-ed420251cbcf"}, "82f68cc3-5c41-4375-8edf-7c00ee4c603b": {"doc_hash": "c4ce2f264eb3cc2e49ef7b9020d0d2efc91de43f6f2ea1034c15ab8774c186b5", "ref_doc_id": "e8c54f02-c166-4c95-a286-4dbf81c1f4fc"}, "67d6b035-9d28-49d9-86f8-5556137a414d": {"doc_hash": "3ffa1a8210c71073065086b13ff0673a6da23c404c3f0c96be045ee1033e2af2", "ref_doc_id": "fc1a7c7c-a819-4bba-84ca-39fe836c2490"}, "882fe263-a4aa-48b5-84d8-951312774a0a": {"doc_hash": "85a4b562bed297a8309a44233d37e01c8221d2d226d51007364b0ccd5d46a8b1", "ref_doc_id": "a2cbf396-ef19-43c7-a15c-86baf1f001a0"}, "d7d175f3-0b44-47ae-930b-be40fe71822f": {"doc_hash": "4ab7d1e633920f625a42a3b20cc1147206b1ab396cfc3918f94332b7c6769c7b", "ref_doc_id": "cd6af929-dff0-4a99-b9e0-0e460c08b7e4"}, "d9717651-a579-4502-ab35-d82ed167cfdf": {"doc_hash": "64937677552462a3af5d5df9053a430c8752392038f3ddf55e12399b6d1ca680", "ref_doc_id": "c61eae0f-d1bb-4290-829f-c7072f80d08c"}, "93e40bf5-ed62-44ed-9369-776cc871e02a": {"doc_hash": "50fc99c4baf13dfa980af48cec75b9eae5060988eda0f763f04be0e91a04c785", "ref_doc_id": "7349a39a-cfd7-4c23-bf17-ed270c770ea0"}, "e921ee1f-6a78-4c7f-84b3-4d10f68e48a3": {"doc_hash": "f199f8113566352687b1209ebdc6cedca1268ebcb71340671d9803d61009e889", "ref_doc_id": "7349a39a-cfd7-4c23-bf17-ed270c770ea0"}, "1286d26d-c1fa-4703-819f-5c44ec569293": {"doc_hash": "bbaebaa7f848f4c36c02a1bb2f81c0317d3e0c3fd276f819caeacdc9fcd12018", "ref_doc_id": "5c49fe42-d58b-4c45-a243-4d8a016ac7d8"}, "dd3c5156-76e8-4167-ae14-6d1473cfc6ee": {"doc_hash": "66f99e2a7a430e05dc2d1dd9ac9147c540c226e4b2eae8ccf3191214c27e2e87", "ref_doc_id": "f97c04f7-7741-4c83-bf74-c3fac0856c3d"}, "5a352a3e-c2cb-485f-b20e-623314290df8": {"doc_hash": "076bfef5bbda2ba38496e92989e9a4ee842784e7b0e649789b4c52df62e25a1d", "ref_doc_id": "c6859032-32f1-4703-93e8-7b9e6c95453e"}, "7800b59c-ca27-460a-91d7-d67358ef8b74": {"doc_hash": "62009ded2bf95a643d4fdff1bef6ac2c5a88831347ec67da4b85b1538478394b", "ref_doc_id": "bae0c8b0-fd27-4c32-bc8e-d95641bc5c7d"}, "60770af3-e928-4ef7-a7a5-ce103f403bd6": {"doc_hash": "915354c922a4252f0f8cc706c0c70757077e572cd6daaad4540aac8aa10172cb", "ref_doc_id": "ea4b7b0a-1ced-4d51-b85d-de7c2cd9d3d3"}, "80ce37bb-468f-4285-b907-514dfa102301": {"doc_hash": "85c8fbfaa443f6c60da13bf33ceb4bfa29af70973e217b3501653d8f4fa7d6e5", "ref_doc_id": "0eaff0a8-fbd9-4bf3-99bc-947e0effa6ef"}, "a2ac7bbe-0ac4-496b-aafb-a6cc411194bd": {"doc_hash": "5a64b0bd6d3ca1268608b32efce9c023907ec8153a7840cb3b5727fc6a2f25d3", "ref_doc_id": "a1164754-9806-42e9-a6a5-e73ee406388e"}, "08ab5769-c73c-4cd8-8a7e-15995922bad9": {"doc_hash": "b62b8a17bdfec31d9eb3ceb189b37d3f12722fcc9cdb48780af9a71bff8ea81e", "ref_doc_id": "b5216b80-5103-48f6-81af-14b94e50a4a9"}, "d2ea0a61-1d3d-4591-9f29-38d28fa41419": {"doc_hash": "6cd176b7726cdff303505a2d8529a5a26a09db06930ba91c5fb6e3a181fb9d22", "ref_doc_id": "f92cccab-29d5-40ba-bf6f-f614dcebd768"}, "87062351-14ce-4d5c-91df-3f5bd151dfee": {"doc_hash": "869128bdca6a57f7b37d4a3f7a3dfca44121ee58082ee73ff1284b3e797a2bf1", "ref_doc_id": "96afeab7-28ca-494a-9bfa-b5aa49aed86d"}, "9ca53843-413c-4d39-9747-4e3dfc8dcd08": {"doc_hash": "99ca902bfa21b09da39bfd5284b1315ca7cd9d6f3bd6339ec5fd72dddefcb6da", "ref_doc_id": "37a87002-5d33-4cb1-89c6-0e3a9cdd3b3a"}, "b4299dd8-9899-4475-8ffa-f8e85d8467a4": {"doc_hash": "c28760a41edb4f6e3121c2b87d65339bcc5db0ce5a4e2280e1396b23556b3d93", "ref_doc_id": "4ec1ac3c-5335-442b-9c87-94a92d636327"}, "6f84e8d3-a6a7-49d0-b888-7f371bc51ca9": {"doc_hash": "988118a83c63190a937e0db3b11f397c7e0c8b6105dce17f65ba6957270c9287", "ref_doc_id": "6dedce11-ddbd-4f1e-8b8a-93aa99d7c5e1"}, "7070e3dc-f567-4ced-938a-01254dabc7d5": {"doc_hash": "39f0076e6c6e639a99a49ee474e08589518d334d5eee6d38fe3a9ff62232a12e", "ref_doc_id": "b52cf43b-8adc-4e65-92b5-66d57870a103"}, "7a55b3b2-e00b-4b85-8c8a-09fd52b64178": {"doc_hash": "e59c51386f7ba0c4ed80444c8af5ea18633d88990681034731f98e371943c135", "ref_doc_id": "956b801d-f4b6-4869-b17b-2f9fb6f04d0d"}, "10adc40f-f4d9-4ee9-8bec-1b477f3f803d": {"doc_hash": "646d3713d15851c55dd9214442cd70e7a6800709c98adf49ff329b9d7125c555", "ref_doc_id": "b5c75f33-816b-4863-b28d-51cec4d3b529"}, "4837b13e-1b1f-4272-9899-b00bd4ca680f": {"doc_hash": "658abcb16f9a1fa0c0477b0daee46fe3f53efa5a3a515445841a5e5e6c655406", "ref_doc_id": "3d390d59-9101-4607-ae69-71824268fa87"}, "879c8edd-27d5-431f-8584-fc7696be59fb": {"doc_hash": "8dae6ee19c18c378f5790b9c8fe6a83f4af0c7249a00e31cc68519e2a596126b", "ref_doc_id": "cd9ad72c-0414-4e21-8086-23886873827a"}, "ed544c8f-c7ed-44bc-b9a4-46848d4d576e": {"doc_hash": "45e26182c87d4bfe7224e89fd94beb20163db794b12aacf28fbc41f54d13dc82", "ref_doc_id": "b240c279-dcae-4e0e-b619-149cf2442806"}, "2b638283-572f-417a-8a7a-3b9534898d44": {"doc_hash": "e80728fde2d4a4b59aa34895c9cefcfff966676b17e474a43a7fbf2bd4fc3a7a", "ref_doc_id": "b2bb905c-3558-404b-9ed2-fdee69cb81ea"}, "fd8919c8-3344-43e7-9a98-2a6ac70ddc63": {"doc_hash": "4edc87ed9f4095ccd3bdcfee8c1bafe3c43861fa6aad811507ca1f1f04b25ad8", "ref_doc_id": "6550ff3c-0c39-488f-a6fb-ec7fdb79e127"}, "2942296e-debd-4d75-bcf3-9b390ed5b156": {"doc_hash": "a1434435646fb03dcc0d7bdd1f7eb7a81a8ca21b26b410a898fc3aaac3ee079d", "ref_doc_id": "230d3219-36f4-4bde-bb69-355cda70d5f6"}, "13df8067-684e-43a3-a787-f600f16df205": {"doc_hash": "0860600b76360fc1d621a83631094c865397a5df22b57d3eb12559ae5015c73f", "ref_doc_id": "efcb2069-1933-4eae-93bd-ab290586a3b2"}, "381d8c3c-760b-49be-a448-a88abd5a39c1": {"doc_hash": "499f13af47aee3d30302cd9f036d0bb03d2b4e572ba1d132b70d4576c98afc3f", "ref_doc_id": "fbb23cff-c529-46fe-9fbe-92a9ea2b3750"}, "9b2d9b4e-3618-4ecd-992d-3788fbc2855c": {"doc_hash": "f9959652fe86c508697497e2dde04c04b094e0d77ac9aa1b2cf329f045d5a3d4", "ref_doc_id": "97f6bb7c-60ed-4be7-89cf-2a2d820277ed"}, "313de4db-a1e7-4bd5-ac63-36afc713995e": {"doc_hash": "2f82094fd8f905980cb1e41314cc7148d646d6928f2ea3d68e8f528772888888", "ref_doc_id": "4f62c1df-e1f8-47a2-b79f-b79839306c0d"}, "dd27ee08-683a-45c8-9d79-f090516961f0": {"doc_hash": "da88d67e460e61febb2636bf73e93dc2d7e4c80b3022e568c4982e46731b3694", "ref_doc_id": "4a007f77-a9da-47a7-97ee-58ec53715948"}, "7e32ece6-2eb9-4a02-933c-dee36e2ec95c": {"doc_hash": "d71d530e639d6608be90dfc46515e8666a4bc4e4f887f10bb2ecf6c1ec174a93", "ref_doc_id": "9bddfd4f-f097-4a4f-aaa6-7e0824ef3948"}, "cb50dfdc-9814-4ba4-8722-4b29dd122255": {"doc_hash": "325fa7656227616ffa923f3cac8f48124357e1c1c77cf14ee67cc0e7cacc84bd", "ref_doc_id": "f286dde6-5e05-4582-876a-f6e6b14c2966"}, "eb554ecb-673a-4cf8-a9dd-4619ca1477f1": {"doc_hash": "a8cf0613b34daea9b6850bdc747622bb60f8ce8fc51be7568bae45b17b3d9eb4", "ref_doc_id": "6a233649-a7fb-4d92-96f8-cdd486554d17"}, "db87863c-e864-493c-a267-5ce64cd7d207": {"doc_hash": "9d589de1abe421743b376a7f049cc4047a67a965e75953dfd6f2abf929c87213", "ref_doc_id": "670f214d-2fee-4eba-ae05-e4af5389371a"}, "80a92745-daf2-496d-b18a-10226dafa0c3": {"doc_hash": "5f483c5c19a490ade80d06ce659e58c96d7354a191b2a81039bfa9792763211a", "ref_doc_id": "bafb9a70-4250-43d8-9cce-d99225bc788e"}, "0d930596-c103-49c0-8a95-2e72011abe61": {"doc_hash": "d907ee46ea36989ac4ef8058a024b1bedfc66457e371b5965542f3855485b034", "ref_doc_id": "4ce3996c-b193-4014-a767-c4ef44f971ff"}, "b5ed8d35-6dbf-46db-8c2b-3ba8d25b60db": {"doc_hash": "850f68b841eea8a6839dc14e48e9712b17e4a193d61c9c0742ee9145b0e6cd5a", "ref_doc_id": "47530729-e34b-4a50-8d0f-fd3bd80a70b0"}, "01db5fe1-00c8-435d-a84b-271d0af75ddf": {"doc_hash": "66cdd5c7613b7ea1b09efda4763d86e49f744a76fdad005837410d03ac2fcc94", "ref_doc_id": "450b9095-9db1-4a7d-aca0-28221ac7ba21"}, "633ce579-ef01-4537-ba05-686fc669e45a": {"doc_hash": "8d1ad5e5b93e560e308bafb9eec82186d31b58cfd7888452cc3cd0ac7b99af4f", "ref_doc_id": "d703b4c9-83d1-4332-a78f-4e0e8b936d38"}, "15c77bf3-753c-4bea-a375-a79ec14f5ff1": {"doc_hash": "19e20828643635ee0280e487ba539a4f04df8916309550404ca361013dd69acf", "ref_doc_id": "eed11105-b8b4-4fec-8c7b-e937d9f7e54d"}, "f6df8a8a-2ed5-499a-b155-f75a0b87cf23": {"doc_hash": "f78541b7d1aafbc6eec19d532dfafcd0ee81b3f31860f6df37ff736f6b88dca1", "ref_doc_id": "ddca60da-d5c3-4473-b5ec-10869091ec7f"}, "96bde251-5198-438d-b02c-8d6625beb6e8": {"doc_hash": "4c1cd0912b16f376b5cbd7d13c70c6e030391b1e0f2481cc009d51b04b9a3b0e", "ref_doc_id": "11c28e0f-bfcf-4f20-905b-d935c59c84aa"}, "34847f06-ae2e-462b-bf75-47863b86e34e": {"doc_hash": "74a612978fb5d67fa5008c044a201f236c25fcc614fc9ae6ce50635d56c0e816", "ref_doc_id": "206a325b-2ba2-4b1a-a37f-3395c772c50b"}, "58d6437f-0355-4229-8e67-6c34b1badf96": {"doc_hash": "022cd616d4c20b6c6d35db7fdec28e5298aca5267ff9b7110c26d6903ecea22b", "ref_doc_id": "49df4625-0661-4a0c-a7e8-cc152a295e96"}, "8d27ce5e-57a1-49bd-a1c9-a724bb8a5298": {"doc_hash": "6226b1b040b381cf6cc75e8f37e2ee5f749c565792d37ba9002edb2ce713cf46", "ref_doc_id": "1858b1e7-41bd-4c4d-ac0b-727fc158ad60"}, "ea0b0dc0-1f8c-4769-b26d-9ed3fdf8c822": {"doc_hash": "bb034340a346d958c050c943cdaa0882fc8b7909226e3b45dd76c6ad20a53b35", "ref_doc_id": "98932691-9658-4b72-9f32-cca87b88640d"}, "1d57b5c2-b2c7-4e24-b772-e6347289ff6e": {"doc_hash": "5430e6f5c70034a3dc92a2a8e56aa7f4d7e44fae3fd02efbbbdbd27d867b37d4", "ref_doc_id": "d5b3c507-d5b4-446d-bc96-be2669b748cb"}, "a27a8566-5435-4a67-a0a3-213347bb755f": {"doc_hash": "666055487b7f630ac9fd34a977a90a52f34351712211b41088036c920506376a", "ref_doc_id": "5406a7e8-6a98-42a4-a0e3-0e0e6c15835d"}, "57d40cf2-50cc-4343-bf93-ff8856b49744": {"doc_hash": "5d8cd3e45e6cf707dcf8d56eb468ed76b0628de60c16cfd996e718094a0cc843", "ref_doc_id": "f7cb44b3-d4a6-40f6-b0e7-dd790c594e57"}, "8445e8e4-f07d-4a5f-ad3b-4cc0de8c1e9a": {"doc_hash": "e7d5770e44da59ac63a082bba827e3ac43f2e9a5a50c901403df1969d0115ebd", "ref_doc_id": "2470caf3-782b-4802-ae14-e833646fc961"}, "d8c22092-6ccc-4aa0-8a50-cd6e9b889fcf": {"doc_hash": "21b80c9fd168bee621b7c8788a067c2f3ae899422594d2d00c7c6a62586966f5", "ref_doc_id": "27c4933c-9cf8-4176-8bab-cab08935e61a"}, "0390de83-b6b8-452a-b65f-e3f179f27420": {"doc_hash": "866cbe06427fc3601d5f2bbd1f973a888d2641ec286b12a4750d726be0864d76", "ref_doc_id": "7ffc6755-54d2-4d01-a257-2d7a5191a139"}, "95d47cc4-2a4e-49e7-9cfe-c0f3d9795cb3": {"doc_hash": "4c310a6e10db3f9e69a401399da62413d714e4567d320719c82cf10b82f3d482", "ref_doc_id": "543b0ae1-2f13-48cf-a344-cb67045ef7bb"}, "3b1b5764-703b-4ce6-8b18-41ffaa464fef": {"doc_hash": "13a79aa855a251b21e188de1ed69ddad7572de742876b5283d355c50fba32f99", "ref_doc_id": "52ca2beb-b4ec-4cbe-95dd-9bb2a35460bb"}, "91f07f65-bed8-4b2e-9209-2884625942b9": {"doc_hash": "6a4891cc8bc917807841d0226a76f4485b51262fadae87f68e87892d479c016d", "ref_doc_id": "d207774e-acd5-408d-871a-c87c3a53fe93"}, "26011f24-7f32-492d-b756-2af370ee2832": {"doc_hash": "5e06eb904c9832adf4b3762219dac6bc40a43383e1b795c996d790b6a8fedcaf", "ref_doc_id": "02d625f2-bb2a-43d9-a4ae-aae00c226437"}, "7edff24c-a136-4660-b1c0-2a12cb23919d": {"doc_hash": "18c5e1abf5caa7b470e54ab0f6397875fbd79c92ea431d96d1e3630499622341", "ref_doc_id": "621e5e1d-65fd-4868-9e3d-575f78528742"}, "7777ca6a-9d67-4eb6-a6d2-5ca06d0aa0bc": {"doc_hash": "c3b4347b95c913b8a844a7f3f21f8803bdb6b123c7deb4cb0b56cda8231bd85d", "ref_doc_id": "e857529b-17d6-40b7-a5d3-dd246e1731f9"}, "d2415395-05d9-4e08-a8dd-3ebe2c06eeb3": {"doc_hash": "ec703bb9d3fe9dc64a9f243d4aca9016c6cb2994020e8312f09891cce6406220", "ref_doc_id": "652e63cd-50e9-47c5-b5c0-05069ec9a45e"}, "3457e16e-6058-4483-af57-b1966b724f1b": {"doc_hash": "1112781a930f20a18157b0a451f95ff7e5c339d65fa449373f6fa79d25d16552", "ref_doc_id": "7377f3b8-0bd3-4cb8-b3bd-337711129bcb"}, "5932ad7d-368d-4cc2-8818-1fcbb82eee36": {"doc_hash": "9420e19f128e1884262bd76e7b2045eb34864691cddf570a5eb924f876940ebc", "ref_doc_id": "7f4af078-8631-4a7e-9b92-c9eca377e19e"}, "72e1c06c-abdf-421c-84e4-d7a0d073ed78": {"doc_hash": "ecbff2a06f4f3710d3ac115467d8a5533c0f437bacf50baa5865a6e8ba141d4b", "ref_doc_id": "ae3c8357-e8f3-4ce7-a39b-8976e9687996"}, "45150d4b-45b4-4834-b1bc-96c457b5043b": {"doc_hash": "b3e7cfa0c603e8784e994ca3e8012d65dd66135a0e5734c960628bd550e01f30", "ref_doc_id": "58f6aa3f-f410-41ce-b9bb-d2c4fb818eb0"}, "070b9fee-43ca-4fb0-92b9-6af843183dd4": {"doc_hash": "3d4f0a98315660affd35657b817ec04787eebebeffdabdd691393db048540934", "ref_doc_id": "a5383756-ad7d-473b-8748-8e5ca3db9873"}, "eb93d18d-b068-4afa-8230-62cf3ccd2af8": {"doc_hash": "f829bed42ab3ef0f9b94721f2e9dd3d5c3308ad135800ee875d67f7451a59b3e", "ref_doc_id": "32057854-cb08-4bfc-9a7d-5a8390ab5503"}, "e057538f-3cad-4d04-a9d9-68b8d1218ab8": {"doc_hash": "d0e3780116fda385e853784e179fd822a7240293677b28fa97f3c163bd04d9a9", "ref_doc_id": "dd7371e0-94b6-4347-9e8f-e40fa9bb20b7"}, "ebbceecc-e1c2-4157-89c6-20c84dedd341": {"doc_hash": "c5bcbb5b4d5fee0bae7e0c94a14a97bd10ef3806dea7f5c355d95b1c97e94cfc", "ref_doc_id": "b2f05688-c05e-4d48-97aa-dba15fa156d1"}, "7a5c971c-1131-4e5f-a82c-5f0d271f5219": {"doc_hash": "c1c091f04cf20fad84f6ba6f4a330eb9da15e12efcf2d6de978fbc8ff2ea5826", "ref_doc_id": "8e7a6c66-f944-40a2-8a74-ad7a15bf7e0c"}, "923a8673-c5b2-460a-b726-824d420550bd": {"doc_hash": "d8eba4bced56858144572b9eed9a17b1347a2871b1c1b88a5d712f7126030f70", "ref_doc_id": "8f8aab45-0db5-45c5-87da-2ddbec6fac83"}, "846c5a10-6bbf-4656-8b74-4780c29815b7": {"doc_hash": "11b73bcfd635d575f5c4bb063a7570da03b7855a9a06a4647eb9305bd7c4004f", "ref_doc_id": "8b79e3c1-a36c-456c-9af4-78184a34c259"}, "d05e87ea-7e8d-48d1-886a-bad0a45e158c": {"doc_hash": "68cc38092b003d31d168e40469c88d2106dfecf0fa1192a9bfa1e08e83bb8a7b", "ref_doc_id": "22385d06-cfcf-4cf8-a4ad-c8782f270e7c"}, "8786e5bc-66a4-4a93-a884-94534e1936fc": {"doc_hash": "6e5b80fcfca394d5a046f89acf9d920a7a2a8dc39b42ffebcf2cdac1fd472718", "ref_doc_id": "bb6a9b07-3ea9-4e54-9ca6-374759e2ab12"}, "93ce2744-83ac-47f5-aecf-18ea79d02ef8": {"doc_hash": "b9a62d1a8f3bfa451204672f00143a4a61932156298938b380e8d042011e8f95", "ref_doc_id": "4b88b751-f134-4b4f-86a5-f267d9404d34"}, "3c5fff63-83cd-45b2-ae3f-e500e8d914d0": {"doc_hash": "040ce38312f28454a95b2fc77ee6da5a1bb10f4027b32aa1427d62803d9f3040", "ref_doc_id": "6fac8246-f819-492c-8c3c-13db2df74f81"}, "5bd4f037-2568-440b-a903-bb44557f612b": {"doc_hash": "4a7cdc3497eb545e4e8923afc579341d5d42d4ded38b6523c5e3765ef835bd09", "ref_doc_id": "0936e118-139e-4968-92aa-c2b13fdf6a56"}, "d76d16f4-a7f7-4abe-b1cd-fdba9a4ab019": {"doc_hash": "970196cb488c364bc2ff47b28d8fd5fbac064f08c0e432ec85e4f4b728ea6a84", "ref_doc_id": "36440607-c683-4dff-9733-34b32fde5961"}, "5157dd00-551c-40c6-94c8-d6ae20ed1faa": {"doc_hash": "8afb18b729c2fad9a27199d48adad1159b1474c9a6ff6da5bd6794aa6e67ba26", "ref_doc_id": "0395d880-e48b-4cbd-9c60-b3423d610b38"}, "c7701a9c-f8f4-4606-9bd1-267259ec85aa": {"doc_hash": "81422ebe22937a2c67838217c0b255082cbb8f498bdbd2e0bfbc1ebe0a61f8c5", "ref_doc_id": "87547b46-efa1-4f0c-b5bf-b4d3e868f54b"}, "ec04ceea-3dbb-4b91-a03b-ad04e45b9103": {"doc_hash": "b8c3e0ec9cc69f58b5207b9095c23dc5f5457f823f5c82be03736886d3908096", "ref_doc_id": "d3248e59-364f-453b-93c4-3810ea964e4e"}, "641f8d16-fda8-4f97-a374-3f386be13462": {"doc_hash": "3d6d4a394c035f951bc23da4a9858546cd72e0603b2af3ae1082870da8855c9c", "ref_doc_id": "2137841b-687f-44b0-900a-9f46763c4e98"}, "b07f7d64-1fed-4ff5-b063-da2d40b54f0a": {"doc_hash": "6ed153aab064a3c4eb9b485081bcf3e82568d09b89cbc13fe4a71e51f8c5d42f", "ref_doc_id": "afd89625-11d5-441b-84e9-a7a2e4372c6d"}, "8b8b7998-0be7-402a-9296-4e7075781390": {"doc_hash": "84869560fbecb24cd3f3b70f4498afc374287b36ba53ceb4fb6b80be200d76a2", "ref_doc_id": "4387c6f1-280d-46a5-a45b-06b5ecdfd831"}, "ea847dca-62b0-41e9-99f7-33075d1042d2": {"doc_hash": "35045967254b31c564900a485d546eeeb917841354a52453d0f26f815c8d8c33", "ref_doc_id": "c844f8f7-2cad-4320-959b-0a2c86c1745f"}, "d06f85b0-44b8-4673-abf8-ccf145c2bda1": {"doc_hash": "957bf475be07e52c43b696f15afdd345ad2700f854eb01148f6e97778b6cff57", "ref_doc_id": "8bac2634-4280-4e50-af81-46bf0ffd6d18"}, "68d138bb-c2d8-4192-8fa7-937fad5d3817": {"doc_hash": "f9cb51afb1b937b7eaf6d908b7c91153d7e36e6a4bdb774f0174cb6ee1454797", "ref_doc_id": "f22c021b-18a4-4c68-b7db-f919355c79f1"}, "77df77e6-5e86-44ed-b8c1-0da6fcdf6e8a": {"doc_hash": "33f9b07ff2aa67c8ce11b421ef2d9480ad0cf21685e0d0adeba091d2dc4aec82", "ref_doc_id": "ba329b1f-4c71-4067-8634-8fb764e0809a"}, "db654770-d2e3-4b41-a241-16039dba6935": {"doc_hash": "bf70c160114aeeb131138b302bd1a967bc204040308a53765eece3856384e6c1", "ref_doc_id": "2db6666a-2f77-4fe6-835c-6b4923f29798"}, "11f66497-d53e-458a-a96d-b4088b14633d": {"doc_hash": "6a624a93a8ecff94ca61cc353f822ff8c07c0f33b43038ac7ea055abcb30233b", "ref_doc_id": "6162e212-cdf6-4d23-8336-f548aea9e5d4"}, "2ced5460-0d85-4c52-bb57-a1475b5eae8e": {"doc_hash": "7c9b1e2a3be42060407f2d7162187a178bb79549eec7781dd29de539cb52b4f1", "ref_doc_id": "188ed599-0bef-4e2f-a6a8-ed897a3a33ea"}, "6d57c268-cf21-4fa0-9ea4-76efbe03a3d4": {"doc_hash": "01192321dc9d14b4d580a032f7014a5c172a391494e8452d65b293fc19a7e55f", "ref_doc_id": "7066ee75-cd40-4dee-a9c3-12e566896e8a"}, "8165b7bb-31c6-4159-8e08-220ce0e2f2cb": {"doc_hash": "94451e3e4bb8a5ec10f41d06c381863fd8cc64cbc76d8182979ea945b488f744", "ref_doc_id": "87870eaf-c124-48e4-90c7-fb047d51fe8a"}, "e0b6e48b-8e90-44e0-917e-cc41596557a0": {"doc_hash": "6b969c365abff1e23d1ab75f2cbbb5e318fe88b92bde618f8a4c7bd93953000d", "ref_doc_id": "256b7554-b675-4320-b4b0-63d1c495fe40"}, "c38b2fb0-3955-4d68-9f22-b664e21540b5": {"doc_hash": "ffbb30a279316231ddf9d8f5264757dd25fe7750cb9cc487a5d3554de2650ac2", "ref_doc_id": "6949dab2-4c32-43a2-a6f2-e97b28262b91"}, "5847cb31-b6d9-4678-9c60-261959ec6c43": {"doc_hash": "be9270dfd3805f839a3c240bf4702c237accfacae8bf7167180f408e4896c5e8", "ref_doc_id": "233713ff-9f4a-4d52-bab8-e89bae908ac0"}, "73f6fb7e-7cc1-4a7a-8c28-e4def73b0d76": {"doc_hash": "72e8f8d9e7028623fe121279f79b40f00f485e9e5a5d8c80074d18a7eeb29e57", "ref_doc_id": "dbb25a45-cab6-4951-9892-f16aefb15e8f"}, "1dc34870-0f21-4cb1-8c72-21b328f3fee4": {"doc_hash": "6f81c7b40c8b704ef072b992ea1996f5b5ba5bd6f2eeee967c2e90d94097a7a0", "ref_doc_id": "edaed966-2e8e-4488-8778-c7be3d46cd79"}, "3228b3c3-e854-4a53-8c46-90b420024efa": {"doc_hash": "4997e116b24ad37ce4c680138e530c7101e10d0e77df65bba0c2830e4c5ea831", "ref_doc_id": "769eb413-31c1-496b-a30d-c6fd5a77cc53"}, "fa897d6e-3699-4280-9a2b-18153e0cd29d": {"doc_hash": "b96e5e4c5b1924b5bfe9a8115aef815844e40091109683f999b710f97cb2e624", "ref_doc_id": "ec8a90ef-0811-47a7-986f-9b493c5416ca"}, "47841a92-e73f-4ce9-b2ac-3a4f27bc52cd": {"doc_hash": "1e7e0d3559444ede3a575c5ddc99c28083241cc36b755d9f2f09fde6259c8b6e", "ref_doc_id": "39839b0c-3cc8-4b2b-88f6-6759d9c35c5c"}, "b5e66dc3-f231-4e25-9092-5229d46dbe0d": {"doc_hash": "6307fceea4b70b2e3872539e687b1eefe73fe6a60a86eac69ae8b6d279eb3d18", "ref_doc_id": "011ed87b-4187-4f74-94fc-c87a96a166b1"}, "bb3cd242-89f9-480b-ade2-db8cfb11d920": {"doc_hash": "211bcaba115cd664654d47730764f3eeeea0644a71fbd3ed82ccce498588a0c2", "ref_doc_id": "b9e152ee-53fa-4018-902a-7c51d497294f"}, "d1f64769-24dc-4070-8618-978cbbe8bec1": {"doc_hash": "87ba502692c63ed029293e76477c5d65b85cd4c6fc42cb6135698fc82c103d85", "ref_doc_id": "e0b257d8-3d67-4f8a-bc49-0bd2a65d4b21"}, "48295fdd-e8a5-44bc-bfce-f020fb85f4eb": {"doc_hash": "332ad9414c5228ff3b58d66821137f01acb210ba19caa8f19fb21da324d0b89c", "ref_doc_id": "6ddb4227-96d3-4160-99b4-dcc7527c59be"}, "e674765f-8b58-490d-90ea-a460e2749993": {"doc_hash": "043fb595ce3ee8c71e7165bc10d0d1979b05aca23092e34cdffe860e95a0b1e4", "ref_doc_id": "8282f81f-04fa-4306-b0e2-158ac8560fc9"}, "75673f48-d3d0-47e3-9b32-8b3955087ae2": {"doc_hash": "761947ab513d2f3c516b12b2167ae66056f5405880f6ccdc456fc2059ff24b75", "ref_doc_id": "3493933d-f315-4006-9040-419979d2e68b"}, "413aaf1d-5361-4689-bcf5-a3b21ca31b8e": {"doc_hash": "945b1cff09eafa6acaf7021093902fe76ca5509fd664e3f7cc7de015520e5bf3", "ref_doc_id": "779388a3-56a8-46d5-9e95-71fb414b8c25"}, "9b2cc135-d7f5-4c0c-b191-0be9af6e2295": {"doc_hash": "29b5e4ba4f189d95a809bd8e32e7e327018c19562c549d7fc00a89eaf881743f", "ref_doc_id": "dbe2a69f-e4ba-4656-8ab8-169e83be275c"}}, "docstore/ref_doc_info": {"6094fcc0-b443-44dd-8634-5068f3ad0090": {"node_ids": ["b2e8fe1a-8646-4db8-a3e8-ddda282679ba"], "metadata": {"page_label": "1", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "60689eb1-3fe9-4624-99ba-8e46b9203b22": {"node_ids": ["39c8a6dc-7107-43e4-92dd-297be9523764"], "metadata": {"page_label": "2", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "899fc809-a5e2-4860-b33d-5006dd337076": {"node_ids": ["f34ea1b8-4f15-43b7-9276-4ebc06388179"], "metadata": {"page_label": "3", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "c60b287f-80a6-43c0-9f72-d1c55c2e13a9": {"node_ids": ["1f0a3d74-fb21-4b0c-9637-3fccb799e058"], "metadata": {"page_label": "4", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "5810e7e5-fdf8-42b5-8543-db2a98d25510": {"node_ids": ["bccdb4cc-146d-4828-b0f3-b6c5e91fa930"], "metadata": {"page_label": "5", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "ea6a59f1-258d-44a5-817e-04c665a9ea93": {"node_ids": ["2930f7f9-4c01-4d4e-8d7e-34b3eed65ffa"], "metadata": {"page_label": "3", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "fd9ee88d-ef27-4bad-af59-a1e83479a348": {"node_ids": ["061f059c-8726-4e53-bb9c-7fd15c60ffd9"], "metadata": {"page_label": "4", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "878e7182-3c06-41c5-bb5c-8222ff1a30f8": {"node_ids": ["f367d0ab-6788-4362-bf3c-5add6d606d3f"], "metadata": {"page_label": "5", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "0164c163-7026-4979-826c-001726140c3a": {"node_ids": ["21f0534b-ed8d-41af-a628-13405fdbbf8e"], "metadata": {"page_label": "6", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "7bceafa1-d21a-4ac8-aabd-19eec5b0b212": {"node_ids": ["7cf5a801-dd90-4825-91d3-c211d857d9c2"], "metadata": {"page_label": "7", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "0fdabdfd-7bf7-429c-95fd-2843c6327b8e": {"node_ids": ["ada3a148-aeeb-4866-8e6a-fd1be013c7bc"], "metadata": {"page_label": "8", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "9f08d760-a4dd-4617-8ca6-5d1bcaeeb8aa": {"node_ids": ["989c8e88-2806-452c-ac07-551c9740379e"], "metadata": {"page_label": "9", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "d9ec69d6-ca26-4231-a9d8-cb5302aadb81": {"node_ids": ["ff3eed74-6e15-4ecc-910f-1b6e8f348ec6"], "metadata": {"page_label": "10", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "fa31846b-7ff6-4025-b334-5531cfd98702": {"node_ids": ["047b3d80-a6bf-4716-9efe-9c706661f461"], "metadata": {"page_label": "11", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "8a4bc4f0-d445-467c-b12f-dd07a019b214": {"node_ids": ["dc7a3ff6-8c5d-4cbc-82c7-e9c361311577"], "metadata": {"page_label": "12", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "89d97c01-a8ef-4666-84e0-9e1236ab18f1": {"node_ids": ["6dad2aa7-f344-48a3-9748-57e19f4e1513"], "metadata": {"page_label": "13", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "3fb7e556-4484-4834-ae7d-1d06d2a19cd1": {"node_ids": ["28954239-7003-4d07-8fed-232f158d5ac3"], "metadata": {"page_label": "14", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "f24aea0b-aeb1-463b-acd4-d86761bb6f62": {"node_ids": ["3947e646-5404-4758-9786-1048c046d6d4"], "metadata": {"page_label": "15", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "33965993-5109-4044-a90e-841c29acf34a": {"node_ids": ["6ac6f13a-9319-471e-96de-ceff489389bd"], "metadata": {"page_label": "16", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "7f3d4a4c-f62a-44a4-a220-26330c1afbe4": {"node_ids": ["6621799e-d1f5-40d6-afb3-6e9a18d60898"], "metadata": {"page_label": "17", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "3947ecce-c8b3-4082-ae15-cb5cdb942e1c": {"node_ids": ["b0de6215-749b-403e-a470-6f6b68f7ce5e"], "metadata": {"page_label": "18", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "120b9a6a-2cb6-4c6d-a97f-74212c30e337": {"node_ids": ["bd056e1b-08f1-4ba9-9a7f-a8f6cbaeda87"], "metadata": {"page_label": "19", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "e916bb8a-c3a6-4624-b2f4-54763c967b89": {"node_ids": ["5d0730f4-a764-43e9-8501-dce3ed11d4d2"], "metadata": {"page_label": "20", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "65be8e2b-575d-4634-b303-5158d365fe84": {"node_ids": ["93559659-41b7-4fc8-ae7c-5f9972d6b9a2"], "metadata": {"page_label": "21", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "5639c14e-d399-4eb7-a1af-a4fd7e42bbbd": {"node_ids": ["8a44d185-30d1-4bc6-986e-04a7eba7e313"], "metadata": {"page_label": "22", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "74e4b992-1891-4633-ba1b-db568a01b6df": {"node_ids": ["340b2b95-5b28-4067-976a-fd1d97e84f2e"], "metadata": {"page_label": "23", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "39263651-0b90-4dba-8f01-d1be11ceff5f": {"node_ids": ["aa5fed15-a60b-4f14-97fa-66b3a2683635"], "metadata": {"page_label": "24", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "8bb33091-ca08-46bb-958a-b1fe79851bac": {"node_ids": ["74c94af6-d202-4cfb-920b-f37847f73fa4"], "metadata": {"page_label": "25", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "cdb06cdc-5247-405f-bbcf-b114ec1912f3": {"node_ids": ["6a389593-16ee-4d41-9df2-d6f19a7fc207"], "metadata": {"page_label": "26", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "cc4df5ae-574d-4fbb-9a35-07ae63fe45e9": {"node_ids": ["4f0b2333-72a2-4e70-9176-d9261008f955"], "metadata": {"page_label": "27", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "6951bad1-e967-4bea-ba56-06374af12b22": {"node_ids": ["09f3d72a-83dc-4328-bddd-ab2bd989623c"], "metadata": {"page_label": "28", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "84f6a62c-c321-476d-84c1-c03c49274070": {"node_ids": ["12cb3e97-6ecb-4147-b3cd-37ec6277c76e"], "metadata": {"page_label": "29", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "82084c07-7813-466b-ac59-f31939648ddc": {"node_ids": ["b1d2bc2e-054f-4ea9-b8f5-d768700e39ae"], "metadata": {"page_label": "30", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "1b59759f-8749-412e-9c09-aec7260d6490": {"node_ids": ["b851b373-85f2-4f1b-a519-55d7e5a25478"], "metadata": {"page_label": "31", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "d1ff5289-b125-4279-ae7d-f25aa654d9c1": {"node_ids": ["29d88efd-250d-4691-a2fc-0a50e7d0c600"], "metadata": {"page_label": "32", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "46430440-97d6-44cb-b6e1-30f0be686743": {"node_ids": ["1402c676-c4f4-4e9e-861f-a9c638e04fcb"], "metadata": {"page_label": "33", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "7e5136e0-957e-4acd-9c39-81b48e3aa2be": {"node_ids": ["2a75f063-ea78-4686-9b6f-44cb41361b75"], "metadata": {"page_label": "34", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "5691d812-406c-4a90-b6f4-d14b890dfed4": {"node_ids": ["d933a600-b9bf-4034-8f80-99dba27f274c"], "metadata": {"page_label": "35", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "0446435b-2869-43da-94ca-11e81ed20e45": {"node_ids": ["348b03b8-f316-4d09-8165-c3fb7ca0ff81"], "metadata": {"page_label": "36", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "2b346bf3-5e5e-490c-9356-87ea13d9fadd": {"node_ids": ["c6faea92-87b0-4935-ba16-e7ff1e516fb6"], "metadata": {"page_label": "37", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "0c4b21e2-ab4c-42fd-ad6a-145261c6023b": {"node_ids": ["c08e923f-1a5e-415f-8b95-96ac2f175014"], "metadata": {"page_label": "38", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "784875a5-bcad-4d00-bca6-cf90a03b8861": {"node_ids": ["f5d9629d-4e4f-4a64-ad76-83a8fb9b6e21"], "metadata": {"page_label": "39", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "d642a07d-31db-430c-b5ad-22309d3056c9": {"node_ids": ["ed302f67-ff74-4d01-8549-fdc1e7817f48"], "metadata": {"page_label": "40", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "b8136704-eb51-4908-b0bb-4ea95f5bea8c": {"node_ids": ["62530c90-823e-45b5-8ae8-2dd1729084bd"], "metadata": {"page_label": "41", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "f22f944c-8fd3-4a9d-864b-72e74a5e44d1": {"node_ids": ["38834211-5145-4b56-9c04-9fb00f303606"], "metadata": {"page_label": "42", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "680f93a1-410f-4b25-8d00-9e260af20180": {"node_ids": ["f01ded40-aa2c-432f-91f2-680b7ccc3065"], "metadata": {"page_label": "43", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "fac1b167-091e-4246-88c3-519a3a708ef6": {"node_ids": ["1f945d7c-470a-4b90-a491-b3b5652c144f"], "metadata": {"page_label": "44", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "07017951-119b-4a87-baee-cb8a70ab735b": {"node_ids": ["f6de8d81-a37f-474d-b9b5-3f594d72cc23"], "metadata": {"page_label": "45", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "006a20de-39fe-46f0-8cf9-dc6353d132d8": {"node_ids": ["4cef62bc-a2d1-4790-b6c5-accfea1fb134"], "metadata": {"page_label": "46", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "b93ae5d7-c1d6-4a9e-a5e0-3e78eb88ef67": {"node_ids": ["c2b9b6e7-3824-49fd-aacf-0a184780cd99"], "metadata": {"page_label": "47", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "b5a55f82-e6fa-41aa-9469-b6f50187b3b0": {"node_ids": ["da4195fd-c83e-4d71-84bc-b889f61c3b34"], "metadata": {"page_label": "48", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "9f5bb9af-4b8c-4008-92f9-9df33a2c1f79": {"node_ids": ["345c9541-9073-49c9-9e62-229afb98afef"], "metadata": {"page_label": "49", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "e467744e-4b41-4485-a417-77e93a794191": {"node_ids": ["ee35189c-756c-4aeb-a633-40969834379c"], "metadata": {"page_label": "50", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "3477f662-34bc-4321-a72e-9aee677858b8": {"node_ids": ["43c46c4c-db2e-4bae-9667-6ecd323b402b"], "metadata": {"page_label": "51", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "a309121d-5285-4fed-9729-74a1185a37f3": {"node_ids": ["2b3051ae-73c5-4c54-9acb-9307537ab897"], "metadata": {"page_label": "52", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "a5854754-25e7-4ead-bd07-5ba54221941e": {"node_ids": ["f6ed877c-aea2-4160-8b8a-73602e7f3f39"], "metadata": {"page_label": "53", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "9584486d-8aa8-4cfe-8281-5a59b69cd8ee": {"node_ids": ["50289ea6-4d57-4d67-86f2-7387f0e2682e"], "metadata": {"page_label": "54", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "8cda7db5-b1a5-40c0-b518-132c38ff3874": {"node_ids": ["28d70e9b-a6fe-4702-ad1c-4ea4b8f517d5"], "metadata": {"page_label": "55", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "7254cc94-c98c-430f-8a32-ba8d5bbadb58": {"node_ids": ["bc21ac7a-9d62-4f42-940e-2b6642c9808c"], "metadata": {"page_label": "56", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "fe67addb-a6d7-477d-a4f3-e0da12d12eb2": {"node_ids": ["da7c61ca-5061-4556-818f-d6bb4d65de52"], "metadata": {"page_label": "57", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "891c5a56-9517-408f-ba99-3568d7d04bdd": {"node_ids": ["75624f8e-28cd-4399-8640-b5b5aeff2694"], "metadata": {"page_label": "58", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "f737561b-3951-41b4-929c-55a9fd235238": {"node_ids": ["6197b9a8-694c-4f78-a126-9e337536b8ef"], "metadata": {"page_label": "59", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "ddf66051-632a-4c96-8c64-0243cae5af9d": {"node_ids": ["e65271b7-4772-4d81-8f3a-6317a47041b3"], "metadata": {"page_label": "60", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "6ed7fefb-4f6b-4b74-b434-63c25726b1be": {"node_ids": ["8813068b-9fbc-41ac-963a-011a3e73bc41"], "metadata": {"page_label": "61", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "e6e0b8cd-6d94-4b86-bd0b-cf43bb78c998": {"node_ids": ["9d66076d-5939-4335-874c-5934255d43bb"], "metadata": {"page_label": "62", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "d17cc04f-2556-4be4-8492-183bde7500b5": {"node_ids": ["5b51b3f0-e39e-430d-8d1e-60a3ffe60564"], "metadata": {"page_label": "63", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "bc11ff53-4ff5-417e-95fe-d05fcaf270ee": {"node_ids": ["56b9c850-b312-431f-a280-8e7d937d7a2e"], "metadata": {"page_label": "64", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "c41f80f8-2b1a-4e9a-aefa-57ac6eccb785": {"node_ids": ["fe262dcf-bbaa-4787-b585-db30d7df91d3"], "metadata": {"page_label": "65", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "38a47316-3622-4770-ae78-b51ac99dc884": {"node_ids": ["90723a68-e2e5-4821-9340-0591384f0ba6"], "metadata": {"page_label": "66", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "df0c2233-afc8-42c9-b8ac-d724076b60a7": {"node_ids": ["6327fd99-ca61-4c04-ab8b-794ed99e1447"], "metadata": {"page_label": "67", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "48bb777f-5654-4d58-b6cf-b579e21e6fb9": {"node_ids": ["617f5ffe-02dd-4de8-ad20-90888f039f77"], "metadata": {"page_label": "68", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "c51adbaa-8270-46ba-8cf6-bbb9e291580a": {"node_ids": ["364cd22c-e52c-442c-a21a-bc076ed7abf4"], "metadata": {"page_label": "69", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "376fccce-f29f-4d2d-9746-0f3cf167b13b": {"node_ids": ["108c0891-258a-4b24-b3b4-16e7421ddcaa"], "metadata": {"page_label": "70", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "0938c9b3-cb06-4f3f-a9c1-9af26e07643f": {"node_ids": ["fbc3e0b0-beaf-4635-b221-474877d34e8b"], "metadata": {"page_label": "71", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "e48a3946-2f14-4abc-a2ca-4ae46205d74a": {"node_ids": ["c6622d93-31e0-483b-8dd2-fe7dc1071ff5"], "metadata": {"page_label": "72", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "6e5a34ea-6545-4637-b648-bc3a2d1168f0": {"node_ids": ["8d93b1c4-40b3-4fa0-80be-c083cbdf53b3"], "metadata": {"page_label": "73", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "9162b5d8-1bdb-4b21-8319-bb413876ea91": {"node_ids": ["2569e1dc-bc28-4da9-9a2f-faee0fad778a"], "metadata": {"page_label": "74", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "002a56bd-7e0e-474a-8f2c-623dae5ee463": {"node_ids": ["a6c5efa8-1a8d-4aa6-8fb8-9d45551e2153"], "metadata": {"page_label": "75", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "fc36ea8f-7747-42f1-96cb-81e60e3248b1": {"node_ids": ["a01bfb75-7720-4e8d-ad8f-9fce2011072c"], "metadata": {"page_label": "76", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "28484c8b-0ff3-41f0-b500-bbdf5e37ffff": {"node_ids": ["1b5ff690-4513-4452-816b-1f934b6e9c87"], "metadata": {"page_label": "77", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "d1d84a30-f22f-4147-929a-0de8a22c465c": {"node_ids": ["4cbb3c39-a01e-4e92-91da-3d44683f3634"], "metadata": {"page_label": "78", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "9a0f0f46-0926-48d0-9fce-9164f33f84c9": {"node_ids": ["12802952-3737-44bc-a7b0-aff4ffc36ab4"], "metadata": {"page_label": "79", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "7e5a61a8-cec2-4a35-9420-99ba06a81934": {"node_ids": ["498c2406-ae08-4f53-a5da-5b7f308424a4"], "metadata": {"page_label": "80", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "65f1e966-8274-4cc7-9b67-ec592f3f979a": {"node_ids": ["79412024-e706-43cf-9fd3-ca2cd10d7f62"], "metadata": {"page_label": "81", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "3ef2ad90-5594-4ddd-8790-31bac87f157f": {"node_ids": ["eeeae394-009d-482c-826d-2421c4d4a464"], "metadata": {"page_label": "82", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "b780f399-be0b-4809-875c-1f1eee3730d6": {"node_ids": ["a5375ae3-31f1-4295-b489-9dda0704b50f"], "metadata": {"page_label": "83", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "350c6309-8c08-48db-a63c-c82fef335820": {"node_ids": ["1d345b7c-1a86-474a-975b-cd3acad8fdc4"], "metadata": {"page_label": "84", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "485880b5-6aa5-4457-9f9f-c0345431acbc": {"node_ids": ["72ee43d7-7bb6-48e2-bed5-2ad4d5c5ef83"], "metadata": {"page_label": "85", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "3d8f3656-6321-42e2-9002-d86c1cd373e6": {"node_ids": ["f8dbc309-1173-440f-8104-c915eee8a157"], "metadata": {"page_label": "86", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "38fdb2ad-2e13-4759-bdc7-84eb20045d87": {"node_ids": ["809bf121-161a-4f7e-a144-e87f55f45fc5"], "metadata": {"page_label": "87", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "9dae6518-a1e9-47e1-a93c-23fe503b873f": {"node_ids": ["13bc16e4-b716-4244-a923-8fe3778b0c66"], "metadata": {"page_label": "88", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "c6ebfbbf-5ae3-4e87-83ff-f82186330325": {"node_ids": ["197e2008-a477-488f-aa83-feb1e6ce3642"], "metadata": {"page_label": "89", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "26e6a64f-61f6-466e-b9dd-bb2955c286b3": {"node_ids": ["6bdcdce4-a9a5-46f3-a328-1095937fcfb8"], "metadata": {"page_label": "90", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "e3da4913-5c12-4be3-86d2-34abf95892dd": {"node_ids": ["e1c6ff47-7a0f-40bb-b6ab-deff14562aec"], "metadata": {"page_label": "91", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "7eeab61e-ba51-404d-9f9a-eecc8bc5a0b5": {"node_ids": ["c22aab93-ecf4-46e5-8abc-f973ac43cf30"], "metadata": {"page_label": "92", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "089efd66-9a30-4768-961a-5f2d28528cb1": {"node_ids": ["94d11640-707a-4b97-bce1-c44456ab92a7"], "metadata": {"page_label": "93", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "243a720a-7a98-47b9-96fd-98eceb370ca4": {"node_ids": ["3f1001b8-a37c-4607-a40f-e5c3cc471e25"], "metadata": {"page_label": "94", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "75cf788c-6b56-47c9-91ed-d23ad70fe197": {"node_ids": ["c3400d92-0410-4f05-9cef-5b244be18db6"], "metadata": {"page_label": "95", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "73786048-9fed-4015-9ef4-f53b73509fdd": {"node_ids": ["c5cd3f6a-6d93-4e8c-a908-f99df372ca20"], "metadata": {"page_label": "96", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "c36be9fe-2134-4974-9a8a-232520aa1518": {"node_ids": ["7905e80d-7912-4ddb-a623-c6c0b73a1040"], "metadata": {"page_label": "97", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "cd904311-7be8-4d75-a39d-db3e6741e71b": {"node_ids": ["109ed287-9f70-4690-bab7-3d4b4267afbd"], "metadata": {"page_label": "98", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "61fdfd31-cb4d-4fe2-b1ce-7c3f052da139": {"node_ids": ["86b07e53-90a5-45b4-9bff-b15a25733c96"], "metadata": {"page_label": "99", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "7a67226d-9103-4d88-946f-98a131d7c5ab": {"node_ids": ["da21513f-a890-4594-adb1-c83ccdd16530"], "metadata": {"page_label": "100", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "a70caa47-ac2e-4bea-8807-5be978f21f00": {"node_ids": ["13c5249d-dfce-4c69-9ed4-25161852294e"], "metadata": {"page_label": "101", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "21ddfb01-65c5-4553-ae6f-8922232b66a6": {"node_ids": ["7e519b41-ec89-42fe-9403-890916131790"], "metadata": {"page_label": "102", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "5f7611bf-9c7c-443b-8723-8bce5ea724d6": {"node_ids": ["50059248-7fb1-4ca0-9336-fdc25b56d3d0"], "metadata": {"page_label": "103", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "845ce1d0-92de-406e-9d14-f4e68eb5b081": {"node_ids": ["e601a6ec-c22e-4ad0-b3d5-ff7de0d0b28f"], "metadata": {"page_label": "104", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "991ef4fa-8d15-46b2-94a4-a6bebb870d9c": {"node_ids": ["5cd3fe57-80f0-469a-a76b-25659275d9f5"], "metadata": {"page_label": "105", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "f60cce34-67bf-475c-bdaf-24756070597b": {"node_ids": ["0cce7747-d815-4d50-b465-9257b08db87f"], "metadata": {"page_label": "106", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "e54ab27f-2295-4789-9317-288ccaf70d3a": {"node_ids": ["414aed5c-ec53-4922-872c-c145fc79d82c"], "metadata": {"page_label": "107", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "2afa67eb-ad74-41b0-9eb9-cd5e2c06b3b2": {"node_ids": ["8e067e52-a3f9-4a49-8118-8c6a2f8dd9da"], "metadata": {"page_label": "108", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "9ef4bb87-a72a-4f6b-957f-8767f3b01866": {"node_ids": ["6bffc94a-ab80-43fb-8cca-0e93b93b3d9b"], "metadata": {"page_label": "109", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "bbdaf581-ff54-4afb-8449-57a33cb9acb1": {"node_ids": ["f185b236-e9df-4afb-bc78-b2864d943ae7"], "metadata": {"page_label": "110", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "138a481a-6bc6-403a-81a8-2f5c17190273": {"node_ids": ["9237f21f-d5fc-4800-83e9-81a26087d30f"], "metadata": {"page_label": "111", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "07f5054d-8320-4945-8af8-26da39e9bbf1": {"node_ids": ["87497604-ff97-4b4d-b3fb-a087170190ad"], "metadata": {"page_label": "112", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "40b9a1cc-cc28-4030-8dcd-b91b00b82793": {"node_ids": ["9737deaf-25c1-49b0-afbd-67133394a021", "291295e5-b0f8-456b-a7d3-4e044aba2fe1"], "metadata": {"page_label": "113", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "3f07bb6e-22f7-41c6-b8da-d3796324b6ce": {"node_ids": ["10396245-96c9-40e6-a013-eb2c0c13f725"], "metadata": {"page_label": "114", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "66560a34-7c1b-4fa2-ad77-acda932385e7": {"node_ids": ["7415086e-15fb-40d2-a53b-24ce9afb4acc"], "metadata": {"page_label": "115", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "df21464b-c7be-47a4-9b6e-5fd8293bc972": {"node_ids": ["98f4af05-6d23-40bc-b68d-b752447acdee"], "metadata": {"page_label": "116", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "4412e606-8da1-4a92-a9dc-61b86050c16c": {"node_ids": ["e354cf5e-1685-411c-9d04-65869e4ee66e"], "metadata": {"page_label": "117", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "10ec693d-b0a2-4e34-a299-2843c6c3a32e": {"node_ids": ["7b563e6a-a32c-43f5-8c21-e393be6d73c1"], "metadata": {"page_label": "118", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "8758f6df-d987-4bab-95f3-f15fc101735e": {"node_ids": ["24008349-7c43-42b2-baf6-bcf01ffc1048"], "metadata": {"page_label": "119", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "f16b09c7-88d7-4b01-9dd8-fa2b76417ae3": {"node_ids": ["c9b1e539-831b-4684-949b-535526589a08"], "metadata": {"page_label": "120", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "057f1af5-e93b-4dd9-a266-2e699aafde04": {"node_ids": ["46d0b0cc-61a6-45f9-b645-e3875aabc7b7"], "metadata": {"page_label": "121", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "b8beef72-ce8f-46f3-87f5-7aee3b82fe18": {"node_ids": ["3a984861-5316-4625-9016-10bec07df664"], "metadata": {"page_label": "122", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}}, "15507f04-38db-4377-b787-5f281d10192c": {"node_ids": ["9ec96a62-c504-4652-845e-6b70c5b16c29"], "metadata": {"page_label": "i", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "ebae195b-60a4-4ea0-aa2a-6e64e670fe0b": {"node_ids": ["41eed3e4-ee8c-45b6-8e65-25dbe683d959"], "metadata": {"page_label": "ii", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "25d02b27-b012-4ed9-94cd-d3402e3f374e": {"node_ids": ["f28e9cc7-1611-4606-bec7-4ee244b14b28"], "metadata": {"page_label": "iii", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "e2429523-b149-44ce-847c-49e8a30c4dfb": {"node_ids": ["f5edc9e0-e56a-45a5-bb15-df95d93a907c"], "metadata": {"page_label": "iv", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "a3058728-f533-4bec-ac10-f63bc0c64fac": {"node_ids": ["03c01a7a-4a15-4999-818b-077abefc71a8"], "metadata": {"page_label": "v", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "7d7242ee-e0a3-4d27-8c75-32f2e7e1b40d": {"node_ids": ["46ba7a57-3691-4487-b5cd-c83820fef529"], "metadata": {"page_label": "vi", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "1bc58610-56f5-4fe7-922f-7d2ad04c23b0": {"node_ids": ["146c09e5-7e89-48ca-8f56-513358b00b97"], "metadata": {"page_label": "vii", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "f316e2ba-173f-4612-bdcc-2a4c050491f4": {"node_ids": ["2495de7a-5231-44d8-99b8-01bc460a2c66"], "metadata": {"page_label": "viii", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "4b3cf393-5386-442b-9675-dc7e93a304c6": {"node_ids": ["f6970b5b-6c43-4a7b-afd9-aa3092ee5321"], "metadata": {"page_label": "ix", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "1dcdd682-3688-41f3-ba4b-1038cba79e00": {"node_ids": ["4014de6b-3a7b-406a-abf2-a9cdcccd52e2"], "metadata": {"page_label": "x", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "2a62d218-63b5-45f7-b38a-9ea517be099f": {"node_ids": ["d1637837-4685-4bdd-aae8-3b0271964031"], "metadata": {"page_label": "xi", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "4fcdd957-e322-4f9d-adcc-b1725fdac1f1": {"node_ids": ["c76e899a-a93f-4a23-833c-fdcff790fc9c"], "metadata": {"page_label": "xii", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "aeead514-701c-4be7-aa0a-5aa2f0d97e72": {"node_ids": ["d5d1f626-49b8-4ec0-a839-04aef8c5cc80"], "metadata": {"page_label": "xiii", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "2c59b9d6-36c8-47bb-b95b-c3f45b5d9ff2": {"node_ids": ["2c96b25b-3f5a-4369-9ae2-100e93c32e32"], "metadata": {"page_label": "xiv", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "bc31d22d-92d8-431d-a2f7-f317658115a5": {"node_ids": ["d4f095b2-c735-4141-a4fa-aa9798eac5f4"], "metadata": {"page_label": "1", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "d14e76bc-e91c-409a-9a11-a609bb01a8dd": {"node_ids": ["23546f82-a25a-4326-9d07-e05529a31672"], "metadata": {"page_label": "2", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "ae82100d-8a93-498e-845a-f1bd93600563": {"node_ids": ["4978da61-21e6-42ed-8ef6-c9bd449f1f2e"], "metadata": {"page_label": "3", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "bd522413-28da-4ff5-8131-001f353b7a92": {"node_ids": ["fe71ffa5-dc19-43ca-a8a6-75d24526d12b"], "metadata": {"page_label": "4", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "5c36486e-83af-438f-bd99-fe99c67fb514": {"node_ids": ["81061db1-604d-4a22-bc1a-a362f6d6d8ab"], "metadata": {"page_label": "5", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "a9b4e6fe-0ce0-45e1-b1ab-1c665d54d426": {"node_ids": ["61c975e4-a365-4528-a52c-4ee4c60dda6c"], "metadata": {"page_label": "6", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "c59d9ca0-7be7-4ee7-9e65-c3d26bd8bf2c": {"node_ids": ["d0a5cf6f-db77-4a1e-b23f-37dd6816959f"], "metadata": {"page_label": "7", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "72f1d33e-6577-45a6-a546-38613b8d35bb": {"node_ids": ["813c32fd-8061-46ee-8f56-b997ac751fc6"], "metadata": {"page_label": "8", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "8aef010f-d6b9-478c-8951-a1454a2b7aac": {"node_ids": ["4e3c21c4-4203-4c46-8278-17ed9b06c913"], "metadata": {"page_label": "9", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "8f4891f0-5e7c-4ac6-9e30-df58d1c69f90": {"node_ids": ["e9b793d8-48a7-4cea-86c6-9cbd901fca1f"], "metadata": {"page_label": "10", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "f962c606-28d6-43b5-ac10-e20210f42aa8": {"node_ids": ["ddb55c98-adde-40f9-a75a-ed989d620e76"], "metadata": {"page_label": "11", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "adf9f41b-d5e3-4793-b35d-cfe305c0465d": {"node_ids": ["a50fa191-c7cf-402b-bc10-46c4a2f04071"], "metadata": {"page_label": "12", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "2463e660-25cd-4e1e-afe3-78d78eafcbde": {"node_ids": ["fe7a65fa-04aa-4e4e-b697-9b3d03a744d3"], "metadata": {"page_label": "13", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "0c36437e-2907-4549-bdaa-9f0b901f64ab": {"node_ids": ["bc112e93-414c-4d61-bb88-7efab9e2277c"], "metadata": {"page_label": "14", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "4af66db7-e787-43fa-a725-ba6d67512629": {"node_ids": ["b808ee3d-6754-47c3-986c-0ca58d5d2507"], "metadata": {"page_label": "15", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "7f94ea95-0283-45d1-8a81-38da77061a02": {"node_ids": ["a152016d-1484-4154-9491-705f69feddd8"], "metadata": {"page_label": "16", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "3b49b232-5373-4b58-b0c6-4126c3dcefeb": {"node_ids": ["1fb27db6-3aeb-4de5-870c-2970b0315647"], "metadata": {"page_label": "17", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "6b1ffec2-f416-49f1-91a6-6638a3707ebc": {"node_ids": ["42be38a5-6c0c-48f7-9365-42259c6be171"], "metadata": {"page_label": "18", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "c896ec5f-0a2f-4aea-8f04-6806ba251529": {"node_ids": ["8bdb543e-a7b8-462d-9bf9-d3f6cd8670a9"], "metadata": {"page_label": "19", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "69dccb96-3af3-4f1e-b23b-96d9e2f59e6e": {"node_ids": ["168b83c4-ebc5-435b-86e3-33a4368ec5b8"], "metadata": {"page_label": "20", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "60c3a714-f332-45f6-937d-819ad4723549": {"node_ids": ["c0bf57af-e364-4236-a1ef-153ee4563a70"], "metadata": {"page_label": "21", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "180fa6d7-f31e-42ee-835f-b6c88ab890a4": {"node_ids": ["590bbb30-6772-414a-83c4-e58aa98d4753"], "metadata": {"page_label": "22", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "acaff5d2-c796-476a-8f22-40a9fcfc9ac2": {"node_ids": ["c3d62f1f-5df7-4dd1-986f-e1c628481dbc"], "metadata": {"page_label": "23", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "869f0e38-da66-42bf-a449-f40a48963754": {"node_ids": ["12e66299-4608-4caf-8592-fa617027b782"], "metadata": {"page_label": "24", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "a61ec00b-17b6-4540-8a98-e5fe77d57d50": {"node_ids": ["09ef6012-468e-4d9f-bf3b-62dcf9d41179"], "metadata": {"page_label": "25", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "557d23b3-4b99-42a8-b7c5-69683ef71182": {"node_ids": ["cc71cc61-1640-44c3-8acb-0dbba54498f8"], "metadata": {"page_label": "26", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "536a7f32-9a7d-409a-b6d0-b98867339186": {"node_ids": ["e8db72d1-59b9-4e22-970f-4d6b02a1731d"], "metadata": {"page_label": "27", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "1093b147-40d4-4059-933c-184a597b0c74": {"node_ids": ["70b45da8-9608-4d17-9923-d63e58fa0fd2"], "metadata": {"page_label": "28", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "b7ebd51d-26d0-489c-a0e9-4fb094d04361": {"node_ids": ["aa081668-3de0-431f-b7f3-4e56fd1eb9a9"], "metadata": {"page_label": "29", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "6a28a010-75c5-4014-a837-af578a5eea41": {"node_ids": ["16f4e90f-fb76-4e6b-9ce1-683e850be6d3"], "metadata": {"page_label": "30", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "8de3e4ce-4634-4794-b0fb-f57dffb74aa2": {"node_ids": ["957f5aeb-9579-43d4-bad2-4adc476e3a49"], "metadata": {"page_label": "31", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "6f3d497e-d30f-4a9c-96d9-de7c81642a38": {"node_ids": ["30557e0e-3173-465a-a8fe-7d27a34a4401"], "metadata": {"page_label": "32", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "b4661db2-b657-4f5d-a83f-fbb2b37f1a23": {"node_ids": ["0cd5b501-ed12-4851-94b9-b0ec33ec350e"], "metadata": {"page_label": "33", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "a22bb487-4bb7-4be3-ba45-7b85994080a3": {"node_ids": ["ee9a83e0-a25a-4ec5-9dc9-b3e4761ea42d"], "metadata": {"page_label": "34", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "8bc7d0bb-11ca-4e39-8897-1aa693a5d9ec": {"node_ids": ["ebc15953-cd6b-484e-a055-d6a398350d27"], "metadata": {"page_label": "35", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "1751a2c0-07c9-4f6a-8525-243780e2ceeb": {"node_ids": ["624f3045-0292-490b-93d6-87c7104acfd5"], "metadata": {"page_label": "36", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "13d27492-0bd7-4cdd-8fba-c815d36bf260": {"node_ids": ["bd860081-b3ed-4936-a7a6-4a888869bd99"], "metadata": {"page_label": "37", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "603b96d6-97da-459e-ba51-3ccdb83a3663": {"node_ids": ["2db0f05b-01c6-40f6-b7fd-926c6609a282"], "metadata": {"page_label": "38", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "d590e653-6e6d-4ed0-80e8-d9aef585bec7": {"node_ids": ["e7cc1021-af56-488b-881a-431b808a657d"], "metadata": {"page_label": "39", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "e7d2da3a-6b4f-4bd7-9f8d-6712343642f6": {"node_ids": ["2adc12c8-f956-4ca8-8ecb-6328660d645c"], "metadata": {"page_label": "40", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "01908d4d-c8d2-42e1-8049-f2640eecdd99": {"node_ids": ["45e28d1b-2679-4b2c-b6f1-682563c936a3"], "metadata": {"page_label": "41", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "ef346a62-a45b-49fd-979f-c43f84a93d6c": {"node_ids": ["f3925c94-975f-4a1e-8161-ab3a08b4c9da"], "metadata": {"page_label": "42", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "35489d90-8530-4200-af4a-c7cd129d5f74": {"node_ids": ["986c666b-ebb2-4bc5-bb6d-8ca33d636af7"], "metadata": {"page_label": "43", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "8ef7f518-da59-46e5-9185-1c660a27b9ca": {"node_ids": ["9ddbb48b-24cf-49cd-a599-e0fe678a95f2"], "metadata": {"page_label": "44", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "5007ad2a-5776-4f0a-8215-66598905fb4b": {"node_ids": ["0394dec8-a084-423c-a812-26295f9af42a"], "metadata": {"page_label": "45", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "cf876de5-32bd-406d-9907-9f5b44863f4f": {"node_ids": ["d6a62002-712b-410e-b7fb-2e792d4d4260"], "metadata": {"page_label": "46", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "22cfa8ed-f7a4-4c30-9880-cb2f7d833635": {"node_ids": ["55acb479-9866-4b9a-926a-6b28c59e2d4a"], "metadata": {"page_label": "47", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "758942b9-e579-4cf0-abf5-0911dd34133f": {"node_ids": ["14a637ff-0e83-4d4e-ab69-84fe36e1201d"], "metadata": {"page_label": "48", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "a96f86ba-2083-4595-88dd-be386ea2d8d0": {"node_ids": ["28a21f32-3532-47a0-b0f9-42c678e51f56"], "metadata": {"page_label": "49", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "504aad8e-0f9e-4734-b857-341512dc0290": {"node_ids": ["f3e50ea8-4c49-4575-8c72-3a2521da661d"], "metadata": {"page_label": "50", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "3c52983a-6a98-4ef5-abc1-efd71cb6ec04": {"node_ids": ["254eae49-7748-4606-b0cc-bf1abc648c73"], "metadata": {"page_label": "51", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "3f98685c-888c-4bc6-99e3-c56494c5b4f5": {"node_ids": ["49657dc5-9a37-4393-8fcf-72fa9300e991"], "metadata": {"page_label": "52", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "8447d23d-fc15-49a7-9c56-d6f6ec10d158": {"node_ids": ["4b61695e-1cf7-4df0-ae20-58f1736ca80a"], "metadata": {"page_label": "53", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "81494ac2-ec6d-4057-8c61-9b7436e7ad26": {"node_ids": ["e1611f22-9e94-4a05-bb43-d4ad915e1708"], "metadata": {"page_label": "54", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "f906dfbb-3fc4-4f79-9960-662e8d7d420b": {"node_ids": ["3060fbc1-4e87-4961-ba77-3e61b3797c9e"], "metadata": {"page_label": "55", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "acaab71d-b86b-4efa-aa4f-89b766603b61": {"node_ids": ["dceb3e93-456b-4ffe-9c05-b66ee1b4541b"], "metadata": {"page_label": "56", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "d2d95939-b5a1-4964-ab45-6ef117f429d1": {"node_ids": ["06fc6a2b-3fbb-4364-8cc8-3800f45b3bb1"], "metadata": {"page_label": "57", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "a18084ec-e602-4289-a815-065d3a9463c8": {"node_ids": ["32f9d7ad-f5b7-41e4-9d4e-98c32f33b141"], "metadata": {"page_label": "58", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "c416c2a7-e4fb-43d6-a561-4bdfc099c90d": {"node_ids": ["1b48f23e-eaf2-4d85-b657-cd09cb974b41"], "metadata": {"page_label": "59", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "0d775c92-a4e1-48ca-98aa-a483248ffa96": {"node_ids": ["7fd2281c-8330-48f1-baa1-2c29423e5511"], "metadata": {"page_label": "60", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "ff9acfb5-0f0f-41ff-aa0a-707ad5be359c": {"node_ids": ["b5795ca1-c647-4be5-ab7a-ecd46fe3a38c"], "metadata": {"page_label": "61", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "55492c51-b24b-4279-8a54-9c01271b75f9": {"node_ids": ["c36fd0bf-5302-4023-9adc-5f0b61a53fa7"], "metadata": {"page_label": "62", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "bc915d77-e2df-440d-8d5f-ed10ad2a2d4d": {"node_ids": ["1e0d01fd-019f-4dc2-a995-902f1048ff4c"], "metadata": {"page_label": "63", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "f27ef110-576c-468b-a6ff-9f3ff0b79902": {"node_ids": ["7fa30983-9c67-4321-befd-ff90a04122b8"], "metadata": {"page_label": "64", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "b1e923b2-a931-410d-9a5c-ec139dde008f": {"node_ids": ["80ddb8ea-84aa-447a-ae87-d87b0ed33ec0"], "metadata": {"page_label": "65", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "1452b476-5ebc-4dba-918a-36fa434eb4f7": {"node_ids": ["0579e9f4-e3d2-445c-99f5-24635594b890"], "metadata": {"page_label": "66", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "45f89314-f1a8-4e77-b0a7-79e40b747f09": {"node_ids": ["5a8cf1fc-890c-49af-affe-c8916fbfb7e6"], "metadata": {"page_label": "67", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "1b41ca5b-78df-46e3-9b66-eec35a6e5696": {"node_ids": ["37c21c61-d9d3-45a3-8cb6-3d951eabc55d"], "metadata": {"page_label": "68", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "bcd1d024-9d52-4dae-82d9-b775c32a2ea1": {"node_ids": ["3ac16900-04f5-43d0-998b-c250211bb3ee"], "metadata": {"page_label": "69", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "b45a8597-f74b-4a7c-b3e7-56db07efbb41": {"node_ids": ["a68e83bd-6689-40d4-94c6-f575dde9551b"], "metadata": {"page_label": "70", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "5e423553-067d-492a-9b4d-6fab9610a934": {"node_ids": ["4d845ca7-5573-4db0-b141-edf6e5d16bbe"], "metadata": {"page_label": "71", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "77ed86e7-d1c0-4679-b11c-61a720a686cb": {"node_ids": ["29de0072-9fb1-422c-8397-0097cccd5c16"], "metadata": {"page_label": "72", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "b2807ba0-b90d-44c2-b9c7-3188e9e9ead7": {"node_ids": ["f52cbded-985e-4363-afd1-9073a592fcd0"], "metadata": {"page_label": "73", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "59ae4b91-05d4-49e7-a8de-e5c79b75f98a": {"node_ids": ["bb4de0c0-5ce3-400b-b5b7-26592e0b8d9a"], "metadata": {"page_label": "74", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "614df1ea-acb8-4f1f-9310-0a5351b8f3a6": {"node_ids": ["5f6f3059-ed46-48c7-ae2c-13f53c878683"], "metadata": {"page_label": "75", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "30dcaae6-7c4e-46da-afaf-d0bbadf89987": {"node_ids": ["8ab5cd7a-9be0-4bfa-bc2b-fa5852c23ea0"], "metadata": {"page_label": "76", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "5dd6a315-7ca6-4496-ad7c-2eff081c209e": {"node_ids": ["05466cc6-b42b-4ee2-a36a-09a121c43850"], "metadata": {"page_label": "77", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "769dee6e-4071-4a68-9c2b-ff0e351b281d": {"node_ids": ["76a71639-4d25-4409-941f-54820b4353ba"], "metadata": {"page_label": "78", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "cabbca11-7bd4-4788-8231-41180c3d2191": {"node_ids": ["14236b03-0bd5-498c-8ac5-7cec66f8f19c"], "metadata": {"page_label": "79", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "42bbd4ff-4afd-4a72-aab7-22644c0498ef": {"node_ids": ["11b0ab22-4c2f-4e2b-a463-9a1d2521e86b"], "metadata": {"page_label": "80", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "b420f8ab-1fa2-47dc-8b7a-3f40fc7ded0a": {"node_ids": ["3d469b8f-a26b-41b0-9e26-6b7e4b6a89e6"], "metadata": {"page_label": "81", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "d851f174-6c48-4c92-8d21-01fd0d60047f": {"node_ids": ["ccd51dc1-643b-4b1b-a589-88eb22bd18db"], "metadata": {"page_label": "82", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "4e83c8be-7093-44af-a08f-9ccddcf8e5b4": {"node_ids": ["bddff48e-9caf-4123-bb01-690c5c8b01ed"], "metadata": {"page_label": "83", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "97cda3fb-7eef-4871-9303-223ec37599df": {"node_ids": ["6064286d-8683-4b91-b499-10d4ee9271c4"], "metadata": {"page_label": "84", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "ef39870b-c6cf-48c3-9dbe-d52fabb4f8dc": {"node_ids": ["e689b712-e50e-4f59-9b21-324e5ee38131"], "metadata": {"page_label": "85", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "13c72ea9-01bd-48f0-bae0-70e041f91303": {"node_ids": ["65e9d45a-e732-4a1f-bdef-2a99e08616df"], "metadata": {"page_label": "86", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "4e1c2e1a-91be-46a1-84c5-f4e2539b1bb5": {"node_ids": ["2addd8ea-b729-4902-9a4f-3747e8fbde68"], "metadata": {"page_label": "87", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "365ae8ec-3416-4258-a94c-262e762ac13d": {"node_ids": ["015fa942-fcba-4baf-9034-abde3cd25834"], "metadata": {"page_label": "88", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "67ce87c6-2b05-4724-baa2-e4465c0b1b3e": {"node_ids": ["3c44a254-aed9-43af-a7b9-8569e1f91a8b"], "metadata": {"page_label": "89", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "97ebb57b-dfd3-475f-9533-61f427719cfa": {"node_ids": ["1b23afaf-d168-4332-a082-e8dddce986f4"], "metadata": {"page_label": "90", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "d5e68fae-a5ea-4609-a1c9-eca90254087f": {"node_ids": ["7639b672-c2dc-4a9f-9bb6-2bf69e12099a"], "metadata": {"page_label": "91", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "86c1e73a-1941-45c8-ad8b-19fc1e7ba195": {"node_ids": ["6024d058-6e00-4a6d-9be0-36929fd94629"], "metadata": {"page_label": "92", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "c85bb471-d654-49c6-8deb-11d545d8fa1f": {"node_ids": ["84881dad-2ed6-4848-9f96-ddf3798175ce"], "metadata": {"page_label": "93", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "0ce6612b-71b1-4463-83b2-590fa497db15": {"node_ids": ["571ef21b-821c-4eeb-8cb0-adef7773b524"], "metadata": {"page_label": "94", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "8ab767c0-571e-4650-aee7-c658dc20b3f7": {"node_ids": ["cee5b5c9-8bc6-40b8-9202-211f128b9280"], "metadata": {"page_label": "95", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "a280a15a-39a7-4362-af67-477e2cdf9175": {"node_ids": ["5f4f41e5-4b82-48b1-b78c-3e9fb16c6e67"], "metadata": {"page_label": "96", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "31ad2038-8762-47b8-9cd4-f0a45f1f3ad7": {"node_ids": ["a688b663-9e5c-4989-b1db-09b70d6e5893"], "metadata": {"page_label": "97", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "040cb028-0e9f-43e7-b69a-952857ba6040": {"node_ids": ["daedae2f-2fd7-4b3d-a6f6-1e4cc4b7c59c"], "metadata": {"page_label": "98", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "8ab952f3-e453-4e7f-a756-8e734b4d1d75": {"node_ids": ["ce583192-0484-4e10-868d-c445c8e2517b"], "metadata": {"page_label": "99", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "003de270-69d6-448c-baa2-6b7e46152a54": {"node_ids": ["93547004-12f5-442e-a393-2a8b02f99aaf"], "metadata": {"page_label": "100", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "74f297da-b32f-4645-8dfa-1dfd40bc2a55": {"node_ids": ["267790ea-e84b-4f31-acfb-c44038774e76"], "metadata": {"page_label": "101", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "4e88aa98-6890-4e4e-b229-6a1c70407791": {"node_ids": ["ac4471b6-e7e4-4406-8dff-68e58a4e498e"], "metadata": {"page_label": "102", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "250a65c9-cec0-4c1d-aefb-0779d5e80d4a": {"node_ids": ["e3a5f1fd-539c-4d4e-9731-cece5e1cd013"], "metadata": {"page_label": "103", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "e60f9c84-8c93-4125-a5c6-114a305d7829": {"node_ids": ["a0418dd5-d840-41c2-9687-9f8c6cb020a1"], "metadata": {"page_label": "104", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "e3e2c7cb-9915-4558-ab7c-bf8620bb8f6b": {"node_ids": ["b7b752a4-fc6d-423c-94f1-4066d5c3cbc4"], "metadata": {"page_label": "105", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "c8dada01-b453-4cc4-921c-ecacaef9e95d": {"node_ids": ["6a8537b7-e4a8-4b8e-98b9-93b5c3ff4b86"], "metadata": {"page_label": "106", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "ae626a43-3f20-47e7-8aae-eca0c0137fe8": {"node_ids": ["20389d11-730b-4f3c-90d1-2f545a706d9b"], "metadata": {"page_label": "107", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "6be54b0e-7ab1-4bca-88a2-09782cf597eb": {"node_ids": ["132c2436-b060-45cf-8656-a983307dac69"], "metadata": {"page_label": "108", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "0068025e-e003-4fdb-b264-48aa9b734964": {"node_ids": ["5a571a73-2a7c-4fc2-9b64-cb2f50d1c5d5"], "metadata": {"page_label": "109", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "2a0cfdca-432d-4333-a58f-8cf82428ea06": {"node_ids": ["b7876197-3dd3-412d-b3e0-380f2c932421"], "metadata": {"page_label": "110", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "2d135c14-20c1-4295-8581-8af2fb6aff76": {"node_ids": ["177fc440-88eb-4b23-9d62-5ed5680a3143"], "metadata": {"page_label": "111", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "ae8ae80a-9450-4928-aed0-21944aeb8281": {"node_ids": ["8ddb5efc-2977-4657-812a-07cfb19fdb1c"], "metadata": {"page_label": "112", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "74511e02-97eb-47eb-9152-384ddcb7ffc4": {"node_ids": ["388556aa-902b-4591-adb1-70b268285804"], "metadata": {"page_label": "113", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "688377e7-6469-47cd-aba0-04ab8b5bd2af": {"node_ids": ["53e93d68-a103-4e36-a0c1-f00814c889fe"], "metadata": {"page_label": "114", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "b43ac4f8-4158-4435-a615-d10bbae5470a": {"node_ids": ["bdd2cfe3-b1fa-455f-a323-d11f777119b8"], "metadata": {"page_label": "115", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "d8a6a125-77e5-412c-ac7c-e950fa6c434c": {"node_ids": ["7af4a5f6-01f5-4b01-bfdb-864a1287f2f0"], "metadata": {"page_label": "116", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "bb9deeaf-0a2e-4308-a3d3-d1f84da6052a": {"node_ids": ["8d0cf81b-7a9f-446f-9ac0-0121ab8a563c"], "metadata": {"page_label": "117", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "1c5816d1-9597-4aa6-a87a-0b82f79a41a0": {"node_ids": ["e9e45f97-1bcc-444d-8fbb-65a2a8f6861f"], "metadata": {"page_label": "118", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "effe889b-e92f-4ab8-a494-425454bae218": {"node_ids": ["5262af32-5f69-4ca9-ab95-e8717ef7aed8"], "metadata": {"page_label": "119", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "41d4b649-8cd9-425e-af6d-2e8e7e1b377a": {"node_ids": ["3b64a0d7-82fb-403b-aef6-606b3325bb29"], "metadata": {"page_label": "120", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "8ce0863d-1182-44b3-ba89-e9b3dd6a74ec": {"node_ids": ["1ede5beb-ab92-44c9-8c56-e70706fa9294"], "metadata": {"page_label": "121", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "017644d3-1264-45bb-a8cc-3707e157c066": {"node_ids": ["144603d2-4edc-4ac6-ad47-0707cdd019e4"], "metadata": {"page_label": "122", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "723daee9-6cc2-4035-9dff-5e8faa2c90e9": {"node_ids": ["bd849597-5990-49aa-aaf9-b35a05453887"], "metadata": {"page_label": "123", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "46dc5006-f6e9-4704-9b13-1adab09c3c38": {"node_ids": ["b21800fb-f7d8-4b4d-baa6-b70c06a421c9"], "metadata": {"page_label": "124", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "28ac059f-52d7-4cf7-89eb-25746df08244": {"node_ids": ["862835bb-a602-4b05-aca2-24810d11f42e"], "metadata": {"page_label": "125", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "009dc4cc-327b-4f69-9585-8c643b6ba683": {"node_ids": ["3cd32ed3-23fd-46ba-a0be-c12d1c67ff6b"], "metadata": {"page_label": "126", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "84a2f68d-335c-4f75-9586-1b7083af95ca": {"node_ids": ["c576d618-a7f5-4368-8cb6-9c86e5f53142"], "metadata": {"page_label": "127", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "17afc4d4-c3f9-450b-ad51-80711b343dbb": {"node_ids": ["53dda6e0-5bd9-443f-aeab-6a79b84ce0e3"], "metadata": {"page_label": "128", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "002dca2b-5dc4-4eda-8fd7-26af71a50609": {"node_ids": ["1f095d4b-1562-4aec-aa14-7219a4854076"], "metadata": {"page_label": "129", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "1b0ba0db-1592-43fc-a473-689847f3a2d3": {"node_ids": ["4fca823b-3065-4e03-ab9c-8622fe850f41"], "metadata": {"page_label": "130", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "890da9df-2f34-4a22-9326-1866503e1392": {"node_ids": ["8fc9ca66-984a-424c-bd1b-97fd672fdefe"], "metadata": {"page_label": "131", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "e42f5fe4-746e-439b-8104-ee4e3a42cb75": {"node_ids": ["7d1d2b34-57f9-4f05-8f4b-f687a615ce9d"], "metadata": {"page_label": "132", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "790b9be9-7901-4e0a-b87a-0af38770246c": {"node_ids": ["8ae1e5b1-d0a8-4152-8e8d-c0f637051206"], "metadata": {"page_label": "133", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "9b46e1b9-b032-4b6a-a9ef-0a4a5c4e138b": {"node_ids": ["d9ed022f-53ee-414d-8906-8544035c8f2a"], "metadata": {"page_label": "134", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "c02fee95-bfcb-48ef-9542-9fe18b63c0bf": {"node_ids": ["aba59c68-d4eb-4938-b212-a5e7ef22ed01"], "metadata": {"page_label": "135", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "18587f29-849f-440c-81bf-40c704fa1caf": {"node_ids": ["3f6ab1e3-d371-4d13-b768-5b8d7128a4d6"], "metadata": {"page_label": "136", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "6561a6c6-c5c0-46d4-83c9-1ba9a82a44ef": {"node_ids": ["f033922e-74ef-4000-a409-6996d1d88f06"], "metadata": {"page_label": "137", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "f5138277-6011-4b6e-8fe9-783515b62aa6": {"node_ids": ["2a49708c-915f-401e-a755-f54fb14684a0"], "metadata": {"page_label": "138", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "b7d9d557-8d2a-4d96-82c9-afff8c5ade86": {"node_ids": ["5bb5fcb3-4010-45c0-9df1-7ed8d4e7e565"], "metadata": {"page_label": "139", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "690ca45c-89b9-4488-97bf-9944b4d1cd75": {"node_ids": ["d2cfe20f-de2d-4da0-ae51-d9ac9da68d30"], "metadata": {"page_label": "140", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "d40722f0-1784-4a73-ab8c-386fb074542d": {"node_ids": ["f6e0e3fc-4289-4ead-a47c-278ffc9a441a"], "metadata": {"page_label": "141", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "1e0b2afb-eeae-4dec-9eab-a1103bdc0daa": {"node_ids": ["da5dbcaf-8de6-4eff-81d2-530e7de7cbd9"], "metadata": {"page_label": "142", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "b9668b4c-d4c3-4905-9cbe-e3a39df81b69": {"node_ids": ["799976c1-893b-4004-acf9-f1b644ff1780"], "metadata": {"page_label": "143", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "92748345-b2ba-4f27-b35c-7146a901ae29": {"node_ids": ["a0aee713-5f5d-4ad7-a51c-de8221ffe1b1"], "metadata": {"page_label": "144", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "1c28a698-8d74-45b1-b3a7-7b1abc45f04c": {"node_ids": ["9b416f9c-869c-4d53-9324-fa4ae5f79d93"], "metadata": {"page_label": "145", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "1541be20-43de-4eaa-937a-ee2dfcc7efd2": {"node_ids": ["b6e25c4c-118f-491a-b493-9b125e507782"], "metadata": {"page_label": "146", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "74d54fe9-1693-4481-bf87-bc93e2581a93": {"node_ids": ["4f7ab63b-c073-4024-af37-a2bd3c172d9f"], "metadata": {"page_label": "147", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "acedcf90-dfa9-40b5-85a4-1524d2890e31": {"node_ids": ["450273d4-a380-4a7c-81fd-792056ba233b"], "metadata": {"page_label": "148", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "cc6ef1f9-c6d4-46f9-99d0-80611cc80901": {"node_ids": ["13ae1bdd-64b5-4e4e-9a84-5a800dc049b9"], "metadata": {"page_label": "149", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "25850888-dd2e-4bfa-80fb-ee32b3fed858": {"node_ids": ["e5ff1692-e5b6-4a62-89f3-ec1951e6a287"], "metadata": {"page_label": "150", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "c4a4b07e-b0f0-4ec8-80b0-611da8a80d1c": {"node_ids": ["1acefc48-2267-43f2-a44d-78072640a737"], "metadata": {"page_label": "151", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "08fe8ec4-19b2-4f24-ab07-9f15fb558d83": {"node_ids": ["17f7a772-7a2d-45b1-ab44-6d8be2322e4a"], "metadata": {"page_label": "152", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "941a2f2a-f0e7-4985-ad28-aa84aeaf75b0": {"node_ids": ["b837b583-a44a-4016-9e0d-c826e5fa4a2d"], "metadata": {"page_label": "153", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "c406a578-f1f4-4286-8e29-b411aca57da7": {"node_ids": ["fb67eeed-0054-4d4b-b6d5-f4f2bba2648f"], "metadata": {"page_label": "154", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "7afe79d1-666e-4f64-8417-3c9650697574": {"node_ids": ["6d33ab2a-4634-492e-be28-e94e274f158f"], "metadata": {"page_label": "155", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "fd8640cb-a525-4be0-9edf-43e24395e18a": {"node_ids": ["6ff9a6e7-b8dc-46ea-ba90-6013c3fffd43"], "metadata": {"page_label": "156", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "19de5903-10bd-40e4-b9a1-22147898db87": {"node_ids": ["a4d36f28-bd17-4ffd-badb-b4a8b6855ac1"], "metadata": {"page_label": "157", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "c791a107-09fd-48a1-a566-a69a58d48299": {"node_ids": ["fef1d7cc-b56a-4075-8069-a3675a4a4691"], "metadata": {"page_label": "158", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "c6465774-0c35-4cec-9535-5cabdce7d516": {"node_ids": ["6baa806f-e2bb-4503-8d11-d600a208fea5"], "metadata": {"page_label": "159", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "4438ba88-20b1-475c-867c-4749033d1cf6": {"node_ids": ["dada8533-19a6-4585-886b-f7b5be30ca2a"], "metadata": {"page_label": "160", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "76b040cf-eb4f-4490-b4e0-f0fa73bb1f05": {"node_ids": ["1ec86bf8-9986-4fe4-8889-3068ed1aa07c"], "metadata": {"page_label": "161", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "2740a957-5269-4e3c-b5c7-8801afe45c1f": {"node_ids": ["c9e9b02f-d500-4da6-af12-82b3cb43bd3d"], "metadata": {"page_label": "162", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "2e3b474e-8a8d-4a13-8018-cd83473e3068": {"node_ids": ["4ba30823-0fd8-49fd-ac85-f2ea7285b84f"], "metadata": {"page_label": "163", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "3912a02a-d236-4640-b5f0-d8e34dc8f2d5": {"node_ids": ["13f5eba0-1c19-40be-bfb0-ec855f2ca7d5"], "metadata": {"page_label": "164", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "5b6331ee-447e-4cfd-a8fc-4f5b1f5bb0db": {"node_ids": ["4eb85f59-4e13-41fc-8292-0972786ce823"], "metadata": {"page_label": "165", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "98b4a3bd-50a2-434f-8a86-4cd5f91be920": {"node_ids": ["8e936397-905a-48a7-9342-bdce3216696e"], "metadata": {"page_label": "166", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "bed722d8-dcd6-43b2-8ca2-a83b7e40f440": {"node_ids": ["ab48d009-0c7d-4643-af73-b34eb7711219"], "metadata": {"page_label": "167", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "16609c01-c14f-4621-8e29-4287e0689ec9": {"node_ids": ["f7c0d771-ee6a-4ab8-bb11-21d9738a3078"], "metadata": {"page_label": "168", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "3214b439-bafd-422d-9907-feda1b4b6eef": {"node_ids": ["212c6696-5166-4a07-8824-c91c349a8c27"], "metadata": {"page_label": "169", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "78b430b4-7272-415b-a2a2-73912578ee34": {"node_ids": ["3f641b18-ef26-4eb9-aafa-39a636923c41"], "metadata": {"page_label": "170", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "7b493f58-b0b2-495b-bce8-3e145def64b5": {"node_ids": ["bd7c2e12-38d9-41bd-89a6-ea87737c1191"], "metadata": {"page_label": "171", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "58dbf862-3565-4a7d-9930-909136367af6": {"node_ids": ["eacd58ec-ba79-4bb1-a19d-a0271eacb36b"], "metadata": {"page_label": "172", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "f710804d-cb57-4e2d-b4cf-8d27d2aeabf3": {"node_ids": ["6f3726db-6965-44ad-bc4a-cd537fbafe11"], "metadata": {"page_label": "173", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "fc99e1ef-a4f1-47ce-84f1-e2a55b9e12c8": {"node_ids": ["808dd9c4-31fc-43fe-b604-c005c7f32ee5"], "metadata": {"page_label": "174", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "37e13078-b7ed-4e2d-8c99-2518a91b0d78": {"node_ids": ["989bac15-172d-4fdd-91f6-057a74c31dbb"], "metadata": {"page_label": "175", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "d9580c8e-aa0a-4911-804c-d9eca0243c72": {"node_ids": ["70f3672b-6cb0-453f-a1fe-ac41f1e8e730"], "metadata": {"page_label": "176", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "dd1225bf-0b70-46fd-b107-75f893995a76": {"node_ids": ["43d91298-f0ff-455e-a071-5faf90484ff1"], "metadata": {"page_label": "177", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "92ad97ae-9fdc-479e-86b6-9cb8115d9f0d": {"node_ids": ["abf33d71-697a-45a9-9647-fd60a497cb42"], "metadata": {"page_label": "178", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "ec066805-4e13-438c-9bcb-1abf6c3bb143": {"node_ids": ["c6b672ce-c294-4b90-9a31-92791208d2e7"], "metadata": {"page_label": "179", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "97e96042-9d0b-413c-8cf1-1180199dc740": {"node_ids": ["d669b237-6cc6-489e-996c-e83047a55198"], "metadata": {"page_label": "180", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "30c0b150-1c03-48f8-985e-a9ff33fa29a8": {"node_ids": ["90df7dc3-8109-40f3-a595-5ec6ddb3ff94"], "metadata": {"page_label": "181", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "ed626845-4764-4090-9e6a-017499d06b10": {"node_ids": ["8e961401-f8cf-4f63-8736-b65bf3e955b6"], "metadata": {"page_label": "182", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "0943de38-d9c3-48cf-a5cf-ff0d66b26abe": {"node_ids": ["00deeea4-9094-4672-879a-3df8be6ad6c5"], "metadata": {"page_label": "183", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "64bc5256-8465-45a3-8403-07d13c4061a4": {"node_ids": ["5ea5f4e4-0ed5-415f-aa82-10e95e3df23a"], "metadata": {"page_label": "184", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "1c64bded-b9c6-4710-9782-79c552f091f5": {"node_ids": ["37f1706d-f151-49b8-b597-40417ecaa0da"], "metadata": {"page_label": "185", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "ff3ff054-6a2f-4195-82f1-8382df810dc8": {"node_ids": ["093291fd-6bc1-4acd-8110-c4f54a5c1057"], "metadata": {"page_label": "186", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "e745289b-b62d-45e7-9c59-d60bca128b61": {"node_ids": ["4c336bee-aa93-41ec-8d6f-0f5c43d943f7"], "metadata": {"page_label": "187", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "8aee8d6b-81e5-40ea-bb4d-e8c36a37af63": {"node_ids": ["32c2d795-aac8-42b6-bd04-336a1827f21c"], "metadata": {"page_label": "188", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "8339000a-9de5-4800-97a9-f14d097a8f98": {"node_ids": ["92f6e08a-f043-45e6-b417-edc6df733674"], "metadata": {"page_label": "189", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "5f57f609-5886-499b-9fe8-0ced7c3944d3": {"node_ids": ["b79ef5c6-0c66-4911-b425-0b25fc11f737"], "metadata": {"page_label": "190", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "c33e11ef-8da9-49b3-8f98-965587aab7df": {"node_ids": ["88065c62-16f9-416a-bcf6-ba9e40768cbc"], "metadata": {"page_label": "191", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "141329cb-b9d8-42e4-a0a8-ac844cbb395b": {"node_ids": ["0eb1db1a-a1e8-443c-ad3d-2ac783af26a8"], "metadata": {"page_label": "192", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "d85b0bec-594a-4e20-8bf4-6224c2766a3e": {"node_ids": ["b2a1ef57-3152-4336-889c-0dba5e4f6e85"], "metadata": {"page_label": "193", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "61ec9db0-0eaf-4462-89f5-838a802748e5": {"node_ids": ["a2e88f3d-f5e1-4ffa-96a0-51f14c61044b"], "metadata": {"page_label": "194", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "d995420a-1d2c-4c76-aa40-e82fbc1ccdd8": {"node_ids": ["ddb45ea1-60c9-4bd9-866a-739cd4d8f0a5"], "metadata": {"page_label": "195", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "4f7d55c9-73c1-45db-a4bc-b7798681a84b": {"node_ids": ["37226689-635d-4627-aa04-26206f4dab38"], "metadata": {"page_label": "196", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "6c487abf-3a49-4492-9846-1d71e723168c": {"node_ids": ["57b077d8-d531-4e36-bdc1-c09344ef6b28"], "metadata": {"page_label": "197", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "1c30fab1-1d82-4d2f-a9c7-e79da4b3781c": {"node_ids": ["79b47871-2683-46da-8f8b-54e02c18df01"], "metadata": {"page_label": "198", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "74266f51-215b-455f-bb8c-5e0cc9aab1bd": {"node_ids": ["1fd9f536-8556-4cf7-a119-d689740c95c1"], "metadata": {"page_label": "199", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "ddf6f29c-7c52-4056-9f15-e5517257bf7f": {"node_ids": ["2cfe2f4a-748c-4940-bc05-679328690c4b"], "metadata": {"page_label": "200", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "8a323310-010e-44d4-8069-511a61b6b033": {"node_ids": ["80c92746-c83b-4514-ad3e-8d938ac332cf"], "metadata": {"page_label": "201", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "5c5e39cd-5ad3-4427-9b1c-2ca86afa6c94": {"node_ids": ["3d6706de-e0ee-413c-911e-1eaf3ee50683"], "metadata": {"page_label": "202", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "f95450ef-580c-4067-97f4-a2850064e928": {"node_ids": ["38bd51fb-c307-4eb6-92e9-7a649f93afd2"], "metadata": {"page_label": "203", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "6c77cf7a-6d97-41c7-8c75-22ce66eee3df": {"node_ids": ["129059a7-c96c-4e67-8b33-768de33a8395"], "metadata": {"page_label": "204", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "1bb84dbb-5682-470a-a005-33658e4f2130": {"node_ids": ["892bc3c4-6965-4feb-bd37-988c1fcc6d21"], "metadata": {"page_label": "205", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "4d509d84-a327-4d09-ae43-ee5f38f3f4ed": {"node_ids": ["1e640f64-77a3-43e8-980f-73d58fccb425"], "metadata": {"page_label": "206", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "8be1d785-5b3f-470b-a382-1f254cf7fa8a": {"node_ids": ["1b7c1878-8830-40d6-931f-1a9eb42328ae"], "metadata": {"page_label": "207", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "4cb64ad2-e81e-4211-bb56-f08c48a57143": {"node_ids": ["1f01178e-8188-4d03-a5bc-293808050f76"], "metadata": {"page_label": "208", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "20195e71-1e7a-46e4-90c9-f240d4041656": {"node_ids": ["100d86cb-9423-4c7c-9bf5-7198ec31bb2b"], "metadata": {"page_label": "209", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "1177bfe4-3430-4637-97cb-2b957b9cb091": {"node_ids": ["9dd49142-1de6-4f62-9aae-4eefcb803c29"], "metadata": {"page_label": "210", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "b18c590e-95e1-4b38-8f5c-032599242ec6": {"node_ids": ["98807f67-27fd-4937-a4fb-c27fea75f204"], "metadata": {"page_label": "211", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "7222784a-317a-49e9-9f16-19cd60b2bcac": {"node_ids": ["eecbb970-1f2f-4d2c-b934-744b9041e0e5"], "metadata": {"page_label": "212", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "f2b073d5-2327-424e-86b3-f1e568fc00a4": {"node_ids": ["4f8381af-79bf-41eb-9d8d-76356459c5a6"], "metadata": {"page_label": "213", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "299cb7f9-c960-4da6-ad81-914ecc56d4d5": {"node_ids": ["51ff03e0-de60-4ab5-a8da-595486e20c9d"], "metadata": {"page_label": "214", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "ea51f9dc-eb85-48e2-840c-bdd4d02b5b13": {"node_ids": ["fd7dd8b2-714a-40f6-9707-41d940436ad6"], "metadata": {"page_label": "215", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "983d58d4-f85d-4b23-ad2d-72917ec8c1f9": {"node_ids": ["0e3033d2-c5c4-4a88-b829-4f3e1630e7a6"], "metadata": {"page_label": "216", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "15cf80fc-ac05-4e5c-8cdc-8c92fefa1b63": {"node_ids": ["26a29e68-b270-42e2-811f-444ef84cc7eb"], "metadata": {"page_label": "217", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "f94225fc-5d8a-457f-a27a-94a3b0b99c85": {"node_ids": ["708d7042-45e8-4a19-b38f-fe4bf01289ee"], "metadata": {"page_label": "218", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "b5d2ce3d-ef4c-4b57-a17b-5154010af782": {"node_ids": ["0d9165bd-af64-42b2-a785-042065993498"], "metadata": {"page_label": "219", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "fb71f892-6744-476c-b66b-05366fa0b4e0": {"node_ids": ["2b0c1bee-5b53-427a-bd43-a3a1b56e058a"], "metadata": {"page_label": "220", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "567a01c0-2e14-4ca9-8648-6995d6c0d780": {"node_ids": ["7074808d-8a8b-469e-bfb8-ab52ef7d80e8"], "metadata": {"page_label": "221", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "97fbf5fc-62cc-4c7a-8a1a-f4810ce0206d": {"node_ids": ["2f71a0a1-ae8d-4b8b-b164-341bf70908d4"], "metadata": {"page_label": "222", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "8b970236-bcbb-407f-9cbb-c789d878070b": {"node_ids": ["770f5be5-07b4-4c47-ac95-f40129feb453"], "metadata": {"page_label": "223", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "1589d060-28a6-4258-9600-7954de7b610c": {"node_ids": ["05d27cd7-c9c1-4a2f-b587-02a40923f143"], "metadata": {"page_label": "224", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "b87e47d0-6eac-43dd-931e-bc7c9d093c9e": {"node_ids": ["fc006b9c-9740-4c95-9919-d8afbfecf107"], "metadata": {"page_label": "225", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "2e1da7df-8474-4df4-aa8d-4450ab07add5": {"node_ids": ["9265280e-6404-437d-a8f4-1dad38ed0224"], "metadata": {"page_label": "226", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "bd732d7b-1080-43b9-bb48-5f7b313d80b1": {"node_ids": ["f070ccd3-58aa-4066-a1bc-d5679e129d3a"], "metadata": {"page_label": "227", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "a6d0fd9e-6acf-4eef-8e14-4fc8ea23eea7": {"node_ids": ["c22699ae-0f17-4c3d-abd9-92a665bd27d5"], "metadata": {"page_label": "228", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "528130f7-6c0b-4c99-930a-1ffeccb02d6a": {"node_ids": ["095e96e7-91c5-4737-a391-2fd5080fd894"], "metadata": {"page_label": "229", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "58468748-a088-4fe9-95c1-b5efb4d1f77e": {"node_ids": ["e74843d6-e310-4e39-b2a2-12c35c0947b0"], "metadata": {"page_label": "230", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "37984659-24ed-4eb8-93a1-0109b8ba3938": {"node_ids": ["23f99809-d2e4-411d-9e65-60bed9dd9c39"], "metadata": {"page_label": "231", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "365d3f67-4e38-4bfe-b76f-b39a52fd18cc": {"node_ids": ["c6493ac8-de7a-4885-a45a-9b52f593fb05"], "metadata": {"page_label": "232", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "ce55cd0a-ec3d-46d8-b946-8a6d0523b392": {"node_ids": ["c2e5a885-85b8-4add-aba5-2be88ab105c7"], "metadata": {"page_label": "233", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "b5537a91-0cfa-4a1a-8620-0ed97ab0b8e6": {"node_ids": ["5f63ca01-dd7a-460c-8fb2-245aaf9147f2"], "metadata": {"page_label": "234", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "a313570b-8c59-4b5a-aec6-c43d9339d521": {"node_ids": ["bd151ad5-0ffb-4732-a139-71cbd9e211d2"], "metadata": {"page_label": "235", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "c683e188-5d11-4e7b-ae98-d9030bea324a": {"node_ids": ["3ffd2c78-51a6-49ab-8990-81758a7e79f3"], "metadata": {"page_label": "236", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "4bd1b38d-e6fc-43fc-828b-aa957c90ed8f": {"node_ids": ["846728f7-be78-4fdb-861f-bd48ae7288dc"], "metadata": {"page_label": "237", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "e8858b7a-bdc4-47e0-9763-e923ab713dc5": {"node_ids": ["9b224c32-0261-4188-8e83-73ffe87dc31d"], "metadata": {"page_label": "238", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "5fe7f062-582b-4afc-ad82-a1be59b2c4cf": {"node_ids": ["13fe5629-634b-42d1-92eb-e036eabf8b07"], "metadata": {"page_label": "239", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "6c689466-5b87-4a04-85f6-903fb33cbd7f": {"node_ids": ["3b18f951-c536-4fe5-b000-c7cea8aebd3c"], "metadata": {"page_label": "240", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "afbe1199-2c08-4d64-bac7-a6d73abbb760": {"node_ids": ["d66b0cc2-30da-4000-bcdd-d729e6fe5b50"], "metadata": {"page_label": "241", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "be631d02-1b61-48ae-8415-b151ff592cfd": {"node_ids": ["0277977d-8f9d-4402-bc5a-3ebc3ca924db"], "metadata": {"page_label": "242", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "c812b12d-3e92-4c63-9341-f85e32ffb5c6": {"node_ids": ["ee2a87f6-72b3-4a97-8946-01a9ab88235b"], "metadata": {"page_label": "243", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "91782f20-756b-4344-953b-1b7d636a3fc4": {"node_ids": ["ba487b7a-6877-4e84-9d54-8b239aaf133d"], "metadata": {"page_label": "244", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "6b8f628c-4ca5-4e88-92e0-a9dc929dd94d": {"node_ids": ["250eac4b-19f8-4afa-88ad-f1dfcdead992"], "metadata": {"page_label": "245", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "39b8ae14-7e62-4fb5-bd30-05fa87b4b8b2": {"node_ids": ["ffaaffc8-87e8-44d1-a9c7-346e2d234ea9"], "metadata": {"page_label": "246", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "4035b2de-dfb7-42ef-b246-39d1e917c221": {"node_ids": ["450e4647-d842-44fb-80d0-1c8cf35a50ae"], "metadata": {"page_label": "247", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "37ab87d4-79d5-4f8e-9977-8843b13f3033": {"node_ids": ["5801212d-6f81-44a0-8c36-169c28c4b61a"], "metadata": {"page_label": "248", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "c5800b7c-34dd-4ea4-bbb1-7288e1e8288c": {"node_ids": ["f2f2bc27-1c39-4282-8a7c-9b5b956f5451"], "metadata": {"page_label": "249", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "5b6b4be5-1d4a-44cb-94c2-16aeaeca4a39": {"node_ids": ["52ba9edf-10eb-49d7-9f1b-ecc2c22dc26c"], "metadata": {"page_label": "250", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "b028a9ee-877f-47df-b366-303c167478c7": {"node_ids": ["95f59716-7855-44ac-936a-27b925e9c270"], "metadata": {"page_label": "251", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "af0387a8-178c-4e13-bb4e-7ca1a107c4a6": {"node_ids": ["e71487d6-0795-4797-ba61-3e9813e83173"], "metadata": {"page_label": "252", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "4275b810-7d87-4a4f-8355-1332465e1133": {"node_ids": ["b7efa100-207d-4fa4-96a5-f1e6c6c52986"], "metadata": {"page_label": "253", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "c363337f-f492-41ed-acdf-df77440b3772": {"node_ids": ["a29468ce-c643-4e96-a6ec-7e1b8fa618ba"], "metadata": {"page_label": "254", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "b4600a08-325e-424d-8707-101e6dff251c": {"node_ids": ["948b987b-3d71-4b30-804c-e689af9744a2"], "metadata": {"page_label": "255", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "4b291cc6-1c9d-406d-92ea-48c488ba1a1d": {"node_ids": ["d4d538e2-edaa-4276-8e57-488d142b6fa2"], "metadata": {"page_label": "256", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "8a14ac0b-ccd1-42c2-a39a-0af461bbff27": {"node_ids": ["d6d03f6b-ac42-414a-9f93-73850f1807bb"], "metadata": {"page_label": "257", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "6d52553b-bbcb-468c-bb2d-105943b58346": {"node_ids": ["684649f1-f1f3-4ce4-b484-92ede919b206"], "metadata": {"page_label": "258", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "e0ecfe1c-768f-45ed-a5a7-1b656ae8c938": {"node_ids": ["29587923-d2b4-4b8c-ba66-d7a2d1349fd8"], "metadata": {"page_label": "259", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "0d1640d9-f170-430b-911d-a029b32ef988": {"node_ids": ["d0e610e3-3f2e-4329-a135-6d504e106993"], "metadata": {"page_label": "260", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "2205f9be-d3e5-45fa-85be-d6dddd0be8a3": {"node_ids": ["59799741-7f51-4f77-9a07-eb26b7aaad93"], "metadata": {"page_label": "261", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "fb8f9468-7cdf-4fe4-8c31-88a856eac71e": {"node_ids": ["0f38306d-b966-43c9-80bb-5a013b2326c9"], "metadata": {"page_label": "262", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "9bb7b024-485f-4f3c-b531-95ea75c81dc8": {"node_ids": ["a4457f8e-1b7e-42b5-882a-b9137da39417"], "metadata": {"page_label": "263", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "0c95cbc2-14f1-44f9-8775-f2446121473d": {"node_ids": ["16bb5c6b-e74d-4fd6-922e-cf158944d990"], "metadata": {"page_label": "264", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "4e5b6f02-cc0a-43d9-b8cd-9e98c06e4ec1": {"node_ids": ["daf264a2-e62f-45ab-b3ab-e519a994c823"], "metadata": {"page_label": "265", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "61b17e81-1c2f-45ab-ad38-403a4d89862b": {"node_ids": ["7b7f4e99-ef34-49ef-8b25-cd8c63421550"], "metadata": {"page_label": "266", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "ac648878-e6f1-48ec-9807-115c802405f7": {"node_ids": ["54c264db-7628-40a6-bd16-77144685579b"], "metadata": {"page_label": "267", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "82aa7df4-4666-46b5-9ba4-d0d6beb3dee6": {"node_ids": ["95dac4eb-26c2-45cf-9425-9c5f3463a8c3"], "metadata": {"page_label": "268", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "97d5dc5d-e376-4753-8958-3334e88cd9b2": {"node_ids": ["441160b0-59ed-4324-aa32-ec8907752b1b"], "metadata": {"page_label": "269", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "7bd14eba-fc09-4bc0-89c5-2c5c2dd01e1a": {"node_ids": ["a143c1ac-a7a4-4579-ae74-eb38dd2782d2"], "metadata": {"page_label": "270", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "0c0aaf4a-1170-46f5-8932-d9c2af794a90": {"node_ids": ["c0f38dca-aada-463e-a115-10e380657dcd"], "metadata": {"page_label": "271", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "c9a19900-6f4c-444a-8796-5d5bfacd6b5b": {"node_ids": ["b3ee40de-6ff6-4c97-9d20-1bb2fd78d12f"], "metadata": {"page_label": "272", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "c5df4b95-e582-4b35-9771-6bdbea5478d7": {"node_ids": ["1e05c857-8084-44fa-a2bf-6f6013512da1"], "metadata": {"page_label": "273", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "ab5d00d9-4931-41f8-968d-6ac14ffec0ed": {"node_ids": ["25b33a9a-922e-4f48-a8d4-acfbdcc44cfb"], "metadata": {"page_label": "274", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "f87b5543-43c6-4971-9eb3-31906a5f6a3f": {"node_ids": ["279fb32f-3c93-4695-a44b-9e7fb5e9d9c4"], "metadata": {"page_label": "275", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "fed9a6c7-2183-4a4a-a380-44b17676cf27": {"node_ids": ["3129273e-8317-42ee-822e-c6006bee143b"], "metadata": {"page_label": "276", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "ac00b750-e85f-4271-abef-c92778783b27": {"node_ids": ["8319fefb-ad3e-4dc6-8e19-e48fbac048f2"], "metadata": {"page_label": "277", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "40dbb883-ecc4-4f7f-ac3a-694401f56fcb": {"node_ids": ["176a00cc-9caa-4a1c-8bd8-bc4f2411a485"], "metadata": {"page_label": "278", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "fb5c9bb7-3512-4c6e-b1c9-cb6dc5efca8d": {"node_ids": ["171cd2ce-b2a5-409c-bff9-bc7c51011c05"], "metadata": {"page_label": "279", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "f3a22cd8-3fbf-4dc4-9044-72336948ecce": {"node_ids": ["675c36cd-9a80-45ca-99b6-7d2d45c8ca2d"], "metadata": {"page_label": "280", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "f1627189-3761-48b5-9304-b33ea5a90b52": {"node_ids": ["1367af17-1289-418f-8448-54066615ccd9"], "metadata": {"page_label": "281", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "82165b48-2776-4a83-be7b-e8ceec57907e": {"node_ids": ["874a1eca-2d6a-4061-9c47-1948275bfdf6"], "metadata": {"page_label": "282", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "42cfe601-5767-410a-adbc-209738a671c5": {"node_ids": ["f828bec8-92a4-455f-9e6b-df0c0023156e"], "metadata": {"page_label": "283", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "b54f5097-cbf7-4e37-92f9-12fe10928cfa": {"node_ids": ["4392a67f-6a2e-4ece-9654-4a4777de8699"], "metadata": {"page_label": "284", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "ed604304-3672-4c9f-b030-4ef69890d838": {"node_ids": ["5ed5da24-f655-4372-aa18-0327d6d9a8ee"], "metadata": {"page_label": "285", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "01117d63-472d-4d1d-befb-7b50753e6d14": {"node_ids": ["6ff136c8-a4a1-404a-9943-7732f50a9b5d"], "metadata": {"page_label": "286", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "013af61f-0a2f-4a18-98ef-6982616901c6": {"node_ids": ["79db8bda-27f4-4a9a-aba4-95ba99bd7f2d"], "metadata": {"page_label": "287", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "5310c3ff-082b-4199-a04a-13f542506f08": {"node_ids": ["31d569e2-2a8c-4449-9c2c-401c094901f6"], "metadata": {"page_label": "288", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "8ed12660-8fb3-4ebb-bbc1-16464737d75b": {"node_ids": ["558427bd-522b-4862-9e34-5af7433feceb"], "metadata": {"page_label": "289", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "af8f683f-ff06-4f40-b4c4-766dd47576de": {"node_ids": ["1748f0ae-adc8-44e5-b8d2-1d66e082e4a1"], "metadata": {"page_label": "290", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "3e5b8c62-32ec-45ab-a485-1855e9d663cc": {"node_ids": ["d8fa2dd2-7e45-4adf-a33f-6e2116642ad9"], "metadata": {"page_label": "291", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "b932d09b-ace6-446b-9a2c-310f54c38d73": {"node_ids": ["22febf1d-08fc-4aad-8506-4f46e17328b1"], "metadata": {"page_label": "292", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "f622ab2b-aef0-488a-b325-534dfbf2d40e": {"node_ids": ["644c77ec-6250-48c7-8872-dff2e4dd8692"], "metadata": {"page_label": "293", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "c19a2081-d274-41e5-865a-2788c35a50c1": {"node_ids": ["da9bb393-983c-406c-97b0-6de958ebc17b"], "metadata": {"page_label": "294", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "4cdb16f1-42f7-4dd3-9f29-c7150a5fe163": {"node_ids": ["f0047d2f-1670-4805-9119-d2265c0d9828"], "metadata": {"page_label": "295", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "06b0417f-ac73-4f91-80f6-2980f6551940": {"node_ids": ["1f5590c4-ec79-4a3d-9120-7f7a41f6d3e5"], "metadata": {"page_label": "296", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "0fa73be9-e013-4cfb-8b21-71ab9a522688": {"node_ids": ["5cd3f0f6-fd29-474d-bb9c-550167fac11f"], "metadata": {"page_label": "297", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "514eb90c-1cbe-4892-81d1-68464aa8022a": {"node_ids": ["cf498972-e055-4742-9c03-31f25d56e4a7"], "metadata": {"page_label": "298", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "a75ee081-c84c-46d1-8f9c-6fb9275c313b": {"node_ids": ["d7a059f1-e149-41ad-a834-40e8c4b7ca19"], "metadata": {"page_label": "299", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "07d3b489-6490-4da8-b93a-8591cf3ef247": {"node_ids": ["8853cbbc-65c8-4c0e-a552-2aaa57fcf7fe"], "metadata": {"page_label": "300", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "918dd188-3baa-4b8c-9a6b-79fc1e6b307a": {"node_ids": ["e2f5e2c0-454f-431a-91c6-2ecacfb20c17"], "metadata": {"page_label": "301", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "91bf0ea9-93ea-4bcf-9afe-d2542d1fc4d7": {"node_ids": ["377c6b41-04ec-41fb-ad24-ed0801ca3916"], "metadata": {"page_label": "302", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "1598ded2-4a38-49a2-a027-6b7c724dd361": {"node_ids": ["dd355122-7365-4853-b418-a469c5a34456"], "metadata": {"page_label": "303", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "e05d6d91-6042-4560-b39f-60c85b3f6f8f": {"node_ids": ["23b7d867-013f-4db8-9799-801b4004a0aa"], "metadata": {"page_label": "304", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "d43c9a0c-6dea-4d6c-ab58-41537e895770": {"node_ids": ["0f8ac128-806e-48e2-84af-b9c27981ada7"], "metadata": {"page_label": "305", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "e070b5ba-5842-4e1f-888d-adebce7e46ea": {"node_ids": ["6d5f1b0b-159f-44ec-9c58-2e12edc4895b"], "metadata": {"page_label": "306", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "6bab5e62-71f0-4f1d-a0c4-6ea43aa7a949": {"node_ids": ["c98d1768-63c2-40e5-81bf-bc03a8063109"], "metadata": {"page_label": "307", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "7b867c3e-4c02-4595-b142-bc52a060fa2f": {"node_ids": ["472b41d6-f007-4ac5-9405-68134254eb19"], "metadata": {"page_label": "308", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "02a5fecb-4917-482e-aac8-2f164068b592": {"node_ids": ["1f72c991-109e-4a33-9699-13c28c83e122"], "metadata": {"page_label": "309", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "fd1c4dc5-0ad7-47e8-a4b2-f2d46794f0d2": {"node_ids": ["469607df-af30-40a6-85fd-3332491e200b"], "metadata": {"page_label": "310", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "4e616ed4-0694-4627-a179-7d09b338ec3f": {"node_ids": ["3ed2de99-970d-4b72-adbc-1e0a79599a6b"], "metadata": {"page_label": "311", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "fe0285b1-558c-4055-8026-68c1d0603ff1": {"node_ids": ["3475e6f5-f286-479d-8883-fba0b9c06866"], "metadata": {"page_label": "312", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "b75e8a37-dbbb-4281-a233-8724b1f61791": {"node_ids": ["9c9cfc79-e2f0-4b10-923c-12d4fb41b896"], "metadata": {"page_label": "313", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "5fa421c8-c6d5-4344-8edb-d0beb5a07ff1": {"node_ids": ["28c48ae1-fa9e-4885-a906-d939aa972f2a"], "metadata": {"page_label": "314", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "57cb6faf-7526-4f6c-84a6-cf0d26d8b304": {"node_ids": ["dfd16fc0-c97b-4b64-8fc3-a53b723e2e1d"], "metadata": {"page_label": "315", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "7ac5b2e6-848c-436f-8f4e-4e6c7111f0fb": {"node_ids": ["57ea6356-774b-45c5-8997-39626d734828"], "metadata": {"page_label": "316", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "545407a9-a8ee-4f42-baed-db62cd6406c0": {"node_ids": ["8dfb559a-c7fd-4402-ac5c-7153ca21ebf7"], "metadata": {"page_label": "317", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "7a23b51c-4c00-4a6f-9e18-7acbc5a17aef": {"node_ids": ["24e0b27e-b861-42f7-a043-d07221a9c371"], "metadata": {"page_label": "318", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "2a46676e-03eb-4e0a-8478-fd9a79f16f84": {"node_ids": ["ee9e141f-5aa8-4ff4-afc0-d2103cd486fc"], "metadata": {"page_label": "319", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "62e45bc0-2093-4f6c-97fe-2e80ac03579c": {"node_ids": ["b05ed1d3-a11c-4187-9c68-4f828de0020a"], "metadata": {"page_label": "320", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "bdce32a2-766b-45f1-9cd8-417aefbaa106": {"node_ids": ["c6d4e588-eb4d-4985-882a-c698c70baa89"], "metadata": {"page_label": "321", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "5850738f-70fb-4cb6-806a-e520a42e91a4": {"node_ids": ["5b45845e-0527-456f-bc9c-e457ca84ba5f"], "metadata": {"page_label": "322", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "329909d7-18a8-4a55-9067-1f234411dee2": {"node_ids": ["5b96f6e2-87c4-4f5b-a61d-dbe474baf79d"], "metadata": {"page_label": "323", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "11f0b836-c8a9-4817-a40f-c14145370e6e": {"node_ids": ["6c617d4c-fc8c-4040-9917-62349c0a8339"], "metadata": {"page_label": "324", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "25e22f8d-1312-45cf-814e-fc06cd2fb982": {"node_ids": ["6d530190-db2d-439f-be0f-e5a9bbb6ef8f"], "metadata": {"page_label": "325", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "6b3fb3ac-e5ec-4a71-a452-e4231825f348": {"node_ids": ["8f7be33e-6211-429b-a1b1-7c636c62827c"], "metadata": {"page_label": "326", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "5d854e55-f328-4526-b771-6cd53939327a": {"node_ids": ["00b075d3-c19b-4814-b46a-ca5297b5a448"], "metadata": {"page_label": "327", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "c79e5777-7648-4a6f-b184-baf3dd16dae7": {"node_ids": ["9faad3e8-6034-4c93-aa05-c4bd6603f3cb"], "metadata": {"page_label": "328", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "b8b1022c-6c6c-45c4-8259-bf4118122e14": {"node_ids": ["a8af6e7d-eb08-4d41-bff5-06b70c702284"], "metadata": {"page_label": "329", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "416b4138-a42c-4040-96a9-51f2e2879dca": {"node_ids": ["bcee9886-8073-4903-832e-20a2f590e3b8"], "metadata": {"page_label": "330", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "b3ce64d9-37dd-4ad8-a31b-3fbc935f5705": {"node_ids": ["f45a8841-bb1c-4a57-9fbc-3fa3cd9bb6e1"], "metadata": {"page_label": "331", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "4dc5a2be-2636-4920-a92a-a8d3cccb44e5": {"node_ids": ["0a691d3a-b760-40e5-a929-5f65bcb3f0a9"], "metadata": {"page_label": "332", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "d36a0a4d-5441-46da-8604-223bea69548d": {"node_ids": ["81ab8765-d87c-4cbb-8973-ae7a5a0014c2"], "metadata": {"page_label": "333", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "b66d1eae-a309-4856-b8ba-7089e7bcf29d": {"node_ids": ["8012441c-4b6d-4fb0-9c6c-92c659695c8d"], "metadata": {"page_label": "334", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "6745a5a7-0c40-413f-afe3-b36c0797dae7": {"node_ids": ["e7decadf-1315-4b79-a20e-528b5e3b8160"], "metadata": {"page_label": "335", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "75cf121a-b58f-47c5-8978-0155ebe436e5": {"node_ids": ["406a1db5-83c9-464a-a878-345d6556056f"], "metadata": {"page_label": "336", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "67110e7d-6565-4200-805a-df95b4678c5a": {"node_ids": ["fe5d1c15-add9-48f0-8d0b-97b4fd8b4c7a"], "metadata": {"page_label": "337", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "abfcf9b2-4c8f-4095-b8e2-2258c90d947f": {"node_ids": ["7fd15a2f-82a1-4c1c-9dc3-8ab2c3537c99"], "metadata": {"page_label": "338", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "12a25404-e56d-45e4-85d7-ee8724ce8943": {"node_ids": ["6bd84bd7-7ced-4cae-b2f5-9ac95b50a33e"], "metadata": {"page_label": "339", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "683e0d30-7a68-40e9-a1d4-5f0ca0230cc6": {"node_ids": ["2e1bdd5b-00fe-479b-a33f-ad2371a9be4f"], "metadata": {"page_label": "340", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "005133bf-6381-401f-abfb-cbccead6fdd4": {"node_ids": ["322a319a-84ce-41ce-8a01-6df685f601e7"], "metadata": {"page_label": "341", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "81056aa8-30d4-446e-a281-b5e7d2d7373f": {"node_ids": ["93e2b73d-8b0f-4c5f-8ef2-6ce049e42325"], "metadata": {"page_label": "342", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "fa8f088a-df23-498a-b721-18f4b314afe9": {"node_ids": ["9486f255-38da-4734-90ba-34209394dffb"], "metadata": {"page_label": "343", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "eec33efe-81f1-4ccd-8e9e-c2abd15b9bcb": {"node_ids": ["cb235361-ba69-409e-8d79-29a042146674"], "metadata": {"page_label": "344", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "d4f313f2-f6d8-414a-8570-b0d87ea84660": {"node_ids": ["021b96dc-f6d3-45d6-84f3-01554c196bd7"], "metadata": {"page_label": "345", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "daa736cc-f5d7-4dc1-bd38-be113d2b0cf3": {"node_ids": ["985d9edc-4c2a-4105-9a58-433de98bd3c5"], "metadata": {"page_label": "346", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "abadf77c-9f91-4456-8c2d-e07f73c0cd70": {"node_ids": ["994c67d6-052c-4463-b39c-b983ab06794e"], "metadata": {"page_label": "347", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "2c86d817-b5d0-4530-8ef0-105d266f8067": {"node_ids": ["e41b5dce-b645-456e-91d1-fa78c7754a59"], "metadata": {"page_label": "348", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "b479bc95-ece2-4c3b-a6a2-2dc65c02c4af": {"node_ids": ["509a2db2-3311-42dc-a583-29bfbd3ca711"], "metadata": {"page_label": "349", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "7ad29451-98c8-4226-a133-9c4570004c6a": {"node_ids": ["b11698ce-a3c8-43c3-8949-2eeb8fe99857"], "metadata": {"page_label": "350", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "a8ba6e53-d2e7-4ec6-b201-f82740ffd64b": {"node_ids": ["121aa955-d908-49b0-beb7-b844f39a8889"], "metadata": {"page_label": "351", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "e2138144-35e1-4f8a-a96b-911c96d8c45b": {"node_ids": ["35abc1c2-7d75-4d40-8b61-d0bed1127e2c"], "metadata": {"page_label": "352", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "1a0f3163-9c73-4fd7-a425-4a4e1fdee66c": {"node_ids": ["d49d06af-418b-4b00-814f-e76d6f5496dd"], "metadata": {"page_label": "353", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "214b6204-23d7-4e73-b275-87aa7aeba666": {"node_ids": ["c8e13854-d4b6-46cd-81d7-9effc6891b1e"], "metadata": {"page_label": "354", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "9af93a94-6b06-4d3f-92b0-7f083b70eba4": {"node_ids": ["96b4197b-6836-4dc6-ad10-dcfc4916fc62"], "metadata": {"page_label": "355", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "6428f0c8-9897-439a-946b-3395e511bff1": {"node_ids": ["d6a80241-51aa-4fd5-8b06-4dcd76ba92e8"], "metadata": {"page_label": "356", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "e9ccb713-9c28-4d0f-be76-423f1336899d": {"node_ids": ["cb89fe76-e678-4608-99bc-bcd4c90d3273"], "metadata": {"page_label": "357", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "d4e714b7-7159-4342-9a52-768799217228": {"node_ids": ["95d94a5c-e569-49de-896f-a12ab8bf49a5"], "metadata": {"page_label": "358", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "5719b3c3-4da1-47dd-9280-729d6cc7dfd1": {"node_ids": ["cce783bd-b444-4075-9995-5a60fa8a2f35"], "metadata": {"page_label": "359", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "05d6ff72-00a3-414e-9223-c65ccc9af63e": {"node_ids": ["64c1d2f0-b753-4a43-95c8-5282fa898a71"], "metadata": {"page_label": "360", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "030fefb5-c3bc-4a5d-8dfe-3bf991ed6e6e": {"node_ids": ["301fc0ea-c54d-4927-ad95-6e35e6a407d5"], "metadata": {"page_label": "361", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "44fce5fe-120c-461e-aeb2-ad10856241ab": {"node_ids": ["7e42b0ee-890e-4b13-aa80-ee973100cca2"], "metadata": {"page_label": "362", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "2b732d2f-c767-42d7-a3a9-35756260baa1": {"node_ids": ["29ad7a9c-e236-4707-ad16-f446455f31f9"], "metadata": {"page_label": "363", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "ee120089-22d3-4dbf-a9eb-ccf775aab4b3": {"node_ids": ["afbbad1b-2b65-44b6-bebc-8e118185d080"], "metadata": {"page_label": "364", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "b0f2cafb-422c-4582-bd63-9d4ed41150f5": {"node_ids": ["0fd7690c-f313-4333-a4bf-b98b68dfbad9"], "metadata": {"page_label": "365", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "103a701f-14c9-4c2d-91cf-e8a9355ed1d8": {"node_ids": ["9b52aea1-9677-4a5f-a2c2-b4c2f150ef97"], "metadata": {"page_label": "366", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "a181963f-518f-4a3b-ae2e-d20c0808acf1": {"node_ids": ["681767ae-7bbf-4bf5-985d-f9aa9884dda6"], "metadata": {"page_label": "367", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "c8a71c8e-e7f7-4fd8-879a-f2104b11f2cd": {"node_ids": ["a5a7d257-0bfd-41f9-8786-fdf8751bee82"], "metadata": {"page_label": "368", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "dc4c8148-7e73-47fa-97bf-86749ca3234f": {"node_ids": ["5134b113-4f47-4722-9356-af47ad709402"], "metadata": {"page_label": "369", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "873dc934-8f63-4168-a833-aad5b89804f2": {"node_ids": ["56b089b0-3c18-48b2-b64a-f884817618bc"], "metadata": {"page_label": "370", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "d8686e99-b64f-4c80-9ddd-3ec2a9a74adc": {"node_ids": ["57d4e818-a59f-47f2-a671-f7e6129386e4"], "metadata": {"page_label": "371", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "98c32d68-879c-43be-abc9-afc38d096054": {"node_ids": ["2f4fd679-df67-41cb-bc63-de3891d43084"], "metadata": {"page_label": "372", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "dc390077-cb7c-4cf2-8f9b-15aa027719f8": {"node_ids": ["a05c1a32-07fc-4bb9-9eb2-2b4149eaf92f"], "metadata": {"page_label": "373", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "3d37decc-2d4c-4627-b50f-234f99aa6bc2": {"node_ids": ["2cc60300-0467-4fc7-af20-a8cde500438a"], "metadata": {"page_label": "374", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "85f6ea14-fbdb-4b68-9147-9b21663b6679": {"node_ids": ["b9eafb9b-4fca-4e88-9442-7d95f4abf729"], "metadata": {"page_label": "375", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "3da80f2b-1a8f-4655-86c4-4635063958c1": {"node_ids": ["5a82fed2-5b0a-4323-bf81-17fd8a830ad2"], "metadata": {"page_label": "376", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "28a07de2-28c9-48b4-bbc1-0753d324eaae": {"node_ids": ["96b5fad8-fb83-4ff4-8cbd-3aa041c3e20c"], "metadata": {"page_label": "377", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "18e4bacd-621d-40f0-8793-a8fa107defb8": {"node_ids": ["e303dc33-daba-4743-a90b-cc5d70de8589"], "metadata": {"page_label": "378", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "a766100b-6ed6-4a4f-a28a-cabb15ea4820": {"node_ids": ["4dfbab53-1279-4754-ac68-22842d91dfea"], "metadata": {"page_label": "379", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "37725764-f5dc-464e-b305-686052c2b92a": {"node_ids": ["430663ae-fff1-4cdb-9d95-44a2ca025992"], "metadata": {"page_label": "380", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "230fc89a-906b-4925-abbc-29a1e46db99a": {"node_ids": ["9cab3cef-d1b4-4fbb-9104-f9c7a9020b22"], "metadata": {"page_label": "381", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "64f551ba-4e02-41c2-b816-0a4a009496c7": {"node_ids": ["6864e3f1-480e-4da7-9363-39a427f32ff0"], "metadata": {"page_label": "382", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "ab3ed68a-8da5-4b23-8357-753ac8ff28e6": {"node_ids": ["44dcec57-d2e9-477b-bd3e-b1b6dba924c1"], "metadata": {"page_label": "383", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "8b2725dc-2582-4a04-add5-60fcd229c176": {"node_ids": ["12a88d13-871c-44d8-9626-d4e6cbe21f07"], "metadata": {"page_label": "384", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "d366da98-37f9-4ca6-a7fa-329d3aaeaed5": {"node_ids": ["0b83b8f6-4ef7-4fb0-91bb-208be322e234"], "metadata": {"page_label": "385", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "4ba84d37-a2dc-4bd3-965f-9f25d4bb7179": {"node_ids": ["a6475ee6-c201-4b01-8076-b207ebc81dbd"], "metadata": {"page_label": "386", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "08f0afe3-6c02-4ac3-96f8-6b9faf8d6693": {"node_ids": ["18d28e50-c19d-43c7-ae5c-dfb4ab5853e4"], "metadata": {"page_label": "387", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "f2914984-93d6-4064-8052-405569b03054": {"node_ids": ["de1ee434-c695-4da5-9903-c380535a64dc"], "metadata": {"page_label": "388", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "f0f3ea6e-8cd6-4549-95b4-c927e37cab2f": {"node_ids": ["ddba2cc6-75ff-485a-991a-4b2608a6ee62"], "metadata": {"page_label": "389", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "60dfb41b-bcfa-418b-b594-7e9526eb6264": {"node_ids": ["265dc105-c5a8-40b5-96d4-612fe0879569"], "metadata": {"page_label": "390", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "d13ee4fe-7734-43f4-8184-57bbcf381caa": {"node_ids": ["c8daa24f-8184-456f-8e9f-942d193ac94f"], "metadata": {"page_label": "391", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "31aaf4d1-e581-4b33-81ee-c639c0b305af": {"node_ids": ["7707894d-c3fb-4cbd-8a35-9864c41c0e5f"], "metadata": {"page_label": "392", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "9d77d323-e287-47c2-9301-d17f17c1919e": {"node_ids": ["06c7ae4e-65c1-4f81-96b1-9f578d0f5955"], "metadata": {"page_label": "393", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "fe18d9c2-64b3-48e4-ad32-d186e47115b5": {"node_ids": ["ef5ae5d8-046a-4cd3-9e62-d25d161c1d31"], "metadata": {"page_label": "394", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "4f12f874-6da4-49cb-97ba-54897d0ff17f": {"node_ids": ["4dcffb83-de21-437f-b229-ded6fc244bad"], "metadata": {"page_label": "395", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "1296e427-d73e-402e-934d-cf7b8d5d995b": {"node_ids": ["974fb3f9-6a18-43e4-8eb6-b33cb15d9a7d"], "metadata": {"page_label": "396", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "2cc3c423-fdee-4eca-a002-c3755d61dc79": {"node_ids": ["c88079db-3e0d-4a2e-9b61-8a5c6437bdf8"], "metadata": {"page_label": "397", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "b1a62608-225e-4b81-abaf-156355da975e": {"node_ids": ["ff44a9fa-a8f6-47c1-b09f-07d2638ea6f2"], "metadata": {"page_label": "398", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "ce859bd8-19f0-46d1-b8d7-1cad32382376": {"node_ids": ["ec705e36-42ca-4767-8279-8170b3875a9a"], "metadata": {"page_label": "399", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "42fcd292-59de-4dcc-aa75-a73d0fefc07b": {"node_ids": ["b3dc535a-6e31-41f5-bec0-d747421a859c"], "metadata": {"page_label": "400", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "dd79b24e-a810-4def-8a40-d9c7f67fb015": {"node_ids": ["5b1048d4-aa4e-4bf8-8cc8-87501bda2585"], "metadata": {"page_label": "401", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "9a391930-3324-4cae-bbf8-653b6acf4753": {"node_ids": ["f53c02dc-2894-4d51-a8df-a0e1aa24be64"], "metadata": {"page_label": "402", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "bd54a1b5-0b94-47ac-8f86-afe20a648369": {"node_ids": ["8171804d-5ab3-4c5a-bed2-430fbf98b126"], "metadata": {"page_label": "403", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "96b5a6d7-c83d-4479-8fe1-bf88a7a63700": {"node_ids": ["2e260656-243f-41c3-b670-f04111a5d0d4"], "metadata": {"page_label": "404", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "385ccbfc-d3a8-4df8-a6ef-5c3c0ccb579f": {"node_ids": ["3b2d3d32-bc2f-4455-acc2-747593064965"], "metadata": {"page_label": "405", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "34698dc2-68d0-4af2-9f49-a0688b3dd878": {"node_ids": ["e72545b6-f514-44bb-966b-4f770a5be1a9"], "metadata": {"page_label": "406", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "f3ff06ce-ac51-448e-a7ba-5e199aa99211": {"node_ids": ["3b94ba16-6c47-4cce-baef-1df4232f69f2"], "metadata": {"page_label": "407", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "41c78e09-a863-4f35-9f27-b23f8c164a12": {"node_ids": ["c7a18bf3-168b-4df6-acca-4a7276851237"], "metadata": {"page_label": "408", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "e8587ffe-8f61-4299-9015-9abb78d599e1": {"node_ids": ["ed6e6547-185c-4685-9abc-2765f6ab3a83"], "metadata": {"page_label": "409", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "fa8379e5-5515-4c22-b2f7-9077fe880f19": {"node_ids": ["65175249-a0fb-4a8f-aa75-5ae0fffb71a3"], "metadata": {"page_label": "410", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "2c16255e-c467-436a-8722-1f3772903cbf": {"node_ids": ["303bf54d-4eef-49b4-87d6-490ab4fe9b29"], "metadata": {"page_label": "411", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "3e0ccae5-53b5-47f8-b89e-458c985215a9": {"node_ids": ["7b8b244e-d99a-46d5-b853-8f09b495380d"], "metadata": {"page_label": "412", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "61a18081-2ac9-4ed5-8015-f0616ca0e6c0": {"node_ids": ["06fb0e3e-ed03-43b2-9b68-10ad687cdcd7"], "metadata": {"page_label": "413", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "2975c766-d4de-46f4-8217-9ec77d5e5694": {"node_ids": ["5f8e2533-990e-4e66-b17b-37654cbdcab3"], "metadata": {"page_label": "414", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "16cfd766-7ce8-485b-bce6-4f485d6ba437": {"node_ids": ["dc3ce791-7aa6-4a24-b3c4-6b4a48de93a3"], "metadata": {"page_label": "415", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "c0f2746e-34f6-47ab-bf69-8c1160dd163e": {"node_ids": ["fca15130-a494-4d2e-a92f-a7972eece9f8"], "metadata": {"page_label": "416", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "91943f55-eb5a-4882-9722-e58d971b0b11": {"node_ids": ["3fc877f8-e3d2-4030-9ccb-4a6904c18040"], "metadata": {"page_label": "417", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "7ac8c78d-33c4-4ff3-aa6c-cc529a712235": {"node_ids": ["77084eb2-36c0-4697-abaa-876f02972645"], "metadata": {"page_label": "418", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "37ecb6f0-0c17-46aa-ad50-59d15b0d5925": {"node_ids": ["283a87a3-04d4-4f59-aae6-6561e0d68448"], "metadata": {"page_label": "419", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "5d6116c9-2cb9-4eb0-be46-952399e626f6": {"node_ids": ["e6e74a44-6074-44ff-8e3b-eb1107a1a44c"], "metadata": {"page_label": "420", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "0195fa02-c772-48e5-b18f-ffa0963851da": {"node_ids": ["36f0fed9-7f45-4d15-bdd1-3c82ac2e292d"], "metadata": {"page_label": "421", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "4aef79b3-9b44-4fbf-93a2-49760af82b5a": {"node_ids": ["a598cd99-fb4e-45f8-81fb-02b83e3821f2"], "metadata": {"page_label": "422", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "88c043bb-73e7-43f4-882b-254312d41907": {"node_ids": ["a4eb0b79-f647-4bea-a572-b28e53b85800"], "metadata": {"page_label": "423", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "ae002d4b-d816-41c4-aa97-a67e92dd8b95": {"node_ids": ["d68c6ae0-0009-4c6f-82ba-a24ec545d851"], "metadata": {"page_label": "424", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "0706fc31-0438-4e89-84ee-9540c254dc80": {"node_ids": ["5a8f5b51-340e-4be4-9dbe-adb1c2bfb080"], "metadata": {"page_label": "425", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "85df79ec-9b2d-4146-bc90-3d3243546817": {"node_ids": ["586d0211-ab58-4df0-bd21-1f1827425de6"], "metadata": {"page_label": "426", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "a0504346-f2c4-4851-b0b0-57bcff4f9ad8": {"node_ids": ["7c4a3087-0c51-4ebc-a2f8-b76a2ecf7a0a"], "metadata": {"page_label": "427", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "b820238f-9868-44db-8ab2-4f1d7764aefc": {"node_ids": ["7f0adfa2-06fb-48de-a85c-c8ac37d766e1"], "metadata": {"page_label": "428", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "e8646b04-b0c9-40ab-9626-fb4ac1efbce4": {"node_ids": ["9d15f657-3f6a-433b-a1a7-c0e2e8f95456"], "metadata": {"page_label": "429", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "2512d278-3030-4650-b0cb-8e1bcf542f31": {"node_ids": ["316aff54-5d5f-4891-986e-2c13042a2545"], "metadata": {"page_label": "430", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "c016a968-aad6-4f94-b4ce-d44df22c743a": {"node_ids": ["04bb4681-4d9e-4f9b-8d02-a438f5e321cb"], "metadata": {"page_label": "431", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "83d8c6bb-9ba7-462e-92ec-ead4307b3bcb": {"node_ids": ["92717293-a186-4a76-a8d0-c0c8069e2101"], "metadata": {"page_label": "432", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "37752dae-6e4d-4fba-98da-5d18c7b373c2": {"node_ids": ["39450837-1b63-4b44-93f5-2509da2a82be"], "metadata": {"page_label": "433", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "f0346e8c-7911-4514-9234-1663b08b6311": {"node_ids": ["9c36e9b5-688c-4973-9090-6d366ea6f28e"], "metadata": {"page_label": "434", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "36f27c83-4011-435e-b9ac-bd3938e5837d": {"node_ids": ["6c2f8b1e-cd1a-4504-acce-be9274172911"], "metadata": {"page_label": "435", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "494d1a79-cc74-4505-9308-0a491a6bb8b3": {"node_ids": ["35c08bde-6c62-419d-b431-31432392febb"], "metadata": {"page_label": "436", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "e9f5f7c3-c675-4a3f-b33b-031873ea9dee": {"node_ids": ["aeb7fda5-aa5e-43f7-842c-70402d0edd97"], "metadata": {"page_label": "437", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "24ae9e8e-a48a-4171-95c2-f354f172fba6": {"node_ids": ["320d9f0b-dc69-49c8-93a9-66921d93ccb4"], "metadata": {"page_label": "438", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "65fad22e-7ffc-460e-8c4d-bb24313d80c0": {"node_ids": ["f2ad224a-fd52-4853-b7ea-ed54f79081c6"], "metadata": {"page_label": "439", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "b2cfd4ee-de55-46fa-bf3c-fcdba6af709c": {"node_ids": ["e9124981-0080-4951-b4a7-57aad50de1ca"], "metadata": {"page_label": "440", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "b2e1bb14-49eb-45f0-8667-04e87fc85a62": {"node_ids": ["2ca6cfcd-6fa9-4410-bb72-df22d4954021"], "metadata": {"page_label": "441", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "98c1edf5-6380-4f4f-8134-cff7519a4b8f": {"node_ids": ["81abb764-db72-4a06-ad32-ca2eed3e88a0"], "metadata": {"page_label": "442", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "669c54db-c164-42d9-928b-2d495aa437fb": {"node_ids": ["d172ee5d-3250-4b91-a02e-4dd76d851b97"], "metadata": {"page_label": "443", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "3f261dca-43a5-491d-8476-7ff12d1c63b6": {"node_ids": ["8d5b5769-384c-43ac-b0db-472ecbf6c8bf"], "metadata": {"page_label": "444", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "10f96c88-23d0-448a-af3f-769d08394940": {"node_ids": ["980b0587-0eab-4684-8eac-1abf8ee5c29e"], "metadata": {"page_label": "445", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "57ab765a-d484-4a1d-9aa7-65e553c5eeb3": {"node_ids": ["565f6e2f-909d-4c64-8339-5ab5178bad59"], "metadata": {"page_label": "446", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "8c7ddde4-4805-4527-8c8c-5c653cccca41": {"node_ids": ["7a7f1f85-bcdf-4975-98d7-943ddff9cc2c"], "metadata": {"page_label": "447", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "05a6188a-f8fb-4187-9dfb-c234789f9564": {"node_ids": ["1e25043c-a641-4f97-8e26-9f06b5b8d9fa"], "metadata": {"page_label": "448", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "08def93a-0740-4196-8b6d-3559d0da9d33": {"node_ids": ["4c33b4c6-d531-45d3-9fb7-934f3c7c895d"], "metadata": {"page_label": "449", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "0ae15f81-7174-4fe1-8c56-a7b4c93b62ab": {"node_ids": ["0c04b51a-76c3-45d9-967d-604901b52ff3"], "metadata": {"page_label": "450", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "20572619-d5f2-4e6b-a1f0-0cc11ec723f8": {"node_ids": ["618d8338-5b0a-4602-ae38-6c166163bdd5"], "metadata": {"page_label": "451", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "54015bf9-880e-4cd0-93ac-1230cb28da6a": {"node_ids": ["8b939cfa-30d5-4075-96c5-5cee90f63843"], "metadata": {"page_label": "452", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "d8915d10-a1ee-48c0-8cca-ed30acdaf313": {"node_ids": ["f8968ce6-e56d-4ba0-a655-d32fdd66e1d7"], "metadata": {"page_label": "453", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "acddec9c-c458-4f07-a33e-8ea338cfbcd4": {"node_ids": ["f23fe794-3ea1-4871-a851-d6bcdccfc904"], "metadata": {"page_label": "454", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "283339f0-9ebc-4c0e-b353-0aeb23550b71": {"node_ids": ["b69c638a-09ae-470d-9335-a9275df7f663"], "metadata": {"page_label": "455", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "d6cd164c-85e3-4923-949f-2bd6db4a671f": {"node_ids": ["74b3b6b6-cd87-4a47-97b2-b1509ebc0263"], "metadata": {"page_label": "456", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "19a07a60-813a-43f2-956a-e466ed8858d7": {"node_ids": ["75e5c486-02a0-4be2-9301-0cf8fee69db9"], "metadata": {"page_label": "457", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "de795d82-d754-4141-ae66-3b88ff19a6ce": {"node_ids": ["41735202-8403-4ee3-82b1-fe5c0f64a350"], "metadata": {"page_label": "458", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "3e02a2b0-5f09-452a-be8d-7c639cdd6dd2": {"node_ids": ["179626e4-bbca-4d32-8957-5ff747102dbd"], "metadata": {"page_label": "459", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "348691c7-171a-4395-a2d2-a0b9bd3685b7": {"node_ids": ["561d0bf1-5365-4a3d-b7f1-ff1d41334ae6"], "metadata": {"page_label": "460", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "769ae878-bd45-4a88-b877-e317e42087b9": {"node_ids": ["0ef7f2d6-1601-45b3-aa28-6cd0502a60cd"], "metadata": {"page_label": "461", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "86b4e1c4-9966-42e4-9878-2f7b4e96c0d8": {"node_ids": ["1301251c-b25d-4b01-a1c3-a80484dfe91f"], "metadata": {"page_label": "462", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "1e37c8a0-d7b3-4149-bd29-98a638595110": {"node_ids": ["b6ba6624-0890-47c9-89d6-1bbbde8b0928"], "metadata": {"page_label": "463", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "b5e04a0c-f2a5-4941-9ce8-96ee6694e915": {"node_ids": ["928f207d-4d47-4c97-8a29-442612f2a7d9"], "metadata": {"page_label": "464", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "5697515d-9a6e-4ad4-b720-f916cf9ebc9b": {"node_ids": ["ecfdf064-b916-4d25-8173-465c8db9a456"], "metadata": {"page_label": "465", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "7e619656-f68b-4ef8-8f07-4bda71a79a41": {"node_ids": ["3f40aca0-e07f-42e9-9230-9131904cc449"], "metadata": {"page_label": "466", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "0f34d5fa-b64d-480d-a677-3f8b015c5922": {"node_ids": ["6751dfd9-296f-4efe-9fb3-f43f60545431"], "metadata": {"page_label": "467", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "44a84d35-c13e-4ce0-baea-29dbc0944781": {"node_ids": ["3a3c6404-e4cd-48db-a8ac-4518839b8b72"], "metadata": {"page_label": "468", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "f5ff7f61-8596-45b4-8c6c-16a7d43ce2c6": {"node_ids": ["30d7795b-a2a7-4f91-a408-0a4d3c4b009d"], "metadata": {"page_label": "469", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "87b955d7-4288-4bdd-9df6-e51ef925cc0e": {"node_ids": ["3dbb222c-e798-408d-bf54-8f0e23bfa877"], "metadata": {"page_label": "470", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "2b97f61c-ee76-4d52-ac08-40afff2196cb": {"node_ids": ["db950bfe-06fe-4b76-8ebb-0ff02f627f54"], "metadata": {"page_label": "471", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}}, "0f2ad73d-88e7-4e3b-982e-bacdf3d1682c": {"node_ids": ["e62e2700-b799-48e0-89cd-62af6a821370"], "metadata": {"page_label": "i", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "c68a7d62-dc0f-4bb0-afa9-ce7bf196307e": {"node_ids": ["6beb62c9-40e3-408d-ab55-c5e1381c7c58"], "metadata": {"page_label": "ii", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "e46784c0-9dee-4176-a042-1b1cc5afa453": {"node_ids": ["d14c9939-e92b-46fb-92fe-0ba99f996112"], "metadata": {"page_label": "iii", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "4337020f-f90f-48ee-8a3b-d0d401c4a4ed": {"node_ids": ["e257ad70-e07a-4316-8fe3-0277f98cb971"], "metadata": {"page_label": "iv", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "c0f570d6-9e2a-4493-abd2-f0e4b74e3fa6": {"node_ids": ["9c524edf-f84e-4fbf-b6d2-c31053ab9ef7"], "metadata": {"page_label": "v", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "6182aab8-b995-44c1-90f3-6c8200b05ddd": {"node_ids": ["4f0fd80d-101c-43b8-b918-0ad8ef951c6b"], "metadata": {"page_label": "vi", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "2f2e7977-7e08-4f03-95bf-cf2711350739": {"node_ids": ["e258e30d-8a5b-48c8-ac6b-2397cb3860e1"], "metadata": {"page_label": "vii", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "1e82188b-f13a-443e-8a08-633a0f859b0c": {"node_ids": ["7867fea4-e9ae-42b2-897c-b01216c83940"], "metadata": {"page_label": "viii", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "651c8b2e-96e3-440a-ba36-7e88c9108d16": {"node_ids": ["8d88ba50-92a7-4408-963c-48ec1ac0c710"], "metadata": {"page_label": "ix", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "7019f3a7-a74d-4a54-a068-853be1bdf7f5": {"node_ids": ["be33a365-87ba-4221-aaa2-c8db2f329d71"], "metadata": {"page_label": "x", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "a97ccd78-598f-47a7-8996-40ff72c0f6de": {"node_ids": ["7b0bd446-eea3-4f05-b715-302b14ebbb10"], "metadata": {"page_label": "xi", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "ddca0521-04fd-4472-aecf-03b85ecec732": {"node_ids": ["c0767d44-8e44-4dc1-9871-db339fc9901f"], "metadata": {"page_label": "xii", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "a543476b-70d5-4405-8102-903b60de501a": {"node_ids": ["5182c7cd-3f74-4378-b724-a625570fcc04"], "metadata": {"page_label": "xiii", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "d81ebb34-568a-4ca6-87e1-a42a4fa68357": {"node_ids": ["69cbc88c-384a-4d04-9292-de2dadb30449"], "metadata": {"page_label": "xiv", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "280d1ef5-8cb3-451e-abd6-731e72030028": {"node_ids": ["baa80214-e8e6-4411-afa9-5a0c169d51e1"], "metadata": {"page_label": "xv", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "eb1d7f3e-5f5b-4470-a8a7-7f48b68b40d6": {"node_ids": ["23d087e5-0b9a-49cd-9dd2-34661c13b414"], "metadata": {"page_label": "xvi", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "a2fc76b1-5158-4836-8450-b224488c562a": {"node_ids": ["0d3f3b07-a9a7-44a9-8100-74c4e3900561"], "metadata": {"page_label": "1", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "d635c7d7-fe12-4655-b170-64a3be476383": {"node_ids": ["da07b93c-1fa5-4e64-9f59-beee4754ff1f"], "metadata": {"page_label": "2", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "d5d20651-adfb-4f5b-aa51-6e2e6485e9ac": {"node_ids": ["c69f4da4-04b7-4694-b7ab-925cc7945539"], "metadata": {"page_label": "3", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "09aa6e63-55a4-4d58-80bb-5f31c5da631f": {"node_ids": ["5b25d82f-1b58-4db6-8e58-9e69983c7eee"], "metadata": {"page_label": "4", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "3bfccd3d-5e50-44b8-b589-68d25a9fcd7d": {"node_ids": ["02a86635-429e-4fc4-b356-e72553995a03"], "metadata": {"page_label": "5", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "7c1e82c4-7a78-41ad-a36a-6984f463e942": {"node_ids": ["eefec921-39ed-4192-9e5d-8c4777188967"], "metadata": {"page_label": "6", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "344d98cb-3668-4b68-9244-5302150bf8a4": {"node_ids": ["7775c25e-9f92-4ed9-8377-b2f4128b5f5e"], "metadata": {"page_label": "7", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "1c8875e3-2734-4c9d-9ccb-d98e56ab3b88": {"node_ids": ["b651f8d9-d310-4170-9c22-2d2c83cf2015"], "metadata": {"page_label": "8", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "956b72c4-b94d-4bdb-a1f1-d751a76f4f27": {"node_ids": ["0f73c86c-1162-4372-94fc-883d11c80fa4"], "metadata": {"page_label": "9", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "8e86599d-7b92-4369-afb2-697c04aace39": {"node_ids": ["74a77453-e29f-4796-b16b-64cd9c07b2b9"], "metadata": {"page_label": "10", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "1e65bf96-4437-4a91-8a4c-d595e883e6ec": {"node_ids": ["c9a419a7-dcd6-46a5-9eb7-51802c7488b6"], "metadata": {"page_label": "11", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "973a168c-2c19-492d-b3d6-780f1ed423fc": {"node_ids": ["771b2a88-48a5-4711-8fdd-6eded2b2ab3d"], "metadata": {"page_label": "12", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "4befef3e-ceed-43e4-8556-0b23b59c302d": {"node_ids": ["6935c2da-a36d-4996-8f6a-60d1f4f0a83e"], "metadata": {"page_label": "13", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "5bf16086-ab9d-45f9-98d0-b62d20c20e1d": {"node_ids": ["1bb718f6-ed35-4b60-9414-7f5a64bdf230"], "metadata": {"page_label": "14", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "152dc485-c35f-4b67-8054-9485b33fbc31": {"node_ids": ["7f033283-2b30-4d83-ab86-659b0b67f114"], "metadata": {"page_label": "15", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "bc5dcddf-91a6-47b6-ace9-3bde0377f39a": {"node_ids": ["ec107ed3-2444-41b8-bd4d-cb3b552f92ba"], "metadata": {"page_label": "16", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "1866a56e-4726-4022-8cf1-dd68d0b5efe5": {"node_ids": ["15d32515-ba1d-4e6f-b1c4-5921b610b0b4"], "metadata": {"page_label": "17", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "81419660-eaa5-4cde-a282-a31e3e70cc4b": {"node_ids": ["22cb6cc1-fb75-484e-b89b-dba4da317497"], "metadata": {"page_label": "18", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "d0357042-73cb-4bd8-8114-fedd5c80d885": {"node_ids": ["f15d7924-e0e2-42c6-a4a5-38e0f9fc31ef"], "metadata": {"page_label": "19", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "a4f9cfa2-8f00-4beb-9aee-9a43a342409b": {"node_ids": ["8495a69b-bd69-4b34-a2e0-2a2068d8509e"], "metadata": {"page_label": "20", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "c855bfe5-9c80-49fc-9c96-8363897da3f8": {"node_ids": ["fc4ea02b-19a8-4f04-9ef4-fb3e6f2ea994"], "metadata": {"page_label": "21", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "68550d54-4c97-4dfc-8216-52f790a932b7": {"node_ids": ["93468089-4cef-468e-8b3b-dd841ed7dd89"], "metadata": {"page_label": "22", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "8076c518-d4c8-459c-8000-a401e4dda760": {"node_ids": ["496c27c2-50c3-42f0-8741-a083e23ff5f9"], "metadata": {"page_label": "23", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "9a4db0e2-2c1a-4748-a937-e53ad119f5f9": {"node_ids": ["040f73eb-1bd0-48d3-9fc5-da6ff10960a1"], "metadata": {"page_label": "24", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "b80ea78d-1c10-4080-8683-a6c705f6c303": {"node_ids": ["cfb8b5e8-8291-4e8a-ae27-7981082ac8b3"], "metadata": {"page_label": "25", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "d2ae7594-0539-46a9-9879-5197d24318f6": {"node_ids": ["cf017f8b-80a8-416f-9144-149249dbbc4f"], "metadata": {"page_label": "26", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "683e1873-ea0b-4047-99c4-e736cbfb4df7": {"node_ids": ["a0b544c1-5ed7-4e48-8552-af907b38a28d"], "metadata": {"page_label": "27", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "c40cfe6c-1291-4d7f-8568-1c924878b41b": {"node_ids": ["ee55e73e-1a3a-4845-94ca-dc20a8496c6a"], "metadata": {"page_label": "28", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "9a5c1e87-f0c1-4b5a-92d2-0a9044c708f5": {"node_ids": ["abc0cec7-7e75-411f-a64f-80ab8ee8fca2"], "metadata": {"page_label": "29", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "2185e75d-63fe-450f-b510-55fb11f1b243": {"node_ids": ["35c12bd2-823e-4abe-a41a-89e64b429294"], "metadata": {"page_label": "30", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "663b50ef-319d-461a-92e1-ea6fa26adb12": {"node_ids": ["72f23256-f74d-4625-946d-f7d38de94585"], "metadata": {"page_label": "31", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "7b66b086-3b1b-4414-8ec7-c09b3506cecd": {"node_ids": ["e7b4c20c-cb8f-4c4c-84df-7d9f395b4179"], "metadata": {"page_label": "32", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "b9a31115-dc09-4e1a-8519-c6a13be8e4ec": {"node_ids": ["9a250d43-f9a0-4819-b66c-4fd0549a3e87"], "metadata": {"page_label": "33", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "efffe59f-d078-4e64-b174-dac797b53c5e": {"node_ids": ["bea2dca8-5e0b-4718-87e8-3eeaffd7e465"], "metadata": {"page_label": "34", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "88ad0fd3-b84a-41aa-8528-3a85c17356fd": {"node_ids": ["d9b48161-3c41-4418-b68d-2fe3646d7676"], "metadata": {"page_label": "35", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "1d1ce352-2bf2-4df7-a62f-7cd32edac32a": {"node_ids": ["c20c91b8-c61f-4cc5-ba7b-0b9ae371f7ef"], "metadata": {"page_label": "36", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "d77a8f98-01c6-4092-9d4e-689fe0317269": {"node_ids": ["38b56389-b8b7-4e49-94b0-f86ef1a6c077"], "metadata": {"page_label": "37", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "4634ecb0-a0c3-43dc-8858-e31e6b1bd7d3": {"node_ids": ["70acec92-85cf-4853-b0b8-c7b3378d6cd9"], "metadata": {"page_label": "38", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "eca65b99-5757-4ccd-88bc-c88c827b6260": {"node_ids": ["14171535-18e7-439e-8fc4-196432228d13"], "metadata": {"page_label": "39", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "3370b1ac-4fe6-40b2-a719-2fc3adf37b17": {"node_ids": ["4aceddb4-bef6-46d0-b780-b152ef7afb70"], "metadata": {"page_label": "40", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "cca66543-14d9-43cf-a912-787d24d8b983": {"node_ids": ["cde1b907-4031-41fd-8390-f8937948d57c"], "metadata": {"page_label": "41", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "37c93f88-d09b-48a5-82cd-2ee15bbe4919": {"node_ids": ["d9721c4d-68e0-4d40-b20e-cb2d16c3649b"], "metadata": {"page_label": "42", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "932e586a-ea57-4faa-9b32-e98017647917": {"node_ids": ["15c68cdc-8c78-4217-b15b-5fa78fc8ddac"], "metadata": {"page_label": "43", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "9196b97d-32dd-4675-b2cb-5bd7ee365e84": {"node_ids": ["4cab2d74-a52f-45ba-8e2f-18de11d07d1c"], "metadata": {"page_label": "44", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "697667af-5a60-4eb9-9ec5-833d915af0ca": {"node_ids": ["57b34d24-928b-4af2-8af8-e06d6f404175"], "metadata": {"page_label": "45", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "05033585-b194-4dd8-a521-25ad5f082f25": {"node_ids": ["fba38032-63d4-4215-8917-bce375464cd6"], "metadata": {"page_label": "46", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "2d23763b-35ba-49be-bc24-4131a38ad76f": {"node_ids": ["386f25b4-a00e-4d69-89f8-11bfe7537874"], "metadata": {"page_label": "47", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "6b2fd142-77ba-4699-a5da-c500089fb5fd": {"node_ids": ["c6250e35-e200-449b-a7f7-c5895e72d2d0"], "metadata": {"page_label": "48", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "2ad50fd9-2ab9-4cfc-a072-841d182ec51a": {"node_ids": ["dcce9de0-f090-4cc7-9b55-dda442826a14"], "metadata": {"page_label": "49", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "0e73c68b-c872-4f49-bea7-b611cf550d66": {"node_ids": ["3a9ca515-8699-478d-8eff-8412cea9d39a"], "metadata": {"page_label": "50", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "30dcfeb3-14cc-4762-aece-246f70900bc5": {"node_ids": ["46855935-001f-404e-a6ac-ff8719e81b9e"], "metadata": {"page_label": "51", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "3e05bfa8-c92a-4deb-bc29-41b02384c11c": {"node_ids": ["79964cb6-b6e3-4379-a8db-5014305132fc"], "metadata": {"page_label": "52", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "84a672db-38a0-492c-b1a5-311657511aa0": {"node_ids": ["075b6f66-a8d7-4010-bb69-93c1ae78f005"], "metadata": {"page_label": "53", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "1ed1b7d0-1dbc-4a38-b0f9-cdb2a04038e5": {"node_ids": ["c8be58e9-5042-480d-bb7a-e08eb1878a56"], "metadata": {"page_label": "54", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "36f7aede-a271-408c-b941-5d244187cec6": {"node_ids": ["56882d47-827c-49b4-b1f4-633d482a5bb3"], "metadata": {"page_label": "55", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "fba8b88e-f446-4304-8d71-330148a57061": {"node_ids": ["6f5d1d11-31a1-4064-bd3b-e3650ee66979"], "metadata": {"page_label": "56", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "e5e9cf33-f963-4e5b-be81-902b7d729e3e": {"node_ids": ["2f152088-2abc-443a-a0e2-847f88a5ad14"], "metadata": {"page_label": "57", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "7e9e1df1-be84-4958-9b3f-dfda49f6687f": {"node_ids": ["598dd297-f754-4755-b85a-fca15bf4b98c"], "metadata": {"page_label": "58", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "d2b381fa-9355-4137-9a68-23eb1092ae4a": {"node_ids": ["69ee86f2-caa2-458c-a7be-1971f4246313"], "metadata": {"page_label": "59", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "36fe691f-205b-4047-9ce0-6ccc5212d21a": {"node_ids": ["142e72cc-0e7d-4de0-84cc-6124c4ccc4ed"], "metadata": {"page_label": "60", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "b3938c7b-3acf-4dfd-8488-82cea76a1fb4": {"node_ids": ["03d37f0b-e01b-4632-a3f4-b7d65fe406d7"], "metadata": {"page_label": "61", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "498cf7b6-cd23-463d-bbee-b2c842c7a2a3": {"node_ids": ["ae729902-9efd-4aa1-8d38-da6c5c850954"], "metadata": {"page_label": "62", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "7321359f-4efa-4f13-b8e3-b8e7c7312892": {"node_ids": ["c2424a6c-d09c-4283-a6f4-44c09a174521"], "metadata": {"page_label": "63", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "58cb4199-06c6-4b17-8e1b-3218710060e8": {"node_ids": ["34f7a3cd-3829-4978-bac5-5ff9d1947e62"], "metadata": {"page_label": "64", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "63d0467a-f106-43bf-b1b8-da5ead3d9d2b": {"node_ids": ["9b34d375-d9f8-4fa8-a311-d05bc2885d0b"], "metadata": {"page_label": "65", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "f4451f81-d9da-4015-8cfa-79ddcaf0c12a": {"node_ids": ["901b2e12-f44d-4619-8661-19ac01f05098"], "metadata": {"page_label": "66", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "1009a73d-8a24-41b2-8483-dbdb402b1fd6": {"node_ids": ["d0826cd7-cf62-45d1-93fe-68d78d5a7fba"], "metadata": {"page_label": "67", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "7ee1ca01-f1d4-470c-8c43-bb37ec4ba637": {"node_ids": ["3f95405b-f342-4847-99fb-cb393a46d758"], "metadata": {"page_label": "68", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "93c62cb1-d685-4125-adf6-d5fc2dd88ba5": {"node_ids": ["c2c0d875-a1d7-459a-b5b7-e71f5e71e5cc"], "metadata": {"page_label": "69", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "d11a3b4e-4006-4167-a098-ae288a1e5324": {"node_ids": ["b54039b2-32d9-4b7f-9999-13d24acef723"], "metadata": {"page_label": "70", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "f6091ce1-e72f-4bd1-a09d-4707988d049d": {"node_ids": ["bb5ecb37-aed6-48d9-afd2-918ceb6ee97f"], "metadata": {"page_label": "71", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "2a50a332-8620-4e66-8502-287080f4d095": {"node_ids": ["42d9b396-1186-4859-b8bd-db60c25e78c6"], "metadata": {"page_label": "72", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "a8d3e2bb-4d57-46c9-95cd-5faa902912b4": {"node_ids": ["90cf82b2-ed30-41b6-acf2-eefa40cdf3fe"], "metadata": {"page_label": "73", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "9894277f-383d-49ae-a636-5262a65b1d47": {"node_ids": ["3462da16-cc57-4141-9129-9dbaff173a92"], "metadata": {"page_label": "74", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "91b6425b-bf38-400e-a2a2-d299fa21384d": {"node_ids": ["5e978f30-bafa-4e10-9e2e-1317f23d9f85"], "metadata": {"page_label": "75", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "2f8a1e58-a6cf-4c3f-8a97-93182960a306": {"node_ids": ["a75b1b2e-fd9f-46bf-a9b3-b6fdec328c43"], "metadata": {"page_label": "76", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "b3bbf62d-35ef-41ff-b428-ff566db5cf5d": {"node_ids": ["1aea0126-77fe-4035-8db4-9bec3434f5d0"], "metadata": {"page_label": "77", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "d8bbda8b-6ab4-4cc1-9229-6c71788eb0f6": {"node_ids": ["ac214987-e3b6-42f4-9b3e-4419bc179794"], "metadata": {"page_label": "78", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "bd5c140b-fecc-401a-b29a-32c1adbaa36d": {"node_ids": ["3441647d-a11f-4e5b-8420-861873f6c06c"], "metadata": {"page_label": "79", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "76ddaffa-ecee-4bad-a52a-d54a1c8d87c1": {"node_ids": ["6cce71b0-534f-4f81-aaab-4d8beb6da159"], "metadata": {"page_label": "80", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "ec2135cb-9306-4b52-bdb4-63450dc6a4bf": {"node_ids": ["175b681d-e83e-41b4-9a80-9370d27c4b06"], "metadata": {"page_label": "81", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "c861cb9d-72f6-4c6c-9aee-9480abb6151a": {"node_ids": ["e7da6d52-d7ea-45b1-9773-3a294d84e23a"], "metadata": {"page_label": "82", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "273e2821-351c-40ae-9c16-900fdaede65e": {"node_ids": ["e2c74e83-2066-469f-8259-2f152dab1136"], "metadata": {"page_label": "83", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "9680772c-abe7-4643-af32-3fd6362a69a2": {"node_ids": ["99e40f34-a0e5-4ae2-9d29-febfd3269635"], "metadata": {"page_label": "84", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "d56537dc-afbf-4ee7-9f36-5e08c8fc5286": {"node_ids": ["72c44ae2-8d3a-4da8-8e95-081e69258e1c"], "metadata": {"page_label": "85", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "435c5adf-f14b-4447-9255-552d8181d7e0": {"node_ids": ["524004e3-8023-413e-b6c0-9a22d7ebfda1"], "metadata": {"page_label": "86", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "562d087a-40e1-42f9-8e7c-d317123cebc9": {"node_ids": ["961cf719-bf87-4e77-a2b2-989780ea4f48"], "metadata": {"page_label": "87", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "90506a4f-15be-48d3-9194-8d35024120a9": {"node_ids": ["8b1ca6ae-7b2d-4366-8ad8-862b25ed696c"], "metadata": {"page_label": "88", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "a3c5c037-2bac-47ed-b9cb-f4fc212d840d": {"node_ids": ["4de0cf26-73d2-4753-b018-6b57e1b74a51"], "metadata": {"page_label": "89", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "dc9781bd-bc47-4042-892a-8db01033f748": {"node_ids": ["591b8ddb-0831-4592-831e-c90e04ecac5e"], "metadata": {"page_label": "90", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "b5deaaca-1c55-46f5-a1c6-d481774fc841": {"node_ids": ["e8ba99ed-61e2-4827-ad6f-3c4c82d721d4"], "metadata": {"page_label": "91", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "b36931ea-65f1-4711-a5cc-a378faefa500": {"node_ids": ["2ea63e1d-48c0-4987-968a-a03b849d14ed"], "metadata": {"page_label": "92", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "199e26da-eb33-4fac-97e6-d1803764d2f1": {"node_ids": ["1fce0605-88ef-406b-94fc-881d14060f6f"], "metadata": {"page_label": "93", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "fe8e008d-ea58-480b-a286-0cdb28d9a604": {"node_ids": ["a2ed6b39-753a-49ff-9d60-cd2f7ff93e51"], "metadata": {"page_label": "94", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "a9b94e00-a888-413c-b8f4-3420afa80c5b": {"node_ids": ["dc60851e-2c24-4298-94d4-f868bb890351"], "metadata": {"page_label": "95", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "a1870a83-c7fa-4092-9c4e-a9889f501047": {"node_ids": ["982c29a5-f101-4f18-9203-15e1cee46049"], "metadata": {"page_label": "96", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "dc0ad0ec-e5d6-42fc-984e-6d5691607475": {"node_ids": ["bd916cd9-d390-40dc-8974-8dbc0d8a84ff"], "metadata": {"page_label": "97", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "febe3ae6-916a-4c37-943a-b9c05240c5b7": {"node_ids": ["8333a2ee-4513-4858-8497-ddc137a695d6"], "metadata": {"page_label": "98", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "7a136fce-67ff-4ddb-b5bf-7a7377ec75ff": {"node_ids": ["e5a220a6-b5fd-4c8f-93ac-7c463ea40454"], "metadata": {"page_label": "99", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "ffb4c078-72e6-408c-ae24-76e5cdd3fd78": {"node_ids": ["73825b4e-7bd8-4e9f-9609-219503b43506"], "metadata": {"page_label": "100", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "7b67d5dc-89de-4fd2-8ecc-710be02c9855": {"node_ids": ["044daf39-a8fc-4989-8215-05d7ab27d421"], "metadata": {"page_label": "101", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "eee46f57-c693-448f-9e7c-adb9abb485dd": {"node_ids": ["cad3c2f8-37af-4b77-a595-1c68e809f1a3"], "metadata": {"page_label": "102", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "43044aa5-2546-47c5-8e1e-f1aeb73e1cfc": {"node_ids": ["7f59925b-0470-4fc5-916c-c67299362a2b"], "metadata": {"page_label": "103", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "1d86a1ee-65eb-45b8-afc3-9e57273f4fe8": {"node_ids": ["75012f6a-cf08-4d1d-9fd0-333865667432"], "metadata": {"page_label": "104", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "5d02052a-34ea-4819-891d-11a2312c1828": {"node_ids": ["7bd8f7b0-ab98-41ba-9886-896f117f479e"], "metadata": {"page_label": "105", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "5917bdd7-9ec3-4f32-8bcf-9ff7cc3e279d": {"node_ids": ["b3a033e2-b6c1-4305-a6e0-b5e177f68af7"], "metadata": {"page_label": "106", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "3196a3f9-f559-4e86-b44c-228d81110463": {"node_ids": ["284a56ea-3c2a-4765-97e8-e97eebca5e7c"], "metadata": {"page_label": "107", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "02f2c303-ad6b-47e5-9134-4c25a6e5464f": {"node_ids": ["ffe91331-33b4-4297-b13b-d924fb5f3f0d"], "metadata": {"page_label": "108", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "60249dc1-fc98-4ebf-9d18-1f7d43673985": {"node_ids": ["b6ac1c45-e50f-422b-a938-6804d4058eb7"], "metadata": {"page_label": "109", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "5aa575d7-f034-40fc-b742-8a0b1b04e100": {"node_ids": ["a394bdc2-adac-475e-b215-93a6d43cf05d"], "metadata": {"page_label": "110", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "d3aea701-7ec6-496a-b362-fa9fe9b299c3": {"node_ids": ["e8712ab1-4cff-4e76-ad5f-e08e18190c70"], "metadata": {"page_label": "111", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "2d15f9ed-7204-4e0b-a1e6-2099182190d9": {"node_ids": ["220f2e28-39f5-4311-a3ca-14822f721676"], "metadata": {"page_label": "112", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "e204b58e-60cb-4908-b049-2c171e8bab72": {"node_ids": ["c6ac63f2-d1ca-4840-980d-20b12c9a561b"], "metadata": {"page_label": "113", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "39767329-45db-4aaf-95f0-a6f2285695dd": {"node_ids": ["20bd6444-cb51-4a08-b2bf-114a6a6b3900"], "metadata": {"page_label": "114", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "31c20893-d85e-4963-a8f5-e5285b504299": {"node_ids": ["317d594a-7f69-4672-9d79-4054f4b9196e"], "metadata": {"page_label": "115", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "6103f60c-7fbf-4be5-94bb-e32239114463": {"node_ids": ["a7bb8614-f306-4ada-b13e-8003783a4c60"], "metadata": {"page_label": "116", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "7e94187b-c817-4cd4-8acc-b437afb94869": {"node_ids": ["e632d6f2-4f98-4979-9607-9a7303bca4b9"], "metadata": {"page_label": "117", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "6b04f224-ff87-4171-b25f-b8556681d8b6": {"node_ids": ["5e88d5c2-5677-4e43-bb6c-01839c53291b"], "metadata": {"page_label": "118", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "265717ab-2465-4c4a-82a8-d340e63a3411": {"node_ids": ["02a6d677-7315-4066-a942-237b8532519f"], "metadata": {"page_label": "119", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "326fa34b-ec7e-4744-ab1d-a0b04c7e2cfe": {"node_ids": ["67d59b96-1dbf-4199-aab5-7cec70a00263"], "metadata": {"page_label": "120", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "9cb4f445-3389-4d7b-9fb0-b9f35e77b145": {"node_ids": ["bad1b73f-a25a-424d-8910-d0d3d790bf18"], "metadata": {"page_label": "121", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "8d976c50-a93f-4676-97d2-cf4d09bc4f77": {"node_ids": ["8b3445e7-a970-4bc4-94c7-410510901028"], "metadata": {"page_label": "122", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "f93b4a25-2d76-4d92-9baa-a8c705f8c976": {"node_ids": ["4e082aa5-5fc1-45a2-9fec-907b77c96540"], "metadata": {"page_label": "123", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "75f8bf61-daeb-4328-9617-0e75e519b03d": {"node_ids": ["c5ca9bf6-0cb9-4c5d-a0f6-226f38d70f69"], "metadata": {"page_label": "124", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "4f528974-17ce-4df6-b72f-b402932b594a": {"node_ids": ["c5e7b92b-6c6d-4698-bd8d-6b90e058e1df"], "metadata": {"page_label": "125", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "e35e1b96-8216-480e-89d5-8dad2ecd1959": {"node_ids": ["7f838087-58df-4ffd-beb1-14c33816da8f"], "metadata": {"page_label": "126", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "0218830a-93cd-4ddf-9b70-1531207acffb": {"node_ids": ["428ede47-ac3d-43fb-8ca1-be48062d4246"], "metadata": {"page_label": "127", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "3bd46432-3105-408b-b5fb-1df51633d112": {"node_ids": ["c33af5cd-5c40-4c0a-bf66-a12c59d3c134"], "metadata": {"page_label": "128", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "c65ed8ec-dcef-46ad-81ae-e3bfab4e096a": {"node_ids": ["84f18cf7-c9db-4866-8f9d-3cefd3e16302"], "metadata": {"page_label": "129", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "fcc414ee-c97c-44d1-ac15-83403f0816e0": {"node_ids": ["9eda0a4b-7866-4603-a7fc-256ac3c4a833"], "metadata": {"page_label": "130", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "699d186a-2014-4281-8fa5-d46c478de3b4": {"node_ids": ["9361f137-c8b7-4337-a132-a6a6c03df00f"], "metadata": {"page_label": "131", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "15f2eb40-eab0-44b4-b3d1-9ca18e860200": {"node_ids": ["5333567e-19ce-4535-b99d-256b75f3f906"], "metadata": {"page_label": "132", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "b59756c7-e125-4dc5-8310-7740c56d2132": {"node_ids": ["ecc20ef3-fe28-492d-9ee6-795858e0a4b1"], "metadata": {"page_label": "133", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "99b8fbff-ceaf-400a-8790-f450481a81c5": {"node_ids": ["257f66d5-7976-455c-b861-b226b36d6f05"], "metadata": {"page_label": "134", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "52a8bac8-ba8e-43cd-9994-6757ed38ca84": {"node_ids": ["901dac7e-973f-4814-bd3a-5dfdca4ee665"], "metadata": {"page_label": "135", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "46e36a31-3fa3-42e8-8c36-6858c5d24a05": {"node_ids": ["ea0a0344-b7e7-4c10-bb20-d9f7fb149701"], "metadata": {"page_label": "136", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "5b5f51d4-1b43-4a4c-a9ab-6b7694ddacf8": {"node_ids": ["10687a53-502d-419b-9cbf-5f4f8bac0be9"], "metadata": {"page_label": "137", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "84a8ec6f-e69a-4b5d-9355-ce958d5d9aff": {"node_ids": ["086e127e-d1a3-4b9d-9e05-090d2992fa2b"], "metadata": {"page_label": "138", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "6ce1cfde-5fa9-46ef-a905-d1d867cac1c4": {"node_ids": ["f296e3eb-36b6-4292-8e14-6b8e2ca9ece8"], "metadata": {"page_label": "139", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "7fe67618-9175-4136-a06f-776277c64206": {"node_ids": ["cfeff32b-d4c7-4417-8369-09350ef5e306"], "metadata": {"page_label": "140", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "002bd546-8fb4-433c-9df6-152aa8a5ff19": {"node_ids": ["1d2e305e-8b22-444d-a461-f4283953d971"], "metadata": {"page_label": "141", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "c463ac14-cb93-49be-9759-3ca4874c7d4a": {"node_ids": ["3f732582-6365-46c3-ab8b-08477bc89495"], "metadata": {"page_label": "142", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "a0169162-c9aa-4f25-a0d8-aa7f91f9aa5d": {"node_ids": ["789440ce-5414-4224-8d3c-68243b6a2a81"], "metadata": {"page_label": "143", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "9060b0b9-b204-4e80-a786-77048b462699": {"node_ids": ["bd6618b0-d6ba-48e2-9e37-bfd3bc0b7092"], "metadata": {"page_label": "144", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "4d06aa7c-a9f3-434c-9426-ac9b12d5e2d2": {"node_ids": ["2a37e9d8-92f9-412f-a2eb-5a99985a592b", "2c969372-a9bf-4e6c-ae2a-5e35d8440a5f"], "metadata": {"page_label": "145", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "bb7f030c-667a-4363-b213-aaee39cf1873": {"node_ids": ["4c136cfd-eecb-4cec-a3cd-e95aac4e4eef"], "metadata": {"page_label": "146", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "ae5806c0-979f-448e-a6ed-7e428fcd265a": {"node_ids": ["74b7c3f8-124d-4411-9ab5-2ee690f93785"], "metadata": {"page_label": "147", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "42659385-bfb6-4873-b34e-7525fa958db3": {"node_ids": ["035c3369-ba1b-4ddf-941a-1008af940660"], "metadata": {"page_label": "148", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "0732527c-dd95-441a-b4fe-24ba2ec7b5df": {"node_ids": ["95a5da59-6f84-4a97-92cb-00b54a29db60", "b65aa98e-e119-4792-8384-d198d5bf3f6f"], "metadata": {"page_label": "149", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "0c5b937c-d14e-4a49-b616-b9550e7661e7": {"node_ids": ["88c1e8a8-353e-4a70-a512-a77d6be0d856"], "metadata": {"page_label": "150", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "1fc2926b-98cc-4241-b175-fa310ee487b1": {"node_ids": ["31ff4633-8026-45f0-bfed-e36f193bc65d"], "metadata": {"page_label": "151", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "b560c667-42e6-4dae-a4a0-68ad5f748dc1": {"node_ids": ["6c3e6c17-d5d2-467c-8f5a-9b0e8f38ea0f"], "metadata": {"page_label": "152", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "6ad8d613-51a5-4b6b-a904-ba28c148e829": {"node_ids": ["76cc272c-e7d7-492a-af83-7806603b7990"], "metadata": {"page_label": "153", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "51c0b513-82a8-4b17-963d-cbe03130de46": {"node_ids": ["3304653f-3c48-46a4-8fb9-0cc13a887bb7"], "metadata": {"page_label": "154", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "9dd8f58b-5aee-4377-9be3-8a887b277c9c": {"node_ids": ["9c7e80f6-e6c0-49ef-b443-f0687a3e93c2"], "metadata": {"page_label": "155", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "e00e6f10-9ba3-41ec-9218-fe0353c7302d": {"node_ids": ["3500af81-67ee-4043-ada8-ae193aacf7e8"], "metadata": {"page_label": "156", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "9a38bf55-d1f8-43db-9135-b8a5eb7234bd": {"node_ids": ["e67475c4-ab22-42b4-9e66-d2f76ecdde76"], "metadata": {"page_label": "157", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "3a14a873-0aa6-4f64-93e7-700cabe07a43": {"node_ids": ["63c347ae-64e6-4b5e-aea5-9f5332dbbcea"], "metadata": {"page_label": "158", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "e21103d5-3542-4241-84a8-f90830408018": {"node_ids": ["d0a612c4-a28f-4543-88d1-0c52abaeb4bc"], "metadata": {"page_label": "159", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "c904c305-b9ff-43e2-bf65-aa00c4b32d47": {"node_ids": ["94c63026-b261-45ba-a2f2-c390d2423e05"], "metadata": {"page_label": "160", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "268728d4-1915-441c-9749-728825864547": {"node_ids": ["c269f3ab-72b0-4f4f-b763-25ffd47d0fa1"], "metadata": {"page_label": "161", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "b94964ad-a356-472c-a3b6-6513e5cd1399": {"node_ids": ["3a11db6c-f849-47ae-99e3-81dd44b2072b"], "metadata": {"page_label": "162", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "1659456d-6c48-4485-b66d-29a8e888b5e3": {"node_ids": ["4fc199c9-9697-49ed-8789-9839c3ae40b4"], "metadata": {"page_label": "163", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "ecfee4e3-1bc3-4036-998d-8d6ac11dc233": {"node_ids": ["a4f4d368-31e5-4d07-8c41-16f578178710"], "metadata": {"page_label": "164", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "d8279567-0178-450d-99fe-7663cc70a647": {"node_ids": ["f0472724-0c08-406e-9987-96334d28d087"], "metadata": {"page_label": "165", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "b9b8f471-9334-454e-b476-81ba82380962": {"node_ids": ["340c6769-f912-4752-a1ca-854069b21453"], "metadata": {"page_label": "166", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "8bd1feeb-d8b9-4dba-b07e-daaa351a87f6": {"node_ids": ["d923e7d1-8a81-43b8-8f54-a8f68cb4f129"], "metadata": {"page_label": "167", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "9a287a74-41ae-414d-9f0e-230784312d87": {"node_ids": ["bb958323-1122-4b3d-95b6-297ec99b78ad"], "metadata": {"page_label": "168", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "898ab30b-be46-499e-9e5e-e82f786c2943": {"node_ids": ["2057b859-a0ba-4af9-993b-a8c4a499763e"], "metadata": {"page_label": "169", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "39696ee9-ce77-4e48-8b45-51c188dfc290": {"node_ids": ["d0217b4d-5dcb-463b-9985-2c00fce93e25"], "metadata": {"page_label": "170", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "0aaf3074-b4ab-415d-9eee-e021be762ddc": {"node_ids": ["c8db6d4f-7ed0-4045-b7ac-5200402e3e5f"], "metadata": {"page_label": "171", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "314e1552-438a-482f-9f47-bc3b6c06c590": {"node_ids": ["6d59ef69-ded6-4bae-aa74-00e064708238", "072f1939-10ae-44f5-9e13-b260f3b47148"], "metadata": {"page_label": "172", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "93283ef8-2219-4af8-b8bb-e01ec47b8324": {"node_ids": ["ff8be358-f666-4b55-99b6-7ea055e73c39"], "metadata": {"page_label": "173", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "83647637-a00d-407e-9dee-4e415aba5bfc": {"node_ids": ["75f2998f-9c75-4881-b9ac-68903762e4dc"], "metadata": {"page_label": "174", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "5eb4f8f5-22b1-49ca-bcc3-a3d4d93df0d0": {"node_ids": ["b289b273-67c9-4472-a5da-e6edb8270f29"], "metadata": {"page_label": "175", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "0546b4e7-9792-4b2f-bfd7-3580bddab0dc": {"node_ids": ["016379d4-5297-4ccd-a9c9-a9d572dff0f0"], "metadata": {"page_label": "176", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "6fa991ff-1293-486d-8f1a-7cde0bee63a9": {"node_ids": ["965fc71d-0187-49b4-84f3-9a8cbabe1b6b"], "metadata": {"page_label": "177", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "56eca053-5fb4-40e5-a33a-2e8a956f63e4": {"node_ids": ["a4dd0396-5b7f-4d05-ab88-1247e00ed4d9"], "metadata": {"page_label": "178", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "dfd9824b-4139-4220-a085-6e79fadeb1b0": {"node_ids": ["5bfc91b2-9d10-4647-bf6a-787aefcde9e9"], "metadata": {"page_label": "179", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "1cd8ec23-45b6-4031-a37b-b6008799a3e7": {"node_ids": ["27389124-f307-4325-85fa-dcc8353c863b"], "metadata": {"page_label": "180", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "95b41ca9-87bf-4121-b8be-c939626b7ee6": {"node_ids": ["46619238-b8bd-4861-9c09-b1471ad4d84d"], "metadata": {"page_label": "181", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "32ca113b-746c-403d-bee1-5c64f59a2049": {"node_ids": ["32f44fc2-63af-491e-9a64-e66979d9e29d"], "metadata": {"page_label": "182", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "1cab42b5-27ed-4a89-843a-9d2a65abacc3": {"node_ids": ["a6bda2d8-1e04-4b7d-a422-45f122c73917"], "metadata": {"page_label": "183", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "2e90e617-26c7-427d-ab7f-6840dbd88823": {"node_ids": ["b99a3e6b-d4ac-4e38-bf03-1bd36b383c4c"], "metadata": {"page_label": "184", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "e2d6d3ca-a1db-4c39-a81d-6bdb49cb8006": {"node_ids": ["6c0ca391-979b-41e0-b869-b945156897e0"], "metadata": {"page_label": "185", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "7db4c44f-e699-4b87-a87d-a71b1282bfd3": {"node_ids": ["88c63a3f-078c-4c93-a2c2-37568d828d2b"], "metadata": {"page_label": "186", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "fe71d34e-096f-44f8-bd51-d16c6d9f942b": {"node_ids": ["48ef7ccc-4fee-4c52-a9c5-5e35c1b1070a"], "metadata": {"page_label": "187", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "bcaeee74-de9b-48d3-b6a5-0c39dfd2b997": {"node_ids": ["3746a184-0d81-42f3-aa0a-901acbfa9265"], "metadata": {"page_label": "188", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "ee6f8217-0f1b-4a54-b1ce-25d66344683d": {"node_ids": ["78e4731a-0e0c-445e-89d3-34e831db10a9"], "metadata": {"page_label": "189", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "8845a7ca-e9f0-4e95-87c5-f97e8f16352c": {"node_ids": ["18bafec4-2c17-4c4a-873b-0b4ad3a63590"], "metadata": {"page_label": "190", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "5722fd4b-957c-4761-9d5d-b6796708cdb4": {"node_ids": ["42bcfd59-5be0-41e4-b115-4756ececde8d"], "metadata": {"page_label": "191", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "8d687fc6-126f-49fa-bc37-2069d9cf8125": {"node_ids": ["83a644a9-f0ab-4173-a4b4-2b9bd51603d9"], "metadata": {"page_label": "192", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "97f812bd-d27a-46d1-9106-dda1f768a5a9": {"node_ids": ["f58823a9-2594-44d0-aa5d-0032ad9ce8fc"], "metadata": {"page_label": "193", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "daa48086-e858-46db-9e9c-2b1318fb72ea": {"node_ids": ["6bc3bf95-7e63-4fe7-a184-296a59bd4b0b"], "metadata": {"page_label": "194", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "c4d7bfbe-4e35-4d2c-9bf2-3153f0973629": {"node_ids": ["7139a2b5-8f27-4c9e-82a8-29f2537eea5e"], "metadata": {"page_label": "195", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "7c9955c0-b712-4e08-8630-56445c97aa45": {"node_ids": ["72ee9c1e-bc51-48b6-b09b-491ce95a2c1d"], "metadata": {"page_label": "196", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "a1dfc68b-6bd4-4957-83c0-f6f4a6cfe3c3": {"node_ids": ["6894e621-273c-4c5f-aaf0-4dd7eef21385"], "metadata": {"page_label": "197", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "75c67807-ec76-42e0-8ed1-54faea0f7543": {"node_ids": ["8ae06f2b-498f-47df-8854-b9357df4a29c"], "metadata": {"page_label": "198", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "78afd476-d315-42fe-9d5d-8d6acd3df7cb": {"node_ids": ["b243197a-f233-49c7-8cfb-d1e641eda111"], "metadata": {"page_label": "199", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "af980508-4ddc-4e09-abe7-31c2d2f42d6c": {"node_ids": ["b8312523-7e40-4569-8df0-c5e54eea0537"], "metadata": {"page_label": "200", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "0788d96a-90d1-40e3-9550-8e7ee829a559": {"node_ids": ["05c83dd7-7006-48cb-831a-bcd8c17460ee"], "metadata": {"page_label": "201", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "da33cebe-16b2-4be5-bdc3-515357f7ff68": {"node_ids": ["36137c6a-a6eb-43e1-b8dd-f26f08699ec2"], "metadata": {"page_label": "202", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "85eae4b5-95a7-4951-afe4-2fdf2e96ff44": {"node_ids": ["7c007a53-d6d2-4dec-8423-a7f08c441b1f"], "metadata": {"page_label": "203", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "eb03315b-7300-4936-8769-2f90151fc43d": {"node_ids": ["adaf78a4-f6c7-4ac7-b1e7-33208eeab63a"], "metadata": {"page_label": "204", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "36ecb6e4-ae9a-4767-902f-b4d377d79ea8": {"node_ids": ["d0767f80-1684-44bc-8d62-6b746972f3bd"], "metadata": {"page_label": "205", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "875e9265-07d9-4691-9e67-4827c9f9054d": {"node_ids": ["a7d02906-87fb-4945-a22c-de2bdea311db"], "metadata": {"page_label": "206", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "f4236a4f-9f76-4230-84a1-d483eb3591c8": {"node_ids": ["b74ff94e-a03a-4e9a-b864-74f6c41af5e3"], "metadata": {"page_label": "207", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "bf6d08df-33a8-44c7-acdc-c852b72d8a43": {"node_ids": ["a12c3bdc-da7b-43db-9048-2c933fd39c60"], "metadata": {"page_label": "208", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "9ed703b9-be5d-4830-9a26-4c3f704bae39": {"node_ids": ["268fe9f8-6b42-4fac-9597-14fb651fd7a9"], "metadata": {"page_label": "209", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "3415773f-3081-4401-87f0-285c4ed96389": {"node_ids": ["b08f0a1a-451f-41f0-b57f-173fe7e32276"], "metadata": {"page_label": "210", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "9b7f85d6-71a8-4224-8d8c-291b2b2abb47": {"node_ids": ["63bd1377-37bd-4b38-bd64-7e1f2a996412"], "metadata": {"page_label": "211", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "30c6c8ed-deb6-455b-aee5-d17a13d86c9a": {"node_ids": ["74d769c4-97ed-438f-9bb5-95a9bdac71b6"], "metadata": {"page_label": "212", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "d57d3330-42e3-48ba-82b7-3cfcc8951174": {"node_ids": ["5c5e9a30-132b-49b5-902d-8d420185de62"], "metadata": {"page_label": "213", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "43d1eaef-fe3a-44ff-8167-656c98ce10ae": {"node_ids": ["cd3068d0-db6f-4f83-acaf-d6123102dcf0"], "metadata": {"page_label": "214", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "323954ca-6773-4339-8e96-c58e22f5b2c7": {"node_ids": ["db9e48c2-49d5-4737-b2e2-01b2abbf708f"], "metadata": {"page_label": "215", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "f8870fa7-fdf8-40b9-a806-1278060db34c": {"node_ids": ["1ce5b567-8733-4a12-bef2-7c6319225df0"], "metadata": {"page_label": "216", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "b68c90e4-25f7-415e-9e76-a667a48e40fc": {"node_ids": ["d33923c9-8f15-4537-a691-755c81fda63a"], "metadata": {"page_label": "217", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "81891401-d971-4807-b1f4-8214c74b3403": {"node_ids": ["fd5f9550-4e51-4074-8d13-fa023d7a1120"], "metadata": {"page_label": "218", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "b4332993-7547-4bc4-a723-ca5fcb14d801": {"node_ids": ["b63d0326-d32b-47a1-91dc-1bf341825276"], "metadata": {"page_label": "219", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "5242e3f9-5a43-4f86-a065-dbbe58304f50": {"node_ids": ["56a1f123-95db-4e6a-8e04-03d606e0dc66"], "metadata": {"page_label": "220", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "18b72851-6d5e-48cc-b00e-1e442f2d9590": {"node_ids": ["d43ab2e7-4f6c-4fd7-af38-dcdf7785339b"], "metadata": {"page_label": "221", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "63c1c3f1-e428-445e-b41a-786e757dc764": {"node_ids": ["2ee0239a-ed12-43f0-b9a1-d9069665172a"], "metadata": {"page_label": "222", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "b442e95e-8120-494a-9aec-d4bd544fd529": {"node_ids": ["d6c89e94-dd91-4d8f-b625-7e4c0a078be3"], "metadata": {"page_label": "223", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "6fa11236-ec5a-4812-b768-8cef0a8568f2": {"node_ids": ["5db3f0a8-2896-48b9-9571-546c0bdf9383"], "metadata": {"page_label": "224", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "6fe96df6-7687-421a-952d-2798e7c01060": {"node_ids": ["70528964-8697-4085-8d1e-f1638b055679"], "metadata": {"page_label": "225", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "6f303d2f-a1d2-4424-bbe1-fb9c7d0c13a8": {"node_ids": ["788a234a-c975-4c6a-bc7c-c7cea6b490b8"], "metadata": {"page_label": "226", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "38a8820d-d0ff-46fc-a09e-5a963749ff0f": {"node_ids": ["30d1e881-140e-4203-a4f6-a671e641eafd"], "metadata": {"page_label": "227", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "6eea69db-d2af-4e66-8e7a-8be1d149e02a": {"node_ids": ["32f086bb-6413-47fb-ace0-395e4a85461e"], "metadata": {"page_label": "228", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "18488222-f311-4bb6-8cde-a63c5bb357c8": {"node_ids": ["54b4bdbf-27c2-41be-9054-ae9fe1f3be4a"], "metadata": {"page_label": "229", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "b3a481e2-5388-4dfe-b0e2-9118b4a8abc7": {"node_ids": ["d1528309-abe3-4d29-bbb9-b0a569aab059"], "metadata": {"page_label": "230", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "290412b5-4303-4786-81cf-b04b75e5de1e": {"node_ids": ["e4644f69-f392-4cd4-97ba-d2a254a59b23"], "metadata": {"page_label": "231", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "7b54a0ff-24c9-415d-ac1e-eef923aacb8a": {"node_ids": ["af0daeb8-655d-47a9-8b19-29719cf893f9"], "metadata": {"page_label": "232", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "8570e97b-6fd5-488b-a397-6cc2e940e33a": {"node_ids": ["09c8a28e-6718-4027-b375-23c4ec5fe313"], "metadata": {"page_label": "233", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "8f8655f5-0f66-4219-888b-62f33291abd6": {"node_ids": ["45a5cb18-ac79-450e-b445-f43e9d00069f"], "metadata": {"page_label": "234", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "0f800b55-5208-4311-9cd0-a8bc59e6f84c": {"node_ids": ["1c2bb72d-d2b3-42d3-814a-27b4db6fb679"], "metadata": {"page_label": "235", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "fafd2ba6-5df4-48b3-a6ac-12ecd07c9f37": {"node_ids": ["c55de6f5-1c5f-4e94-a720-b4f0033fa9a2"], "metadata": {"page_label": "236", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "2808b57a-9ed3-44a0-b321-a62572cda607": {"node_ids": ["8317ff91-8560-45bf-a7a4-58ad987d6844"], "metadata": {"page_label": "237", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "cb55b00e-11c1-46b0-b030-43539644fae3": {"node_ids": ["81dd0e12-d9a7-4eb2-8255-fdade662ae54"], "metadata": {"page_label": "238", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "36b27665-c63e-49da-b97a-420e5399eb89": {"node_ids": ["8e1785f2-adee-4e84-b242-0f45515b9da5"], "metadata": {"page_label": "239", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "d3973197-d432-4b86-889d-0ab950f29f57": {"node_ids": ["81ec5d99-30b9-4f78-ba96-0cc531fed29b"], "metadata": {"page_label": "240", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "1b9091e5-a412-47ce-b495-247a75c10ef5": {"node_ids": ["9feb501b-5e5d-4639-b0fd-3d14c432995a"], "metadata": {"page_label": "241", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "b9f36262-f69e-4e01-959e-ed420251cbcf": {"node_ids": ["9b5d7e8b-2fb7-4b57-9c99-ac8b305a1a6a"], "metadata": {"page_label": "242", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "e8c54f02-c166-4c95-a286-4dbf81c1f4fc": {"node_ids": ["82f68cc3-5c41-4375-8edf-7c00ee4c603b"], "metadata": {"page_label": "243", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "fc1a7c7c-a819-4bba-84ca-39fe836c2490": {"node_ids": ["67d6b035-9d28-49d9-86f8-5556137a414d"], "metadata": {"page_label": "244", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "a2cbf396-ef19-43c7-a15c-86baf1f001a0": {"node_ids": ["882fe263-a4aa-48b5-84d8-951312774a0a"], "metadata": {"page_label": "245", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "cd6af929-dff0-4a99-b9e0-0e460c08b7e4": {"node_ids": ["d7d175f3-0b44-47ae-930b-be40fe71822f"], "metadata": {"page_label": "246", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "c61eae0f-d1bb-4290-829f-c7072f80d08c": {"node_ids": ["d9717651-a579-4502-ab35-d82ed167cfdf"], "metadata": {"page_label": "247", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "7349a39a-cfd7-4c23-bf17-ed270c770ea0": {"node_ids": ["93e40bf5-ed62-44ed-9369-776cc871e02a", "e921ee1f-6a78-4c7f-84b3-4d10f68e48a3"], "metadata": {"page_label": "248", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "5c49fe42-d58b-4c45-a243-4d8a016ac7d8": {"node_ids": ["1286d26d-c1fa-4703-819f-5c44ec569293"], "metadata": {"page_label": "249", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "f97c04f7-7741-4c83-bf74-c3fac0856c3d": {"node_ids": ["dd3c5156-76e8-4167-ae14-6d1473cfc6ee"], "metadata": {"page_label": "250", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "c6859032-32f1-4703-93e8-7b9e6c95453e": {"node_ids": ["5a352a3e-c2cb-485f-b20e-623314290df8"], "metadata": {"page_label": "251", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "bae0c8b0-fd27-4c32-bc8e-d95641bc5c7d": {"node_ids": ["7800b59c-ca27-460a-91d7-d67358ef8b74"], "metadata": {"page_label": "252", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "ea4b7b0a-1ced-4d51-b85d-de7c2cd9d3d3": {"node_ids": ["60770af3-e928-4ef7-a7a5-ce103f403bd6"], "metadata": {"page_label": "253", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "0eaff0a8-fbd9-4bf3-99bc-947e0effa6ef": {"node_ids": ["80ce37bb-468f-4285-b907-514dfa102301"], "metadata": {"page_label": "254", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "a1164754-9806-42e9-a6a5-e73ee406388e": {"node_ids": ["a2ac7bbe-0ac4-496b-aafb-a6cc411194bd"], "metadata": {"page_label": "255", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "b5216b80-5103-48f6-81af-14b94e50a4a9": {"node_ids": ["08ab5769-c73c-4cd8-8a7e-15995922bad9"], "metadata": {"page_label": "256", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "f92cccab-29d5-40ba-bf6f-f614dcebd768": {"node_ids": ["d2ea0a61-1d3d-4591-9f29-38d28fa41419"], "metadata": {"page_label": "257", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "96afeab7-28ca-494a-9bfa-b5aa49aed86d": {"node_ids": ["87062351-14ce-4d5c-91df-3f5bd151dfee"], "metadata": {"page_label": "258", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "37a87002-5d33-4cb1-89c6-0e3a9cdd3b3a": {"node_ids": ["9ca53843-413c-4d39-9747-4e3dfc8dcd08"], "metadata": {"page_label": "259", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "4ec1ac3c-5335-442b-9c87-94a92d636327": {"node_ids": ["b4299dd8-9899-4475-8ffa-f8e85d8467a4"], "metadata": {"page_label": "260", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "6dedce11-ddbd-4f1e-8b8a-93aa99d7c5e1": {"node_ids": ["6f84e8d3-a6a7-49d0-b888-7f371bc51ca9"], "metadata": {"page_label": "261", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "b52cf43b-8adc-4e65-92b5-66d57870a103": {"node_ids": ["7070e3dc-f567-4ced-938a-01254dabc7d5"], "metadata": {"page_label": "262", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "956b801d-f4b6-4869-b17b-2f9fb6f04d0d": {"node_ids": ["7a55b3b2-e00b-4b85-8c8a-09fd52b64178"], "metadata": {"page_label": "263", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "b5c75f33-816b-4863-b28d-51cec4d3b529": {"node_ids": ["10adc40f-f4d9-4ee9-8bec-1b477f3f803d"], "metadata": {"page_label": "264", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "3d390d59-9101-4607-ae69-71824268fa87": {"node_ids": ["4837b13e-1b1f-4272-9899-b00bd4ca680f"], "metadata": {"page_label": "265", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "cd9ad72c-0414-4e21-8086-23886873827a": {"node_ids": ["879c8edd-27d5-431f-8584-fc7696be59fb"], "metadata": {"page_label": "266", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "b240c279-dcae-4e0e-b619-149cf2442806": {"node_ids": ["ed544c8f-c7ed-44bc-b9a4-46848d4d576e"], "metadata": {"page_label": "267", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "b2bb905c-3558-404b-9ed2-fdee69cb81ea": {"node_ids": ["2b638283-572f-417a-8a7a-3b9534898d44"], "metadata": {"page_label": "268", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "6550ff3c-0c39-488f-a6fb-ec7fdb79e127": {"node_ids": ["fd8919c8-3344-43e7-9a98-2a6ac70ddc63"], "metadata": {"page_label": "269", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "230d3219-36f4-4bde-bb69-355cda70d5f6": {"node_ids": ["2942296e-debd-4d75-bcf3-9b390ed5b156"], "metadata": {"page_label": "270", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "efcb2069-1933-4eae-93bd-ab290586a3b2": {"node_ids": ["13df8067-684e-43a3-a787-f600f16df205"], "metadata": {"page_label": "271", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "fbb23cff-c529-46fe-9fbe-92a9ea2b3750": {"node_ids": ["381d8c3c-760b-49be-a448-a88abd5a39c1"], "metadata": {"page_label": "272", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "97f6bb7c-60ed-4be7-89cf-2a2d820277ed": {"node_ids": ["9b2d9b4e-3618-4ecd-992d-3788fbc2855c"], "metadata": {"page_label": "273", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "4f62c1df-e1f8-47a2-b79f-b79839306c0d": {"node_ids": ["313de4db-a1e7-4bd5-ac63-36afc713995e"], "metadata": {"page_label": "274", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "4a007f77-a9da-47a7-97ee-58ec53715948": {"node_ids": ["dd27ee08-683a-45c8-9d79-f090516961f0"], "metadata": {"page_label": "275", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "9bddfd4f-f097-4a4f-aaa6-7e0824ef3948": {"node_ids": ["7e32ece6-2eb9-4a02-933c-dee36e2ec95c"], "metadata": {"page_label": "276", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "f286dde6-5e05-4582-876a-f6e6b14c2966": {"node_ids": ["cb50dfdc-9814-4ba4-8722-4b29dd122255"], "metadata": {"page_label": "277", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "6a233649-a7fb-4d92-96f8-cdd486554d17": {"node_ids": ["eb554ecb-673a-4cf8-a9dd-4619ca1477f1"], "metadata": {"page_label": "278", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "670f214d-2fee-4eba-ae05-e4af5389371a": {"node_ids": ["db87863c-e864-493c-a267-5ce64cd7d207"], "metadata": {"page_label": "279", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "bafb9a70-4250-43d8-9cce-d99225bc788e": {"node_ids": ["80a92745-daf2-496d-b18a-10226dafa0c3"], "metadata": {"page_label": "280", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "4ce3996c-b193-4014-a767-c4ef44f971ff": {"node_ids": ["0d930596-c103-49c0-8a95-2e72011abe61"], "metadata": {"page_label": "281", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "47530729-e34b-4a50-8d0f-fd3bd80a70b0": {"node_ids": ["b5ed8d35-6dbf-46db-8c2b-3ba8d25b60db"], "metadata": {"page_label": "282", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "450b9095-9db1-4a7d-aca0-28221ac7ba21": {"node_ids": ["01db5fe1-00c8-435d-a84b-271d0af75ddf"], "metadata": {"page_label": "283", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "d703b4c9-83d1-4332-a78f-4e0e8b936d38": {"node_ids": ["633ce579-ef01-4537-ba05-686fc669e45a"], "metadata": {"page_label": "284", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "eed11105-b8b4-4fec-8c7b-e937d9f7e54d": {"node_ids": ["15c77bf3-753c-4bea-a375-a79ec14f5ff1"], "metadata": {"page_label": "285", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "ddca60da-d5c3-4473-b5ec-10869091ec7f": {"node_ids": ["f6df8a8a-2ed5-499a-b155-f75a0b87cf23"], "metadata": {"page_label": "286", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "11c28e0f-bfcf-4f20-905b-d935c59c84aa": {"node_ids": ["96bde251-5198-438d-b02c-8d6625beb6e8"], "metadata": {"page_label": "287", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "206a325b-2ba2-4b1a-a37f-3395c772c50b": {"node_ids": ["34847f06-ae2e-462b-bf75-47863b86e34e"], "metadata": {"page_label": "288", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "49df4625-0661-4a0c-a7e8-cc152a295e96": {"node_ids": ["58d6437f-0355-4229-8e67-6c34b1badf96"], "metadata": {"page_label": "289", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "1858b1e7-41bd-4c4d-ac0b-727fc158ad60": {"node_ids": ["8d27ce5e-57a1-49bd-a1c9-a724bb8a5298"], "metadata": {"page_label": "290", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "98932691-9658-4b72-9f32-cca87b88640d": {"node_ids": ["ea0b0dc0-1f8c-4769-b26d-9ed3fdf8c822"], "metadata": {"page_label": "291", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "d5b3c507-d5b4-446d-bc96-be2669b748cb": {"node_ids": ["1d57b5c2-b2c7-4e24-b772-e6347289ff6e"], "metadata": {"page_label": "292", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "5406a7e8-6a98-42a4-a0e3-0e0e6c15835d": {"node_ids": ["a27a8566-5435-4a67-a0a3-213347bb755f"], "metadata": {"page_label": "293", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "f7cb44b3-d4a6-40f6-b0e7-dd790c594e57": {"node_ids": ["57d40cf2-50cc-4343-bf93-ff8856b49744"], "metadata": {"page_label": "294", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "2470caf3-782b-4802-ae14-e833646fc961": {"node_ids": ["8445e8e4-f07d-4a5f-ad3b-4cc0de8c1e9a"], "metadata": {"page_label": "295", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "27c4933c-9cf8-4176-8bab-cab08935e61a": {"node_ids": ["d8c22092-6ccc-4aa0-8a50-cd6e9b889fcf"], "metadata": {"page_label": "296", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "7ffc6755-54d2-4d01-a257-2d7a5191a139": {"node_ids": ["0390de83-b6b8-452a-b65f-e3f179f27420"], "metadata": {"page_label": "297", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "543b0ae1-2f13-48cf-a344-cb67045ef7bb": {"node_ids": ["95d47cc4-2a4e-49e7-9cfe-c0f3d9795cb3"], "metadata": {"page_label": "298", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "52ca2beb-b4ec-4cbe-95dd-9bb2a35460bb": {"node_ids": ["3b1b5764-703b-4ce6-8b18-41ffaa464fef"], "metadata": {"page_label": "299", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "d207774e-acd5-408d-871a-c87c3a53fe93": {"node_ids": ["91f07f65-bed8-4b2e-9209-2884625942b9"], "metadata": {"page_label": "300", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "02d625f2-bb2a-43d9-a4ae-aae00c226437": {"node_ids": ["26011f24-7f32-492d-b756-2af370ee2832"], "metadata": {"page_label": "301", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "621e5e1d-65fd-4868-9e3d-575f78528742": {"node_ids": ["7edff24c-a136-4660-b1c0-2a12cb23919d"], "metadata": {"page_label": "302", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "e857529b-17d6-40b7-a5d3-dd246e1731f9": {"node_ids": ["7777ca6a-9d67-4eb6-a6d2-5ca06d0aa0bc"], "metadata": {"page_label": "303", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "652e63cd-50e9-47c5-b5c0-05069ec9a45e": {"node_ids": ["d2415395-05d9-4e08-a8dd-3ebe2c06eeb3"], "metadata": {"page_label": "304", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "7377f3b8-0bd3-4cb8-b3bd-337711129bcb": {"node_ids": ["3457e16e-6058-4483-af57-b1966b724f1b"], "metadata": {"page_label": "305", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "7f4af078-8631-4a7e-9b92-c9eca377e19e": {"node_ids": ["5932ad7d-368d-4cc2-8818-1fcbb82eee36"], "metadata": {"page_label": "306", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "ae3c8357-e8f3-4ce7-a39b-8976e9687996": {"node_ids": ["72e1c06c-abdf-421c-84e4-d7a0d073ed78"], "metadata": {"page_label": "307", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "58f6aa3f-f410-41ce-b9bb-d2c4fb818eb0": {"node_ids": ["45150d4b-45b4-4834-b1bc-96c457b5043b"], "metadata": {"page_label": "308", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "a5383756-ad7d-473b-8748-8e5ca3db9873": {"node_ids": ["070b9fee-43ca-4fb0-92b9-6af843183dd4"], "metadata": {"page_label": "309", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "32057854-cb08-4bfc-9a7d-5a8390ab5503": {"node_ids": ["eb93d18d-b068-4afa-8230-62cf3ccd2af8"], "metadata": {"page_label": "310", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "dd7371e0-94b6-4347-9e8f-e40fa9bb20b7": {"node_ids": ["e057538f-3cad-4d04-a9d9-68b8d1218ab8"], "metadata": {"page_label": "311", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "b2f05688-c05e-4d48-97aa-dba15fa156d1": {"node_ids": ["ebbceecc-e1c2-4157-89c6-20c84dedd341"], "metadata": {"page_label": "312", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "8e7a6c66-f944-40a2-8a74-ad7a15bf7e0c": {"node_ids": ["7a5c971c-1131-4e5f-a82c-5f0d271f5219"], "metadata": {"page_label": "313", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "8f8aab45-0db5-45c5-87da-2ddbec6fac83": {"node_ids": ["923a8673-c5b2-460a-b726-824d420550bd"], "metadata": {"page_label": "314", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "8b79e3c1-a36c-456c-9af4-78184a34c259": {"node_ids": ["846c5a10-6bbf-4656-8b74-4780c29815b7"], "metadata": {"page_label": "315", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "22385d06-cfcf-4cf8-a4ad-c8782f270e7c": {"node_ids": ["d05e87ea-7e8d-48d1-886a-bad0a45e158c"], "metadata": {"page_label": "316", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "bb6a9b07-3ea9-4e54-9ca6-374759e2ab12": {"node_ids": ["8786e5bc-66a4-4a93-a884-94534e1936fc"], "metadata": {"page_label": "317", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "4b88b751-f134-4b4f-86a5-f267d9404d34": {"node_ids": ["93ce2744-83ac-47f5-aecf-18ea79d02ef8"], "metadata": {"page_label": "318", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "6fac8246-f819-492c-8c3c-13db2df74f81": {"node_ids": ["3c5fff63-83cd-45b2-ae3f-e500e8d914d0"], "metadata": {"page_label": "319", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "0936e118-139e-4968-92aa-c2b13fdf6a56": {"node_ids": ["5bd4f037-2568-440b-a903-bb44557f612b"], "metadata": {"page_label": "320", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "36440607-c683-4dff-9733-34b32fde5961": {"node_ids": ["d76d16f4-a7f7-4abe-b1cd-fdba9a4ab019"], "metadata": {"page_label": "321", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "0395d880-e48b-4cbd-9c60-b3423d610b38": {"node_ids": ["5157dd00-551c-40c6-94c8-d6ae20ed1faa"], "metadata": {"page_label": "322", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "87547b46-efa1-4f0c-b5bf-b4d3e868f54b": {"node_ids": ["c7701a9c-f8f4-4606-9bd1-267259ec85aa"], "metadata": {"page_label": "323", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "d3248e59-364f-453b-93c4-3810ea964e4e": {"node_ids": ["ec04ceea-3dbb-4b91-a03b-ad04e45b9103"], "metadata": {"page_label": "324", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "2137841b-687f-44b0-900a-9f46763c4e98": {"node_ids": ["641f8d16-fda8-4f97-a374-3f386be13462"], "metadata": {"page_label": "325", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "afd89625-11d5-441b-84e9-a7a2e4372c6d": {"node_ids": ["b07f7d64-1fed-4ff5-b063-da2d40b54f0a"], "metadata": {"page_label": "326", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "4387c6f1-280d-46a5-a45b-06b5ecdfd831": {"node_ids": ["8b8b7998-0be7-402a-9296-4e7075781390"], "metadata": {"page_label": "327", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "c844f8f7-2cad-4320-959b-0a2c86c1745f": {"node_ids": ["ea847dca-62b0-41e9-99f7-33075d1042d2"], "metadata": {"page_label": "328", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "8bac2634-4280-4e50-af81-46bf0ffd6d18": {"node_ids": ["d06f85b0-44b8-4673-abf8-ccf145c2bda1"], "metadata": {"page_label": "329", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "f22c021b-18a4-4c68-b7db-f919355c79f1": {"node_ids": ["68d138bb-c2d8-4192-8fa7-937fad5d3817"], "metadata": {"page_label": "330", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "ba329b1f-4c71-4067-8634-8fb764e0809a": {"node_ids": ["77df77e6-5e86-44ed-b8c1-0da6fcdf6e8a"], "metadata": {"page_label": "331", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "2db6666a-2f77-4fe6-835c-6b4923f29798": {"node_ids": ["db654770-d2e3-4b41-a241-16039dba6935"], "metadata": {"page_label": "332", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "6162e212-cdf6-4d23-8336-f548aea9e5d4": {"node_ids": ["11f66497-d53e-458a-a96d-b4088b14633d"], "metadata": {"page_label": "333", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "188ed599-0bef-4e2f-a6a8-ed897a3a33ea": {"node_ids": ["2ced5460-0d85-4c52-bb57-a1475b5eae8e"], "metadata": {"page_label": "334", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "7066ee75-cd40-4dee-a9c3-12e566896e8a": {"node_ids": ["6d57c268-cf21-4fa0-9ea4-76efbe03a3d4"], "metadata": {"page_label": "335", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "87870eaf-c124-48e4-90c7-fb047d51fe8a": {"node_ids": ["8165b7bb-31c6-4159-8e08-220ce0e2f2cb"], "metadata": {"page_label": "336", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "256b7554-b675-4320-b4b0-63d1c495fe40": {"node_ids": ["e0b6e48b-8e90-44e0-917e-cc41596557a0"], "metadata": {"page_label": "337", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "6949dab2-4c32-43a2-a6f2-e97b28262b91": {"node_ids": ["c38b2fb0-3955-4d68-9f22-b664e21540b5"], "metadata": {"page_label": "338", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "233713ff-9f4a-4d52-bab8-e89bae908ac0": {"node_ids": ["5847cb31-b6d9-4678-9c60-261959ec6c43"], "metadata": {"page_label": "339", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "dbb25a45-cab6-4951-9892-f16aefb15e8f": {"node_ids": ["73f6fb7e-7cc1-4a7a-8c28-e4def73b0d76"], "metadata": {"page_label": "340", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "edaed966-2e8e-4488-8778-c7be3d46cd79": {"node_ids": ["1dc34870-0f21-4cb1-8c72-21b328f3fee4"], "metadata": {"page_label": "341", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "769eb413-31c1-496b-a30d-c6fd5a77cc53": {"node_ids": ["3228b3c3-e854-4a53-8c46-90b420024efa"], "metadata": {"page_label": "342", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "ec8a90ef-0811-47a7-986f-9b493c5416ca": {"node_ids": ["fa897d6e-3699-4280-9a2b-18153e0cd29d"], "metadata": {"page_label": "343", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "39839b0c-3cc8-4b2b-88f6-6759d9c35c5c": {"node_ids": ["47841a92-e73f-4ce9-b2ac-3a4f27bc52cd"], "metadata": {"page_label": "344", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "011ed87b-4187-4f74-94fc-c87a96a166b1": {"node_ids": ["b5e66dc3-f231-4e25-9092-5229d46dbe0d"], "metadata": {"page_label": "345", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "b9e152ee-53fa-4018-902a-7c51d497294f": {"node_ids": ["bb3cd242-89f9-480b-ade2-db8cfb11d920"], "metadata": {"page_label": "346", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "e0b257d8-3d67-4f8a-bc49-0bd2a65d4b21": {"node_ids": ["d1f64769-24dc-4070-8618-978cbbe8bec1"], "metadata": {"page_label": "347", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "6ddb4227-96d3-4160-99b4-dcc7527c59be": {"node_ids": ["48295fdd-e8a5-44bc-bfce-f020fb85f4eb"], "metadata": {"page_label": "348", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "8282f81f-04fa-4306-b0e2-158ac8560fc9": {"node_ids": ["e674765f-8b58-490d-90ea-a460e2749993"], "metadata": {"page_label": "349", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "3493933d-f315-4006-9040-419979d2e68b": {"node_ids": ["75673f48-d3d0-47e3-9b32-8b3955087ae2"], "metadata": {"page_label": "350", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "779388a3-56a8-46d5-9e95-71fb414b8c25": {"node_ids": ["413aaf1d-5361-4689-bcf5-a3b21ca31b8e"], "metadata": {"page_label": "351", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}, "dbe2a69f-e4ba-4656-8ab8-169e83be275c": {"node_ids": ["9b2cc135-d7f5-4c0c-b191-0be9af6e2295"], "metadata": {"page_label": "352", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}}}, "docstore/data": {"b2e8fe1a-8646-4db8-a3e8-ddda282679ba": {"__data__": {"id_": "b2e8fe1a-8646-4db8-a3e8-ddda282679ba", "embedding": null, "metadata": {"page_label": "1", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6094fcc0-b443-44dd-8634-5068f3ad0090", "node_type": "4", "metadata": {"page_label": "1", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "00d10a506eafc7a3831229cf5e062d59b437c849aa006bcb0bf468c53263ba6a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Foundations and TrendsR\u20dd in\nMachine Learning\nVol. 3, No. 1 (2010) 1\u2013122\nc\u20dd 2011 S. Boyd, N. Parikh, E. Chu, B. Peleato\nand J. Eckstein\nDOI: 10.1561/2200000016\nDistributed Optimization and Statistical\nLearning via the Alternating Direction\nMethod of Multipliers\nStephen Boyd1, Neal Parikh2, Eric Chu3\nBorja Peleato4 and Jonathan Eckstein5\n1 Electrical Engineering Department, Stanford University, Stanford, CA\n94305, USA, boyd@stanford.edu\n2 Computer Science Department, Stanford University, Stanford, CA 94305,\nUSA, npparikh@cs.stanford.edu\n3 Electrical Engineering Department, Stanford University, Stanford, CA\n94305, USA, echu508@stanford.edu\n4 Electrical Engineering Department, Stanford University, Stanford, CA\n94305, USA, peleato@stanford.edu\n5 Management Science and Information Systems Department and\nRUTCOR, Rutgers University, Piscataway, NJ 08854, USA,\njeckstei@rci.rutgers.edu", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 888, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "39c8a6dc-7107-43e4-92dd-297be9523764": {"__data__": {"id_": "39c8a6dc-7107-43e4-92dd-297be9523764", "embedding": null, "metadata": {"page_label": "2", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60689eb1-3fe9-4624-99ba-8e46b9203b22", "node_type": "4", "metadata": {"page_label": "2", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "dcfb1239b2bf612e7facdd74c0fd8a9f17c6099c030c92986684121022943bde", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Contents\n1 Introduction 3\n2 Precursors 7\n2.1 Dual Ascent 7\n2.2 Dual Decomposition 9\n2.3 Augmented Lagrangians and the Method of Multipliers 10\n3 Alternating Direction Method of Multipliers 13\n3.1 Algorithm 13\n3.2 Convergence 15\n3.3 Optimality Conditions and Stopping Criterion 18\n3.4 Extensions and Variations 20\n3.5 Notes and References 23\n4 General Patterns 25\n4.1 Proximity Operator 25\n4.2 Quadratic Objective Terms 26\n4.3 Smooth Objective Terms 30\n4.4 Decomposition 31\n5 Constrained Convex Optimization 33\n5.1 Convex Feasibility 34\n5.2 Linear and Quadratic Programming 36", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 575, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f34ea1b8-4f15-43b7-9276-4ebc06388179": {"__data__": {"id_": "f34ea1b8-4f15-43b7-9276-4ebc06388179", "embedding": null, "metadata": {"page_label": "3", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "899fc809-a5e2-4860-b33d-5006dd337076", "node_type": "4", "metadata": {"page_label": "3", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "3f8c3a2036c5851d1e56b8c544fd0e8493f057943b37b7eb5aceb3f74aca42d1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6 \u21131-Norm Problems 38\n6.1 Least Absolute Deviations 39\n6.2 Basis Pursuit 41\n6.3 General \u2113\n1 Regularized Loss Minimization 42\n6.4 Lasso 43\n6.5 Sparse Inverse Covariance Selection 45\n7 Consensus and Sharing 48\n7.1 Global Variable Consensus Optimization 48\n7.2 General Form Consensus Optimization 53\n7.3 Sharing 56\n8 Distributed Model Fitting 61\n8.1 Examples 62\n8.2 Splitting across Examples 64\n8.3 Splitting across Features 66\n9 Nonconvex Problems 73\n9.1 Nonconvex Constraints 73\n9.2 Bi-convex Problems 76\n10 Implementation 78\n10.1 Abstract Implementation 78\n10.2 MPI 80\n10.3 Graph Computing Frameworks 81\n10.4 MapReduce 82\n11 Numerical Examples 87\n11.1 Small Dense Lasso 88\n11.2 Distributed \u2113\n1 Regularized Logistic Regression 92\n11.3 Group Lasso with Feature Splitting 95\n11.4 Distributed Large-Scale Lasso with MPI 97\n11.5 Regressor Selection 100", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 847, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1f0a3d74-fb21-4b0c-9637-3fccb799e058": {"__data__": {"id_": "1f0a3d74-fb21-4b0c-9637-3fccb799e058", "embedding": null, "metadata": {"page_label": "4", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c60b287f-80a6-43c0-9f72-d1c55c2e13a9", "node_type": "4", "metadata": {"page_label": "4", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "e9a95f65a1052b84946946eae225c63e0f4a3208cb9c52328abeed8ba2203a0c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12 Conclusions 103\nAcknowledgments 105\nA Convergence Proof 106\nReferences 111", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 77, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bccdb4cc-146d-4828-b0f3-b6c5e91fa930": {"__data__": {"id_": "bccdb4cc-146d-4828-b0f3-b6c5e91fa930", "embedding": null, "metadata": {"page_label": "5", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5810e7e5-fdf8-42b5-8543-db2a98d25510", "node_type": "4", "metadata": {"page_label": "5", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "30988781d8e965f02d724e8c016146ccdb8570bb73bde3c6a33c5355ec374f11", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Abstract\nMany problems of recent interest in statistics and machine learning\ncan be posed in the framework of convex optimization. Due to the\nexplosion in size and complexity of modern datasets, it is increasingly\nimportant to be able to solve problems with a very large number of fea-\ntures or training examples. As a result, both the decentralized collection\nor storage of these datasets as well as accompanying distributed solu-\ntion methods are either necessary or at least highly desirable. In this\nreview, we argue that the alternating direction method of multipliers\nis well suited to distributed convex optimization, and in particular to\nlarge-scale problems arising in statistics, machine learning, and related\nareas. The method was developed in the 1970s, with roots in the 1950s,\nand is equivalent or closely related to many other algorithms, such\nas dual decomposition, the method of multipliers, Douglas\u2013Rachford\nsplitting, Spingarn\u2019s method of partial inverses, Dykstra\u2019s alternating\nprojections, Bregman iterative algorithms for \u2113\n1 problems, proximal\nmethods, and others. After brie\ufb02y surveying the theory and history of\nthe algorithm, we discuss applications to a wide variety of statistical\nand machine learning problems of recent interest, including the lasso,\nsparse logistic regression, basis pursuit, covariance selection, support\nvector machines, and many others. We also discuss general distributed\noptimization, extensions to the nonconvex setting, and e\ufb03cient imple-\nmentation, including some details on distributed MPI and Hadoop\nMapReduce implementations.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1583, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2930f7f9-4c01-4d4e-8d7e-34b3eed65ffa": {"__data__": {"id_": "2930f7f9-4c01-4d4e-8d7e-34b3eed65ffa", "embedding": null, "metadata": {"page_label": "3", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea6a59f1-258d-44a5-817e-04c665a9ea93", "node_type": "4", "metadata": {"page_label": "3", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "cda702855f29c341c990013588963afc720eeb861b0de14291182c5c3521ed1a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1\nIntroduction\nIn all applied \ufb01elds, it is now commonplace to attack problems through\ndata analysis, particularly through the use of statistical and machine\nlearning algorithms on what are often large datasets. In industry, this\ntrend has been referred to as \u2018Big Data\u2019, and it has had a signi\ufb01cant\nimpact in areas as varied as arti\ufb01cial intelligence, internet applications,\ncomputational biology, medicine, \ufb01nance, marketing, journalism, net-\nwork analysis, and logistics.\nThough these problems arise in diverse application domains, they\nshare some key characteristics. First, the datasets are often extremely\nlarge, consisting of hundreds of millions or billions of training examples;\nsecond, the data is often very high-dimensional, because it is now possi-\nble to measure and store very detailed information about each example;\nand third, because of the large scale of many applications, the data is\noften stored or even collected in a distributed manner. As a result, it\nhas become of central importance to develop algorithms that are both\nrich enough to capture the complexity of modern data, and scalable\nenough to process huge datasets in a parallelized or fully decentral-\nized fashion. Indeed, some researchers [92] have suggested that even\nhighly complex and structured problems may succumb most easily to\nrelatively simple models trained on vast datasets.\n3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1369, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "061f059c-8726-4e53-bb9c-7fd15c60ffd9": {"__data__": {"id_": "061f059c-8726-4e53-bb9c-7fd15c60ffd9", "embedding": null, "metadata": {"page_label": "4", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd9ee88d-ef27-4bad-af59-a1e83479a348", "node_type": "4", "metadata": {"page_label": "4", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "451ed6f26ff7f92cb7cab7632585ffe9bf90998ddf049f5719e2df0a07957a5e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4 Introduction\nMany such problems can be posed in the framework of convex opti-\nmization. Given the signi\ufb01cant work on decomposition methods and\ndecentralized algorithms in the optimization community, it is natural\nto look to parallel optimization algorithms as a mechanism for solving\nlarge-scale statistical tasks. This approach also has the bene\ufb01t that one\nalgorithm could be \ufb02exible enough to solve many problems.\nThis review discusses the alternating direction method of multipli-\ners (ADMM), a simple but powerful algorithm that is well suited to\ndistributed convex optimization, and in particular to problems aris-\ning in applied statistics and machine learning. It takes the form of a\ndecomposition-coordinationprocedure, in which the solutions to small\nlocal subproblems are coordinated to \ufb01nd a solution to a large global\nproblem. ADMM can be viewed as an attempt to blend the bene\ufb01ts\nof dual decomposition and augmented Lagrangian methods for con-\nstrained optimization, two earlier approaches that we review in \u00a72. It\nturns out to be equivalent or closely related to many other algorithms\nas well, such as Douglas-Rachford splitting from numerical analysis,\nSpingarn\u2019s method of partial inverses, Dykstra\u2019s alternating projec-\ntions method, Bregman iterative algorithms for \u2113\n1 problems in signal\nprocessing, proximal methods, and many others. The fact that it has\nbeen re-invented in di\ufb00erent \ufb01elds over the decades underscores the\nintuitive appeal of the approach.\nIt is worth emphasizing that the algorithm itself is not new, and that\nwe do not present any new theoretical results. It was \ufb01rst introduced\nin the mid-1970s by Gabay, Mercier, Glowinski, and Marrocco, though\nsimilar ideas emerged as early as the mid-1950s. The algorithm was\nstudied throughout the 1980s, and by the mid-1990s, almost all of the\ntheoretical results mentioned here had been established. The fact that\nADMM was developed so far in advance of the ready availability of\nlarge-scale distributed computing systems and massive optimization\nproblems may account for why it is not as widely known today as we\nbelieve it should be.\nThe main contributions of this review can be summarized as follows:\n(1) We provide a simple, cohesive discussion of the extensive\nliterature in a way that emphasizes and uni\ufb01es the aspects\nof primary importance in applications.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2345, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f367d0ab-6788-4362-bf3c-5add6d606d3f": {"__data__": {"id_": "f367d0ab-6788-4362-bf3c-5add6d606d3f", "embedding": null, "metadata": {"page_label": "5", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "878e7182-3c06-41c5-bb5c-8222ff1a30f8", "node_type": "4", "metadata": {"page_label": "5", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "1d5ff12a15cb1493f73d9947b794d25265c94ab04b19c921bfa0f487e53a88d5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5\n(2) We show, through a number of examples, that the algorithm\nis well suited for a wide variety of large-scale distributed mod-\nern problems. We derive methods for decomposing a wide\nclass of statistical problems by training examples and by fea-\ntures, which is not easily accomplished in general.\n(3) We place a greater emphasis on practical large-scale imple-\nmentation than most previous references. In particular, we\ndiscuss the implementation of the algorithm in cloud com-\nputing environments using standard frameworks and provide\neasily readable implementations of many of our examples.\nThroughout, the focus is on applications rather than theory, and a main\ngoal is to provide the reader with a kind of \u2018toolbox\u2019 that can be applied\nin many situations to derive and implement a distributed algorithm of\npractical use. Though the focus here is on parallelism, the algorithm\ncan also be used serially, and it is interesting to note that with no\ntuning, ADMM can be competitive with the best known methods for\nsome problems.\nWhile we have emphasized applications that can be concisely\nexplained, the algorithm would also be a natural \ufb01t for more compli-\ncated problems in areas like graphical models. In addition, though our\nfocus is on statistical learning problems, the algorithm is readily appli-\ncable in many other cases, such as in engineering design, multi-period\nportfolio optimization, time series analysis, network \ufb02ow, or scheduling.\nOutline\nWe begin in \u00a72 with a brief review of dual decomposition and the\nmethod of multipliers, two important precursors to ADMM. This sec-\ntion is intended mainly for background and can be skimmed. In \u00a73,\nwe present ADMM, including a basic convergence theorem, some vari-\nations on the basic version that are useful in practice, and a survey of\nsome of the key literature. A complete convergence proof is given in\nappendix A.\nIn \u00a74, we describe some general patterns that arise in applications\nof the algorithm, such as cases when one of the steps in ADMM can", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2012, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "21f0534b-ed8d-41af-a628-13405fdbbf8e": {"__data__": {"id_": "21f0534b-ed8d-41af-a628-13405fdbbf8e", "embedding": null, "metadata": {"page_label": "6", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0164c163-7026-4979-826c-001726140c3a", "node_type": "4", "metadata": {"page_label": "6", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "db58b076050df190d1883399079c4175ba60b6a884c56fa2fe7da217975b7485", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6 Introduction\nbe carried out particularly e\ufb03ciently. These general patterns will recur\nthroughout our examples. In\u00a75, we consider the use of ADMM for some\ngeneric convex optimization problems, such as constrained minimiza-\ntion and linear and quadratic programming. In \u00a76, we discuss a wide\nvariety of problems involving the \u2113\n1 norm. It turns out that ADMM\nyields methods for these problems that are related to many state-of-the-\nart algorithms. This section also clari\ufb01es why ADMM is particularly\nwell suited to machine learning problems.\nIn \u00a77, we present consensus and sharing problems, which provide\ngeneral frameworks for distributed optimization. In \u00a78, we consider\ndistributed methods for generic model \ufb01tting problems, including reg-\nularized regression models like the lasso and classi\ufb01cation models like\nsupport vector machines.\nIn \u00a79, we consider the use of ADMM as a heuristic for solving some\nnonconvex problems. In\u00a710, we discuss some practical implementation\ndetails, including how to implement the algorithm in frameworks suit-\nable for cloud computing applications. Finally, in \u00a711, we present the\ndetails of some numerical experiments.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1155, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7cf5a801-dd90-4825-91d3-c211d857d9c2": {"__data__": {"id_": "7cf5a801-dd90-4825-91d3-c211d857d9c2", "embedding": null, "metadata": {"page_label": "7", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7bceafa1-d21a-4ac8-aabd-19eec5b0b212", "node_type": "4", "metadata": {"page_label": "7", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "e3804dcaceaa6245dc606c909aabca949e28ace254ef7e0f8db91edd3d0c894a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2\nPrecursors\nIn this section, we brie\ufb02y review two optimization algorithms that are\nprecursors to the alternating direction method of multipliers. While\nwe will not use this material in the sequel, it provides some useful\nbackground and motivation.\n2.1 Dual Ascent\nConsider the equality-constrained convex optimization problem\nminimize f(x)\nsubject to Ax = b, (2.1)\nwith variable x \u2208 Rn, where A \u2208 Rm\u00d7n and f : Rn \u2192 R is convex.\nThe Lagrangian for problem (2.1) is\nL(x,y)= f(x)+ yT (Ax \u2212 b)\nand the dual function is\ng(y) = inf\nx\nL(x,y)= \u2212f\u2217(\u2212AT y) \u2212 bT y,\nwhere y is the dual variable or Lagrange multiplier, andf\u2217 is the convex\nconjugate of f; see [20, \u00a73.3] or [140, \u00a712] for background. The dual\n7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 700, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ada3a148-aeeb-4866-8e6a-fd1be013c7bc": {"__data__": {"id_": "ada3a148-aeeb-4866-8e6a-fd1be013c7bc", "embedding": null, "metadata": {"page_label": "8", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0fdabdfd-7bf7-429c-95fd-2843c6327b8e", "node_type": "4", "metadata": {"page_label": "8", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "757605b2edecd0856f24ded5f2771b0b0f1e247fc81b521ed29abd7957c1ba61", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8 Precursors\nproblem is\nmaximize g(y),\nwith variable y \u2208 Rm. Assuming that strong duality holds, the optimal\nvalues of the primal and dual problems are the same. We can recover\na primal optimal point x\n\u22c6 from a dual optimal point y\u22c6 as\nx\u22c6 = argmin\nx\nL(x,y\u22c6 ),\nprovided there is only one minimizer of L(x,y\u22c6 ). (This is the case\nif, e.g., f is strictly convex.) In the sequel, we will use the notation\nargminx F(x) to denote any minimizer of F, even when F does not\nhave a unique minimizer.\nIn the dual ascent method, we solve the dual problem using gradient\nascent. Assuming that g is di\ufb00erentiable, the gradient \u2207g(y) can be\nevaluated as follows. We \ufb01rst \ufb01nd x+ = argminx L(x,y); then we have\n\u2207g(y)= Ax+ \u2212 b, which is the residual for the equality constraint. The\ndual ascent method consists of iterating the updates\nxk+1 := argmin\nx\nL(x,yk) (2.2)\nyk+1 := yk + \u03b1k(Axk+1 \u2212 b), (2.3)\nwhere \u03b1k > 0 is a step size, and the superscript is the iteration counter.\nThe \ufb01rst step (2.2) is anx-minimization step, and the second step (2.3)\nis a dual variable update. The dual variable y can be interpreted as\na vector of prices, and the y-update is then called a price update or\nprice adjustment step. This algorithm is called dual ascent since, with\nappropriate choice of \u03b1k, the dual function increases in each step, i.e.,\ng(yk+1) >g (yk).\nThe dual ascent method can be used even in some cases when g is\nnot di\ufb00erentiable. In this case, the residualAxk+1 \u2212 b is not the gradi-\nent of g, but the negative of a subgradient of \u2212g. This case requires a\ndi\ufb00erent choice of the\u03b1k than when g is di\ufb00erentiable, and convergence\nis not monotone; it is often the case thatg(yk+1) \u0338>g (yk). In this case,\nthe algorithm is usually called the dual subgradient method [152].\nIf \u03b1k is chosen appropriately and several other assumptions hold,\nthen xk converges to an optimal point and yk converges to an optimal", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1887, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "989c8e88-2806-452c-ac07-551c9740379e": {"__data__": {"id_": "989c8e88-2806-452c-ac07-551c9740379e", "embedding": null, "metadata": {"page_label": "9", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9f08d760-a4dd-4617-8ca6-5d1bcaeeb8aa", "node_type": "4", "metadata": {"page_label": "9", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "65edf9de872e1c3c6cead23fac0047dcfc8e9211acb8fd6b51174b96d32e150c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.2 Dual Decomposition 9\ndual point. However, these assumptions do not hold in many applica-\ntions, so dual ascent often cannot be used. As an example, if f is a\nnonzero a\ufb03ne function of any component ofx, then the x-update (2.2)\nfails, since L is unbounded below in x for most y.\n2.2 Dual Decomposition\nThe major bene\ufb01t of the dual ascent method is that it can lead to a\ndecentralized algorithm in some cases. Suppose, for example, that the\nobjective f is separable (with respect to a partition or splitting of the\nvariable into subvectors), meaning that\nf(x)=\nN\u2211\ni=1\nfi(xi),\nwhere x =( x1,...,x N ) and the variables xi \u2208 Rni are subvectors of x.\nPartitioning the matrix A conformably as\nA =[ A1 \u00b7\u00b7\u00b7 AN ],\nso Ax = \u2211 N\ni=1 Aixi, the Lagrangian can be written as\nL(x,y)=\nN\u2211\ni=1\nLi(xi,y)=\nN\u2211\ni=1\n(\nfi(xi)+ yT Aixi \u2212 (1/N)yT b\n)\n,\nwhich is also separable in x. This means that the x-minimization\nstep (2.2) splits intoN separate problems that can be solved in parallel.\nExplicitly, the algorithm is\nxk+1\ni := argmin\nxi\nLi(xi,yk) (2.4)\nyk+1 := yk + \u03b1k(Axk+1 \u2212 b). (2.5)\nThe x-minimization step (2.4) is carried out independently, in parallel,\nfor each i =1 ,...,N . In this case, we refer to the dual ascent method\nas dual decomposition.\nIn the general case, each iteration of the dual decomposition method\nrequires a broadcast and a gather operation. In the dual update\nstep (2.5), the equality constraint residual contributions Aixk+1\ni are", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1439, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ff3eed74-6e15-4ecc-910f-1b6e8f348ec6": {"__data__": {"id_": "ff3eed74-6e15-4ecc-910f-1b6e8f348ec6", "embedding": null, "metadata": {"page_label": "10", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9ec69d6-ca26-4231-a9d8-cb5302aadb81", "node_type": "4", "metadata": {"page_label": "10", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "2e68dd883055e639ded9ebc5e86e80a419ff0677d39214f62bea54a6163b0d58", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10 Precursors\ncollected (gathered) in order to compute the residualAxk+1 \u2212 b. Once\nthe (global) dual variable yk+1 is computed, it must be distributed\n(broadcast) to the processors that carry out the N individual xi mini-\nmization steps (2.4).\nDual decomposition is an old idea in optimization, and traces back\nat least to the early 1960s. Related ideas appear in well known work\nby Dantzig and Wolfe [44] and Benders [13] on large-scale linear pro-\ngramming, as well as in Dantzig\u2019s seminal book [43]. The general idea\nof dual decomposition appears to be originally due to Everett [69],\nand is explored in many early references [107, 84, 117, 14]. The use\nof nondi\ufb00erentiable optimization, such as the subgradient method, to\nsolve the dual problem is discussed by Shor [152]. Good references on\ndual methods and decomposition include the book by Bertsekas [16,\nchapter 6] and the survey by Nedi\u00b4c and Ozdaglar [131] on distributed\noptimization, which discusses dual decomposition methods and con-\nsensus problems. A number of papers also discuss variants on standard\ndual decomposition, such as [129].\nMore generally, decentralized optimization has been an active topic\nof research since the 1980s. For instance, Tsitsiklis and his co-authors\nworked on a number of decentralized detection and consensus problems\ninvolving the minimization of a smooth function f known to multi-\nple agents [160, 161, 17]. Some good reference books on parallel opti-\nmization include those by Bertsekas and Tsitsiklis [17] and Censor and\nZenios [31]. There has also been some recent work on problems where\neach agent has its own convex, potentially nondi\ufb00erentiable, objective\nfunction [130]. See [54] for a recent discussion of distributed methods\nfor graph-structured optimization problems.\n2.3 Augmented Lagrangians and the Method of Multipliers\nAugmented Lagrangian methods were developed in part to bring\nrobustness to the dual ascent method, and in particular, to yield con-\nvergence without assumptions like strict convexity or \ufb01niteness of f.\nThe augmented Lagrangian for (2.1) is\nL\n\u03c1(x,y)= f(x)+ yT (Ax \u2212 b)+( \u03c1/2)\u2225Ax \u2212 b\u22252\n2, (2.6)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2124, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "047b3d80-a6bf-4716-9efe-9c706661f461": {"__data__": {"id_": "047b3d80-a6bf-4716-9efe-9c706661f461", "embedding": null, "metadata": {"page_label": "11", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa31846b-7ff6-4025-b334-5531cfd98702", "node_type": "4", "metadata": {"page_label": "11", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "2d6db71f9c6891994b3726dd0d6c092b2494169907f7b93c0e0b78e028adcffd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.3 Augmented Lagrangians and the Method of Multipliers 11\nwhere \u03c1> 0 is called the penalty parameter. (Note that L0 is the\nstandard Lagrangian for the problem.) The augmented Lagrangian\ncan be viewed as the (unaugmented) Lagrangian associated with the\nproblem\nminimize f(x)+( \u03c1/2)\u2225Ax \u2212 b\u2225\n2\n2\nsubject to Ax = b.\nThis problem is clearly equivalent to the original problem (2.1), since\nfor any feasiblex the term added to the objective is zero. The associated\ndual function is g\u03c1(y) = infx L\u03c1(x,y).\nThe bene\ufb01t of including the penalty term is thatg\u03c1 can be shown to\nbe di\ufb00erentiable under rather mild conditions on the original problem.\nThe gradient of the augmented dual function is found the same way as\nwith the ordinary Lagrangian,i.e., by minimizing overx, and then eval-\nuating the resulting equality constraint residual. Applying dual ascent\nto the modi\ufb01ed problem yields the algorithm\nx\nk+1 := argmin\nx\nL\u03c1(x,yk) (2.7)\nyk+1 := yk + \u03c1(Axk+1 \u2212 b), (2.8)\nwhich is known as the method of multipliers for solving (2.1). This is\nthe same as standard dual ascent, except that thex-minimization step\nuses the augmented Lagrangian, and the penalty parameter \u03c1 is used\nas the step size\u03b1k. The method of multipliers converges under far more\ngeneral conditions than dual ascent, including cases when f takes on\nthe value +\u221e or is not strictly convex.\nIt is easy to motivate the choice of the particular step size \u03c1 in\nthe dual update (2.8). For simplicity, we assume here that f is di\ufb00er-\nentiable, though this is not required for the algorithm to work. The\noptimality conditions for (2.1) are primal and dual feasibility, i.e.,\nAx\n\u22c6 \u2212 b =0 , \u2207f(x\u22c6 )+ AT y\u22c6 =0 ,\nrespectively. By de\ufb01nition, xk+1 minimizes L\u03c1(x,yk), so\n0= \u2207xL\u03c1(xk+1,yk)\n= \u2207xf(xk+1)+ AT\n(\nyk + \u03c1(Axk+1 \u2212 b)\n)\n= \u2207xf(xk+1)+ AT yk+1.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1789, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dc7a3ff6-8c5d-4cbc-82c7-e9c361311577": {"__data__": {"id_": "dc7a3ff6-8c5d-4cbc-82c7-e9c361311577", "embedding": null, "metadata": {"page_label": "12", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a4bc4f0-d445-467c-b12f-dd07a019b214", "node_type": "4", "metadata": {"page_label": "12", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "ba6fdbb83797d3a611711ed0597ddc26030efd8f6452d8330055cbbdfcd44984", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12 Precursors\nWe see that by using \u03c1 as the step size in the dual update, the iterate\n(xk+1,yk+1) is dual feasible. As the method of multipliers proceeds, the\nprimal residual Axk+1 \u2212 b converges to zero, yielding optimality.\nThe greatly improved convergence properties of the method of mul-\ntipliers over dual ascent comes at a cost. Whenf is separable, the aug-\nmented LagrangianL\u03c1 is not separable, so thex-minimization step (2.7)\ncannot be carried out separately in parallel for eachxi. This means that\nthe basic method of multipliers cannot be used for decomposition. We\nwill see how to address this issue next.\nAugmented Lagrangians and the method of multipliers for con-\nstrained optimization were \ufb01rst proposed in the late 1960s by Hestenes\n[97, 98] and Powell [138]. Many of the early numerical experiments on\nthe method of multipliers are due to Miele et al. [124, 125, 126]. Much\nof the early work is consolidated in a monograph by Bertsekas [15],\nwho also discusses similarities to older approaches using Lagrangians\nand penalty functions [6, 5, 71], as well as a number of generalizations.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1101, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6dad2aa7-f344-48a3-9748-57e19f4e1513": {"__data__": {"id_": "6dad2aa7-f344-48a3-9748-57e19f4e1513", "embedding": null, "metadata": {"page_label": "13", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "89d97c01-a8ef-4666-84e0-9e1236ab18f1", "node_type": "4", "metadata": {"page_label": "13", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "2252d55c82f90ea24b6c6faed24546e7030abb3c968463af9c50f8bfe06fe6de", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3\nAlternating Direction Method of Multipliers\n3.1 Algorithm\nADMM is an algorithm that is intended to blend the decomposability\nof dual ascent with the superior convergence properties of the method\nof multipliers. The algorithm solves problems in the form\nminimize f(x)+ g(z)\nsubject to Ax + Bz = c (3.1)\nwith variables x \u2208 R\nn and z \u2208 Rm, where A \u2208 Rp\u00d7n, B \u2208 Rp\u00d7m, and\nc \u2208 Rp. We will assume thatf and g are convex; more speci\ufb01c assump-\ntions will be discussed in \u00a73.2. The only di\ufb00erence from the general\nlinear equality-constrained problem (2.1) is that the variable, called x\nthere, has been split into two parts, calledx and z here, with the objec-\ntive function separable across this splitting. The optimal value of the\nproblem (3.1) will be denoted by\np\n\u22c6 = inf{f(x)+ g(z) | Ax + Bz = c}.\nAs in the method of multipliers, we form the augmented Lagrangian\nL\u03c1(x,z,y )= f(x)+ g(z)+ yT (Ax + Bz \u2212 c)+( \u03c1/2)\u2225Ax + Bz \u2212 c\u22252\n2.\n13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 928, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "28954239-7003-4d07-8fed-232f158d5ac3": {"__data__": {"id_": "28954239-7003-4d07-8fed-232f158d5ac3", "embedding": null, "metadata": {"page_label": "14", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3fb7e556-4484-4834-ae7d-1d06d2a19cd1", "node_type": "4", "metadata": {"page_label": "14", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "e22f23c0501dacd2a77c6e01eebd4e87c01e06430f45a65bf9c11d308d7d1643", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14 Alternating Direction Method of Multipliers\nADMM consists of the iterations\nxk+1 := argmin\nx\nL\u03c1(x,zk,yk) (3.2)\nzk+1 := argmin\nz\nL\u03c1(xk+1,z,y k) (3.3)\nyk+1 := yk + \u03c1(Axk+1 + Bzk+1 \u2212 c), (3.4)\nwhere \u03c1> 0. The algorithm is very similar to dual ascent and the\nmethod of multipliers: it consists of an x-minimization step (3.2), a\nz-minimization step (3.3), and a dual variable update (3.4). As in the\nmethod of multipliers, the dual variable update uses a step size equal\nto the augmented Lagrangian parameter \u03c1.\nThe method of multipliers for (3.1) has the form\n(x\nk+1,zk+1) := argmin\nx,z\nL\u03c1(x,z,y k)\nyk+1 := yk + \u03c1(Axk+1 + Bzk+1 \u2212 c).\nHere the augmented Lagrangian is minimized jointly with respect to\nthe two primal variables. In ADMM, on the other hand, x and z are\nupdated in an alternating or sequential fashion, which accounts for the\nterm alternating direction. ADMM can be viewed as a version of the\nmethod of multipliers where a single Gauss-Seidel pass [90, \u00a710.1] over\nx and z is used instead of the usual joint minimization. Separating the\nminimization over x and z into two steps is precisely what allows for\ndecomposition when f or g are separable.\nThe algorithm state in ADMM consists ofz\nk and yk. In other words,\n(zk+1,yk+1) is a function of (zk,yk). The variable xk is not part of the\nstate; it is an intermediate result computed from the previous state\n(z\nk\u22121,yk\u22121).\nIf we switch (re-label) x and z, f and g, and A and B in the prob-\nlem (3.1), we obtain a variation on ADMM with the order of the x-\nupdate step (3.2) and z-update step (3.3) reversed. The roles of x and\nz are almost symmetric, but not quite, since the dual update is done\nafter the z-update but before the x-update.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1700, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3947e646-5404-4758-9786-1048c046d6d4": {"__data__": {"id_": "3947e646-5404-4758-9786-1048c046d6d4", "embedding": null, "metadata": {"page_label": "15", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f24aea0b-aeb1-463b-acd4-d86761bb6f62", "node_type": "4", "metadata": {"page_label": "15", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "6066069acc8a8cddef9ebe910c700100cfae5d2c7f6395e2cde1cb23a5dec4cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.2 Convergence 15\n3.1.1 Scaled Form\nADMM can be written in a slightly di\ufb00erent form, which is often\nmore convenient, by combining the linear and quadratic terms in the\naugmented Lagrangian and scaling the dual variable. De\ufb01ning the resid-\nual r = Ax + Bz \u2212 c,w eh a v e\ny\nT r +( \u03c1/2)\u2225r\u22252\n2 =( \u03c1/2)\u2225r +( 1/\u03c1)y\u22252\n2 \u2212 (1/2\u03c1)\u2225y\u22252\n2\n=( \u03c1/2)\u2225r + u\u22252\n2 \u2212 (\u03c1/2)\u2225u\u22252\n2,\nwhere u =( 1/\u03c1)y is the scaled dual variable. Using the scaled dual vari-\nable, we can express ADMM as\nxk+1 := argmin\nx\n(\nf(x)+( \u03c1/2)\u2225Ax + Bzk \u2212 c + uk\u22252\n2\n)\n(3.5)\nzk+1 := argmin\nz\n(\ng(z)+( \u03c1/2)\u2225Axk+1 + Bz \u2212 c + uk\u22252\n2\n)\n(3.6)\nuk+1 := uk + Axk+1 + Bzk+1 \u2212 c. (3.7)\nDe\ufb01ning the residual at iterationk as rk = Axk + Bzk \u2212 c, we see that\nuk = u0 +\nk\u2211\nj=1\nrj,\nthe running sum of the residuals.\nWe call the \ufb01rst form of ADMM above, given by (3.2\u20133.4), the\nunscaled form, and the second form (3.5\u20133.7) the scaled form, since it\nis expressed in terms of a scaled version of the dual variable. The two\nare clearly equivalent, but the formulas in the scaled form of ADMM\nare often shorter than in the unscaled form, so we will use the scaled\nform in the sequel. We will use the unscaled form when we wish to\nemphasize the role of the dual variable or to give an interpretation\nthat relies on the (unscaled) dual variable.\n3.2 Convergence\nThere are many convergence results for ADMM discussed in the liter-\nature; here, we limit ourselves to a basic but still very general result\nthat applies to all of the examples we will consider. We will make one", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1502, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6ac6f13a-9319-471e-96de-ceff489389bd": {"__data__": {"id_": "6ac6f13a-9319-471e-96de-ceff489389bd", "embedding": null, "metadata": {"page_label": "16", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "33965993-5109-4044-a90e-841c29acf34a", "node_type": "4", "metadata": {"page_label": "16", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "02ae8ff140bf4bf3815dadf8b9386c5f55ae1cc6769b914df3776aefc0ae39ae", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "16 Alternating Direction Method of Multipliers\nassumption about the functions f and g, and one assumption about\nproblem (3.1).\nAssumption 1. The (extended-real-valued) functions f : Rn \u2192 R \u222a\n{+\u221e} and g : Rm \u2192 R \u222a{ +\u221e} are closed, proper, and convex.\nThis assumption can be expressed compactly using the epigraphs of\nthe functions: The function f satis\ufb01es assumption 1 if and only if its\nepigraph\nepif = {(x,t) \u2208 Rn \u00d7 R | f(x) \u2264 t}\nis a closed nonempty convex set.\nAssumption 1 implies that the subproblems arising in thex-update\n(3.2) andz-update (3.3) aresolvable, i.e., there existx and z, not neces-\nsarily unique (without further assumptions onA and B), that minimize\nthe augmented Lagrangian. It is important to note that assumption 1\nallows f and g to be nondi\ufb00erentiable and to assume the value + \u221e.\nFor example, we can take f to be the indicator function of a closed\nnonempty convex set C, i.e., f(x)=0f o r x \u2208C and f(x)=+ \u221e other-\nwise. In this case, the x-minimization step (3.2) will involve solving a\nconstrained quadratic program over C, the e\ufb00ective domain of f.\nAssumption 2. The unaugmented Lagrangian L0 has a saddle point.\nExplicitly, there exist (x\u22c6 ,z\u22c6 ,y\u22c6 ), not necessarily unique, for which\nL0(x\u22c6 ,z\u22c6 ,y) \u2264 L0(x\u22c6 ,z\u22c6 ,y\u22c6 ) \u2264 L0(x,z,y \u22c6 )\nholds for all x, z, y .\nBy assumption 1, it follows that L0(x\u22c6 ,z\u22c6 ,y\u22c6 ) is \ufb01nite for any sad-\ndle point (x\u22c6 ,z\u22c6 ,y\u22c6 ). This implies that ( x\u22c6 ,z\u22c6 ) is a solution to (3.1),\nso Ax\u22c6 + Bz\u22c6 = c and f(x\u22c6 ) < \u221e, g(z\u22c6 ) < \u221e. It also implies that y\u22c6\nis dual optimal, and the optimal values of the primal and dual prob-\nlems are equal, i.e., that strong duality holds. Note that we make no\nassumptions about A, B,o r c, except implicitly through assumption 2;\nin particular, neither A nor B is required to be full rank.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1772, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6621799e-d1f5-40d6-afb3-6e9a18d60898": {"__data__": {"id_": "6621799e-d1f5-40d6-afb3-6e9a18d60898", "embedding": null, "metadata": {"page_label": "17", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7f3d4a4c-f62a-44a4-a220-26330c1afbe4", "node_type": "4", "metadata": {"page_label": "17", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "a2515477d90d25502a96e713e05f20cf0aeeb4349492b8282c401f58e03075b9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.2 Convergence 17\n3.2.1 Convergence\nUnder assumptions 1 and 2, the ADMM iterates satisfy the following:\n\u2022 Residual convergence. rk \u2192 0a s k \u2192\u221e , i.e., the iterates\napproach feasibility.\n\u2022 Objective convergence. f(xk)+ g(zk) \u2192 p\u22c6 as k \u2192\u221e , i.e.,\nthe objective function of the iterates approaches the optimal\nvalue.\n\u2022 Dual variable convergence.y\nk \u2192 y\u22c6 as k \u2192\u221e , where y\u22c6 is a\ndual optimal point.\nA proof of the residual and objective convergence results is given in\nappendix A. Note that xk and zk need not converge to optimal values,\nalthough such results can be shown under additional assumptions.\n3.2.2 Convergence in Practice\nSimple examples show that ADMM can be very slow to converge to\nhigh accuracy. However, it is often the case that ADMM converges to\nmodest accuracy\u2014su\ufb03cient for many applications\u2014within a few tens\nof iterations. This behavior makes ADMM similar to algorithms like\nthe conjugate gradient method, for example, in that a few tens of iter-\nations will often produce acceptable results of practical use. However,\nthe slow convergence of ADMM also distinguishes it from algorithms\nsuch as Newton\u2019s method (or, for constrained problems, interior-point\nmethods), where high accuracy can be attained in a reasonable amount\nof time. While in some cases it is possible to combine ADMM with\na method for producing a high accuracy solution from a low accu-\nracy solution [64], in the general case ADMM will be practically useful\nmostly in cases when modest accuracy is su\ufb03cient. Fortunately, this\nis usually the case for the kinds of large-scale problems we consider.\nAlso, in the case of statistical and machine learning problems, solving\na parameter estimation problem to very high accuracy often yields lit-\ntle to no improvement in actual prediction performance, the real metric\nof interest in applications.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1827, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b0de6215-749b-403e-a470-6f6b68f7ce5e": {"__data__": {"id_": "b0de6215-749b-403e-a470-6f6b68f7ce5e", "embedding": null, "metadata": {"page_label": "18", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3947ecce-c8b3-4082-ae15-cb5cdb942e1c", "node_type": "4", "metadata": {"page_label": "18", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "1969e68c643d41b454de3c20f0da8718b4bf96a530b6ae73a643cc516c0eb9a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "18 Alternating Direction Method of Multipliers\n3.3 Optimality Conditions and Stopping Criterion\nThe necessary and su\ufb03cient optimality conditions for the ADMM prob-\nlem (3.1) are primal feasibility,\nAx\u22c6 + Bz\u22c6 \u2212 c =0 , (3.8)\nand dual feasibility,\n0 \u2208 \u2202f(x\u22c6 )+ AT y\u22c6 (3.9)\n0 \u2208 \u2202g(z\u22c6 )+ BT y\u22c6 . (3.10)\nHere, \u2202 denotes the subdi\ufb00erential operator; see, e.g., [140, 19, 99].\n(When f and g are di\ufb00erentiable, the subdi\ufb00erentials \u2202f and \u2202g can\nbe replaced by the gradients\u2207f and \u2207g, and \u2208 can be replaced by =.)\nSince zk+1 minimizes L\u03c1(xk+1,z,y k) by de\ufb01nition, we have that\n0 \u2208 \u2202g(zk+1)+ BT yk + \u03c1BT (Axk+1 + Bzk+1 \u2212 c)\n= \u2202g(zk+1)+ BT yk + \u03c1BT rk+1\n= \u2202g(zk+1)+ BT yk+1.\nThis means that zk+1 and yk+1 always satisfy (3.10), so attaining opti-\nmality comes down to satisfying (3.8) and (3.9). This phenomenon is\nanalogous to the iterates of the method of multipliers always being dual\nfeasible; see page 11.\nSince x\nk+1 minimizes L\u03c1(x,zk,yk) by de\ufb01nition, we have that\n0 \u2208 \u2202f(xk+1)+ AT yk + \u03c1AT (Axk+1 + Bzk \u2212 c)\n= \u2202f(xk+1)+ AT (yk + \u03c1rk+1 + \u03c1B(zk \u2212 zk+1))\n= \u2202f(xk+1)+ AT yk+1 + \u03c1AT B(zk \u2212 zk+1),\nor equivalently,\n\u03c1AT B(zk+1 \u2212 zk) \u2208 \u2202f(xk+1)+ AT yk+1.\nThis means that the quantity\nsk+1 = \u03c1AT B(zk+1 \u2212 zk)\ncan be viewed as a residual for the dual feasibility condition (3.9).\nWe will refer to sk+1 as the dual residual at iteration k + 1, and to\nrk+1 = Axk+1 + Bzk+1 \u2212 c as the primal residual at iteration k +1 .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1403, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bd056e1b-08f1-4ba9-9a7f-a8f6cbaeda87": {"__data__": {"id_": "bd056e1b-08f1-4ba9-9a7f-a8f6cbaeda87", "embedding": null, "metadata": {"page_label": "19", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "120b9a6a-2cb6-4c6d-a97f-74212c30e337", "node_type": "4", "metadata": {"page_label": "19", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "03c5285d84fb619a09d1840c12fe5b0b303703050f7683734826b9c1885b5d83", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.3 Optimality Conditions and Stopping Criterion 19\nIn summary, the optimality conditions for the ADMM problem con-\nsist of three conditions, (3.8\u20133.10). The last condition (3.10) always\nholds for (x\nk+1,zk+1,yk+1); the residuals for the other two, (3.8) and\n(3.9), are the primal and dual residuals rk+1 and sk+1, respectively.\nThese two residuals converge to zero as ADMM proceeds. (In fact, the\nconvergence proof in appendix A showsB(z\nk+1 \u2212 zk) converges to zero,\nwhich implies sk converges to zero.)\n3.3.1 Stopping Criteria\nThe residuals of the optimality conditions can be related to a bound on\nthe objective suboptimality of the current point, i.e., f(xk)+ g(zk) \u2212\np\u22c6 . As shown in the convergence proof in appendix A, we have\nf(xk)+ g(zk) \u2212 p\u22c6 \u2264\u2212 (yk)T rk +( xk \u2212 x\u22c6 )T sk. (3.11)\nThis shows that when the residuals rk and sk are small, the objective\nsuboptimality also must be small. We cannot use this inequality directly\nin a stopping criterion, however, since we do not know x\n\u22c6 . But if we\nguess or estimate that \u2225xk \u2212 x\u22c6 \u22252 \u2264 d, we have that\nf(xk)+ g(zk) \u2212 p\u22c6 \u2264\u2212 (yk)T rk + d\u2225sk\u22252 \u2264\u2225 yk\u22252\u2225rk\u22252 + d\u2225sk\u22252.\nThe middle or righthand terms can be used as an approximate bound\non the objective suboptimality (which depends on our guess of d).\nThis suggests that a reasonable termination criterion is that the\nprimal and dual residuals must be small, i.e.,\n\u2225rk\u22252 \u2264 \u03f5pri and \u2225sk\u22252 \u2264 \u03f5dual, (3.12)\nwhere \u03f5pri > 0 and \u03f5dual > 0 are feasibility tolerances for the primal and\ndual feasibility conditions (3.8) and (3.9), respectively. These tolerances\ncan be chosen using an absolute and relative criterion, such as\n\u03f5\npri = \u221ap\u03f5 abs + \u03f5rel max{\u2225Axk\u22252,\u2225Bzk\u22252,\u2225c\u22252},\n\u03f5dual = \u221an\u03f5 abs + \u03f5rel\u2225AT yk\u22252,\nwhere \u03f5abs > 0 is an absolute tolerance and \u03f5rel > 0 is a relative toler-\nance. (The factors\u221ap and \u221an account for the fact that the\u21132 norms are\nin Rp and Rn, respectively.) A reasonable value for the relative stopping", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1914, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5d0730f4-a764-43e9-8501-dce3ed11d4d2": {"__data__": {"id_": "5d0730f4-a764-43e9-8501-dce3ed11d4d2", "embedding": null, "metadata": {"page_label": "20", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e916bb8a-c3a6-4624-b2f4-54763c967b89", "node_type": "4", "metadata": {"page_label": "20", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "567d69e74a93bc18e353ab25e82d45d33b65a2bb49fe929138c939fcbf27d27e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "20 Alternating Direction Method of Multipliers\ncriterion might be \u03f5rel =1 0\u22123 or 10\u22124, depending on the application.\nThe choice of absolute stopping criterion depends on the scale of the\ntypical variable values.\n3.4 Extensions and Variations\nMany variations on the classic ADMM algorithm have been explored in\nthe literature. Here we brie\ufb02y survey some of these variants, organized\ninto groups of related ideas. Some of these methods can give superior\nconvergence in practice compared to the standard ADMM presented\nabove. Most of the extensions have been rigorously analyzed, so the\nconvergence results described above are still valid (in some cases, under\nsome additional conditions).\n3.4.1 Varying Penalty Parameter\nA standard extension is to use possibly di\ufb00erent penalty parameters\u03c1\nk\nfor each iteration, with the goal of improving the convergence in prac-\ntice, as well as making performance less dependent on the initial choice\nof the penalty parameter. In the context of the method of multipliers,\nthis approach is analyzed in [142], where it is shown that superlinear\nconvergence may be achieved if \u03c1\nk \u2192\u221e . Though it can be di\ufb03cult to\nprove the convergence of ADMM when\u03c1 varies by iteration, the \ufb01xed-\n\u03c1 theory still applies if one just assumes that \u03c1 becomes \ufb01xed after a\n\ufb01nite number of iterations.\nA simple scheme that often works well is (see, e.g., [96, 169]):\n\u03c1k+1 :=\n\uf8f1\n\uf8f4\uf8f4\n\uf8f2\n\uf8f4\uf8f4\n\uf8f3\n\u03c4\nincr\u03c1k if \u2225rk\u22252 >\u00b5 \u2225sk\u22252\n\u03c1k/\u03c4decr if \u2225sk\u22252 >\u00b5 \u2225rk\u22252\n\u03c1k otherwise,\n(3.13)\nwhere \u00b5> 1, \u03c4incr > 1, and \u03c4decr > 1 are parameters. Typical choices\nmight be \u00b5 = 10 and \u03c4incr = \u03c4decr = 2. The idea behind this penalty\nparameter update is to try to keep the primal and dual residual norms\nwithin a factor of \u00b5 of one another as they both converge to zero.\nThe ADMM update equations suggest that large values of\u03c1 place a\nlarge penalty on violations of primal feasibility and so tend to produce", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1881, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "93559659-41b7-4fc8-ae7c-5f9972d6b9a2": {"__data__": {"id_": "93559659-41b7-4fc8-ae7c-5f9972d6b9a2", "embedding": null, "metadata": {"page_label": "21", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "65be8e2b-575d-4634-b303-5158d365fe84", "node_type": "4", "metadata": {"page_label": "21", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "226671591eeced618832c6fe113e38f70510d01d6c7a632fba36db58bb003598", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.4 Extensions and Variations 21\nsmall primal residuals. Conversely, the de\ufb01nition ofsk+1 suggests that\nsmall values of\u03c1 tend to reduce the dual residual, but at the expense of\nreducing the penalty on primal feasibility, which may result in a larger\nprimal residual. The adjustment scheme (3.13) in\ufb02ates\u03c1 by \u03c4\nincr when\nthe primal residual appears large compared to the dual residual, and\nde\ufb02ates \u03c1 by \u03c4\ndecr when the primal residual seems too small relative\nto the dual residual. This scheme may also be re\ufb01ned by taking into\naccount the relative magnitudes of \u03f5\npri and \u03f5dual.\nWhen a varying penalty parameter is used in the scaled form of\nADMM, the scaled dual variable uk =( 1/\u03c1)yk must also be rescaled\nafter updating \u03c1; for example, if \u03c1 is halved, uk should be doubled\nbefore proceeding.\n3.4.2 More General Augmenting Terms\nAnother idea is to allow for a di\ufb00erent penalty parameter for each\nconstraint, or more generally, to replace the quadratic term (\u03c1/2)\u2225r\u22252\n2\nwith (1/2)rT Pr , whereP is a symmetric positive de\ufb01nite matrix. When\nP is constant, we can interpret this generalized version of ADMM as\nstandard ADMM applied to a modi\ufb01ed initial problem with the equality\nconstraints r = 0 replaced with Fr = 0, where F\nT F = P.\n3.4.3 Over-relaxation\nIn the z- and y-updates, the quantity Axk+1 can be replaced with\n\u03b1kAxk+1 \u2212 (1 \u2212 \u03b1k)(Bzk \u2212 c),\nwhere \u03b1k \u2208 (0,2) is arelaxation parameter; when\u03b1k > 1, this technique\nis called over-relaxation, and when\u03b1k < 1, it is calledunder-relaxation.\nThis scheme is analyzed in [63], and experiments in [59, 64] suggest that\nover-relaxation with \u03b1\nk \u2208 [1.5,1.8] can improve convergence.\n3.4.4 Inexact Minimization\nADMM will converge even when the x- and z-minimization steps\nare not carried out exactly, provided certain suboptimality measures", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1786, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8a44d185-30d1-4bc6-986e-04a7eba7e313": {"__data__": {"id_": "8a44d185-30d1-4bc6-986e-04a7eba7e313", "embedding": null, "metadata": {"page_label": "22", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5639c14e-d399-4eb7-a1af-a4fd7e42bbbd", "node_type": "4", "metadata": {"page_label": "22", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "4b3ed4eca3160b37c386350475894c0caefd6185282449d5d6f6f571baf99f2a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "22 Alternating Direction Method of Multipliers\nin the minimizations satisfy an appropriate condition, such as being\nsummable. This result is due to Eckstein and Bertsekas [63], building\non earlier results by Gol\u2019shtein and Tret\u2019yakov [89]. This technique is\nimportant in situations where thex-o r z-updates are carried out using\nan iterative method; it allows us to solve the minimizations only approx-\nimately at \ufb01rst, and then more accurately as the iterations progress.\n3.4.5 Update Ordering\nSeveral variations on ADMM involve performing the x-, z-, and y-\nupdates in varying orders or multiple times. For example, we can divide\nthe variables into k blocks, and update each of them in turn, possibly\nmultiple times, before performing each dual variable update; see, e.g.,\n[146]. Carrying out multiple x- and z-updates before the y-update can\nbe interpreted as executing multiple Gauss-Seidel passes instead of just\none; if many sweeps are carried out before each dual update, the result-\ning algorithm is very close to the standard method of multipliers [17,\n\u00a73.4.4]. Another variation is to perform an additionaly-update between\nthe x- and z-update, with half the step length [17].\n3.4.6 Related Algorithms\nThere are also a number of other algorithms distinct from but inspired\nby ADMM. For instance, Fukushima [80] applies ADMM to a dual\nproblem formulation, yielding a \u2018dual ADMM\u2019 algorithm, which is\nshown in [65] to be equivalent to the \u2018primal Douglas-Rachford\u2019 method\ndiscussed in [57, \u00a73.5.6]. As another example, Zhu et al. [183] discuss\nvariations of distributed ADMM (discussed in \u00a77, \u00a78, and \u00a710) that\ncan cope with various complicating factors, such as noise in the mes-\nsages exchanged for the updates, or asynchronous updates, which can\nbe useful in a regime when some processors or subsystems randomly\nfail. There are also algorithms resembling a combination of ADMM\nand the proximal method of multipliers [141], rather than the standard\nmethod of multipliers; see, e.g., [33, 60]. Other representative publica-\ntions include [62, 143, 59, 147, 158, 159, 42].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2078, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "340b2b95-5b28-4067-976a-fd1d97e84f2e": {"__data__": {"id_": "340b2b95-5b28-4067-976a-fd1d97e84f2e", "embedding": null, "metadata": {"page_label": "23", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "74e4b992-1891-4633-ba1b-db568a01b6df", "node_type": "4", "metadata": {"page_label": "23", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "867b51ade86b059041936231fd9eef43d5d81323432a96fad46cba7bd65931fd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.5 Notes and References 23\n3.5 Notes and References\nADMM was originally proposed in the mid-1970s by Glowinski and\nMarrocco [86] and Gabay and Mercier [82]. There are a number of other\nimportant papers analyzing the properties of the algorithm, including\n[76, 81, 75, 87, 157, 80, 65, 33]. In particular, the convergence of ADMM\nhas been explored by many authors, including Gabay [81] and Eckstein\nand Bertsekas [63].\nADMM has also been applied to a number of statistical prob-\nlems, such as constrained sparse regression [18], sparse signal recov-\nery [70], image restoration and denoising [72, 154, 134], trace norm\nregularized least squares minimization [174], sparse inverse covari-\nance selection [178], the Dantzig selector [116], and support vector\nmachines [74], among others. For examples in signal processing, see\n[42, 40, 41, 150, 149] and the references therein.\nMany papers analyzing ADMM do so from the perspective of max-\nimal monotone operators [23, 141, 142, 63, 144]. Brie\ufb02y, a wide variety\nof problems can be posed as \ufb01nding a zero of a maximal monotone\noperator; for example, if f is closed, proper, and convex, then the sub-\ndi\ufb00erential operator \u2202f is maximal monotone, and \ufb01nding a zero of\u2202f\nis simply minimization off; such a minimization may implicitly contain\nconstraints if f is allowed to take the value +\u221e. Rockafellar\u2019s proximal\npoint algorithm [142] is a general method for \ufb01nding a zero of a max-\nimal monotone operator, and a wide variety of algorithms have been\nshown to be special cases, including proximal minimization (see \u00a74.1),\nthe method of multipliers, and ADMM. For a more detailed review of\nthe older literature, see [57, \u00a72].\nThe method of multipliers was shown to be a special case of the\nproximal point algorithm by Rockafellar [141]. Gabay [81] showed that\nADMM is a special case of a method called Douglas-Rachford split-\nting for monotone operators [53, 112], and Eckstein and Bertsekas\n[63] showed in turn that Douglas-Rachford splitting is a special case\nof the proximal point algorithm. (The variant of ADMM that per-\nforms an extra y-update between the x- and z-updates is equiva-\nlent to Peaceman-Rachford splitting [137, 112] instead, as shown by\nGlowinski and Le Tallec [87].) Using the same framework, Eckstein", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2267, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aa5fed15-a60b-4f14-97fa-66b3a2683635": {"__data__": {"id_": "aa5fed15-a60b-4f14-97fa-66b3a2683635", "embedding": null, "metadata": {"page_label": "24", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "39263651-0b90-4dba-8f01-d1be11ceff5f", "node_type": "4", "metadata": {"page_label": "24", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "36c1ad9148e19309e6e0f51a832c4019f7ae628cf11c6e1da2ae9b0725351330", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "24 Alternating Direction Method of Multipliers\nand Bertsekas [63] also showed the relationships between a number of\nother algorithms, such as Spingarn\u2019s method of partial inverses [153].\nLawrence and Spingarn [108] develop an alternative framework show-\ning that Douglas-Rachford splitting, hence ADMM, is a special case\nof the proximal point algorithm; Eckstein and Ferris [64] o\ufb00er a more\nrecent discussion explaining this approach.\nThe major importance of these results is that they allow the pow-\nerful convergence theory for the proximal point algorithm to apply\ndirectly to ADMM and other methods, and show that many of these\nalgorithms are essentially identical. (But note that our proof of con-\nvergence of the basic ADMM algorithm, given in appendix A, is self-\ncontained and does not rely on this abstract machinery.) Research on\noperator splitting methods and their relation to decomposition algo-\nrithms continues to this day [66, 67].\nA considerable body of recent research considers replacing the\nquadratic penalty term in the standard method of multipliers with a\nmore general deviation penalty, such as one derived from a Bregman\ndivergence [30, 58]; see [22] for background material. Unfortunately,\nthese generalizations do not appear to carry over in a straightforward\nmanner from non-decomposition augmented Lagrangian methods to\nADMM: There is currently no proof of convergence known for ADMM\nwith nonquadratic penalty terms.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1445, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "74c94af6-d202-4cfb-920b-f37847f73fa4": {"__data__": {"id_": "74c94af6-d202-4cfb-920b-f37847f73fa4", "embedding": null, "metadata": {"page_label": "25", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8bb33091-ca08-46bb-958a-b1fe79851bac", "node_type": "4", "metadata": {"page_label": "25", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "7741f7f60e83685ea4cac8bfaaaf7909af838051edc4ad3676790c7f0674e239", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4\nGeneral Patterns\nStructure in f, g, A, and B can often be exploited to carry out the\nx- and z-updates more e\ufb03ciently. Here we consider three general cases\nthat we will encounter repeatedly in the sequel: quadratic objective\nterms, separable objective and constraints, and smooth objective terms.\nOur discussion will be written for the x-update but applies to the z-\nupdate by symmetry. We express the x-update step as\nx\n+ = argmin\nx\n(\nf(x)+( \u03c1/2)\u2225Ax \u2212 v\u22252\n2\n)\n,\nwhere v = \u2212Bz + c \u2212 u is a known constant vector for the purposes of\nthe x-update.\n4.1 Proximity Operator\nFirst, consider the simple case where A = I, which appears frequently\nin the examples. Then the x-update is\nx+ = argmin\nx\n(\nf(x)+( \u03c1/2)\u2225x \u2212 v\u22252\n2\n)\n.\nAs a function of v, the righthand side is denoted proxf,\u03c1(v) and is\ncalled the proximity operator of f with penalty \u03c1 [127]. In variational\n25", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 862, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6a389593-16ee-4d41-9df2-d6f19a7fc207": {"__data__": {"id_": "6a389593-16ee-4d41-9df2-d6f19a7fc207", "embedding": null, "metadata": {"page_label": "26", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cdb06cdc-5247-405f-bbcf-b114ec1912f3", "node_type": "4", "metadata": {"page_label": "26", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "68561c8ae84a9e5f0cd840e526fefd3125b778cab5c308458508d7e8188620ea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "26 General Patterns\nanalysis,\n\u02dcf(v) = inf\nx\n(\nf(x)+( \u03c1/2)\u2225x \u2212 v\u22252\n2\n)\nis known as theMoreau envelopeor Moreau-Yosida regularizationof f,\nand is connected to the theory of the proximal point algorithm [144].\nThe x-minimization in the proximity operator is generally referred to\nas proximal minimization. While these observations do not by them-\nselves allow us to improve the e\ufb03ciency of ADMM, it does tie the\nx-minimization step to other well known ideas.\nWhen the function f is simple enough, thex-update (i.e., the prox-\nimity operator) can be evaluated analytically; see [41] for many exam-\nples. For instance, if f is the indicator function of a closed nonempty\nconvex set C, then the x-update is\nx\n+ = argmin\nx\n(\nf(x)+( \u03c1/2)\u2225x \u2212 v\u22252\n2\n)\n=\u03a0 C(v),\nwhere \u03a0C denotes projection (in the Euclidean norm) ontoC. This holds\nindependently of the choice of \u03c1. As an example, if f is the indicator\nfunction of the nonnegative orthantRn\n+,w eh a v ex+ =( v)+, the vector\nobtained by taking the nonnegative part of each component of v.\n4.2 Quadratic Objective Terms\nSuppose f is given by the (convex) quadratic function\nf(x)=( 1 /2)xT Px + qT x + r,\nwhere P \u2208 Sn\n+, the set of symmetric positive semide\ufb01nite n \u00d7 n matri-\nces. This includes the cases when f is linear or constant, by setting P,\nor both P and q, to zero. Then, assuming P + \u03c1AT A is invertible, x+\nis an a\ufb03ne function of v given by\nx+ =( P + \u03c1AT A)\u22121(\u03c1AT v \u2212 q). (4.1)\nIn other words, computing the x-update amounts to solving a linear\nsystem with positive de\ufb01nite coe\ufb03cient matrix P + \u03c1AT A and right-\nhand side\u03c1AT v \u2212 q. As we show below, an appropriate use of numerical\nlinear algebra can exploit this fact and substantially improve perfor-\nmance. For general background on numerical linear algebra, see [47] or\n[90]; see [20, appendix C] for a short overview of direct methods.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1838, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4f0b2333-72a2-4e70-9176-d9261008f955": {"__data__": {"id_": "4f0b2333-72a2-4e70-9176-d9261008f955", "embedding": null, "metadata": {"page_label": "27", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc4df5ae-574d-4fbb-9a35-07ae63fe45e9", "node_type": "4", "metadata": {"page_label": "27", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "cdc6289d9edf93a4a9f9157ff0acc853b7d15bb43b2a791eedb56d385613a427", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.2 Quadratic Objective Terms 27\n4.2.1 Direct Methods\nWe assume here that adirect methodis used to carry out thex-update;\nthe case when an iterative method is used is discussed in \u00a74.3. Direct\nmethods for solving a linear systemFx = g are based on \ufb01rst factoring\nF = F1F2 \u00b7\u00b7\u00b7 Fk into a product of simpler matrices, and then computing\nx = F\u22121b by solving a sequence of problems of the form Fizi = zi\u22121,\nwhere z1 = F\u22121\n1 g and x = zk. The solve step is sometimes also called\na back-solve. The computational cost of factorization and back-solve\noperations depends on the sparsity pattern and other properties of F.\nThe cost of solving Fx = g is the sum of the cost of factoring F and\nthe cost of the back-solve.\nIn our case, the coe\ufb03cient matrix is F = P + \u03c1AT A and the right-\nhand side is g = \u03c1AT v \u2212 q, where P \u2208 Sn\n+ and A \u2208 Rp\u00d7n. Suppose we\nexploit no structure in A or P, i.e., we use generic methods that work\nfor any matrix. We can form F = P + \u03c1AT A at a cost of O(pn2) \ufb02ops\n(\ufb02oating point operations). We then carry out a Cholesky factorization\nof F at a cost of O(n\n3) \ufb02ops; the back-solve cost isO(n2). (The cost of\nforming g is negligible compared to the costs listed above.) When p is\non the order of, or more than n, the overall cost is O(pn2). (When p is\nless than n in order, the matrix inversion lemma described below can\nbe used to carry out the update in O(p2n) \ufb02ops.)\n4.2.2 Exploiting Sparsity\nWhen A and P are such that F is sparse (i.e., has enough zero entries\nto be worth exploiting), much more e\ufb03cient factorization and back-\nsolve routines can be employed. As an extreme case, if P and A are\ndiagonal n \u00d7 n matrices, then both the factor and solve costs are\nO(n). If P and A are banded, then so is F.I f F is banded with\nbandwidth k, the factorization cost is O(nk\n2) and the back-solve cost\nis O(nk). In this case, the x-update can be carried out at a cost\nO(nk2), plus the cost of forming F. The same approach works when\nP + \u03c1AT A has a more general sparsity pattern; in this case, a permuted\nCholesky factorization can be used, with permutations chosen to reduce\n\ufb01ll-in.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2097, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "09f3d72a-83dc-4328-bddd-ab2bd989623c": {"__data__": {"id_": "09f3d72a-83dc-4328-bddd-ab2bd989623c", "embedding": null, "metadata": {"page_label": "28", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6951bad1-e967-4bea-ba56-06374af12b22", "node_type": "4", "metadata": {"page_label": "28", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "857e42a0227cd42c27d450c8d87bf48c76bfad303a6bcec7635cf6ecef30fd74", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "28 General Patterns\n4.2.3 Caching Factorizations\nNow suppose we need to solve multiple linear systems, say,Fx(i) = g(i),\ni =1 ,...,N , with the same coe\ufb03cient matrix but di\ufb00erent righthand\nsides. This occurs in ADMM when the parameter \u03c1 is not changed. In\nthis case, the factorization of the coe\ufb03cient matrixF can be computed\nonce and then back-solves can be carried out for each righthand side.\nIf t is the factorization cost and s is the back-solve cost, then the total\ncost becomes t + Ns instead of N(t + s), which would be the cost if\nwe were to factor F each iteration. As long as \u03c1 does not change, we\ncan factor P + \u03c1A\nT A once, and then use this cached factorization in\nsubsequent solve steps. For example, if we do not exploit any structure\nand use the standard Cholesky factorization, the x-update steps can\nbe carried out a factor n more e\ufb03ciently than a naive implementation,\nin which we solve the equations from scratch in each iteration.\nWhen structure is exploited, the ratio between t and s is typically\nless than n but often still signi\ufb01cant, so here too there are performance\ngains. However, in this case, there is less bene\ufb01t to \u03c1 not changing, so\nwe can weigh the bene\ufb01t of varying\u03c1 against the bene\ufb01t of not recom-\nputing the factorization of P + \u03c1A\nT A. In general, an implementation\nshould cache the factorization of P + \u03c1AT A and then only recompute\nit if and when \u03c1 changes.\n4.2.4 Matrix Inversion Lemma\nWe can also exploit structure using thematrix inversion lemma, which\nstates that the identity\n(P + \u03c1AT A)\u22121 = P\u22121 \u2212 \u03c1P\u22121AT (I + \u03c1AP\u22121AT )\u22121AP\u22121\nholds when all the inverses exist. This means that if linear systems\nwith coe\ufb03cient matrix P can be solved e\ufb03ciently, and p is small, or\nat least no larger than n in order, then the x-update can be computed\ne\ufb03ciently as well. The same trick as above can also be used to obtain\nan e\ufb03cient method for computing multiple updates: The factorization\nof I + \u03c1AP\n\u22121AT can be cached and cheaper back-solves can be used\nin computing the updates.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2010, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "12cb3e97-6ecb-4147-b3cd-37ec6277c76e": {"__data__": {"id_": "12cb3e97-6ecb-4147-b3cd-37ec6277c76e", "embedding": null, "metadata": {"page_label": "29", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "84f6a62c-c321-476d-84c1-c03c49274070", "node_type": "4", "metadata": {"page_label": "29", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "0fae1ce30082a5784aa305e2277f7823edb5286508e2125cdd7c2f17a3f0bdd5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.2 Quadratic Objective Terms 29\nAs an example, suppose that P is diagonal and that p \u2264 n. A naive\nmethod for computing the update costs O(n3) \ufb02ops in the \ufb01rst itera-\ntion and O(n2) \ufb02ops in subsequent iterations, if we store the factors of\nP + \u03c1AT A. Using the matrix inversion lemma (i.e., using the righthand\nside above) to compute the x-update costs O(np2) \ufb02ops, an improve-\nment by a factor of ( n/p)2 over the naive method. In this case, the\ndominant cost is forming AP\u22121AT . The factors of I + \u03c1AP\u22121AT can\nbe saved after the \ufb01rst update, so subsequent iterations can be car-\nried out at cost O(np) \ufb02ops, a savings of a factor of p over the \ufb01rst\nupdate.\nUsing the matrix inversion lemma to compute x\n+ can also make\nit less costly to vary \u03c1 in each iteration. When P is diagonal, for\nexample, we can compute AP\u22121AT once, and then form and factor\nI + \u03c1kAP\u22121AT in iteration k at a cost of O(p3) \ufb02ops. In other words,\nthe update costs an additionalO(np) \ufb02ops, so ifp2 is less than or equal\nto n in order, there is no additional cost (in order) to carrying out\nupdates with \u03c1 varying in each iteration.\n4.2.5 Quadratic Function Restricted to an A\ufb03ne Set\nThe same comments hold for the slightly more complex case of a convex\nquadratic function restricted to an a\ufb03ne set:\nf(x)=( 1 /2)xT Px + qT x + r, domf = {x | Fx = g}.\nHere, x+ is still an a\ufb03ne function ofv, and the update involves solving\nthe KKT (Karush-Kuhn-Tucker) system\n[ P + \u03c1I F T\nF 0\n][xk+1\n\u03bd\n]\n+\n[ q \u2212 \u03c1(zk \u2212 uk)\n\u2212g\n]\n=0 .\nAll of the comments above hold here as well: Factorizations can be\ncached to carry out additional updates more e\ufb03ciently, and structure\nin the matrices can be exploited to improve the e\ufb03ciency of the factor-\nization and back-solve steps.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1724, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b1d2bc2e-054f-4ea9-b8f5-d768700e39ae": {"__data__": {"id_": "b1d2bc2e-054f-4ea9-b8f5-d768700e39ae", "embedding": null, "metadata": {"page_label": "30", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "82084c07-7813-466b-ac59-f31939648ddc", "node_type": "4", "metadata": {"page_label": "30", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "ebb1b3d3f7cdc9767f33564a7c487ffbc2541a5118f010d1b93a86a7b59ab59e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "30 General Patterns\n4.3 Smooth Objective Terms\n4.3.1 Iterative Solvers\nWhen f is smooth, general iterative methods can be used to carry\nout the x-minimization step. Of particular interest are methods that\nonly require the ability to compute \u2207f(x) for a given x, to multiply a\nvector by A, and to multiply a vector by AT . Such methods can scale\nto relatively large problems. Examples include the standard gradient\nmethod, the (nonlinear) conjugate gradient method, and the limited-\nmemory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) algorithm [113,\n26]; see [135] for further details.\nThe convergence of these methods depends on the conditioning of\nthe function to be minimized. The presence of the quadratic penalty\nterm (\u03c1/2)\u2225Ax \u2212 v\u2225\n2\n2 tends to improve the conditioning of the problem\nand thus improve the performance of an iterative method for updating\nx. Indeed, one method for adjusting the parameter \u03c1 from iteration to\niteration is to increase it until the iterative method used to carry out\nthe updates converges quickly enough.\n4.3.2 Early Termination\nA standard technique to speed up the algorithm is to terminate the\niterative method used to carry out the x-update (or z-update) early,\ni.e., before the gradient of f(x)+( \u03c1/2)\u2225Ax \u2212 v\u2225\n2\n2 is very small. This\ntechnique is justi\ufb01ed by the convergence results for ADMM with inexact\nminimization in the x- and z-update steps. In this case, the required\naccuracy should be low in the initial iterations of ADMM and then\nrepeatedly increased in each iteration. Early termination in the x-o r\nz-updates can result in more ADMM iterations, but much lower cost\nper iteration, giving an overall improvement in e\ufb03ciency.\n4.3.3 Warm Start\nAnother standard trick is to initialize the iterative method used in\nthe x-update at the solution x\nk obtained in the previous iteration.\nThis is called a warm start. The previous ADMM iterate often gives\na good enough approximation to result in far fewer iterations (of the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1963, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b851b373-85f2-4f1b-a519-55d7e5a25478": {"__data__": {"id_": "b851b373-85f2-4f1b-a519-55d7e5a25478", "embedding": null, "metadata": {"page_label": "31", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b59759f-8749-412e-9c09-aec7260d6490", "node_type": "4", "metadata": {"page_label": "31", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "44b254a64b81aefad06a7cf0ce2d0e1bf51a83e0b106889662a43d0cd1f67fa9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.4 Decomposition 31\niterative method used to compute the updatexk+1) than if the iterative\nmethod were started at zero or some other default initialization. This\nis especially the case when ADMM has almost converged, in which case\nthe updates will not change signi\ufb01cantly from their previous values.\n4.3.4 Quadratic Objective Terms\nEven when f is quadratic, it may be worth using an iterative method\nrather than a direct method for the x-update. In this case, we can use\na standard (possibly preconditioned) conjugate gradient method. This\napproach makes sense when direct methods do not work (e.g., because\nthey require too much memory) or when A is dense but a fast method\nis available for multiplying a vector by A or A\nT . This is the case, for\nexample, when A represents the discrete Fourier transform [90].\n4.4 Decomposition\n4.4.1 Block Separability\nSuppose x =( x1,...,x N ) is a partition of the variablex into subvectors\nand that f is separable with respect to this partition, i.e.,\nf(x)= f1(x1)+ \u00b7\u00b7\u00b7 + fN (xN ),\nwhere xi \u2208 Rni and \u2211 N\ni=1 ni = N. If the quadratic term \u2225Ax\u22252\n2 is also\nseparable with respect to the partition, i.e., AT A is block diagonal\nconformably with the partition, then the augmented Lagrangian L\u03c1 is\nseparable. This means that thex-update can be carried out in parallel,\nwith the subvectors xi updated by N separate minimizations.\n4.4.2 Component Separability\nIn some cases, the decomposition extends all the way to individual\ncomponents of x, i.e.,\nf(x)= f1(x1)+ \u00b7\u00b7\u00b7 + fn(xn),\nwhere fi : R \u2192 R, and AT A is diagonal. The x-minimization step can\nthen be carried out via n scalar minimizations, which can in some\ncases be expressed analytically (but in any case can be computed very\ne\ufb03ciently). We will call this component separability.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1769, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "29d88efd-250d-4691-a2fc-0a50e7d0c600": {"__data__": {"id_": "29d88efd-250d-4691-a2fc-0a50e7d0c600", "embedding": null, "metadata": {"page_label": "32", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d1ff5289-b125-4279-ae7d-f25aa654d9c1", "node_type": "4", "metadata": {"page_label": "32", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "5b676317ce1aff5e28001a2d99ccefa054ff3fdc02e3367b73e143aa0e7df332", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "32 General Patterns\n4.4.3 Soft Thresholding\nFor an example that will come up often in the sequel, consider f(x)=\n\u03bb\u2225x\u22251 (with \u03bb> 0) and A = I. In this case the (scalar) xi-update is\nx+\ni := argmin\nxi\n(\n\u03bb|xi| +( \u03c1/2)(xi \u2212 vi)2)\n.\nEven though the \ufb01rst term is not di\ufb00erentiable, we can easily compute\na simple closed-form solution to this problem by using subdi\ufb00erential\ncalculus; see [140, \u00a723] for background. Explicitly, the solution is\nx\n+\ni := S\u03bb/\u03c1(vi),\nwhere the soft thresholding operator S is de\ufb01ned as\nS\u03ba(a)=\n\uf8f1\n\uf8f4\uf8f4\n\uf8f2\n\uf8f4\uf8f4\n\uf8f3\na \u2212 \u03ba a>\u03ba\n0 |a|\u2264 \u03ba\na + \u03baa < \u2212\u03ba,\nor equivalently,\nS\n\u03ba(a)=( a \u2212 \u03ba)+ \u2212 (\u2212a \u2212 \u03ba)+.\nYet another formula, which shows that the soft thresholding operator\nis a shrinkage operator (i.e., moves a point toward zero), is\nS\u03ba(a)=( 1 \u2212 \u03ba/|a|)+a (4.2)\n(for a \u0338= 0). We refer to updates that reduce to this form as element-\nwise soft thresholding. In the language of \u00a74.1, soft thresholding is the\nproximity operator of the \u21131 norm.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 943, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1402c676-c4f4-4e9e-861f-a9c638e04fcb": {"__data__": {"id_": "1402c676-c4f4-4e9e-861f-a9c638e04fcb", "embedding": null, "metadata": {"page_label": "33", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "46430440-97d6-44cb-b6e1-30f0be686743", "node_type": "4", "metadata": {"page_label": "33", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "4e84bc1cdc15ce6c97b15a12e7495249247ea93f92011a5d80ac914c360b48b9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5\nConstrained Convex Optimization\nThe generic constrained convex optimization problem is\nminimize f(x)\nsubject to x \u2208C , (5.1)\nwith variable x \u2208 Rn, where f and C are convex. This problem can be\nrewritten in ADMM form (3.1) as\nminimize f(x)+ g(z)\nsubject to x \u2212 z =0 ,\nwhere g is the indicator function of C.\nThe augmented Lagrangian (using the scaled dual variable) is\nL\u03c1(x,z,u )= f(x)+ g(z)+( \u03c1/2)\u2225x \u2212 z + u\u22252\n2,\nso the scaled form of ADMM for this problem is\nxk+1 := argmin\nx\n(\nf(x)+( \u03c1/2)\u2225x \u2212 zk + uk\u22252\n2\n)\nzk+1 := \u03a0C(xk+1 + uk)\nuk+1 := uk + xk+1 \u2212 zk+1.\n33", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 561, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2a75f063-ea78-4686-9b6f-44cb41361b75": {"__data__": {"id_": "2a75f063-ea78-4686-9b6f-44cb41361b75", "embedding": null, "metadata": {"page_label": "34", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7e5136e0-957e-4acd-9c39-81b48e3aa2be", "node_type": "4", "metadata": {"page_label": "34", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "e1738db25e459b714e8428411bcea74f7ccd8a1ccfc3c60d735f02775996dc15", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "34 Constrained Convex Optimization\nThe x-update involves minimizing f plus a convex quadratic function,\ni.e., evaluation of the proximal operator associated with f. The z-\nupdate is Euclidean projection onto C. The objective f need not be\nsmooth here; indeed, we can include additional constraints (i.e., beyond\nthose represented byx \u2208C ) by de\ufb01ningf to be +\u221e where they are vio-\nlated. In this case, the x-update becomes a constrained optimization\nproblem over domf = {x | f(x) < \u221e}.\nAs with all problems where the constraint is x \u2212 z = 0, the primal\nand dual residuals take the simple form\nrk = xk \u2212 zk,s k = \u2212\u03c1(zk \u2212 zk\u22121).\nIn the following sections we give some more speci\ufb01c examples.\n5.1 Convex Feasibility\n5.1.1 Alternating Projections\nA classic problem is to \ufb01nd a point in the intersection of two closed\nnonempty convex sets. The classical method, which dates back to the\n1930s, is von Neumann\u2019salternating projectionsalgorithm [166, 36, 21]:\nx\nk+1 := \u03a0C(zk)\nzk+1 := \u03a0D(xk+1),\nwhere \u03a0C and \u03a0D are Euclidean projection onto the sets C and D,\nrespectively.\nIn ADMM form, the problem can be written as\nminimize f(x)+ g(z)\nsubject to x \u2212 z =0 ,\nwhere f is the indicator function of C and g is the indicator function\nof D. The scaled form of ADMM is then\nxk+1 := \u03a0C(zk \u2212 uk)\nzk+1 := \u03a0D(xk+1 + uk) (5.2)\nuk+1 := uk + xk+1 \u2212 zk+1,\nso both the x and z steps involve projection onto a convex set, as in\nthe classical method. This is exactly Dykstra\u2019s alternating projections", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1472, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d933a600-b9bf-4034-8f80-99dba27f274c": {"__data__": {"id_": "d933a600-b9bf-4034-8f80-99dba27f274c", "embedding": null, "metadata": {"page_label": "35", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5691d812-406c-4a90-b6f4-d14b890dfed4", "node_type": "4", "metadata": {"page_label": "35", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "2446a16eff93bddd79655330f18868afd0e459d7743f547b95dcc95dd94c7ea7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5.1 Convex Feasibility 35\nmethod [56, 9], which is far more e\ufb03cient than the classical method\nthat does not use the dual variable u.\nHere, the norm of the primal residual \u2225xk \u2212 zk\u22252 has a nice inter-\npretation. Since xk \u2208C and zk \u2208D , \u2225xk \u2212 zk\u22252 is an upper bound on\ndist(C,D), the Euclidean distance between C and D. If we terminate\nwith \u2225rk\u22252 \u2264 \u03f5pri, then we have found a pair of points, one in C and\none in D, that are no more than \u03f5pri far apart. Alternatively, the point\n(1/2)(xk + zk) is no more than a distance \u03f5pri/2 from both C and D.\n5.1.2 Parallel Projections\nThis method can be applied to the problem of \ufb01nding a point in the\nintersection of N closed convex sets A1,..., AN in Rn by running the\nalgorithm in RnN with\nC = A1 \u00d7 \u00b7\u00b7\u00b7 \u00d7 AN , D = {(x1,...,x N ) \u2208 RnN | x1 = x2 = \u00b7\u00b7\u00b7 = xN }.\nIf x =( x1,...,x N ) \u2208 RnN , then projection onto C is\n\u03a0C(x)=( \u03a0 A1 (x1),..., \u03a0AN (xN )),\nand projection onto D is\n\u03a0D(x)=( x,x,..., x),\nwhere x =( 1/N)\u2211 N\ni=1 xi is the average of x1,...,x N . Thus, each step\nof ADMM can be carried out by projecting points onto each of the sets\nA\ni in parallel and then averaging the results:\nxk+1\ni := \u03a0Ai(zk \u2212 uk\ni )\nzk+1 := 1\nN\nN\u2211\ni=1\n(xk+1\ni + uk\ni )\nuk+1\ni := uk\ni + xk+1\ni \u2212 zk+1.\nHere the \ufb01rst and third steps are carried out in parallel, fori =1 ,...,N .\n(The description above involves a small abuse of notation in dropping\nthe index i from z\ni, since they are all the same.) This can be viewed as a\nspecial case of constrained optimization, as described in\u00a74.4, where the\nindicator function of A1 \u2229 \u00b7\u00b7\u00b7 \u2229 AN splits into the sum of the indicator\nfunctions of each Ai.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1608, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "348b03b8-f316-4d09-8165-c3fb7ca0ff81": {"__data__": {"id_": "348b03b8-f316-4d09-8165-c3fb7ca0ff81", "embedding": null, "metadata": {"page_label": "36", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0446435b-2869-43da-94ca-11e81ed20e45", "node_type": "4", "metadata": {"page_label": "36", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "30581976b063000d64d260331607ff2828ff5542e18c66efeed37043762b0133", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "36 Constrained Convex Optimization\nWe note for later reference a simpli\ufb01cation of the ADMM algorithm\nabove. Taking the average (overi) of the last equation we obtain\nuk+1 = uk + xk+1 \u2212 zk,\ncombined with zk+1 = xk+1 + uk (from the second equation) we see\nthat uk+1 = 0. So after the \ufb01rst step, the average of ui is zero. This\nmeans that zk+1 reduces to xk+1. Using these simpli\ufb01cations, we arrive\nat the simple algorithm\nxk+1\ni := \u03a0Ai(xk \u2212 uk\ni )\nuk+1\ni := uk\ni +( xk+1\ni \u2212 xk+1).\nThis shows thatuk\ni is the running sum of the the \u2018discrepancies\u2019xk\ni \u2212 xk\n(assuming u0 = 0). The \ufb01rst step is a parallel projection onto the sets\nCi; the second involves averaging the projected points.\nThere is a large literature on successive projection algorithms and\ntheir many applications; see the survey by Bauschke and Borwein [10]\nfor a general overview, Combettes [39] for applications to image pro-\ncessing, and Censor and Zenios [31, \u00a75] for a discussion in the context\nof parallel optimization.\n5.2 Linear and Quadratic Programming\nThe standard form quadratic program (QP) is\nminimize (1 /2)xT Px + qT x\nsubject to Ax = b, x \u2265 0, (5.3)\nwith variable x \u2208 Rn; we assume that P \u2208 Sn\n+. When P = 0, this\nreduces to the standard form linear program (LP).\nWe express it in ADMM form as\nminimize f(x)+ g(z)\nsubject to x \u2212 z =0 ,\nwhere\nf(x)=( 1 /2)xT Px + qT x, domf = {x | Ax = b}\nis the original objective with restricted domain and g is the indicator\nfunction of the nonnegative orthant Rn\n+.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1480, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c6faea92-87b0-4935-ba16-e7ff1e516fb6": {"__data__": {"id_": "c6faea92-87b0-4935-ba16-e7ff1e516fb6", "embedding": null, "metadata": {"page_label": "37", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2b346bf3-5e5e-490c-9356-87ea13d9fadd", "node_type": "4", "metadata": {"page_label": "37", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "fd3165e2dab3bd0e1565783118e328713ce08f7eedf51c4c6b8533447487663b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5.2 Linear and Quadratic Programming 37\nThe scaled form of ADMM consists of the iterations\nxk+1 := argmin\nx\n(\nf(x)+( \u03c1/2)\u2225x \u2212 zk + uk\u22252\n2\n)\nzk+1 := (xk+1 + uk)+\nuk+1 := uk + xk+1 \u2212 zk+1.\nAs described in \u00a74.2.5, the x-update is an equality-constrained least\nsquares problem with optimality conditions\n[ P + \u03c1I A T\nA 0\n][xk+1\n\u03bd\n]\n+\n[ q \u2212 \u03c1(zk \u2212 uk)\n\u2212b\n]\n=0 .\nAll of the comments on e\ufb03cient computation from\u00a74.2, such as storing\nfactorizations so that subsequent iterations can be carried out cheaply,\nalso apply here. For example, if P is diagonal, possibly zero, the \ufb01rst\nx-update can be carried out at a cost of O(np\n2) \ufb02ops, where p is the\nnumber of equality constraints in the original quadratic program. Sub-\nsequent updates only cost O(np) \ufb02ops.\n5.2.1 Linear and Quadratic Cone Programming\nMore generally, any conic constraintx \u2208K can be used in place of the\nconstraint x \u2265 0, in which case the standard quadratic program (5.3)\nbecomes a quadratic conic program. The only change to ADMM is in\nthe z-update, which then involves projection onto K. For example, we\ncan solve a semide\ufb01nite program with x \u2208 S\nn\n+ by projecting xk+1 + uk\nonto Sn\n+ using an eigenvalue decomposition.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1181, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c08e923f-1a5e-415f-8b95-96ac2f175014": {"__data__": {"id_": "c08e923f-1a5e-415f-8b95-96ac2f175014", "embedding": null, "metadata": {"page_label": "38", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0c4b21e2-ab4c-42fd-ad6a-145261c6023b", "node_type": "4", "metadata": {"page_label": "38", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "ef7b580d01cb404b41ca9ec84f24d77eea7300376101612328792aad37ea7ae6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6\n\u21131-Norm Problems\nThe problems addressed in this section will help illustrate why ADMM\nis a natural \ufb01t for machine learning and statistical problems in particu-\nlar. The reason is that, unlike dual ascent or the method of multipliers,\nADMM explicitly targets problems that split into two distinct parts,f\nand g, that can then be handled separately. Problems of this form are\npervasive in machine learning, because a signi\ufb01cant number of learning\nproblems involve minimizing a loss function together with a regulariza-\ntion term or side constraints. In other cases, these side constraints are\nintroduced through problem transformations like putting the problem\nin consensus form, as will be discussed in \u00a77.1.\nThis section contains a variety of simple but important problems\ninvolving\u2113\n1 norms. There is widespread current interest in many of these\nproblems across statistics, machine learning, and signal processing, and\napplying ADMM yields interesting algorithms that are state-of-the-art,\nor closely related to state-of-the-art methods. We will see that ADMM\nnaturally lets us decouple the nonsmooth\u2113\n1 term from the smooth loss\nterm, which is computationally advantageous. In this section, we focus on\nthe non-distributed versions of these problems for simplicity; the problem\nof distributed model \ufb01tting will be treated in the sequel.\n38", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1343, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f5d9629d-4e4f-4a64-ad76-83a8fb9b6e21": {"__data__": {"id_": "f5d9629d-4e4f-4a64-ad76-83a8fb9b6e21", "embedding": null, "metadata": {"page_label": "39", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "784875a5-bcad-4d00-bca6-cf90a03b8861", "node_type": "4", "metadata": {"page_label": "39", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "371453ddecfdf3d3c3da10fd1e156fede5a622f722c8d362570b424de215b627", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6.1 Least Absolute Deviations 39\nThe idea of \u21131 regularization is decades old, and traces back to\nHuber\u2019s [100] work on robust statistics and a paper of Claerbout [38]\nin geophysics. There is a vast literature, but some important modern\npapers are those on total variation denoising [145], soft thresholding\n[49], the lasso [156], basis pursuit [34], compressed sensing [50, 28, 29],\nand structure learning of sparse graphical models [123].\nBecause of the now widespread use of models incorporating an \u2113\n1\npenalty, there has also been considerable research on optimization algo-\nrithms for such problems. A recent survey by Yang et al. [173] com-\npares and benchmarks a number of representative algorithms, includ-\ning gradient projection [73, 102], homotopy methods [52], iterative\nshrinkage-thresholding [45], proximal gradient [132, 133, 11, 12], aug-\nmented Lagrangian methods [175], and interior-point methods [103].\nThere are other approaches as well, such as Bregman iterative algo-\nrithms [176] and iterative thresholding algorithms [51] implementable\nin a message-passing framework.\n6.1 Least Absolute Deviations\nA simple variant on least squares \ufb01tting is least absolute deviations,\nin which we minimize \u2225Ax \u2212 b\u22251 instead of \u2225Ax \u2212 b\u22252\n2. Least absolute\ndeviations provides a more robust \ufb01t than least squares when the data\ncontains large outliers, and has been used extensively in statistics and\neconometrics. See, for example, [95, \u00a710.6], [171, \u00a79.6], and [20, \u00a76.1.2].\nIn ADMM form, the problem can be written as\nminimize \u2225z\u2225\n1\nsubject to Ax \u2212 z = b,\nso f = 0 and g = \u2225\u00b7\u2225 1. Exploiting the special form of f and g, and\nassuming AT A is invertible, ADMM can be expressed as\nxk+1 := (AT A)\u22121AT (b + zk \u2212 uk)\nzk+1 := S1/\u03c1(Axk+1 \u2212 b + uk)\nuk+1 := uk + Axk+1 \u2212 zk+1 \u2212 b,\nwhere the soft thresholding operator is interpreted elementwise. As in\n\u00a74.2, the matrix AT A can be factored once; the factors are then used\nin cheaper back-solves in subsequent x-updates.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1967, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ed302f67-ff74-4d01-8549-fdc1e7817f48": {"__data__": {"id_": "ed302f67-ff74-4d01-8549-fdc1e7817f48", "embedding": null, "metadata": {"page_label": "40", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d642a07d-31db-430c-b5ad-22309d3056c9", "node_type": "4", "metadata": {"page_label": "40", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "28967c75293714ae9e4c1193e9cd39732d45ab63f92dc59dea3b3d0415b72914", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "40 \u21131-Norm Problems\nThe x-update step is the same as carrying out a least squares \ufb01t\nwith coe\ufb03cient matrixA and righthand sideb + zk \u2212 uk. Thus ADMM\ncan be interpreted as a method for solving a least absolute deviations\nproblem by iteratively solving the associated least squares problem with\na modi\ufb01ed righthand side; the modi\ufb01cation is then updated using soft\nthresholding. With factorization caching, the cost of subsequent least\nsquares iterations is much smaller than the initial one, often making\nthe time required to carry out least absolute deviations very nearly the\nsame as the time required to carry out least squares.\n6.1.1 Huber Fitting\nA problem that lies in between least squares and least absolute devia-\ntions is Huber function \ufb01tting,\nminimize g\nhub(Ax \u2212 b),\nwhere the Huber penalty functionghub is quadratic for small arguments\nand transitions to an absolute value for larger values. For scalar a,i t\nis given by\nghub(a)=\n{\na2/2 |a|\u2264 1\n|a|\u2212 1/2 |a| > 1\nand extends to vector arguments as the sum of the Huber functions\nof the components. (For simplicity, we consider the standard Huber\nfunction, which transitions from quadratic to absolute value at the\nlevel 1.)\nThis can be put into ADMM form as above, and the ADMM algo-\nrithm is the same except that thez-update involves the proximity oper-\nator of the Huber function rather than that of the \u2113\n1 norm:\nzk+1 := \u03c1\n1+ \u03c1\n(\nAxk+1 \u2212 b + uk\n)\n+ 1\n1+ \u03c1S1+1/\u03c1(Axk+1 \u2212 b + uk).\nWhen the least squares \ufb01t xls =( AT A)\u22121b satis\ufb01es |xls\ni |\u2264 1 for all i,i t\nis also the Huber \ufb01t. In this case, ADMM terminates in two steps.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1583, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "62530c90-823e-45b5-8ae8-2dd1729084bd": {"__data__": {"id_": "62530c90-823e-45b5-8ae8-2dd1729084bd", "embedding": null, "metadata": {"page_label": "41", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8136704-eb51-4908-b0bb-4ea95f5bea8c", "node_type": "4", "metadata": {"page_label": "41", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "c8daba564c781bd811fa1c23967265644d6791612a2da1702f3d4a99a3447ce5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6.2 Basis Pursuit 41\n6.2 Basis Pursuit\nBasis pursuit is the equality-constrained \u21131 minimization problem\nminimize \u2225x\u22251\nsubject to Ax = b,\nwith variable x \u2208 Rn, data A \u2208 Rm\u00d7n, b \u2208 Rm, with m<n . Basis pur-\nsuit is often used as a heuristic for \ufb01nding a sparse solution to an\nunderdetermined system of linear equations. It plays a central role in\nmodern statistical signal processing, particularly the theory of com-\npressed sensing; see [24] for a recent survey.\nIn ADMM form, basis pursuit can be written as\nminimize f(x)+ \u2225z\u2225\n1\nsubject to x \u2212 z =0 ,\nwhere f is the indicator function of {x \u2208 Rn | Ax = b}. The ADMM\nalgorithm is then\nxk+1 := \u03a0(zk \u2212 uk)\nzk+1 := S1/\u03c1(xk+1 + uk)\nuk+1 := uk + xk+1 \u2212 zk+1,\nwhere \u03a0 is projection onto {x \u2208 Rn | Ax = b}. The x-update, which\ninvolves solving a linearly-constrained minimum Euclidean norm prob-\nlem, can be written explicitly as\nx\nk+1 := (I \u2212 AT (AAT )\u22121A)(zk \u2212 uk)+ AT (AAT )\u22121b.\nAgain, the comments on e\ufb03cient computation from \u00a74.2 apply; by\ncaching a factorization of AAT , subsequent projections are much\ncheaper than the \ufb01rst one. We can interpret ADMM for basis pur-\nsuit as reducing the solution of a least \u2113\n1 norm problem to solving a\nsequence of minimum Euclidean norm problems. For a discussion of\nsimilar algorithms for related problems in image processing, see [2].\nA recent class of algorithms called Bregman iterative methods have\nattracted considerable interest for solving\u2113\n1 problems like basis pursuit.\nFor basis pursuit and related problems,Bregman iterative regularization\n[176] is equivalent to the method of multipliers, and thesplit Bregman\nmethod [88] is equivalent to ADMM [68].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1647, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "38834211-5145-4b56-9c04-9fb00f303606": {"__data__": {"id_": "38834211-5145-4b56-9c04-9fb00f303606", "embedding": null, "metadata": {"page_label": "42", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f22f944c-8fd3-4a9d-864b-72e74a5e44d1", "node_type": "4", "metadata": {"page_label": "42", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "cf27511ea6e8517a9215091edd399ffe11eea2e966a982bb57dfe1884bad188b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "42 \u21131-Norm Problems\n6.3 General \u21131 Regularized Loss Minimization\nConsider the generic problem\nminimize l(x)+ \u03bb\u2225x\u22251, (6.1)\nwhere l is any convex loss function.\nIn ADMM form, this problem can be written as\nminimize l(x)+ g(z)\nsubject to x \u2212 z =0 ,\nwhere g(z)= \u03bb\u2225z\u22251. The algorithm is\nxk+1 := argmin\nx\n(\nl(x)+( \u03c1/2)\u2225x \u2212 zk + uk\u22252\n2\n)\nzk+1 := S\u03bb/\u03c1(xk+1 + uk)\nuk+1 := uk + xk+1 \u2212 zk+1.\nThe x-update is a proximal operator evaluation. Ifl is smooth, this can\nbe done by any standard method, such as Newton\u2019s method, a quasi-\nNewton method such as L-BFGS, or the conjugate gradient method.\nIf l is quadratic, the x-minimization can be carried out by solving lin-\near equations, as in \u00a74.2. In general, we can interpret ADMM for \u2113\n1\nregularized loss minimization as reducing it to solving a sequence of\u21132\n(squared) regularized loss minimization problems.\nA very wide variety of models can be represented with the loss\nfunction l, including generalized linear models [122] and generalized\nadditive models [94]. In particular, generalized linear models subsume\nlinear regression, logistic regression, softmax regression, and Poisson\nregression, since they allow for any exponential family distribution. For\ngeneral background on models like\u2113\n1 regularized logistic regression, see,\ne.g., [95, \u00a74.4.4].\nIn order to use a regularizerg(z) other than\u2225z\u22251, we simply replace\nthe soft thresholding operator in thez-update with the proximity oper-\nator of g,a si n \u00a74.1.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1453, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f01ded40-aa2c-432f-91f2-680b7ccc3065": {"__data__": {"id_": "f01ded40-aa2c-432f-91f2-680b7ccc3065", "embedding": null, "metadata": {"page_label": "43", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "680f93a1-410f-4b25-8d00-9e260af20180", "node_type": "4", "metadata": {"page_label": "43", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "b7b4e0d61f4732e6801497317c0bd410f9c777b5bb1bc8ab8311a195a93f867e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6.4 Lasso 43\n6.4 Lasso\nAn important special case of (6.1) is \u21131 regularized linear regression,\nalso called the lasso [156]. This involves solving\nminimize (1 /2)\u2225Ax \u2212 b\u22252\n2 + \u03bb\u2225x\u22251, (6.2)\nwhere \u03bb> 0 is a scalar regularization parameter that is usually cho-\nsen by cross-validation. In typical applications, there are many more\nfeatures than training examples, and the goal is to \ufb01nd a parsimo-\nnious model for the data. For general background on the lasso, see [95,\n\u00a73.4.2]. The lasso has been widely applied, particularly in the analy-\nsis of biological data, where only a small fraction of a huge number of\npossible factors are actually predictive of some outcome of interest; see\n[95, \u00a718.4] for a representative case study.\nIn ADMM form, the lasso problem can be written as\nminimize f(x)+ g(z)\nsubject to x \u2212 z =0 ,\nwhere f(x)=( 1 /2)\u2225Ax \u2212 b\u2225\n2\n2 and g(z)= \u03bb\u2225z\u22251.B y \u00a74.2 and \u00a74.4,\nADMM becomes\nxk+1 := (AT A + \u03c1I)\u22121(AT b + \u03c1(zk \u2212 uk))\nzk+1 := S\u03bb/\u03c1(xk+1 + uk)\nuk+1 := uk + xk+1 \u2212 zk+1.\nNote that AT A + \u03c1I is always invertible, since \u03c1> 0. The x-update\nis essentially a ridge regression (i.e., quadratically regularized least\nsquares) computation, so ADMM can be interpreted as a method for\nsolving the lasso problem by iteratively carrying out ridge regression.\nWhen using a direct method, we can cache an initial factorization to\nmake subsequent iterations much cheaper. See [1] for an example of an\napplication in image processing.\n6.4.1 Generalized Lasso\nThe lasso problem can be generalized to\nminimize (1 /2)\u2225Ax \u2212 b\u2225\n2\n2 + \u03bb\u2225Fx\u22251, (6.3)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1546, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1f945d7c-470a-4b90-a491-b3b5652c144f": {"__data__": {"id_": "1f945d7c-470a-4b90-a491-b3b5652c144f", "embedding": null, "metadata": {"page_label": "44", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fac1b167-091e-4246-88c3-519a3a708ef6", "node_type": "4", "metadata": {"page_label": "44", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "612655915f16a22cbcb6ceb0ff2121a31c496b56fb5bfc38fed4699d096ce09b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "44 \u21131-Norm Problems\nwhere F is an arbitrary linear transformation. An important special\ncase is when F \u2208 R(n\u22121)\u00d7n is the di\ufb00erence matrix,\nFij =\n\uf8f1\n\uf8f2\n\uf8f3\n1 j = i +1\n\u22121 j = i\n0 otherwise ,\nand A = I, in which case the generalization reduces to\nminimize (1 /2)\u2225x \u2212 b\u22252\n2 + \u03bb\u2211 n\u22121\ni=1 |xi+1 \u2212 xi|. (6.4)\nThe second term is thetotal variation of x. This problem is often called\ntotal variation denoising [145], and has applications in signal process-\ning. When A = I and F is a second di\ufb00erence matrix, the problem (6.3)\nis called \u21131 trend \ufb01ltering [101].\nIn ADMM form, the problem (6.3) can be written as\nminimize (1 /2)\u2225Ax \u2212 b\u22252\n2 + \u03bb\u2225z\u22251\nsubject to Fx \u2212 z =0 ,\nwhich yields the ADMM algorithm\nxk+1 := (AT A + \u03c1FT F)\u22121(AT b + \u03c1FT (zk \u2212 uk))\nzk+1 := S\u03bb/\u03c1(Fxk+1 + uk)\nuk+1 := uk + Fxk+1 \u2212 zk+1.\nFor the special case of total variation denoising (6.4),AT A + \u03c1FT F\nis tridiagonal, so thex-update can be carried out inO(n) \ufb02ops [90,\u00a74.3].\nFor \u21131 trend \ufb01ltering, the matrix is pentadiagonal, so the x-update is\nstill O(n) \ufb02ops.\n6.4.2 Group Lasso\nAs another example, consider replacing the regularizer \u2225x\u22251 with\u2211 N\ni=1 \u2225xi\u22252, where x =( x1,...,x N ), with xi \u2208 Rni. When ni = 1 and\nN = n, this reduces to the \u21131 regularized problem (6.1). Here the reg-\nularizer is separable with respect to the partition x1,...,x N but not\nfully separable. This extension of \u21131 norm regularization is called the\ngroup lasso [177] or, more generally,sum-of-norms regularization [136].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1456, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f6de8d81-a37f-474d-b9b5-3f594d72cc23": {"__data__": {"id_": "f6de8d81-a37f-474d-b9b5-3f594d72cc23", "embedding": null, "metadata": {"page_label": "45", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "07017951-119b-4a87-baee-cb8a70ab735b", "node_type": "4", "metadata": {"page_label": "45", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "65b49a3495849bbbf9f197906d97b5800714eb74c62c30b38fa29bf8f1848f5f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6.5 Sparse Inverse Covariance Selection 45\nADMM for this problem is the same as above with the z-update\nreplaced with block soft thresholding\nzk+1\ni = S\u03bb/\u03c1(xk+1\ni + uk),i =1 ,...,N,\nwhere the vector soft thresholding operator S\u03ba : Rm \u2192 Rm is\nS\u03ba(a)=( 1 \u2212 \u03ba/\u2225a\u22252)+a,\nwith S\u03ba(0) = 0. This formula reduces to the scalar soft threshold-\ning operator when a is a scalar, and generalizes the expression given\nin (4.2).\nThis can be extended further to handle overlapping groups, which\nis often useful in bioinformatics and other applications [181, 118]. In\nthis case, we have N potentially overlapping groups G\ni \u2286{ 1,...,n } of\nvariables, and the objective is\n(1/2)\u2225Ax \u2212 b\u22252\n2 + \u03bb\nN\u2211\ni=1\n\u2225xGi\u22252,\nwhere xGi is the subvector of x with entries in Gi. Because the groups\ncan overlap, this kind of objective is di\ufb03cult to optimize with many\nstandard methods, but it is straightforward with ADMM. To use\nADMM, introduceN new variablesx\ni \u2208 R|Gi| and consider the problem\nminimize (1 /2)\u2225Az \u2212 b\u22252\n2 + \u03bb\u2211 N\ni=1 \u2225xi\u22252\nsubject to xi \u2212 \u02dczi =0 ,i =1 ,...,N,\nwith local variables xi and global variable z. Here, \u02dczi is the global\nvariable z\u2019s idea of what the local variable xi should be, and is given\nby a linear function of z. This follows the notation for general form\nconsensus optimization outlined in full detail in \u00a77.2; the overlapping\ngroup lasso problem above is a special case.\n6.5 Sparse Inverse Covariance Selection\nGiven a dataset consisting of samples from a zero mean Gaussian dis-\ntribution in R\nn,\nai \u223cN (0,\u03a3),i =1 ,...,N,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1520, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4cef62bc-a2d1-4790-b6c5-accfea1fb134": {"__data__": {"id_": "4cef62bc-a2d1-4790-b6c5-accfea1fb134", "embedding": null, "metadata": {"page_label": "46", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "006a20de-39fe-46f0-8cf9-dc6353d132d8", "node_type": "4", "metadata": {"page_label": "46", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "8ef5427196ad40eab469ad17e4abdce0cc3e55037a3c7b87981b14b00a03a0d5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "46 \u21131-Norm Problems\nconsider the task of estimating the covariance matrix \u03a3 under the prior\nassumption that \u03a3 \u22121 is sparse. Since (\u03a3 \u22121)ij is zero if and only if\nthe ith and jth components of the random variable are conditionally\nindependent, given the other variables, this problem is equivalent to the\nstructure learningproblem of estimating the topology of the undirected\ngraphical model representation of the Gaussian [104]. Determining the\nsparsity pattern of the inverse covariance matrix \u03a3\n\u22121 is also called the\ncovariance selection problem.\nFor n very small, it is feasible to search over all sparsity patterns\nin \u03a3\u22121 since for a \ufb01xed sparsity pattern, determining the maximum\nlikelihood estimate of \u03a3 is a tractable (convex optimization) problem.\nA good heuristic that scales to much larger values of n is to minimize\nthe negative log-likelihood (with respect to the parameter X =\u03a3\n\u22121)\nwith an \u21131 regularization term to promote sparsity of the estimated\ninverse covariance matrix [7]. If S is the empirical covariance matrix\n(1/N)\u2211 N\ni=1 aiaT\ni , then the estimation problem can be written as\nminimize Tr(SX) \u2212 logdet X + \u03bb\u2225X\u22251,\nwith variable X \u2208 Sn\n+, where \u2225\u00b7\u2225 1 is de\ufb01ned elementwise, i.e.,a st h e\nsum of the absolute values of the entries, and the domain of logdet is\nS\nn\n++, the set of symmetric positive de\ufb01nite n \u00d7 n matrices. This is a\nspecial case of the general \u21131 regularized problem (6.1) with (convex)\nloss function l(X)= Tr(SX) \u2212 logdet X.\nThe idea of covariance selection is originally due to Dempster [48]\nand was \ufb01rst studied in the sparse, high-dimensional regime by Mein-\nshausen and B\u00a8uhlmann [123]. The form of the problem above is due to\nBanerjee et al. [7]. Some other recent papers on this problem include\nFriedman et al.\u2019s graphical lasso [79], Duchi et al. [55], Lu [115], Yuan\n[178], and Scheinberg et al. [148], the last of which shows that ADMM\noutperforms state-of-the-art methods for this problem.\nThe ADMM algorithm for sparse inverse covariance selection is\nX\nk+1 := argmin\nX\n(\nTr(SX) \u2212 logdet X +( \u03c1/2)\u2225X \u2212 Zk + Uk\u22252\nF\n)\nZk+1 := argmin\nZ\n(\n\u03bb\u2225Z\u22251 +( \u03c1/2)\u2225Xk+1 \u2212 Z + Uk\u22252\nF\n)\nUk+1 := Uk + Xk+1 \u2212 Zk+1,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2143, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c2b9b6e7-3824-49fd-aacf-0a184780cd99": {"__data__": {"id_": "c2b9b6e7-3824-49fd-aacf-0a184780cd99", "embedding": null, "metadata": {"page_label": "47", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b93ae5d7-c1d6-4a9e-a5e0-3e78eb88ef67", "node_type": "4", "metadata": {"page_label": "47", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "33e33fdd9d7eebe08314094bde870cbad17bcb2a26472a69bbbb7ed2b99f0537", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6.5 Sparse Inverse Covariance Selection 47\nwhere \u2225\u00b7\u2225 F is the Frobenius norm, i.e., the square root of the sum of\nthe squares of the entries.\nThis algorithm can be simpli\ufb01ed much further. TheZ-minimization\nstep is elementwise soft thresholding,\nZk+1\nij := S\u03bb/\u03c1(Xk+1\nij + Uk\nij),\nand the X-minimization also turns out to have an analytical solution.\nThe \ufb01rst-order optimality condition is that the gradient should vanish,\nS \u2212 X\u22121 + \u03c1(X \u2212 Zk + Uk)=0 ,\ntogether with the implicit constraint X \u227b 0. Rewriting, this is\n\u03c1X \u2212 X\u22121 = \u03c1(Zk \u2212 Uk) \u2212 S. (6.5)\nWe will construct a matrixX that satis\ufb01es this condition and thus min-\nimizes the X-minimization objective. First, take the orthogonal eigen-\nvalue decomposition of the righthand side,\n\u03c1(Zk \u2212 Uk) \u2212 S = Q\u039bQT ,\nwhere \u039b = diag(\u03bb1,...,\u03bb n), and QT Q = QQT = I. Multiplying (6.5)\nby QT on the left and by Q on the right gives\n\u03c1 \u02dcX \u2212 \u02dcX\u22121 =\u039b ,\nwhere \u02dcX = QT XQ. We can now construct a diagonal solution of this\nequation, i.e., \ufb01nd positive numbers \u02dcXii that satisfy \u03c1 \u02dcXii \u2212 1/ \u02dcXii = \u03bbi.\nBy the quadratic formula,\n\u02dcXii =\n\u03bbi +\n\u221a\n\u03bb2\ni +4 \u03c1\n2\u03c1 ,\nwhich are always positive since \u03c1> 0. It follows that X = Q \u02dcXQT sat-\nis\ufb01es the optimality condition (6.5), so this is the solution to the X-\nminimization. The computational e\ufb00ort of the X-update is that of an\neigenvalue decomposition of a symmetric matrix.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1342, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "da4195fd-c83e-4d71-84bc-b889f61c3b34": {"__data__": {"id_": "da4195fd-c83e-4d71-84bc-b889f61c3b34", "embedding": null, "metadata": {"page_label": "48", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b5a55f82-e6fa-41aa-9469-b6f50187b3b0", "node_type": "4", "metadata": {"page_label": "48", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "7d3336647328297742ddfb76f8368a03d07957e385a1e0fdae5815de54076024", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7\nConsensus and Sharing\nHere we describe two generic optimization problems, consensus and\nsharing, and ADMM-based methods for solving them using distributed\noptimization. Consensus problems have a long history, especially in\nconjunction with ADMM; see, e.g., Bertsekas and Tsitsiklis [17] for a\ndiscussion of distributed consensus problems in the context of ADMM\nfrom the 1980s. Some more recent examples include a survey by Nedi\u00b4c\nand Ozdaglar [131] and several application papers by Giannakis and\nco-authors in the context of signal processing and wireless communica-\ntions, such as [150, 182, 121].\n7.1 Global Variable Consensus Optimization\nWe \ufb01rst consider the case with a single global variable, with the objec-\ntive and constraint terms split into N parts:\nminimize f(x)= \u2211\nN\ni=1 fi(x),\nwhere x \u2208 Rn, and fi : Rn \u2192 R \u222a{ +\u221e} are convex. We refer to fi as\nthe ith term in the objective. Each term can also encode constraints\nby assigning fi(x)=+ \u221e when a constraint is violated. The goal is to\n48", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1001, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "345c9541-9073-49c9-9e62-229afb98afef": {"__data__": {"id_": "345c9541-9073-49c9-9e62-229afb98afef", "embedding": null, "metadata": {"page_label": "49", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9f5bb9af-4b8c-4008-92f9-9df33a2c1f79", "node_type": "4", "metadata": {"page_label": "49", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "ed9ce6454b3adee7db07da2f1a14a22ed952be161fbc2fd52b6cb819c00ab0bf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7.1 Global Variable Consensus Optimization 49\nsolve the problem above in such a way that each term can be handled\nby its own processing element, such as a thread or processor.\nThis problem arises in many contexts. In model \ufb01tting, for exam-\nple, x represents the parameters in a model and fi represents the loss\nfunction associated with the ith block of data or measurements. In this\ncase, we would say that x is found by collaborative \ufb01ltering, since the\ndata sources are \u2018collaborating\u2019 to develop a global model.\nThis problem can be rewritten with local variables xi \u2208 Rn and a\ncommon global variable z:\nminimize \u2211 N\ni=1 fi(xi)\nsubject to xi \u2212 z =0 ,i =1 ,...,N. (7.1)\nThis is called the global consensus problem, since the constraint is that\nall the local variables should agree, i.e., be equal. Consensus can be\nviewed as a simple technique for turning additive objectives\u2211 N\ni=1 fi(x),\nwhich show up frequently but do not split due to the variable being\nshared across terms, into separable objectives \u2211\nN\ni=1 fi(xi), which split\neasily. For a useful recent discussion of consensus algorithms, see [131]\nand the references therein.\nADMM for the problem (7.1) can be derived either directly from\nthe augmented Lagrangian\nL\n\u03c1(x1,...,x N ,z,y )=\nN\u2211\ni=1\n(\nfi(xi)+ yT\ni (xi \u2212 z)+( \u03c1/2)\u2225xi \u2212 z\u22252\n2\n)\n,\nor simply as a special case of the constrained optimization problem (5.1)\nwith variable (x1,...,x N ) \u2208 RnN and constraint set\nC = {(x1,...,x N ) | x1 = x2 = \u00b7\u00b7\u00b7 = xN }.\nThe resulting ADMM algorithm is the following:\nxk+1\ni := argmin\nxi\n(\nfi(xi)+ ykT\ni (xi \u2212 zk)+( \u03c1/2)\u2225xi \u2212 zk\u22252\n2\n)\nzk+1 := 1\nN\nN\u2211\ni=1\n(\nxk+1\ni +( 1/\u03c1)yk\ni\n)\nyk+1\ni := yk\ni + \u03c1(xk+1\ni \u2212 zk+1).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1661, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ee35189c-756c-4aeb-a633-40969834379c": {"__data__": {"id_": "ee35189c-756c-4aeb-a633-40969834379c", "embedding": null, "metadata": {"page_label": "50", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e467744e-4b41-4485-a417-77e93a794191", "node_type": "4", "metadata": {"page_label": "50", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "df36f30ba578174c73a880b95667cc9860e23baaa3f034aa20bc91ffccbebc8c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "50 Consensus and Sharing\nHere, we write ykT instead of (yk)T to lighten the notation. The \ufb01rst\nand last steps are carried out independently for each i =1 ,...,N .I n\nthe literature, the processing element that handles the global variable\nz is sometimes called the central collector or the fusion center. Note\nthat the z-update is simply the projection of x\nk+1 +( 1/\u03c1)yk onto the\nconstraint set C of \u2018block-constant\u2019 vectors.\nThis algorithm can be simpli\ufb01ed further. With the average (over\ni =1 ,...,N ) of a vector denoted with an overline, thez-update can be\nwritten\nzk+1 = xk+1 +( 1/\u03c1)yk.\nSimilarly, averaging the y-update gives\nyk+1 = yk + \u03c1(xk+1 \u2212 zk+1).\nSubstituting the \ufb01rst equation into the second shows that yk+1 =0 ,\ni.e., the dual variables have average value zero after the \ufb01rst iteration.\nUsing zk = xk, ADMM can be written as\nxk+1\ni := argmin\nxi\n(\nfi(xi)+ ykT\ni (xi \u2212 xk)+( \u03c1/2)\u2225xi \u2212 xk\u22252\n2\n)\nyk+1\ni := yk\ni + \u03c1(xk+1\ni \u2212 xk+1).\nWe have already seen a special case of this in parallel projections (see\n\u00a75.1.2), which is consensus ADMM for the case whenfi are all indicator\nfunctions of convex sets.\nThis is a very intuitive algorithm. The dual variables are separately\nupdated to drive the variables into consensus, and quadratic regular-\nization helps pull the variables toward their average value while still\nattempting to minimize each local f\ni.\nWe can interpret consensus ADMM as a method for solving prob-\nlems in which the objective and constraints are distributed across mul-\ntiple processors. Each processor only has to handle its own objective\nand constraint term, plus a quadratic term which is updated each iter-\nation. The quadratic terms (or more accurately, the linear parts of the\nquadratic terms) are updated in such a way that the variables converge\nto a common value, which is the solution of the full problem.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1843, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "43c46c4c-db2e-4bae-9667-6ecd323b402b": {"__data__": {"id_": "43c46c4c-db2e-4bae-9667-6ecd323b402b", "embedding": null, "metadata": {"page_label": "51", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3477f662-34bc-4321-a72e-9aee677858b8", "node_type": "4", "metadata": {"page_label": "51", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "40cfa9fa55514d9ba589c0288a542c87b8113f63e747502a99f76858d91ebaf8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7.1 Global Variable Consensus Optimization 51\nFor consensus ADMM, the primal and dual residuals are\nrk =( xk\n1 \u2212 xk,...,x k\nN\n\u2212 xk),s k = \u2212\u03c1(xk \u2212 xk\u22121,..., xk \u2212 xk\u22121),\nso their (squared) norms are\n\u2225rk\u22252\n2 =\nN\u2211\ni=1\n\u2225xk\ni \u2212 xk\u22252\n2\n, \u2225sk\u22252\n2 = N\u03c12\u2225xk \u2212 xk\u22121\u22252\n2.\nThe \ufb01rst term is N times the standard deviation of the points\nx1,...,x N , a natural measure of (lack of) consensus.\nWhen the original consensus problem is a parameter \ufb01tting problem,\nthe x-update step has an intuitive statistical interpretation. Suppose\nfi is the negative log-likelihood function for the parameterx, given the\nmeasurements or data available to the ith processing element. Then\nxk+1\ni is precisely the maximum a posteriori (MAP) estimate of the\nparameter, given the Gaussian prior distribution N(xk +( 1/\u03c1)yk\ni ,\u03c1I ).\nThe expression for the prior mean is also intuitive: It is the average\nvalue\nxk of the local parameter estimates in the previous iteration,\ntranslated slightly by yk\ni , the \u2018price\u2019 of the ith processor disagree-\ning with the consensus in the previous iteration. Note also that the\nuse of di\ufb00erent forms of penalty in the augmented term, as discussed\nin \u00a73.4, will lead to corresponding changes in this prior distribution;\nfor example, using a matrix penalty P rather than a scalar \u03c1 will\nmean that the Gaussian prior distribution has covariance P rather\nthan \u03c1I.\n7.1.1 Global Variable Consensus with Regularization\nIn a simple variation on the global variable consensus problem, an\nobjective term g, often representing a simple constraint or regulariza-\ntion, is handled by the central collector:\nminimize \u2211\nN\ni=1 fi(xi)+ g(z)\nsubject to xi \u2212 z =0 ,i =1 ,...,N.\n(7.2)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1663, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2b3051ae-73c5-4c54-9acb-9307537ab897": {"__data__": {"id_": "2b3051ae-73c5-4c54-9acb-9307537ab897", "embedding": null, "metadata": {"page_label": "52", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a309121d-5285-4fed-9729-74a1185a37f3", "node_type": "4", "metadata": {"page_label": "52", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "338ad8d49dc681697fa6685405a660908391c1fd4f35f316bbee7f333333dbcb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "52 Consensus and Sharing\nThe resulting ADMM algorithm is\nxk+1\ni := argmin\nxi\n(\nfi(xi)+ ykT\ni (xi \u2212 zk)+( \u03c1/2)\u2225xi \u2212 zk\u22252\n2\n)\n(7.3)\nzk+1 := argmin\nz\n(\ng(z)+\nN\u2211\ni=1\n(\u2212ykT\ni z +( \u03c1/2)\u2225xk+1\ni \u2212 z\u22252\n2)\n)\n(7.4)\nyk+1\ni := yk\ni + \u03c1(xk+1\ni \u2212 zk+1). (7.5)\nBy collecting the linear and quadratic terms, we can express the z-\nupdate as an averaging step, as in consensus ADMM, followed by a\nproximal step involving g:\nz\nk+1 := argmin\nz\n(\ng(z)+( N\u03c1/2)\u2225z \u2212 xk+1 \u2212 (1/\u03c1)yk\u22252\n2\n)\n.\nIn the case with nonzero g, we do not in general have yk =0 , s o w e\ncannot drop the yi terms from z-update as in consensus ADMM.\nAs an example, for g(z)= \u03bb\u2225z\u22251, with \u03bb> 0, the second step of the\nz-update is a soft threshold operation:\nzk+1 := S\u03bb/N\u03c1(xk+1 \u2212 (1/\u03c1)yk).\nAs another simple example, suppose g is the indicator function of Rn\n+,\nwhich means that the g term enforces nonnegativity of the variable. In\nthis case, the update is\nzk+1 := (xk+1 \u2212 (1/\u03c1)yk)+.\nThe scaled form of ADMM for this problem also has an appealing\nform, which we record here for convenience:\nxk+1\ni := argmin\nxi\n(\nfi(xi)+( \u03c1/2)\u2225xi \u2212 zk + uk\ni \u22252\n2\n)\n(7.6)\nzk+1 := argmin\nz\n(\ng(z)+( N\u03c1/2)\u2225z \u2212 xk+1 \u2212 uk\u22252\n2\n)\n(7.7)\nuk+1\ni := uk\ni + xk+1\ni \u2212 zk+1. (7.8)\nIn many cases, this version is simpler and easier to work with than the\nunscaled form.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1281, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f6ed877c-aea2-4160-8b8a-73602e7f3f39": {"__data__": {"id_": "f6ed877c-aea2-4160-8b8a-73602e7f3f39", "embedding": null, "metadata": {"page_label": "53", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a5854754-25e7-4ead-bd07-5ba54221941e", "node_type": "4", "metadata": {"page_label": "53", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "fa458e4b5be66356352fa62838880b766b69e03eb2cbe107af0590db10ae2ece", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7.2 General Form Consensus Optimization 53\n7.2 General Form Consensus Optimization\nWe now consider a more general form of the consensus minimization\nproblem, in which we have local variables xi \u2208 Rni, i =1 ,...,N , with\nthe objective f1(x1)+ \u00b7\u00b7\u00b7 + fN (xN ) separable in the xi. Each of these\nlocal variables consists of a selection of the components of the global\nvariable z \u2208 R\nn; that is, each component of each local variable corre-\nsponds to some global variable componentzg. The mapping from local\nvariable indices into global variable index can be written asg = G(i,j ),\nwhich means that local variable component (xi)j corresponds to global\nvariable component zg.\nAchieving consensus between the local variables and the global vari-\nable means that\n(xi)j = zG(i,j),i =1 ,...,N, j =1 ,...,n i.\nIf G(i,j )= j for all i, then each local variable is just a copy of\nthe global variable, and consensus reduces to global variable consen-\nsus, x\ni = z. General consensus is of interest in cases where ni \u226a n,\nso each local vector only contains a small number of the global\nvariables.\nIn the context of model \ufb01tting, the following is one way that general\nform consensus naturally arises. The global variable z is the full fea-\nture vector (i.e., vector of model parameters or independent variables\nin the data), and di\ufb00erent subsets of the data are spread out amongN\nprocessors. Then x\ni can be viewed as the subvector of z corresponding\nto (nonzero) features that appear in the ith block of data. In other\nwords, each processor handles only its block of data and only the sub-\nset of model coe\ufb03cients that are relevant for that block of data. If in\neach block of data all regressors appear with nonzero values, then this\nreduces to global consensus.\nFor example, if each training example is a document, then the fea-\ntures may include words or combinations of words in the document; it\nwill often be the case that some words are only used in a small sub-\nset of the documents, in which case each processor can just deal with\nthe words that appear in its local corpus. In general, datasets that are\nhigh-dimensional but sparse will bene\ufb01t from this approach.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2155, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "50289ea6-4d57-4d67-86f2-7387f0e2682e": {"__data__": {"id_": "50289ea6-4d57-4d67-86f2-7387f0e2682e", "embedding": null, "metadata": {"page_label": "54", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9584486d-8aa8-4cfe-8281-5a59b69cd8ee", "node_type": "4", "metadata": {"page_label": "54", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "0b8ebb72d8c916c29ceb3129887d467617d7f176beeba8636226639caaffd392", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "54 Consensus and Sharing\nFig. 7.1. General form consensus optimization. Local objective terms are on the left; global\nvariable components are on the right. Each edge in the bipartite graph is a consistency\nconstraint, linking a local variable and a global variable component.\nFor ease of notation, let \u02dc zi \u2208 Rni be de\ufb01ned by (\u02dc zi)j = zG(i,j).\nIntuitively, \u02dczi is the global variable\u2019s idea of what the local variable\nxi should be; the consensus constraint can then be written very simply\nas xi \u2212 \u02dczi =0 , i =1 ,...,N .\nThe general form consensus problem is\nminimize \u2211 N\ni=1 fi(xi)\nsubject to xi \u2212 \u02dczi =0 ,i =1 ,...,N, (7.9)\nwith variables x1,...,x N and z (\u02dczi are linear functions of z).\nA simple example is shown in Figure 7.1. In this example, we have\nN = 3 subsystems, global variable dimension n = 4, and local variable\ndimensions n1 =4 , n2 = 2, and n3 = 3. The objective terms and global\nvariables form a bipartite graph, with each edge representing a con-\nsensus constraint between a local variable component and a global\nvariable.\nThe augmented Lagrangian for (7.9) is\nL\n\u03c1(x,z,y )=\nN\u2211\ni=1\n(\nfi(xi)+ yT\ni (xi \u2212 \u02dczi)+( \u03c1/2)\u2225xi \u2212 \u02dczi\u22252\n2\n)\n,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1149, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "28d70e9b-a6fe-4702-ad1c-4ea4b8f517d5": {"__data__": {"id_": "28d70e9b-a6fe-4702-ad1c-4ea4b8f517d5", "embedding": null, "metadata": {"page_label": "55", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8cda7db5-b1a5-40c0-b518-132c38ff3874", "node_type": "4", "metadata": {"page_label": "55", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "438bfb132913c3e229d5299474ddae4443330a759022d801826a2c488de6e8a6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7.2 General Form Consensus Optimization 55\nwith dual variable yi \u2208 Rni. Then ADMM consists of the iterations\nxk+1\ni := argmin\nxi\n(\nfi(xi)+ ykT\ni xi +( \u03c1/2)\u2225xi \u2212 \u02dczk\ni \u22252\n2\n)\nzk+1 := argmin\nz\n( m\u2211\ni=1\n(\n\u2212ykT\ni \u02dczi +( \u03c1/2)\u2225xk+1\ni \u2212 \u02dczi\u22252\n2\n))\nyk+1\ni := yk\ni + \u03c1(xk+1\ni \u2212 \u02dczk+1\ni ),\nwhere the xi- and yi-updates can be carried out independently in par-\nallel for each i.\nThe z-update step decouples across the components of z, since L\u03c1\nis fully separable in its components:\nzk+1\ng :=\n\u2211\nG(i,j)=g\n(\n(xk+1\ni )j +( 1/\u03c1)(yk\ni )j\n)\n\u2211\nG(i,j)=g 1 ,\nso zg is found by averaging all entries ofxk+1\ni +( 1/\u03c1)yk\ni that correspond\nto the global index g. Applying the same type of argument as in the\nglobal variable consensus case, we can show that after the \ufb01rst iteration,\n\u2211\nG(i,j)=g\n(yk\ni )j =0 ,\ni.e., the sum of the dual variable entries that correspond to any given\nglobal index g is zero. The z-update step can thus be written in the\nsimpler form\nzk+1\ng := (1/kg)\n\u2211\nG(i,j)=g\n(xk+1\ni )j,\nwhere kg is the number of local variable entries that correspond to\nglobal variable entryzg. In other words, thez-update is local averaging\nfor each component zg rather than global averaging; in the language of\ncollaborative \ufb01ltering, we could say that only the processing elements\nthat have an opinion on a feature z\ng will vote on zg.\n7.2.1 General Form Consensus with Regularization\nAs in the global consensus case, the general form consensus problem\ncan be generalized by allowing the global variable nodes to handle an", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1500, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bc21ac7a-9d62-4f42-940e-2b6642c9808c": {"__data__": {"id_": "bc21ac7a-9d62-4f42-940e-2b6642c9808c", "embedding": null, "metadata": {"page_label": "56", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7254cc94-c98c-430f-8a32-ba8d5bbadb58", "node_type": "4", "metadata": {"page_label": "56", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "162eed5dea7e44996551b1e3b0f0fc73a91fa380e23a756f5543219efb76c3fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "56 Consensus and Sharing\nobjective term. Consider the problem\nminimize \u2211 N\ni=1 fi(xi)+ g(z)\nsubject to xi \u2212 \u02dczi =0 ,i =1 ,...,N, (7.10)\nwhere g is a regularization function. The z-update involves the local\naveraging step from the unregularized setting, followed by an applica-\ntion of the proximity operatorprox\ng,kg\u03c1 to the results of this averaging,\njust as in the global variable consensus case.\n7.3 Sharing\nAnother canonical problem that will prove useful in the sequel is the\nsharing problem\nminimize \u2211\nN\ni=1 fi(xi)+ g(\u2211 N\ni=1 xi) (7.11)\nwith variablesxi \u2208 Rn, i =1 ,...,N , where fi is a local cost function for\nsubsystem i, and g is the shared objective, which takes as argument the\nsum of the variables. We can think of the variablexi as being the choice\nof agenti; the sharing problem involves each agent adjusting its variable\nto minimize its individual cost fi(xi), as well as the shared objective\nterm g(\u2211 N\ni=1 xi). The sharing problem is important both because many\nuseful problems can be put into this form and because it enjoys a dual\nrelationship with the consensus problem, as discussed below.\nSharing can be written in ADMM form by copying all the variables:\nminimize \u2211\nN\ni=1 fi(xi)+ g(\u2211 N\ni=1 zi)\nsubject to xi \u2212 zi =0 ,i =1 ,...,N, (7.12)\nwith variables xi,z i \u2208 Rn, i =1 ,...,N . The scaled form of ADMM is\nxk+1\ni := argmin\nxi\n(\nfi(xi)+( \u03c1/2)\u2225xi \u2212 zk\ni + uk\ni \u22252\n2\n)\nzk+1 := argmin\nz\n(\ng(\u2211 N\ni=1 zi)+( \u03c1/2)\u2211 N\ni=1 \u2225zi \u2212 uk\ni \u2212 xk+1\ni \u22252\n2\n)\nuk+1\ni := uk\ni + xk+1\ni \u2212 zk+1\ni .\nThe \ufb01rst and last steps can be carried out independently in parallel for\neach i =1 ,...,N . As written, the z-update requires solving a problem", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1641, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "da7c61ca-5061-4556-818f-d6bb4d65de52": {"__data__": {"id_": "da7c61ca-5061-4556-818f-d6bb4d65de52", "embedding": null, "metadata": {"page_label": "57", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fe67addb-a6d7-477d-a4f3-e0da12d12eb2", "node_type": "4", "metadata": {"page_label": "57", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "4c2383e6c978e145c1068485bc7270aa587db60040e6736111af70fda9e9182f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7.3 Sharing 57\nin Nn variables, but we will show that it is possible to carry it out by\nsolving a problem in only n variables.\nFor simplicity of notation, let ai = uk\ni + xk+1\ni . Then the z-update\ncan be rewritten as\nminimize g(Nz)+( \u03c1/2)\u2211 N\ni=1 \u2225zi \u2212 ai\u22252\n2\nsubject to z =( 1/N)\u2211 N\ni=1 zi,\nwith additional variablez \u2208 Rn. Minimizing overz1,...,z N with z \ufb01xed\nhas the solution\nzi = ai + z \u2212 a, (7.13)\nso the z-update can be computed by solving the unconstrained problem\nminimize g(Nz)+( \u03c1/2)\u2211 N\ni=1 \u2225z \u2212 a\u22252\n2\nfor z \u2208 Rn and then applying (7.13). Substituting (7.13) for zk+1\ni in\nthe u-update gives\nuk+1\ni = uk + xk+1 \u2212 zk+1, (7.14)\nwhich shows that the dual variablesuk\ni are all equal (i.e., in consensus)\nand can be replaced with a single dual variable u \u2208 Rm. Substituting\nin the expression for zk\ni in the x-update, the \ufb01nal algorithm becomes\nxk+1\ni := argmin\nxi\n(\nfi(xi)+( \u03c1/2)\u2225xi \u2212 xk\ni + xk \u2212 zk + uk\u22252\n2\n)\nzk+1 := argmin\nz\n(\ng(Nz)+( N\u03c1/2)\u2225z \u2212 uk \u2212 xk+1\u22252\n2\n)\nuk+1 := uk + xk+1 \u2212 zk+1.\nThe x-update can be carried out in parallel, for i =1 ,...,N . The z-\nupdate step requires gathering xk+1\ni to form the averages, and then\nsolving a problem with n variables. After the u-update, the new value\nof xk+1 \u2212 zk+1 + uk+1 is scattered to the subsystems.\n7.3.1 Duality\nAttaching Lagrange multipliers \u03bdi to the constraints xi \u2212 zi = 0, the\ndual function \u0393 of the ADMM sharing problem (7.12) is given by\n\u0393(\u03bd1,...,\u03bd N )=\n{ \u2212g\u2217(\u03bd1) \u2212 \u2211\ni f\u2217\ni (\u2212\u03bdi)i f \u03bd1 = \u03bd2 = \u00b7\u00b7\u00b7 = \u03bdN\n\u2212\u221e otherwise.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1485, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "75624f8e-28cd-4399-8640-b5b5aeff2694": {"__data__": {"id_": "75624f8e-28cd-4399-8640-b5b5aeff2694", "embedding": null, "metadata": {"page_label": "58", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "891c5a56-9517-408f-ba99-3568d7d04bdd", "node_type": "4", "metadata": {"page_label": "58", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "9ed17cebd8c2ba0a1792b2c32febfddb6d02464df18556e9135101d557e8b50f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "58 Consensus and Sharing\nLetting \u03c8 = g\u2217 and hi(\u03bd)= f\u2217\ni (\u2212\u03bd), the dual sharing problem can be\nwritten as\nminimize \u2211 N\ni=1 hi(\u03bdi)+ \u03c8(\u03bd)\nsubject to \u03bdi \u2212 \u03bd =0 , (7.15)\nwith variables \u03bd \u2208 Rn, \u03bdi \u2208 Rn, i =1 ,...,N . This is identical to the\nregularized global variable consensus problem (7.2). Assuming strong\nduality holds, this implies that y\nk = \u03c1uk \u2192 \u03bd\u22c6 in ADMM, where \u03bd\u22c6 is\nan optimal point of (7.15).\nConsider the reverse direction. Attaching Lagrange multipliers\ndi \u2208 Rn to the constraints\u03bdi \u2212 \u03bd = 0, the dual of the regularized global\nconsensus problem is\nminimize \u2211 N\ni=1 fi(di)+ g(\u2211 N\ni=1 di)\nwith variables di \u2208 Rn, which is exactly the sharing problem (7.11).\n(This follows because f and g are assumed to be convex and closed, so\nf\u2217\u2217 = f and g\u2217\u2217 = g.) Assuming strong duality holds, running ADMM\non the consensus problem (7.15) gives that dk\ni \u2192 x\u22c6\ni , where x\u22c6\ni is an\noptimal point of the sharing problem (7.11).\nThus, there is a close dual relationship between the consensus prob-\nlem (7.15) and the sharing problem (7.11). In fact, the global consensus\nproblem can be solved by running ADMM on its dual sharing problem,\nand vice versa. This is related to work by Fukushima [80] on \u2018dual\nADMM\u2019 methods.\n7.3.2 Optimal Exchange\nHere, we highlight an important special case of the sharing problem\nwith an appealing economic interpretation. The exchange problem is\nminimize \u2211\nN\ni=1 fi(xi)\nsubject to \u2211 N\ni=1 xi =0 ,\n(7.16)\nwith variables xi \u2208 Rn, i =1 ,...,N , where fi represents the cost func-\ntion for subsystem i. This is a sharing problem where the shared objec-\ntive g is the indicator function of the set {0}. The components of\nthe vectors xi represent quantities of commodities that are exchanged", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1710, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6197b9a8-694c-4f78-a126-9e337536b8ef": {"__data__": {"id_": "6197b9a8-694c-4f78-a126-9e337536b8ef", "embedding": null, "metadata": {"page_label": "59", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f737561b-3951-41b4-929c-55a9fd235238", "node_type": "4", "metadata": {"page_label": "59", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "ee99b0e0bff28283a739511bf45d477fe9673cd2ee070307a5f5bb8a0b70a243", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7.3 Sharing 59\namong N agents or subsystems. When (xi)j is nonnegative, it can be\nviewed as the amount of commodityj receivedby subsystem i from the\nexchange. When (xi)j is negative, its magnitude|(xi)j| can be viewed as\nthe amount of commodityj contributed by subsystemi to the exchange.\nThe equilibrium constraint that each commodity clears, or balances, is\nsimply \u2211 N\ni=1 xi = 0. As this interpretation suggests, this and related\nproblems have a long history in economics, particularly in the theories\nof market exchange, resource allocation, and general equilibrium; see,\nfor example, the classic works by Walras [168], Arrow and Debreu [4],\nand Uzawa [162, 163].\nThe exchange problem can be solved via ADMM either by applying\nthe generic sharing algorithm above and simplifying, or by treating it\nas a generic constrained convex problem (5.1), with\nC =\n{\nx \u2208 R\nnN | x1 + \u00b7\u00b7\u00b7 + xN =0 }.\nThis gives the exchange ADMM algorithm\nxk+1\ni := argmin\nxi\n(\nfi(xi)+( \u03c1/2)\u2225xi \u2212 xk\ni + xk + uk\u22252\n2\n)\nuk+1 := uk + xk+1.\nIt is also instructive to consider the unscaled form of ADMM for this\nproblem:\nxk+1\ni := argmin\nxi\n(\nfi(xi)+ ykT xi +( \u03c1/2)\u2225xi \u2212 (xk\ni \u2212 xk)\u22252\n2\n)\nyk+1 := yk + \u03c1xk+1.\nThe variable yk converges to an optimal dual variable, which is readily\ninterpreted as a set of optimal or clearing prices for the exchange. The\nproximal term in the x-update is a penalty for x\nk+1 deviating from xk,\nprojected onto the feasible set. The x-update in exchange ADMM can\nbe carried out independently in parallel, fori =1 ,...,N . The u-update\nrequires gathering the xk+1\ni (or otherwise averaging), and broadcasting\nxk+1 + uk+1 back to the processors handling the xi updates.\nExchange ADMM can be viewed as a form oft\u02c6atonnement or price\nadjustment process [168, 163] from Walras\u2019 theory of general equilib-\nrium. T\u02c6atonnement represents the mechanism of the competitive mar-\nket working towards market equilibrium; the idea is that the market", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1934, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e65271b7-4772-4d81-8f3a-6317a47041b3": {"__data__": {"id_": "e65271b7-4772-4d81-8f3a-6317a47041b3", "embedding": null, "metadata": {"page_label": "60", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ddf66051-632a-4c96-8c64-0243cae5af9d", "node_type": "4", "metadata": {"page_label": "60", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "6d0c3b1ae2e1e17683d6546847957f5fdc91ef7713b380d3f934389273b63944", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "60 Consensus and Sharing\nacts via price adjustment,i.e., increasing or decreasing the price of each\ngood depending on whether there is an excess demand or excess supply\nof the good, respectively.\nDual decomposition is the simplest algorithmic expression of\nt\u02c6atonnement. In this setting, each agent adjusts his consumption x\ni\nto minimize his individual cost fi(xi) adjusted by the costyT xi, where\ny is the price vector. The central collector (called the \u2018secretary of mar-\nket\u2019 in [163]) works toward equilibrium by adjusting the pricesy up or\ndown depending on whether each commodity or good is overproduced\nor underproduced. ADMM di\ufb00ers only in the inclusion of the proximal\nregularization term in the updates for each agent. As y\nk converges to\nan optimal price vector y\u22c6 , the e\ufb00ect of the proximal regularization\nterm vanishes. The proximal regularization term can be interpreted as\neach agent\u2019s commitment to help clear the market.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 939, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8813068b-9fbc-41ac-963a-011a3e73bc41": {"__data__": {"id_": "8813068b-9fbc-41ac-963a-011a3e73bc41", "embedding": null, "metadata": {"page_label": "61", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6ed7fefb-4f6b-4b74-b434-63c25726b1be", "node_type": "4", "metadata": {"page_label": "61", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "ceab352590e23c9b8d9aee10c4ff971a59dc77e626849545cb137759b2dcf164", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8\nDistributed Model Fitting\nA general convex model \ufb01tting problem can be written in the form\nminimize l(Ax \u2212 b)+ r(x), (8.1)\nwith parameters x \u2208 Rn, where A \u2208 Rm\u00d7n is the feature matrix, b \u2208\nRm is the output vector, l : Rm \u2192 R is a convex loss function, and r\nis a convex regularization function. We assume that l is additive, so\nl(Ax \u2212 b)=\nm\u2211\ni=1\nli(aT\ni x \u2212 bi),\nwhere li : R \u2192 R is the loss for the ith training example, ai \u2208 Rn is\nthe feature vector for example i, and bi is the output or response for\nexample i. Each li can be di\ufb00erent, though in practice they are usually\nall the same.\nWe also assume that the regularization functionr is separable. The\nmost common examples are r(x)= \u03bb\u2225x\u22252\n2 (called Tikhonov regulariza-\ntion,o ra ridge penalty in statistical settings) and r(x)= \u03bb\u2225x\u22251 (some-\ntimes generically called a lasso penalty in statistical settings), where\n\u03bb is a positive regularization parameter, though more elaborate regu-\nlarizers can be used just as easily. In some cases, one or more model\n61", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1014, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9d66076d-5939-4335-874c-5934255d43bb": {"__data__": {"id_": "9d66076d-5939-4335-874c-5934255d43bb", "embedding": null, "metadata": {"page_label": "62", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e6e0b8cd-6d94-4b86-bd0b-cf43bb78c998", "node_type": "4", "metadata": {"page_label": "62", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "61d266f7b5d932fc0f9fdc7b6ef074a3aa2fa330549be4a14b098f9d29132f2c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "62 Distributed Model Fitting\nparameters are not regularized, such as the o\ufb00set parameter in a clas-\nsi\ufb01cation model. This corresponds to, for example, r(x)= \u03bb\u2225x1:n\u22121\u22251,\nwhere x1:n\u22121 is the subvector of x consisting of all but the last compo-\nnent of x; with this choice of r, the last component of x is not regular-\nized.\nThe next section discusses some examples that have the general\nform above. We then consider two ways to solve (8.1) in a distributed\nmanner, namely, by splitting across training examples and by splitting\nacross features. While we work with the assumption that l and r are\nseparable at the component level, we will see that the methods we\ndescribe work with appropriate block separability as well.\n8.1 Examples\n8.1.1 Regression\nConsider a linear modeling problem with measurements of the form\nbi = aT\ni x + vi,\nwhere ai is the ith feature vector and the measurement noises vi\nare independent with log-concave densities pi; see, e.g., [20, \u00a77.1.1].\nThen the negative log-likelihood function is l(Ax \u2212 b), with li(\u03c9)=\n\u2212logpi(\u2212\u03c9). If r = 0, then the general \ufb01tting problem (8.1) can be\ninterpreted as maximum likelihood estimation of x under noise model\npi.I f ri is taken to be the negative log prior density of xi, then the\nproblem can be interpreted as MAP estimation.\nFor example, the lasso follows the form above with quadratic loss\nl(u)=( 1 /2)\u2225u\u22252\n2 and \u21131 regularization r(x)= \u03bb\u2225x\u22251, which is equiva-\nlent to MAP estimation of a linear model with Gaussian noise and a\nLaplacian prior on the parameters [156, \u00a75].\n8.1.2 Classi\ufb01cation\nMany classi\ufb01cation problems can also be put in the form of the general\nmodel \ufb01tting problem (8.1), withA, b, l, andr appropriately chosen. We\nfollow the standard setup from statistical learning theory, as described\nin, e.g., [8]. Let p\ni \u2208 Rn\u22121 denote the feature vector of theith example", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1847, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5b51b3f0-e39e-430d-8d1e-60a3ffe60564": {"__data__": {"id_": "5b51b3f0-e39e-430d-8d1e-60a3ffe60564", "embedding": null, "metadata": {"page_label": "63", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d17cc04f-2556-4be4-8492-183bde7500b5", "node_type": "4", "metadata": {"page_label": "63", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "ccad3d8d1eec22cf6f16609e35f553ef7c4c83e66539c1bb4d4172e8bdcb96e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8.1 Examples 63\nand let qi \u2208{ \u22121,1} denote the binary outcome or class label, for i =\n1,...,m . The goal is to \ufb01nd a weight vectorw \u2208 Rn\u22121 and o\ufb00set v \u2208 R\nsuch that\nsign(pT\ni w + v)= qi\nholds for many examples. Viewed as a function of pi, the expression\npT\ni w + v is called a discriminant function. The condition that the sign\nof the discriminant function and the response should agree can also be\nwritten as \u00b5\ni > 0, where \u00b5i = qi(pT\ni w + v) is called the margin of the\nith training example.\nIn the context of classi\ufb01cation, loss functions are generally written\nas a function of the margin, so the loss for the ith example is\nli(\u00b5i)= li(qi(pT\ni w + v)).\nA classi\ufb01cation error is made if and only if the margin is negative, so\nli should be positive and decreasing for negative arguments and zero\nor small for positive arguments. To \ufb01nd the parameters w and v,w e\nminimize the average loss plus a regularization term on the weights:\n1\nm\nm\u2211\ni=1\nli(qi(pT\ni w + v)) + rwt(w). (8.2)\nThis has the generic model \ufb01tting form (8.1), with x =( w,v ), ai =\n(qipi,\u2212qi), bi = 0, and regularizerr(x)= rwt(w). (We also need to scale\nli by 1/m.) In the sequel, we will address such problems using the form\n(8.1) without comment, assuming that this transformation has been\ncarried out.\nIn statistical learning theory, the problem (8.2) is referred to as\npenalized empirical risk minimization or structural risk minimization.\nWhen the loss function is convex, this is sometimes termed convex\nrisk minimization. In general, \ufb01tting a classi\ufb01er by minimizing a sur-\nrogate loss function , i.e., a convex upper bound to 0-1 loss, is a\nwell studied and widely used approach in machine learning; see, e.g.,\n[165, 180, 8].\nMany classi\ufb01cation models in machine learning correspond to dif-\nferent choices of loss function l\ni and regularization or penalty rwt.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1835, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "56b9c850-b312-431f-a280-8e7d937d7a2e": {"__data__": {"id_": "56b9c850-b312-431f-a280-8e7d937d7a2e", "embedding": null, "metadata": {"page_label": "64", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bc11ff53-4ff5-417e-95fe-d05fcaf270ee", "node_type": "4", "metadata": {"page_label": "64", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "cfc6e2a3d8397e3def6e76d686e567f020493a894a7cacf8ef3d254934e4fbe5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "64 Distributed Model Fitting\nSome common loss functions are hinge loss (1 \u2212 \u00b5i)+, exponential\nloss exp(\u2212\u00b5i), and logistic loss log(1 + exp(\u2212\u00b5i)); the most com-\nmon regularizers are \u21131 and \u21132 (squared). The support vector machine\n(SVM) [151] corresponds to hinge loss with a quadratic penalty, while\nexponential loss yields boosting [78] and logistic loss yields logistic\nregression.\n8.2 Splitting across Examples\nHere we discuss how to solve the model \ufb01tting problem (8.1) with\na modest number of features but a very large number of training\nexamples. Most classical statistical estimation problems belong to this\nregime, with large volumes of relatively low-dimensional data. The goal\nis to solve the problem in a distributed way, with each processor han-\ndling a subset of the training data. This is useful either when there are\nso many training examples that it is inconvenient or impossible to pro-\ncess them on a single machine or when the data is naturally collected\nor stored in a distributed fashion. This includes, for example, online\nsocial network data, webserver access logs, wireless sensor networks,\nand many cloud computing applications more generally.\nWe partition A and b by rows,\nA =\n\uf8ee\n\uf8ef\uf8f0\nA\n1\n..\n.\nA\nN\n\uf8f9\n\uf8fa\uf8fb,b =\n\uf8ee\n\uf8ef\uf8f0\nb1\n..\n.\nb\nN\n\uf8f9\n\uf8fa\uf8fb,\nwith Ai \u2208 Rmi\u00d7n and bi \u2208 Rmi, where \u2211 N\ni=1 mi = m. Thus, Ai and bi\nrepresent theith block of data and will be handled by theith processor.\nWe \ufb01rst put the model \ufb01tting problem in the consensus form\nminimize \u2211 N\ni=1 li(Aixi \u2212 bi)+ r(z)\nsubject to xi \u2212 z =0 ,i =1 ,...,N,\n(8.3)\nwith variables xi \u2208 Rn and z \u2208 Rn. Here, li refers (with some abuse of\nnotation) to the loss function for theith block of data. The problem can\nnow be solved by applying the generic global variable consensus ADMM", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1741, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fe262dcf-bbaa-4787-b585-db30d7df91d3": {"__data__": {"id_": "fe262dcf-bbaa-4787-b585-db30d7df91d3", "embedding": null, "metadata": {"page_label": "65", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c41f80f8-2b1a-4e9a-aefa-57ac6eccb785", "node_type": "4", "metadata": {"page_label": "65", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "a0f1e0407e8d5e001bdb8aa2aa800b6f948bc1e482453895edf10abd55d4e258", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8.2 Splitting across Examples 65\nalgorithm described in \u00a77.1, given here with scaled dual variable:\nxk+1\ni := argmin\nxi\n(\nli(Aixi \u2212 bi)+( \u03c1/2)\u2225xi \u2212 zk + uk\ni \u22252\n2\n)\nzk+1 := argmin\nz\n(\nr(z)+( N\u03c1/2)\u2225z \u2212 xk+1 \u2212 uk\u22252\n2\n)\nuk+1\ni := uk\ni + xk+1\ni \u2212 zk+1.\nThe \ufb01rst step, which consists of an \u21132-regularized model \ufb01tting prob-\nlem, can be carried out in parallel for each data block. The second\nstep requires gathering variables to form the average. The minimiza-\ntion in the second step can be carried out componentwise (and usually\nanalytically) when r is assumed to be fully separable.\nThe algorithm described above only requires that the loss function\nl be separable across the blocks of data; the regularizerr does not need\nto be separable at all. (However, whenr is not separable, the z-update\nmay require the solution of a nontrivial optimization problem.)\n8.2.1 Lasso\nFor the lasso, this yields the distributed algorithm\nx\nk+1\ni := argmin\nxi\n(\n(1/2)\u2225Aixi \u2212 bi\u22252\n2 +( \u03c1/2)\u2225xi \u2212 zk + uk\ni \u22252\n2\n)\nzk+1 := S\u03bb/\u03c1N (xk+1 + uk)\nuk+1\ni := uk\ni + xk+1\ni \u2212 zk+1.\nEach xi-update takes the form of a Tikhonov-regularized least squares\n(i.e., ridge regression) problem, with analytical solution\nxk+1\ni := (AT\ni Ai + \u03c1I)\u22121(AT\ni bi + \u03c1(zk \u2212 uk\ni )).\nThe techniques from\u00a74.2 apply: If a direct method is used, then the fac-\ntorization of AT\ni Ai + \u03c1I can be cached to speed up subsequent updates,\nand if mi <n , then the matrix inversion lemma can be applied to let\nus factor the smaller matrix AiAT\ni + \u03c1I instead.\nComparing this distributed-data lasso algorithm with the serial ver-\nsion in \u00a76.4, we see that the only di\ufb00erence is the collection and aver-\naging steps, which couple the computations for the data blocks.\nAn ADMM-based distributed lasso algorithm is described in [121],\nwith applications in signal processing and wireless communications.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1837, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "90723a68-e2e5-4821-9340-0591384f0ba6": {"__data__": {"id_": "90723a68-e2e5-4821-9340-0591384f0ba6", "embedding": null, "metadata": {"page_label": "66", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38a47316-3622-4770-ae78-b51ac99dc884", "node_type": "4", "metadata": {"page_label": "66", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "3be3f936f56723eef0249b53c755f434fc14ae0ed0fec378389a695ba8bcc3d5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "66 Distributed Model Fitting\n8.2.2 Sparse Logistic Regression\nConsider solving (8.1) with logistic loss functions li and \u21131 regulariza-\ntion. We ignore the intercept term for notational simplicity; the algo-\nrithm can be easily modi\ufb01ed to incorporate an intercept. The ADMM\nalgorithm is\nx\nk+1\ni := argmin\nxi\n(\nli(Aixi)+( \u03c1/2)\u2225xi \u2212 zk + uk\ni \u22252\n2\n)\nzk+1 := S\u03bb/\u03c1N (xk+1 + uk)\nuk+1\ni := uk\ni + xk+1\ni \u2212 zk+1.\nThis is identical to the distributed lasso algorithm, except for the xi\nupdate, which here involves an \u21132 regularized logistic regression prob-\nlem that can be e\ufb03ciently solved by algorithms like L-BFGS.\n8.2.3 Support Vector Machine\nUsing the notation of (8.1), the algorithm is\nxk+1\ni := argmin\nxi\n(\n1T (Aixi + 1)+ +( \u03c1/2)\u2225xi \u2212 zk + uk\ni \u22252\n2\n)\nzk+1 := \u03c1\n(1/\u03bb)+ N\u03c1 (xk+1 + uk)\nuk+1\ni := uk\ni + xk+1\ni \u2212 zk+1.\nEach xi-update essentially involves \ufb01tting a support vector machine to\nthe local data Ai (with an o\ufb00set in the quadratic regularization term),\nso this can be carried out e\ufb03ciently using an existing SVM solver for\nserial problems.\nThe use of ADMM to train support vector machines in a distributed\nfashion was described in [74].\n8.3 Splitting across Features\nNow we consider the model \ufb01tting problem (8.1) with a modest num-\nber of examples and a large number of features. Statistical problems\nof this kind frequently arise in areas like natural language processing\nand bioinformatics, where there are often a large number of potential", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1449, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6327fd99-ca61-4c04-ab8b-794ed99e1447": {"__data__": {"id_": "6327fd99-ca61-4c04-ab8b-794ed99e1447", "embedding": null, "metadata": {"page_label": "67", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "df0c2233-afc8-42c9-b8ac-d724076b60a7", "node_type": "4", "metadata": {"page_label": "67", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "5b555e085167623b09379a54e6f6c4b95c465b8875d3726b2d2dd12a8b59c1e0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8.3 Splitting across Features 67\nexplanatory variables for any given outcome. For example, the obser-\nvations may be a corpus of documents, and the features could include\nall words and pairs of adjacent words (bigrams) that appear in each\ndocument. In bioinformatics, there are usually relatively few people\nin a given association study, but there can be a very large number of\npotential features relating to factors like observed DNA mutations in\neach individual. There are many examples in other areas as well, and\nthe goal is to solve such problems in a distributed fashion with each\nprocessor handling a subset of the features. In this section, we show\nhow this can be done by formulating it as a sharing problem from\u00a77.3.\nWe partition the parameter vector x as x =( x\n1,...,x N ), with xi \u2208\nRni, where \u2211 N\ni=1 ni = n. Conformably partition the data matrix A as\nA =[ A1 \u00b7\u00b7\u00b7 AN ], with Ai \u2208 Rm\u00d7ni, and the regularization function as\nr(x)= \u2211 N\ni=1 ri(xi). This implies that Ax = \u2211 N\ni=1 Aixi, i.e., Aixi can\nbe thought of as a \u2018partial\u2019 prediction of b using only the features ref-\nerenced in xi. The model \ufb01tting problem (8.1) becomes\nminimize l\n(\u2211 N\ni=1 Aixi \u2212 b\n)\n+ \u2211 N\ni=1 ri(xi).\nFollowing the approach used for the sharing problem (7.12), we express\nthe problem as\nminimize l\n(\u2211 N\ni=1 zi \u2212 b\n)\n+ \u2211 N\ni=1 ri(xi)\nsubject to Aixi \u2212 zi =0 ,i =1 ,...,N,\nwith new variables zi \u2208 Rm. The derivation and simpli\ufb01cation of\nADMM also follows that for the sharing problem. The scaled form\nof ADMM is\nx\nk+1\ni := argmin\nxi\n(\nri(xi)+( \u03c1/2)\u2225Aixi \u2212 zk\ni + uk\ni \u22252\n2\n)\nzk+1 := argmin\nz\n(\nl(\nN\u2211\ni=1\nzi \u2212 b)+\nN\u2211\ni=1\n(\u03c1/2)\u2225Aixk+1\ni \u2212 zk\ni + uk\ni \u22252\n2\n)\nuk+1\ni := uk\ni + Aixk+1\ni \u2212 zk+1\ni .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1676, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "617f5ffe-02dd-4de8-ad20-90888f039f77": {"__data__": {"id_": "617f5ffe-02dd-4de8-ad20-90888f039f77", "embedding": null, "metadata": {"page_label": "68", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "48bb777f-5654-4d58-b6cf-b579e21e6fb9", "node_type": "4", "metadata": {"page_label": "68", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "4c83aae0fac04c7618915b51157c9b4cd6f3a31d4a3c507e3edcf624b6977d2b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "68 Distributed Model Fitting\nAs in the discussion for the sharing problem, we carry out thez-update\nby \ufb01rst solving for the average zk+1:\nzk+1 := argmin\nz\n(\nl(Nz \u2212 b)+( N\u03c1/2)\u2225z \u2212 Ax\nk+1\n\u2212 uk\u22252\n2\n)\nzk+1\ni := zk+1 + Aixk+1\ni + uk\ni \u2212 Ax\nk+1\n\u2212 uk,\nwhere Ax\nk+1\n=( 1/N)\u2211 N\ni=1 Aixk+1\ni . Substituting the last expression\ninto the update for ui, we \ufb01nd that\nuk+1\ni = Ax\nk+1\n+ uk \u2212 zk+1,\nwhich shows that, as in the sharing problem, all the dual variables are\nequal. Using a single dual variable uk \u2208 Rm, and eliminating zi,w e\narrive at the algorithm\nxk+1\ni := argmin\nxi\n(\nri(xi)+( \u03c1/2)\u2225Aixi \u2212 Aixk\ni \u2212 zk + Ax\nk\n+ uk\u22252\n2\n)\nzk+1 := argmin\nz\n(\nl(Nz \u2212 b)+( N\u03c1/2)\u2225z \u2212 Ax\nk+1\n\u2212 uk\u22252\n2\n)\nuk+1 := uk + Ax\nk+1\n\u2212 zk+1.\nThe \ufb01rst step involves solving N parallel regularized least squares\nproblems in ni variables each. Between the \ufb01rst and second steps, we\ncollect and sum the partial predictors Aixk+1\ni to form Ax\nk+1\n. The sec-\nond step is a single minimization in m variables, a quadratically regu-\nlarized loss minimization problem; the third step is a simple update in\nm variables.\nThis algorithm does not require l to be separable in the training\nexamples, as assumed earlier. If l is separable, then the\nz-update fully\nsplits intom separate scalar optimization problems. Similarly, the regu-\nlarizer r only needs to be separable at the level of the blocks of features.\nFor example, if r is a sum-of-norms, as in \u00a76.4.2, then it would be nat-\nural to have each subsystem handle a separate group.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1488, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "364cd22c-e52c-442c-a21a-bc076ed7abf4": {"__data__": {"id_": "364cd22c-e52c-442c-a21a-bc076ed7abf4", "embedding": null, "metadata": {"page_label": "69", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c51adbaa-8270-46ba-8cf6-bbb9e291580a", "node_type": "4", "metadata": {"page_label": "69", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "858c082c49d956d5d910346a85334649b944d3428e6c1ea500a719b998b8f540", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8.3 Splitting across Features 69\n8.3.1 Lasso\nIn this case, the algorithm above becomes\nxk+1\ni := argmin\nxi\n(\n(\u03c1/2)\u2225Aixi \u2212 Aixk\ni \u2212 zk + Ax\nk\n+ uk\u22252\n2 + \u03bb\u2225xi\u22251\n)\nzk+1 := 1\nN + \u03c1\n(\nb + \u03c1Ax\nk+1\n+ \u03c1uk\n)\nuk+1 := uk + Ax\nk+1\n\u2212 zk+1.\nEach xi-update is a lasso problem withni variables, which can be solved\nusing any single processor lasso method.\nIn the xi-updates, we have xk+1\ni = 0 (meaning that none of the\nfeatures in the ith block are used) if and only if\n\ued79\ued79\n\ued79A\nT\ni (Aixk\ni + zk \u2212 Ax\nk\n\u2212 uk)\n\ued79\ued79\n\ued79\n2\n\u2264 \u03bb/\u03c1.\nWhen this occurs, the xi-update is fast (compared to the case when\nxk+1\ni \u0338= 0). In a parallel implementation, there is no bene\ufb01t to speeding\nup only some of the tasks being executed in parallel, but in a serial\nsetting we do bene\ufb01t.\n8.3.2 Group Lasso\nConsider the group lasso problem with the feature groups coinciding\nwith the blocks of features, and \u2113\n2 norm (not squared) regularization:\nminimize (1 /2)\u2225Ax \u2212 b\u22252\n2 + \u03bb\u2211 N\ni=1 \u2225xi\u22252.\nThe z-update and u-update are the same as for the lasso, but the xi\nupdate becomes\nxk+1\ni := argmin\nxi\n(\n(\u03c1/2)\u2225Aixi \u2212 Aixk\ni \u2212 zk + Ax\nk\n+ uk\u22252\n2 + \u03bb\u2225xi\u22252\n)\n.\n(Only the subscript on the last norm di\ufb00ers from the lasso case.) This\ninvolves minimizing a function of the form\n(\u03c1/2)\u2225Aixi \u2212 v\u22252\n2 + \u03bb\u2225xi\u22252,\nwhich can be carried out as follows. The solution is xi = 0 if and only\nif \u2225AT\ni v\u22252 \u2264 \u03bb/\u03c1. Otherwise, the solution has the form\nxi =( AT\ni Ai + \u03bdI)\u22121AT\ni v,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1401, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "108c0891-258a-4b24-b3b4-16e7421ddcaa": {"__data__": {"id_": "108c0891-258a-4b24-b3b4-16e7421ddcaa", "embedding": null, "metadata": {"page_label": "70", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "376fccce-f29f-4d2d-9746-0f3cf167b13b", "node_type": "4", "metadata": {"page_label": "70", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "51fd6ec308c70c04dbafa6eba133460f8930221194281a314eff9100b1b57efc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "70 Distributed Model Fitting\nfor the value of\u03bd> 0 that gives \u03bd\u2225xi\u22252 = \u03bb/\u03c1. This value can be found\nusing a one-parameter search (e.g., via bisection) over\u03bd. We can speed\nup the computation of xi for several values of \u03bd (as needed for the\nparameter search) by computing and caching an eigendecomposition of\nA\nT\ni Ai. Assuming Ai is tall, i.e., m \u2265 ni (a similar method works when\nm<n i), we compute an orthogonalQ for whichAT\ni Ai = Qdiag(\u03bb)QT ,\nwhere \u03bb is the vector of eigenvalues of AT\ni Ai (i.e., the squares of the\nsingular values of Ai). The cost is O(mn2\ni ) \ufb02ops, dominated (in order)\nby forming AT\ni Ai. We subsequently compute \u2225xi\u22252 using\n\u2225xi\u22252 =\n\ued79\ued79diag(\u03bb + \u03bd1)\u22121QT AT\ni v\n\ued79\ued79\n2 .\nThis can be computed in O(ni) \ufb02ops, once QT AT\ni v is computed, so\nthe search over \u03bd is costless (in order). The cost per iteration is thus\nO(mni) (to compute QT AT\ni v), a factor of ni better than carrying out\nthe xi-update without caching.\n8.3.3 Sparse Logistic Regression\nThe algorithm is identical to the lasso problem above, except that the\nz-update becomes\nzk+1 := argmin\nz\n(\nl(Nz)+( \u03c1/2)\u2225z \u2212 Ax\nk+1\n\u2212 uk\u22252\n2\n)\n,\nwhere l is the logistic loss function. This splits to the component level,\nand involves the proximity operator for l. This can be very e\ufb03ciently\ncomputed by a lookup table that gives the approximate value, followed\nby one or two Newton steps (for a scalar problem).\nIt is interesting to see that in distributed sparse logistic regression,\nthe dominant computation is the solution ofN parallel lasso problems.\n8.3.4 Support Vector Machine\nThe algorithm is\nx\nk+1\ni := argmin\nxi\n(\n(\u03c1/2)\u2225Aixi \u2212 Aixk\ni \u2212 zk + Ax\nk\n+ uk\u22252\n2 + \u03bb\u2225xi\u22252\n2\n)\nzk+1 := argmin\nz\n(\n1T (Nz + 1)+ +( \u03c1/2)\u2225z \u2212 Ax\nk+1\n\u2212 uk\u22252\n2\n)\nuk+1 := uk + Ax\nk+1\n\u2212 zk+1.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1729, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fbc3e0b0-beaf-4635-b221-474877d34e8b": {"__data__": {"id_": "fbc3e0b0-beaf-4635-b221-474877d34e8b", "embedding": null, "metadata": {"page_label": "71", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0938c9b3-cb06-4f3f-a9c1-9af26e07643f", "node_type": "4", "metadata": {"page_label": "71", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "5705cf538860df495d1dc22c10638d5bf8a06267d8da898fb22c5818b385bf9f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8.3 Splitting across Features 71\nThe xi-updates involve quadratic functions, and require solving ridge\nregression problems. The z-update splits to the component level, and\ncan be expressed as the shifted soft thresholding operation\nzk+1\ni :=\n\uf8f1\n\uf8f2\n\uf8f3\nvi \u2212 N/\u03c1 v i > \u22121/N + N/\u03c1\n\u22121/N v i \u2208 [\u22121/N,\u22121/N + N/\u03c1]\nvi vi < \u22121/N,\nwhere v = Ax\nk+1\n+ uk (and here, the subscript denotes the entry in\nthe vector zk+1).\n8.3.5 Generalized Additive Models\nA generalized additive model has the form\nb \u2248\nn\u2211\nj=1\nfj(aj),\nwhere aj is the jth element of the feature vectora, and fj : R \u2192 R are\nthe feature functions. When the feature functions fj are linear, i.e.,o f\nthe form fj(aj)= wjaj, this reduces to standard linear regression.\nWe choose the feature functions by solving the optimization problem\nminimize \u2211 m\ni=1 li(\u2211 n\nj=1 fj(aij) \u2212 bi)+ \u2211 n\nj=1 rj(fj),\nwhere aij is the jth component of the feature vector of theith example,\nand bi is the associated outcome. Here the optimization variables are\nthe functions fj \u2208F j, where Fj is a subspace of functions; rj is now\na regularization functional. Usually fj is linearly parametrized by a\n\ufb01nite number of coe\ufb03cients, which are the underlying optimization\nvariables, but this formulation can also handle the case when F\nj is\nin\ufb01nite-dimensional. In either case, it is clearer to think of the feature\nfunctions f\nj as the variables to be determined.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1377, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c6622d93-31e0-483b-8dd2-fe7dc1071ff5": {"__data__": {"id_": "c6622d93-31e0-483b-8dd2-fe7dc1071ff5", "embedding": null, "metadata": {"page_label": "72", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e48a3946-2f14-4abc-a2ca-4ae46205d74a", "node_type": "4", "metadata": {"page_label": "72", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "a512438c2be611f821e7126d8843fbc7a626c50c78f4161d9f4bc4ffac95c6c0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "72 Distributed Model Fitting\nWe split the features down to individual functions so N = n. The\nalgorithm is\nfk+1\nj :=\nargmin\nfj\u2208Fj\n(\nrj(fj)+( \u03c1/2)\u2211 m\ni=1(fj(aij) \u2212 fk\nj (aij) \u2212 zk\ni + f\nk\ni + uk\ni )2\n)\nzk+1 := argmin\nz\n(\u2211 m\ni=1 li(Nzi \u2212 bi)+( \u03c1/2)\u2211 N\ni=1 \u2225z \u2212 f\nk+1\n\u2212 uk\u22252\n2\n)\nuk+1 := uk + f\nk+1\n\u2212 zk+1,\nwhere f\nk\ni =( 1/n)\u2211 n\nj=1 fk\nj (aij), the average value of the predicted\nresponse \u2211 n\nj=1 fk\nj (aij) for the ith feature.\nThe fj-update is an \u21132 (squared) regularized function \ufb01t. The z-\nupdate can be carried out componentwise.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 531, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8d93b1c4-40b3-4fa0-80be-c083cbdf53b3": {"__data__": {"id_": "8d93b1c4-40b3-4fa0-80be-c083cbdf53b3", "embedding": null, "metadata": {"page_label": "73", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e5a34ea-6545-4637-b648-bc3a2d1168f0", "node_type": "4", "metadata": {"page_label": "73", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "dc57255198d29c5c33c570df8f8e5684a05062db14581055fa396dcb101e73f9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9\nNonconvex Problems\nWe now explore the use of ADMM for nonconvex problems, focusing\non cases in which the individual steps in ADMM, i.e., the x- and z-\nupdates, can be carried out exactly. Even in this case, ADMM need not\nconverge, and when it does converge, it need not converge to an optimal\npoint; it must be considered just another local optimization method.\nThe hope is that it will possibly have better convergence properties\nthan other local optimization methods, where \u2018better convergence\u2019 can\nmean faster convergence or convergence to a point with better objective\nvalue. For nonconvex problems, ADMM can converge to di\ufb00erent (and\nin particular, nonoptimal) points, depending on the initial values x\n0\nand y0 and the parameter \u03c1.\n9.1 Nonconvex Constraints\nConsider the constrained optimization problem\nminimize f(x)\nsubject to x \u2208S ,\n73", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 846, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2569e1dc-bc28-4da9-9a2f-faee0fad778a": {"__data__": {"id_": "2569e1dc-bc28-4da9-9a2f-faee0fad778a", "embedding": null, "metadata": {"page_label": "74", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9162b5d8-1bdb-4b21-8319-bb413876ea91", "node_type": "4", "metadata": {"page_label": "74", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "5e93d625b98b7284a4d6416d54c865dca09bc5002a968e98d1b10a5ac55e2505", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "74 Nonconvex Problems\nwith f convex, but S nonconvex. Here, ADMM has the form\nxk+1 := argmin\nx\n(\nf(x)+( \u03c1/2)\u2225x \u2212 zk + uk\u22252\n2\n)\nzk+1 := \u03a0S(xk+1 + uk)\nuk+1 := uk + xk+1 \u2212 zk+1,\nwhere \u03a0S is projection ontoS. Thex-minimization step (which is evalu-\nating a proximal operator) is convex sincef is convex, but thez-update\nis projection onto a nonconvex set. In general, this is hard to compute,\nbut it can be carried out exactly in some important special cases we\nlist below.\n\u2022 Cardinality. If S = {x | card(x) \u2264 c}, where card gives the\nnumber of nonzero elements, then \u03a0\nS(v) keeps the c largest\nmagnitude elements and zeroes out the rest.\n\u2022 Rank. If S is the set of matrices with rank c, then \u03a0 S(v)\nis determined by carrying out a singular value decomposi-\ntion, v = \u2211\ni \u03c3iuivT\ni , and keeping the top c dyads, i.e., form\n\u03a0S(v)= \u2211 c\ni=1 \u03c3iuivT\ni .\n\u2022 Boolean constraints.If S = {x | xi \u2208{ 0,1}}, then \u03a0S(v) sim-\nply rounds each entry to 0 or 1, whichever is closer. Integer\nconstraints can be handled in the same way.\n9.1.1 Regressor Selection\nAs an example, consider the least squaresregressor selectionor feature\nselection problem,\nminimize \u2225Ax \u2212 b\u2225\n2\n2\nsubject to card(x) \u2264 c,\nwhich is to \ufb01nd the best \ufb01t tob as a linear combination of no more than\nc columns of A. For this problem, ADMM takes the form above, where\nthe x-update involves a regularized least squares problem, and the z-\nupdate involves keeping thec largest magnitude elements ofxk+1 + uk.\nThis is just like ADMM for the lasso, except that soft thresholding is", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1525, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a6c5efa8-1a8d-4aa6-8fb8-9d45551e2153": {"__data__": {"id_": "a6c5efa8-1a8d-4aa6-8fb8-9d45551e2153", "embedding": null, "metadata": {"page_label": "75", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "002a56bd-7e0e-474a-8f2c-623dae5ee463", "node_type": "4", "metadata": {"page_label": "75", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "28aeacff19b32bf06372ffb9cb2f5215cacfc46b2b892821597e66cd1bad146f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9.1 Nonconvex Constraints 75\nreplaced with \u2018hard\u2019 thresholding. This close connection is hardly sur-\nprising, since lasso can be thought of as a heuristic for solving the\nregressor selection problem. From this viewpoint, the lasso controls the\ntrade-o\ufb00 between least squares error and sparsity through the param-\neter \u03bb, whereas in ADMM for regressor selection, the same trade-o\ufb00 is\ncontrolled by the parameter c, the exact cardinality desired.\n9.1.2 Factor Model Fitting\nThe goal is to approximate a symmetric matrix \u03a3 (say, an empiri-\ncal covariance matrix) as a sum of a rank- k and a diagonal positive\nsemide\ufb01nite matrix. Using the Frobenius norm to measure approxima-\ntion error, we have the problem\nminimize (1 /2)\u2225X + diag(d) \u2212 \u03a3\u2225\n2\nF\nsubject to X \u2265 0, Rank(X)= k, d \u2265 0,\nwith variablesX \u2208 Sn, d \u2208 Rn. (Any convex loss function could be used\nin lieu of the Frobenius norm.)\nWe take\nf(X) = inf\nd\u22650\n(1/2)\u2225X + diag(d) \u2212 \u03a3\u22252\nF\n=( 1/2)\n\u2211\ni\u0338=j\n(Xij \u2212 \u03a3ij)2 +( 1/2)\nn\u2211\ni=1\n(Xii \u2212 \u03a3ii)2\n+,\nwith the optimizing d having the form di =( \u03a3ii \u2212 Xii)+, i =1 ,...,n .\nWe take S to be the set of positive semide\ufb01nite rank-k matrices.\nADMM for the factor model \ufb01tting problem is then\nXk+1 := argmin\nX\n(\nf(X)+( \u03c1/2)\u2225X \u2212 Zk + Uk\u22252\nF\n)\nZk+1 := \u03a0S(Xk+1 + Uk)\nUk+1 := Uk + Xk+1 \u2212 Zk+1,\nwhere Z, U \u2208 Sn. The X-update is separable to the component level,\nand can be expressed as\n(Xk+1)ij := (1 /(1 + \u03c1))\n(\n\u03a3ij + \u03c1(Zk\nij \u2212 Uk\nij)\n)\ni \u0338= j\n(Xk+1)ii := (1 /(1 + \u03c1))\n(\n\u03a3ii + \u03c1(Zk\nii \u2212 Uk\nii)\n)\n\u03a3ii \u2264 Zii \u2212 Uii\n(Xk+1)ii := Zk\nii \u2212 Uk\nii \u03a3ii >Z ii \u2212 Uii.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1531, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a01bfb75-7720-4e8d-ad8f-9fce2011072c": {"__data__": {"id_": "a01bfb75-7720-4e8d-ad8f-9fce2011072c", "embedding": null, "metadata": {"page_label": "76", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fc36ea8f-7747-42f1-96cb-81e60e3248b1", "node_type": "4", "metadata": {"page_label": "76", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "d4f3cafd4cd05c02bb84a5affb0c9925403d5448651e9715baf75f9024e8bc65", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "76 Nonconvex Problems\nThe Z-update is carried out by an eigenvalue decomposition, keeping\nonly the dyads associated with the largest k positive eigenvalues.\n9.2 Bi-convex Problems\nAnother problem that admits exact ADMM updates is the general bi-\nconvex problem,\nminimize F(x,z)\nsubject to G(x,z)=0 ,\nwhere F : Rn \u00d7 Rm \u2192 R is bi-convex, i.e., convex in x for each z and\nconvex in z for each x, and G : Rn \u00d7 Rm \u2192 Rp is bi-a\ufb03ne, i.e., a\ufb03ne\nin x for each \ufb01xed z, and a\ufb03ne in z for each \ufb01xed x. When F is\nseparable in x and z, and G is jointly a\ufb03ne in x and z, this reduces\nto the standard ADMM problem form (3.1). For this problem ADMM\nhas the form\nx\nk+1 := argmin\nx\n(\nF(x,zk)+( \u03c1/2)\u2225G(x,zk)+ uk\u22252\n2\n)\nzk+1 := argmin\nz\n(\nF(xk+1,z)+( \u03c1/2)\u2225G(xk+1,z)+ uk\u22252\n2\n)\nuk+1 := uk + G(xk+1,zk+1).\nBoth the x- and z-updates involve convex optimization problems, and\nso are tractable.\nWhen G = 0 (or is simply absent), ADMM reduces to simple alter-\nnating minimization, a standard method for bi-convex minimization.\n9.2.1 Nonnegative Matrix Factorization\nAs an example, consider nonnegative matrix factorization [110]:\nminimize (1 /2)\u2225VW \u2212 C\u22252\nF\nsubject to Vij \u2265 0,W ij \u2265 0,\nwith variables V \u2208 Rp\u00d7r and W \u2208 Rr\u00d7q, and data C \u2208 Rp\u00d7q. In this\nform of the problem, the objective (which includes the constraints) is\nbi-a\ufb03ne, and there are no equality constraints, so ADMM becomes\nthe standard method for nonnegative matrix factorization, which is", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1423, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1b5ff690-4513-4452-816b-1f934b6e9c87": {"__data__": {"id_": "1b5ff690-4513-4452-816b-1f934b6e9c87", "embedding": null, "metadata": {"page_label": "77", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "28484c8b-0ff3-41f0-b500-bbdf5e37ffff", "node_type": "4", "metadata": {"page_label": "77", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "b6d7186cda3c33d1b44903f9b757b141a585573a0080d3557197e6dc15230b46", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9.2 Bi-convex Problems 77\nalternately minimizing overV , with W \ufb01xed, and then minimizing over\nW, with V \ufb01xed.\nWe can also introduce a new variable, moving the bi-linear term\nfrom the objective into the constraints:\nminimize (1 /2)\u2225X \u2212 C\u22252\nF + I+(V )+ I+(W)\nsubject to X \u2212 VW =0 ,\nwith variables X,V,W , where I+ is the indicator function for elemen-\ntwise nonnegative matrices. With (X,V ) serving the role of x, and W\nserving the role of z above, ADMM becomes\n(Xk+1,V k+1) := argmin\nX,V \u22650\n(\n\u2225X \u2212 C\u22252\nF +( \u03c1/2)\u2225X \u2212 VW k + Uk\u22252\nF\n)\nWk+1 := argmin\nW\u22650\n\u2225Xk+1 \u2212 V k+1W + Uk\u22252\nF\nUk+1 := Uk + Xk+1 \u2212 V k+1Wk+1.\nThe \ufb01rst step splits across the rows of X and V , so can be performed\nby solving a set of quadratic programs, in parallel, to \ufb01nd each row of\nX and V separately; the second splits in the columns of W,s oc a nb e\nperformed by solving parallel quadratic programs to \ufb01nd each column.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 887, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4cbb3c39-a01e-4e92-91da-3d44683f3634": {"__data__": {"id_": "4cbb3c39-a01e-4e92-91da-3d44683f3634", "embedding": null, "metadata": {"page_label": "78", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d1d84a30-f22f-4147-929a-0de8a22c465c", "node_type": "4", "metadata": {"page_label": "78", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "c23401ca4385e64bd3f3a6385ad8dcd1ed5e682880c9cf5907833cedaaaa4f1f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10\nImplementation\nThis section addresses the implementation of ADMM in a distributed\ncomputing environment. For simplicity, we focus on the global consen-\nsus problem with regularization,\nminimize \u2211\nN\ni=1 fi(xi)+ g(z)\nsubject to xi \u2212 z =0 ,\nwhere fi is the ith objective function term and g is the global regular-\nizer. Extensions to the more general consensus case are mostly straight-\nforward. We \ufb01rst describe an abstract implementation and then show\nhow this maps onto a variety of software frameworks.\n10.1 Abstract Implementation\nWe refer to xi and ui as local variables stored in subsystem i, and to z\nas the global variable. For a distributed implementation, it is often more\nnatural to group the local computations (i.e., the xi- and ui-updates),\nso we write ADMM as\nui := ui + xi \u2212 z\nxi := argmin\n(\nfi(xi)+( \u03c1/2)\u2225xi \u2212 z + ui\u22252\n2\n)\nz := proxg,N\u03c1 (x + u).\n78", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 866, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "12802952-3737-44bc-a7b0-aff4ffc36ab4": {"__data__": {"id_": "12802952-3737-44bc-a7b0-aff4ffc36ab4", "embedding": null, "metadata": {"page_label": "79", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9a0f0f46-0926-48d0-9fce-9164f33f84c9", "node_type": "4", "metadata": {"page_label": "79", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "ff444ea1a55beadadded4b2a24cd8df77ff443b75718d421369a5135249a49e1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10.1 Abstract Implementation 79\nHere, the iteration indices are omitted because in an actual implemen-\ntation, we can simply overwrite previous values of these variables. Note\nthat the u-update must be done before the x-update in order to match\n(7.6-7.8). If g = 0, then thez-update simply involves computing\nx, and\nthe ui are not part of the aggregation, as discussed in \u00a77.1.\nThis suggests that the main features required to implement ADMM\nare the following:\n\u2022 Mutable state. Each subsystemi must store the current values\nof xi and ui.\n\u2022 Local computation. Each subsystem must be able to solve a\nsmall convex problem, where \u2018small\u2019 means that the problem\nis solvable using a serial algorithm. In addition, each local\nprocess must have local access to whatever data are required\nto specify f\ni.\n\u2022 Global aggregation. There must be a mechanism for averag-\ning local variables and broadcasting the result back to each\nsubsystem, either by explicitly using a central collector or via\nsome other approach like distributed averaging [160, 172]. If\ncomputing z involves a proximal step (i.e.,i f g is nonzero),\nthis can either be performed centrally or at each local node;\nthe latter is easier to implement in some frameworks.\n\u2022 Synchronization. All the local variables must be updated\nbefore performing global aggregation, and the local updates\nmust all use the latest global variable. One way to implement\nthis synchronization is via a barrier, a system checkpoint at\nwhich all subsystems must stop and wait until all other sub-\nsystems reach it.\nWhen actually implementing ADMM, it helps to consider whether to\ntake the \u2018local perspective\u2019 of a subsystem performing local processing\nand communicating with a central collector, or the \u2018global perspective\u2019\nof a central collector coordinating the work of a set of subsystems.\nWhich is more natural depends on the software framework used.\nFrom the local perspective, each node i receives z, updates u\ni and\nthen xi, sends them to the central collector, waits, and then receives the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2026, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "498c2406-ae08-4f53-a5da-5b7f308424a4": {"__data__": {"id_": "498c2406-ae08-4f53-a5da-5b7f308424a4", "embedding": null, "metadata": {"page_label": "80", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7e5a61a8-cec2-4a35-9420-99ba06a81934", "node_type": "4", "metadata": {"page_label": "80", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "81750c971095aaf11bd5a5b7a879b1eb65ed7bdf6886bb56558904c022308940", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "80 Implementation\nupdated z. From the global perspective, the central collector broadcasts\nz to the subsystems, waits for them to \ufb01nish local processing, gathers\nall the xi and ui, and updates z. (Of course, if \u03c1 varies across itera-\ntions, then \u03c1 must also be updated and broadcast when z is updated.)\nThe nodes must also evaluate the stopping criteria and decide when to\nterminate; see below for examples.\nIn the general form consensus case, which we do not discuss here,\na decentralized implementation is possible that does not require z to\nbe centrally stored; each set of subsystems that share a variable can\ncommunicate among themselves directly. In this setting, it can be con-\nvenient to think of ADMM as a message-passing algorithm on a graph,\nwhere each node corresponds to a subsystem and the edges correspond\nto shared variables.\n10.2 MPI\nMessage Passing Interface (MPI) [77] is a language-independent\nmessage-passing speci\ufb01cation used for parallel algorithms, and is the\nmost widely used model for high-performance parallel computing today.\nThere are numerous implementations of MPI on a variety of distributed\nplatforms, and interfaces to MPI are available from a wide variety of\nlanguages, including C, C++, and Python.\nThere are multiple ways to implement consensus ADMM in MPI,\nbut perhaps the simplest is given in Algorithm 1. This pseudocode\nuses a single program, multiple data (SPMD) programming style, in\nwhich each processor or subsystem runs the same program code but\nhas its own set of local variables and can read in a separate subset\nof the data. We assume there are N processors, with each processor\ni storing local variables x\ni and ui, a (redundant) copy of the global\nvariable z, and handling only the local data implicit in the objective\ncomponent fi.\nIn step 4, Allreduce denotes using the MPI Allreduce operation to\ncompute the global sum over all processors of the contents of the vector\nw, and store the result in w on every processor; the same applies to\nthe scalar t. After step 4, then, w = \u2211\nn\ni=1(xi + ui)= N(x + u) and\nt = \u2225r\u22252\n2 = \u2211 n\ni=1 \u2225ri\u22252\n2 on all processors. We use Allreduce because", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2133, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "79412024-e706-43cf-9fd3-ca2cd10d7f62": {"__data__": {"id_": "79412024-e706-43cf-9fd3-ca2cd10d7f62", "embedding": null, "metadata": {"page_label": "81", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "65f1e966-8274-4cc7-9b67-ec592f3f979a", "node_type": "4", "metadata": {"page_label": "81", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "e6c29b27cef43878ed0f32afb322a213078b546d3f37e1c63e72aea4e95eea31", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10.3 Graph Computing Frameworks 81\nAlgorithm 1 Global consensus ADMM in MPI.\ninitialize N processes, along with xi,ui,ri,z.\nrepeat\n1. Update ui := ui + xi \u2212 z.\n2. Update xi := argminx\n(\nfi(x)+( \u03c1/2)\u2225x \u2212 z + ui\u22252\n2\n)\n.\n3. Let w := xi + ui and t := \u2225ri\u22252\n2.\n4. Allreduce w and t.\n5. Let zprev := z and update z := proxg,N\u03c1 (w/N).\n6. exit if \u03c1\n\u221a\nN\u2225z \u2212 zprev\u22252 \u2264 \u03f5conv and\n\u221a\nt \u2264 \u03f5feas.\n7. Update ri := xi \u2212 z.\nits implementation is in general much more scalable than simply\nhaving each subsystem send its results directly to an explicit central\ncollector.\nNext, in steps 5 and 6, all processors (redundantly) compute the\nz-update and perform the termination test. It is possible to have the\nz-update and termination test performed on just one processor and\nbroadcast the results to the other processors, but doing so complicates\nthe code and is generally no faster.\n10.3 Graph Computing Frameworks\nSince ADMM can be interpreted as performing message-passing on a\ngraph, it is natural to implement it in a graph processing framework.\nConceptually, the implementation will be similar to the MPI case dis-\ncussed above, except that the role of the central collector will often be\nhandled abstractly by the system, rather than having an explicit central\ncollector process. In addition, higher-level graph processing frameworks\nprovide a number of built in services that one would otherwise have to\nmanually implement, such as fault tolerance.\nMany modern graph frameworks are based on or inspired by\nValiant\u2019s bulk-synchronous parallel (BSP) model [164] for parallel\ncomputation. A BSP computer consists of a set of processors net-\nworked together, and a BSP computation consists of a series of\nglobal supersteps. Each superstep consists of three stages: parallel", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1755, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eeeae394-009d-482c-826d-2421c4d4a464": {"__data__": {"id_": "eeeae394-009d-482c-826d-2421c4d4a464", "embedding": null, "metadata": {"page_label": "82", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ef2ad90-5594-4ddd-8790-31bac87f157f", "node_type": "4", "metadata": {"page_label": "82", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "2a6e54f1ec17a6847a1fdbef9a06057523f81f473bb14cedfb5dee74909c3ac4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "82 Implementation\ncomputation, in which the processors, in parallel, perform local\ncomputations; communication, in which the processors communicate\namong themselves; and barrier synchronization, in which the processes\nwait until all processes are \ufb01nished communicating.\nThe \ufb01rst step in each ADMM superstep consists of performing local\nui- and xi-updates. The communication step would broadcast the new\nxi and ui values to a central collector node, or globally to each indi-\nvidual processor. Barrier synchronization is then used to ensure that\nall the processors have updated their primal variable before the central\ncollector averages and rebroadcasts the results.\nSpeci\ufb01c frameworks directly based on or inspired by the BSP model\ninclude the Parallel BGL [91], GraphLab [114], and Pregel [119], among\nothers. Since all three follow the general outline above, we refer the\nreader to the individual papers for details.\n10.4 MapReduce\nMapReduce [46] is a popular programming model for distributed batch\nprocessing of very large datasets. It has been widely used in indus-\ntry and academia, and its adoption has been bolstered by the open\nsource project Hadoop, inexpensive cloud computing services avail-\nable through Amazon, and enterprise products and services o\ufb00ered\nby Cloudera. MapReduce libraries are available in many languages,\nincluding Java, C++, and Python, among many others, though Java\nis the primary language for Hadoop. Though it is awkward to express\nADMM in MapReduce, the amount of cloud infrastructure available\nfor MapReduce computing can make it convenient to use in practice,\nespecially for large problems. We brie\ufb02y review some key features of\nHadoop below; see [170] for general background.\nA MapReduce computation consists of a set of Map tasks, which\nprocess subsets of the input data in parallel, followed by a Reduce task,\nwhich combines the results of the Map tasks. Both the Map and Reduce\nfunctions are speci\ufb01ed by the user and operate on key-value pairs. The\nMap function performs the transformation\n(k,v ) \u21a6\u2192 [(k\n\u2032\n1,v\u2032\n1),..., (k\u2032\nm,v\u2032\nm)],", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2075, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a5375ae3-31f1-4295-b489-9dda0704b50f": {"__data__": {"id_": "a5375ae3-31f1-4295-b489-9dda0704b50f", "embedding": null, "metadata": {"page_label": "83", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b780f399-be0b-4809-875c-1f1eee3730d6", "node_type": "4", "metadata": {"page_label": "83", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "2a7a19c316304d43b53583b1663a4ddbc1304de30e8df63b36d52f644e0b0756", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10.4 MapReduce 83\nthat is, it takes a key-value pair and emits a list of intermediate\nkey-value pairs. The engine then collects all the values v\u2032\n1,...,v \u2032\nr that\ncorrespond to the same output key k\u2032 (across all Mappers) and passes\nthem to the Reduce functions, which performs the transformation\n(k\u2032,[v\u2032\n1,...,v \u2032\nr]) \u21a6\u2192 (k\u2032\u2032,R(v\u2032\n1,...,v \u2032\nr)),\nwhere R is a commutative and associative function. For example, R\ncould simply sum v\u2032\ni. In Hadoop, Reducers can emit lists of key-value\npairs rather than just a single pair.\nEach iteration of ADMM can easily be represented as a MapRe-\nduce task: The parallel local computations are performed by Maps,\nand the global aggregation is performed by a Reduce. We will describe\na simple global consensus implementation to give the general \ufb02avor\nand discuss the details below. Here, we have the Reducer compute\nAlgorithm 2 An iteration of global consensus ADMM in Hadoop/ MapReduce.\nfunction map(key i, dataset Di)\n1. Read (xi,ui, \u02c6z) from HBase table.\n2. Compute z := proxg,N\u03c1 ((1/N)\u02c6z).\n3. Update ui := ui + xi \u2212 z.\n4. Update xi := argminx\n(\nfi(x)+( \u03c1/2)\u2225x \u2212 z + ui\u22252\n2\n)\n.\n5. Emit (key central, record (xi,ui)).\nfunction reduce(key central, records (x1,u1),..., (xN ,uN ))\n1. Update \u02c6z := \u2211 N\ni=1 xi + ui.\n2. Emit (key j, record (xj,uj, \u02c6z)) to HBase for j =1 ,...,N .\n\u02c6z = \u2211 N\ni=1(xi + ui) rather than z or \u02dcz because summation is associa-\ntive while averaging is not. We assume N is known (or, alternatively,\nthe Reducer can compute the sum \u2211 N\ni=1 1). We have N Mappers, one\nfor each subsystem, and each Mapper updates ui and xi using the\n\u02c6z from the previous iteration. Each Mapper independently executes\nthe proximal step to compute z, but this is usually a cheap opera-\ntion like soft thresholding. It emits an intermediate key-value pair that\nessentially serves as a message to the central collector. There is a sin-\ngle Reducer, playing the role of a central collector, and its incoming\nvalues are the messages from the Mappers. The updated records are", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2002, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1d345b7c-1a86-474a-975b-cd3acad8fdc4": {"__data__": {"id_": "1d345b7c-1a86-474a-975b-cd3acad8fdc4", "embedding": null, "metadata": {"page_label": "84", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "350c6309-8c08-48db-a63c-c82fef335820", "node_type": "4", "metadata": {"page_label": "84", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "9a2b1bc51866b88a0c6e6bd2df76fafbeeaaed056e8c052c7f20644b9c5a6f23", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "84 Implementation\nthen written out directly to HBase by the Reducer, and a wrapper\nprogram restarts a new MapReduce iteration if the algorithm has not\nconverged. The wrapper will check whether \u03c1\n\u221a\nN\u2225z \u2212 zprev\u22252 \u2264 \u03f5conv\nand ( \u2211 N\ni=1 \u2225xi \u2212 z\u22252\n2)1/2 \u2264 \u03f5feas to determine convergence, as in the\nMPI case. (The wrapper checks the termination criteria instead of the\nReducer because they are not associative to check.)\nThe main di\ufb03culty is that MapReduce tasks are not designed to\nbe iterative and do not preserve state in the Mappers across iter-\nations, so implementing an iterative algorithm like ADMM requires\nsome understanding of the underlying infrastructure. Hadoop con-\ntains a number of components supporting large-scale, fault-tolerant\ndistributed computing applications. The relevant components here are\nHDFS, a distributed \ufb01le system based on Google\u2019s GFS [85], and\nHBase, a distributed database based on Google\u2019s BigTable [32].\nHDFS is a distributed \ufb01lesystem, meaning that it manages the\nstorage of data across an entire cluster of machines. It is designed for\nsituations where a typical \ufb01le may be gigabytes or terabytes in size and\nhigh-speed streaming read access is required. The base units of storage\nin HDFS are blocks, which are 64 MB to 128 MB in size in a typi-\ncal con\ufb01guration. Files stored on HDFS are comprised of blocks; each\nblock is stored on a particular machine (though for redundancy, there\nare replicas of each block on multiple machines), but di\ufb00erent blocks in\nthe same \ufb01le need not be stored on the same machine or even nearby.\nFor this reason, any task that processes data stored on HDFS (e.g., the\nlocal datasets D\ni) should process a single block of data at a time, since\na block is guaranteed to reside wholly on one machine; otherwise, one\nmay cause unnecessary network transfer of data.\nIn general, the input to each Map task is data stored on HDFS, and\nMappers cannot access local disk directly or perform any stateful com-\nputation. The scheduler runs each Mapper as close to its input data\nas possible, ideally on the same node, in order to minimize network\ntransfer of data. To help preserve data locality, each Map task should\nalso be assigned around a block\u2019s worth of data. Note that this is very\ndi\ufb00erent from the implementation presented for MPI, where each pro-\ncess can be told to pick up the local data on whatever machine it is\nrunning on.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2392, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "72ee43d7-7bb6-48e2-bed5-2ad4d5c5ef83": {"__data__": {"id_": "72ee43d7-7bb6-48e2-bed5-2ad4d5c5ef83", "embedding": null, "metadata": {"page_label": "85", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "485880b5-6aa5-4457-9f9f-c0345431acbc", "node_type": "4", "metadata": {"page_label": "85", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "468fe7087160e74be6dd6a9389de32236e39434fc49c9a45f84723d0907c81cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10.4 MapReduce 85\nSince each Mapper only handles a single block of data, there will\nusually be a number of Mappers running on the same machine. To\nreduce the amount of data transferred over the network, Hadoop sup-\nports the use of combiners, which essentially Reduce the results of all\nthe Map tasks on a given node so only one set of intermediate key-\nvalue pairs need to be transferred across machines for the \ufb01nal Reduce\ntask. In other words, the Reduce step should be viewed as a two-step\nprocess: First, the results of all the Mappers on each individual node\nare reduced with Combiners, and then the records across each machine\nare Reduced. This is a major reason why the Reduce function must be\ncommutative and associative.\nSince the input value to a Mapper is a block of data, we also need a\nmechanism for a Mapper to read in local variables, and for the Reducer\nto store the updated variables for the next iteration. Here, we use\nHBase, a distributed database built on top of HDFS that provides\nfast random read-write access. HBase, like BigTable, provides a dis-\ntributed multi-dimensional sorted map. The map is indexed by a row\nkey, a column key, and a timestamp. Each cell in an HBase table can\ncontain multiple versions of the same data indexed by timestamp; in\nour case, we can use the iteration counts as the timestamps to store\nand access data from previous iterations; this is useful for checking ter-\nmination criteria, for example. The row keys in a table are strings, and\nHBase maintains data in lexicographic order by row key. This means\nthat rows with lexicographically adjacent keys will be stored on the\nsame machine or nearby. In our case, variables should be stored with\nthe subsystem identi\ufb01er at the beginning of row key, so information for\nthe same subsystem is stored together and is e\ufb03cient to access. For\nmore details, see [32, 170].\nThe discussion and pseudocode above omits and glosses over many\ndetails for simplicity of exposition. MapReduce frameworks like Hadoop\nalso support much more sophisticated implementations, which may\nbe necessary for very large scale problems. For example, if there\nare too many values for a single Reducer to handle, we can use an\napproach analogous to the one suggested for MPI: Mappers emit pairs\nto \u2018regional\u2019 reduce jobs, and then an additional MapReduce step is car-\nried out that uses an identity mapper and aggregates regional results", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2408, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f8dbc309-1173-440f-8104-c915eee8a157": {"__data__": {"id_": "f8dbc309-1173-440f-8104-c915eee8a157", "embedding": null, "metadata": {"page_label": "86", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3d8f3656-6321-42e2-9002-d86c1cd373e6", "node_type": "4", "metadata": {"page_label": "86", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "4780a549dd00b578966ed29c53c88d55ccf8f209ae1bec9bb55482621ee3a81a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "86 Implementation\ninto a global result. In this section, our goal is merely to give a gen-\neral \ufb02avor of some of the issues involved in implementing ADMM in\na MapReduce framework, and we refer to [46, 170, 111] for further\ndetails. There has also been some recent work on alternative MapRe-\nduce systems that are speci\ufb01cally designed for iterative computation,\nwhich are likely better suited for ADMM [25, 179], though the imple-\nmentations are less mature and less widely available. See [37, 93] for\nexamples of recent papers discussing machine learning and optimiza-\ntion in MapReduce frameworks.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 598, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "809bf121-161a-4f7e-a144-e87f55f45fc5": {"__data__": {"id_": "809bf121-161a-4f7e-a144-e87f55f45fc5", "embedding": null, "metadata": {"page_label": "87", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38fdb2ad-2e13-4759-bdc7-84eb20045d87", "node_type": "4", "metadata": {"page_label": "87", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "d80e60df3f0f17b38f9f0e904cde40a2f9dbda8a6029ff0f987621decb89a451", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11\nNumerical Examples\nIn this section we report numerical results for several examples. The\nexamples are chosen to illustrate a variety of the ideas discussed above,\nincluding caching matrix factorizations, using iterative solvers for the\nupdates, and using consensus and sharing ADMM to solve distributed\nproblems. The implementations of ADMM are written to be as simple\nas possible, with no implementation-level optimization or tuning.\nThe \ufb01rst section discusses a small instance of the lasso problem\nwith a dense coe\ufb03cient matrix. This helps illustrate some of the basic\nbehavior of the algorithm, and the impact of some of the linear algebra-\nbased optimizations suggested in \u00a74. We \ufb01nd, for example, that we can\ncompute the entire regularization path for the lasso in not much more\ntime than it takes to solve a single problem instance, which in turn takes\nnot much more time than solving a single ridge regression problem of\nthe same size.\nWe then discuss a serial implementation of the consensus ADMM\nalgorithm applied to \u2113\n1 regularized logistic regression, where we split\nthe problem across training examples. Here, we focus on details of\nimplementing consensus ADMM for this problem, rather than on actual\n87", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1218, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "13bc16e4-b716-4244-a923-8fe3778b0c66": {"__data__": {"id_": "13bc16e4-b716-4244-a923-8fe3778b0c66", "embedding": null, "metadata": {"page_label": "88", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9dae6518-a1e9-47e1-a93c-23fe503b873f", "node_type": "4", "metadata": {"page_label": "88", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "af83746e5445ca550c28603adecfececb312dc9cfebd93bc23739defe2d378f3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "88 Numerical Examples\ndistributed solutions. The following section has a similar discussion of\nthe group lasso problem, but split across features, with each regular-\nization group corresponding to a distinct subsystem.\nWe then turn to a real large-scale distributed implementation using\nan MPI-based solver written in C. We report on the results of solving\nsome large lasso problems on clusters hosted in Amazon EC2, and \ufb01nd\nthat a fairly basic implementation is able to solve a lasso problem with\n30 GB of data in a few minutes.\nOur last example is regressor selection, a nonconvex problem.\nWe compare the sparsity-\ufb01t trade-o\ufb00 curve obtained using noncon-\nvex ADMM, directly controlling the number of regressors, with the\nsame curve obtained using the lasso regularization path (with poste-\nrior least squares \ufb01t). We will see that the curves are not the same,\nbut give very similar results. This suggests that the regressor selection\nmethod may be preferable to the lasso when the desired sparsity level\nis known in advance: It is much easier to explicitly set the desired spar-\nsity level than tuning the regularization parameter \u03bb to obtain this\nlevel.\nAll examples except the large-scale lasso are implemented in Mat-\nlab, and run on an Intel Core i3 processor running at 3.2 GHz.\nThe large lasso example is implemented in C using MPI for inter-\nprocess communication and the GNU Scienti\ufb01c Library for linear alge-\nbra. Source code and data for these examples (and others) can be\nfound at www.stanford.edu/\u02dcboyd/papers/admm_distr_stats.html\nand most are extensively commented.\n11.1 Small Dense Lasso\nWe consider a small, dense instance of the lasso problem (6.2), where\nthe feature matrix A has m = 1500 examples and n = 5000 features.\nWe generate the data as follows. We \ufb01rst choose A\nij \u223cN (0,1)\nand then normalize the columns to have unit \u21132 norm. A \u2018true\u2019 value\nxtrue \u2208 Rn is generated with 100 nonzero entries, each sampled from an\nN(0,1) distribution. The labelsb are then computed asb = Axtrue + v,\nwhere v \u223cN (0,10\u22123I), which corresponds to a signal-to-noise ratio\n\u2225Axtrue\u22252\n2/\u2225v\u22252\n2 of around 60.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2110, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "197e2008-a477-488f-aa83-feb1e6ce3642": {"__data__": {"id_": "197e2008-a477-488f-aa83-feb1e6ce3642", "embedding": null, "metadata": {"page_label": "89", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c6ebfbbf-5ae3-4e87-83ff-f82186330325", "node_type": "4", "metadata": {"page_label": "89", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "b4e2f26339899310d5a2bf8757500a11e26b5cc0b62a8f20581843b98e8cf85d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11.1 Small Dense Lasso 89\nFig. 11.1. Norms of primal residual (top) and dual residual (bottom) versus iteration, for\na lasso problem. The dashed lines show\u03f5pri (top) and \u03f5dual (bottom).\nWe set the penalty parameter \u03c1 = 1 and set termination tolerances\n\u03f5abs =1 0\u22124 and \u03f5rel =1 0\u22122. The variables u0 and z0 were initialized to\nbe zero.\n11.1.1 Single Problem\nWe \ufb01rst solve the lasso problem with regularization parameter \u03bb =\n0.1\u03bbmax, where \u03bbmax = \u2225AT b\u2225\u221e is the critical value of \u03bb above which\nthe solution of the lasso problem is x = 0. (Although not relevant, this\nchoice correctly identi\ufb01es about 80% of the nonzero entries in xtrue.)\nFigure 11.1 shows the primal and dual residual norms by iteration,\nas well as the associated stopping criterion limits \u03f5pri and \u03f5dual (which\nvary slightly in each iteration since they depend on xk, zk, and yk\nthrough the relative tolerance terms). The stopping criterion was sat-\nis\ufb01ed after 15 iterations, but we ran ADMM for 35 iterations to show\nthe continued progress. Figure 11.2 shows the objective suboptimality\n\u02dcp\nk \u2212 p\u22c6 , where \u02dcpk =( 1/2)\u2225Azk \u2212 b\u22252\n2 + \u03bb\u2225zk\u22251 is the objective value\nat zk. The optimal objective valuep\u22c6 =1 7.4547 was independently ver-\ni\ufb01ed using l1 ls [102].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1221, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6bdcdce4-a9a5-46f3-a328-1095937fcfb8": {"__data__": {"id_": "6bdcdce4-a9a5-46f3-a328-1095937fcfb8", "embedding": null, "metadata": {"page_label": "90", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "26e6a64f-61f6-466e-b9dd-bb2955c286b3", "node_type": "4", "metadata": {"page_label": "90", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "8db009ffbe55b5fe5ba5eec022c1ccd4cbc38cbf26a5c8ac64a6c0c0fdf61913", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "90 Numerical Examples\nFig. 11.2. Objective suboptimality versus iteration for a lasso problem. The stopping crite-\nrion is satis\ufb01ed at iteration 15, indicated by the vertical dashed line.\nSince A is fat (i.e., m<n ), we apply the matrix inversion lemma\nto (AT A + \u03c1I)\u22121 and instead compute the factorization of the smaller\nmatrix I +( 1/\u03c1)AAT , which is then cached for subsequent x-updates.\nThe factor step itself takes about nm2 +( 1/3)m3 \ufb02ops, which is the\ncost of forming AAT and computing the Cholesky factorization. Subse-\nquent updates require two matrix-vector multiplications and forward-\nbackward solves, which require approximately 4mn +2 m\n2 \ufb02ops. (The\ncost of the soft thresholding step in thez-update is negligible.) For these\nproblem dimensions, the \ufb02op count analysis suggests a factor/solve\nratio of around 350, which means that 350 subsequent ADMM itera-\ntions can be carried out for the cost of the initial factorization.\nIn our basic implementation, the factorization step takes about 1\nsecond, and subsequentx-updates take around 30 ms. (This gives a fac-\ntor/solve ratio of only 33, less than predicted, due to a particularly e\ufb03-\ncient matrix-matrix multiplication routine used in Matlab.) Thus the\ntotal cost of solving an entire lasso problem is around 1.5 seconds\u2014only\n50% more than the initial factorization. In terms of parameter estima-\ntion, we can say that computing the lasso estimate requires only 50%", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1433, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e1c6ff47-7a0f-40bb-b6ab-deff14562aec": {"__data__": {"id_": "e1c6ff47-7a0f-40bb-b6ab-deff14562aec", "embedding": null, "metadata": {"page_label": "91", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e3da4913-5c12-4be3-86d2-34abf95892dd", "node_type": "4", "metadata": {"page_label": "91", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "e6dc680ae377a003a8d881602c1d8ea582e88c7c9548896360883f62a0440fab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11.1 Small Dense Lasso 91\nFig. 11.3. Iterations needed versus\u03bb for warm start (solid line) and cold start (dashed line).\nmore time than a ridge regression estimate. (Moreover, in an imple-\nmentation with a higher factor/solve ratio, the additional e\ufb00ort for the\nlasso would have been even smaller.)\nFinally, we report the e\ufb00ect of varying the parameter \u03c1 on conver-\ngence time. Varying\u03c1 over the 100:1 range from 0.1 to 10 yields a solve\ntime ranging between 1.45 seconds to around 4 seconds. (In an imple-\nmentation with a larger factor/solve ratio, the e\ufb00ect of varying\u03c1 would\nhave been even smaller.) Over-relaxation with \u03b1 =1 .5 does not signif-\nicantly change the convergence time with\u03c1 = 1, but it does reduce the\nworst convergence time over the range\u03c1 \u2208 [0.1,10] to only 2.8 seconds.\n11.1.2 Regularization Path\nTo illustrate computing the regularization path, we solve the lasso prob-\nlem for 100 values of \u03bb, spaced logarithmically from 0 .01\u03bb\nmax (where\nx\u22c6 has around 800 nonzeros) to 0 .95\u03bbmax (where x\u22c6 has two nonzero\nentries). We \ufb01rst solve the lasso problem as above for \u03bb =0 .01\u03bbmax,\nand for each subsequent value of \u03bb, we then initialize (warm start) z\nand u at their optimal values for the previous\u03bb. This requires only one\nfactorization for all the computations; warm starting ADMM at the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1306, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c22aab93-ecf4-46e5-8abc-f973ac43cf30": {"__data__": {"id_": "c22aab93-ecf4-46e5-8abc-f973ac43cf30", "embedding": null, "metadata": {"page_label": "92", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7eeab61e-ba51-404d-9f9a-eecc8bc5a0b5", "node_type": "4", "metadata": {"page_label": "92", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "946536e3e68779357dc63c2846d3966c95369578b30a3cd40eed06a643674343", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "92 Numerical Examples\nTask Time (s)\nFactorization 1.1\nx-Update 0.03\nSingle lasso (\u03bb =0 .1\u03bbmax) 1.5\nCold start regularization path (100 values of\u03bb) 160\nWarm start regularization path (100 values of\u03bb)1 3\nTable 11.1. Summary of timings for lasso example.\nprevious value signi\ufb01cantly reduces the number of ADMM iterations\nrequired to solve each lasso problem after the \ufb01rst one.\nFigure 11.3 shows the number of iterations required to solve each\nlasso problem using this warm start initialization, compared to the\nnumber of iterations required using a cold start of z\n0 = u0 = 0 for\neach \u03bb. For the 100 values of \u03bb, the total number of ADMM itera-\ntions required is 428, which takes 13 seconds in all. By contrast, with\ncold starts, we need 2166 total ADMM iterations and 100 factorizations\nto compute the regularization path, or around 160 seconds total. This\ntiming information is summarized in Table 11.1.\n11.2 Distributed \u21131 Regularized Logistic Regression\nIn this example, we use consensus ADMM to \ufb01t an\u21131 regularized logis-\ntic regression model. Following \u00a78, the problem is\nminimize\nm\u2211\ni=1\nlog\n(\n1 + exp(\u2212bi(aT\ni w + v))\n)\n+ \u03bb\u2225w\u22251, (11.1)\nwith optimization variables w \u2208 Rn and v \u2208 R. The training set con-\nsists of m pairs (ai,bi), where ai \u2208 Rn is a feature vector and bi \u2208\n{\u22121,1} is the corresponding label.\nWe generated a problem instance with m =1 06 training examples\nand n =1 04 features. The m examples are distributed among N = 100\nsubsystems, so each subsystem has 104 training examples. Each feature\nvector ai was generated to have approximately 10 nonzero features,\neach sampled independently from a standard normal distribution. We\nchose a \u2018true\u2019 weight vector w\ntrue \u2208 Rn to have 100 nonzero values,\nand these entries, along with the true intercept vtrue, were sampled", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1784, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "94d11640-707a-4b97-bce1-c44456ab92a7": {"__data__": {"id_": "94d11640-707a-4b97-bce1-c44456ab92a7", "embedding": null, "metadata": {"page_label": "93", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "089efd66-9a30-4768-961a-5f2d28528cb1", "node_type": "4", "metadata": {"page_label": "93", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "83242eeda1b4cfc898716c5094142e88da011362f48c40b3b242d3c71dca768c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11.2 Distributed \u21131 Regularized Logistic Regression 93\nindependently from a standard normal distribution. The labels bi were\nthen generated using\nbi = sign(aT\ni wtrue + vtrue + vi),\nwhere vi \u223cN (0,0.1).\nThe regularization parameter is set to \u03bb =0 .1\u03bbmax, where \u03bbmax is\nthe critical value above which the solution of the problem is w\u22c6 =0 .\nHere \u03bbmax is more complicated to describe than in the simple lasso case\ndescribed above. Let \u03b8neg be the fraction of examples withbi = \u22121 and\n\u03b8pos the fraction with bi = 1, and let \u02dcb \u2208 Rm be a vector with entries\n\u03b8neg where bi = 1 and \u2212\u03b8pos where bi = \u22121. Then \u03bbmax = \u2225AT\u02dcb\u2225\u221e (see\n[103, \u00a72.1]). (While not relevant here, the \ufb01nal \ufb01tted model with \u03bb =\n0.1\u03bbmax classi\ufb01ed the training examples with around 90% accuracy.)\nFitting the model involves solving the global consensus problem\n(8.3) in \u00a78.2 with local variables xi =( vi,wi) and consensus variable\nz =( v,w ). As in the lasso example, we used \u03f5abs =1 0\u22124 and \u03f5rel =\n10\u22122 as tolerances and used the initialization u0\ni =0 ,z0 = 0. We use\nthe penalty parameter value \u03c1 = 1 for the iterations.\nWe used L-BFGS to carry out the xi-updates. We used Nocedal\u2019s\nFortran 77 implementation of L-BFGS with no tuning: We used default\nparameters, a memory of 5, and a constant termination tolerance across\nADMM iterations (for a more e\ufb03cient implementation, these toler-\nances would start large and decrease with ADMM iterations). We warm\nstarted the x\ni-updates.\nWe used a serial implementation that performs the xi-updates\nsequentially; in a distributed implementation, of course, thexi-updates\nwould be performed in parallel. To report an approximation of the tim-\ning that would have been achieved in a parallel implementation, we\nreport the maximum time required to update x\ni among the K subsys-\ntems. This corresponds roughly to the maximum number of L-BFGS\niterations required for the x\ni-update.\nFigure 11.4 shows the progress of the primal and dual residual\nnorm by iteration. The dashed line shows when the stopping criterion\nhas been satis\ufb01ed (after 19 iterations), resulting in a primal residual\nnorm of about 1. Since the RMS consensus error can be expressed as\n(1/\u221a\nm)\u2225rk\u22252 where m =1 06, a primal residual norm of about 1 means\nthat on average, the elements of xi agree with z up to the third digit.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2296, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3f1001b8-a37c-4607-a40f-e5c3cc471e25": {"__data__": {"id_": "3f1001b8-a37c-4607-a40f-e5c3cc471e25", "embedding": null, "metadata": {"page_label": "94", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "243a720a-7a98-47b9-96fd-98eceb370ca4", "node_type": "4", "metadata": {"page_label": "94", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "5a647d55350e630b7cedab722c13a10c4b6cbfcf55eb06158e6dca80064d558c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "94 Numerical Examples\nFig. 11.4. Progress of primal and dual residual norm for distributed\u21131 regularized logistic\nregression problem. The dashed lines show\u03f5pri (top) and \u03f5dual (bottom).\nFig. 11.5. Left. Objective suboptimality of distributed \u21131 regularized logistic regression\nversus iteration. Right. Progress versus elapsed time. The stopping criterion is satis\ufb01ed at\niteration 19, indicated by the vertical dashed line.\nFigure 11.5 shows the suboptimality \u02dcpk \u2212 p\u22c6 for the consensus vari-\nable, where\n\u02dcpk =\nm\u2211\ni=1\nlog\n(\n1 + exp(\u2212bi(aT\ni wk + vk))\n)\n+ \u03bb\u2225wk\u22251.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 561, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c3400d92-0410-4f05-9cef-5b244be18db6": {"__data__": {"id_": "c3400d92-0410-4f05-9cef-5b244be18db6", "embedding": null, "metadata": {"page_label": "95", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "75cf788c-6b56-47c9-91ed-d23ad70fe197", "node_type": "4", "metadata": {"page_label": "95", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "2aa403aabb6f1a04bc32fdd487c2ded44e7c601056f682134d17207de8e6c23c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11.3 Group Lasso with Feature Splitting 95\nThe optimal value p\u22c6 =0 .3302 \u00d7 106 was veri\ufb01ed using l1 logreg\n[103]. The lefthand plot shows ADMM progress by iteration, while the\nrighthand plot shows the cumulative time in a parallel implementa-\ntion. It took 19 iterations to satisfy the stopping criterion. The \ufb01rst 4\niterations of ADMM took 2 seconds, while the last 4 iterations (before\nthe stopping criterion is satis\ufb01ed) took less than 0 .5 seconds. This is\nbecause as the iterates approach consensus, L-BFGS requires fewer\niterations due to warm starting.\n11.3 Group Lasso with Feature Splitting\nWe consider the group lasso example, described in \u00a76.4.2,\nminimize (1 /2)\u2225Ax \u2212 b\u22252\n2 + \u03bb\u2211 N\ni=1 \u2225xi\u22252,\nwhere x =( x1,...,x N ), with xi \u2208 Rni. We will solve the problem\nby splitting across feature groups x1,...,x N using the formulation\nin \u00a78.3.\nWe generated a problem instance with N = 200 groups of fea-\ntures, with ni = 100 features per group, for i =1 ,..., 200, for a total\nof n = 20000 features andm = 200 examples. A \u2018true\u2019 valuextrue \u2208 Rn\nwas generated, with 9 nonzero groups, resulting in 900 nonzero fea-\nture values. The feature matrix A is dense, with entries drawn from\nan N(0,1) distribution, and its columns then normalized to have unit\n\u21132 norm (as in the lasso example of \u00a711.1). The outcomes b are gener-\nated by b = Axtrue + v, where v \u223cN (0,0.1I), which corresponds to a\nsignal-to-noise ratio \u2225Axtrue\u22252\n2/\u2225v\u22252\n2 of around 60.\nWe used the penalty parameter \u03c1 = 10 and set termination toler-\nances as \u03f5abs =1 0\u22124 and \u03f5rel =1 0\u22122. The variables u0 and z0 are ini-\ntialized to be zero.\nWe used the regularization parameter value \u03bb =0 .5\u03bbmax, where\n\u03bbmax = max{\u2225AT\n1 b\u22252,..., \u2225AT\nN b\u22252}\nis the critical value of \u03bb above which the solution is x = 0. (Although\nnot relevant, this choice of \u03bb correctly identi\ufb01es 6 of the 9 nonzero\ngroups in xtrue and produces an estimate with 17 nonzero groups.) The\nstopping criterion was satis\ufb01ed after 47 iterations.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1964, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c5cd3f6a-6d93-4e8c-a908-f99df372ca20": {"__data__": {"id_": "c5cd3f6a-6d93-4e8c-a908-f99df372ca20", "embedding": null, "metadata": {"page_label": "96", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "73786048-9fed-4015-9ef4-f53b73509fdd", "node_type": "4", "metadata": {"page_label": "96", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "b90f03e6df9fff77d0ea46bb1ff371216222eaeef56811f6153f78fdc46868d3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "96 Numerical Examples\nFig. 11.6. Norms of primal residual (top) and dual residual (bottom) versus iteration, for\nthe distributed group lasso problem. The dashed lines show\u03f5pri (top) and \u03f5dual (bottom).\nThe xi-update is computed using the method described in \u00a78.3.2,\nwhich involves computing and caching eigendecompositions of AT\ni Ai.\nThe eigenvalue decompositions of AT\ni Ai took around 7 milliseconds;\nsubsequent xi-updates took around 350 microseconds, around a fac-\ntor of 20 faster. For 47 ADMM iterations, these numbers predict a\ntotal runtime in a serial implementation of about 5 seconds; the actual\nruntime was around 7 seconds. For a parallel implementation, we can\nestimate the runtime (neglecting interprocess communication and data\ndistribution) as being about 200 times faster, around 35 milliseconds.\nFigure 11.6 shows the progress of the primal and dual residual norm\nby iteration. The dashed line shows when the stopping criterion is sat-\nis\ufb01ed (after 47 iterations).\nFigure 11.7 shows the suboptimality \u02dcp\nk \u2212 p\u22c6 for the problem versus\niteration, where\n\u02dcpk =( 1/2)\u2225Axk \u2212 b\u22252\n2 + \u03bb\nK\u2211\ni=1\n\u2225xk\ni \u22252.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1115, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7905e80d-7912-4ddb-a623-c6c0b73a1040": {"__data__": {"id_": "7905e80d-7912-4ddb-a623-c6c0b73a1040", "embedding": null, "metadata": {"page_label": "97", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c36be9fe-2134-4974-9a8a-232520aa1518", "node_type": "4", "metadata": {"page_label": "97", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "d2c0b570d17ace054fef1bd066f03eafc2e8c4ba83c6caf994f7b4586d71d2bc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11.4 Distributed Large-Scale Lasso with MPI 97\nFig. 11.7. Suboptimality of distributed group lasso versus iteration. The stopping criterion\nis satis\ufb01ed at iteration 47, indicated by the vertical dashed line.\nThe optimal objective value p\u22c6 = 430.8390 was found by running\nADMM for 1000 iterations.\n11.4 Distributed Large-Scale Lasso with MPI\nIn previous sections, we discussed an idealized version of a distributed\nimplementation that was actually carried out serially for simplicity.\nWe now turn to a much more realistic distributed example, in which\nwe solve a very large instance of the lasso problem (6.2) using a dis-\ntributed solver implemented in C using MPI for inter-process communi-\ncation and the GNU Scienti\ufb01c Library (GSL) for linear algebra. In this\nexample, we split the problem across training examples rather than fea-\ntures. We carried out the experiments in a cluster of virtual machines\nrunning on Amazon\u2019s Elastic Compute Cloud (EC2). Here, we focus\nentirely on scaling and implementation details.\nThe data was generated as in\u00a711.1, except that we now solve a prob-\nlem with m = 400000 examples and n = 8000 features across N =8 0\nsubsystems, so each subsystem handles 5000 training examples. Note", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1217, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "109ed287-9f70-4690-bab7-3d4b4267afbd": {"__data__": {"id_": "109ed287-9f70-4690-bab7-3d4b4267afbd", "embedding": null, "metadata": {"page_label": "98", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd904311-7be8-4d75-a39d-db3e6741e71b", "node_type": "4", "metadata": {"page_label": "98", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "741339cd22ecb2e6b382086c6957f4cc7ecaa6c4df59940726221e17fd89a3b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "98 Numerical Examples\nthat the overall problem has a skinny coe\ufb03cient matrix but each of\nthe subproblems has a fat coe\ufb03cient matrix. We emphasize that the\ncoe\ufb03cient matrix is dense, so the full dataset requires over 30 GB to\nstore and has 3.2 billion nonzero entries in the total coe\ufb03cient matrix\nA. This is far too large to be solved e\ufb03ciently, or at all, using standard\nserial methods on commonly available hardware.\nWe solved the problem using a cluster of 10 machines. We used\nCluster Compute instances, which have 23 GB of RAM, two quad-core\nIntel Xeon X5570 \u2018Nehalem\u2019 chips, and are connected to each other\nwith 10 Gigabit Ethernet. We used hardware virtual machine images\nrunning CentOS 5.4. Since each node had 8 cores, we ran the code\nwith 80 processes, so each subsystem ran on its own core. In MPI,\ncommunication between processes on the same machine is performed\nlocally via the shared-memory Byte Transfer Layer (BTL), which pro-\nvides low latency and high bandwidth communication, while commu-\nnication across machines goes over the network. The data was sized\nso all the processes on a single machine could work entirely in RAM.\nEach node had its own attached Elastic Block Storage (EBS) volume\nthat contained only the local data relevant to that machine, so disk\nthroughput was shared among processes on the same machine but not\nacross machines. This is to emulate a scenario where each machine is\nonly processing the data on its local disk, and none of the dataset is\ntransferred over the network. We emphasize that usage of a cluster set\nup in this fashion costs under $20 per hour.\nWe solved the problem with a deliberately naive implementation of\nthe algorithm, based directly on the discussion of\u00a76.4, \u00a78.2, and \u00a710.2.\nThe implementation consists of a single \ufb01le of C code, under 400 lines\ndespite extensive comments. The linear algebra (BLAS operations and\nthe Cholesky factorization) were performed using a stock installation\nof the GNU Scienti\ufb01c Library.\nWe now report the breakdown of the wall-clock runtime. It took\nroughly 30 seconds to load all the data into memory. It then took\n4-5 minutes to form and then compute the Cholesky factorizations of\nI +( 1/\u03c1)A\niAT\ni . After caching these factorizations, it then took 0.5-2\nseconds for each subsequent ADMM iteration. This includes the back-\nsolves in the x\ni-updates and all the message passing. For this problem,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2390, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "86b07e53-90a5-45b4-9bff-b15a25733c96": {"__data__": {"id_": "86b07e53-90a5-45b4-9bff-b15a25733c96", "embedding": null, "metadata": {"page_label": "99", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "61fdfd31-cb4d-4fe2-b1ce-7c3f052da139", "node_type": "4", "metadata": {"page_label": "99", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "c37402c69595c09ff8f2331f033fd32e122835d39c918f04abbfa5fe3cd4e91c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11.4 Distributed Large-Scale Lasso with MPI 99\nTotal dataset size 30 GB\nNumber of subsystems 80\nTotal dataset dimensions 400000 \u00d7 8000\nSubsystem dimensions 5000 \u00d7 8000\nData loading time 30 seconds\nFactorization time 5 minutes\nSingle iteration time 1 second\nTotal runtime 6 minutes\nTable 11.2. Rough summary of a large dense distributed lasso example.\nADMM converged in 13 iterations, yielding a start-to-\ufb01nish runtime of\nunder 6 minutes to solve the whole problem. Approximate times are\nsummarized in Table 11.2.\nThough we did not compute it as part of this example, the extremely\nlow cost of each iteration means that it would be straightforward\nto compute the entire regularization path for this problem using the\nmethod described in \u00a711.1.2. In that example, it required 428 iterations\nto compute the regularization path for 100 settings of \u03bb, while it took\naround 15 iterations for a single instance to converge, roughly the same\nas in this example. Extrapolating for this case, it is plausible that the\nentire regularization path, even for this very large problem, could easily\nbe obtained in another \ufb01ve to ten minutes.\nIt is clear that by far the dominant computation is forming\nand computing the Cholesky factorization, locally and in parallel, of\neach A\nT\ni Ai + \u03c1I (or I +( 1/\u03c1)AiAT\ni , if the matrix inversion lemma\nis applied). As a result, it is worth keeping in mind that the perfor-\nmance of the linear algebra operations in our basic implementation can\nbe signi\ufb01cantly improved by using LAPACK instead of GSL for the\nCholesky factorization, and by replacing GSL\u2019s BLAS implementation\nwith a hardware-optimized BLAS library produced by ATLAS, a ven-\ndor library like Intel MKL, or a GPU-based linear algebra package. This\ncould easily lead to several orders of magnitude faster performance.\nIn this example, we used a dense coe\ufb03cient matrix so the code\ncould be written using a single simple math library. Many real-world\nexamples of the lasso have larger numbers of training examples or fea-\ntures, but are sparse and do not have billions of nonzero entries, as we\ndo here. The code we provide could be modi\ufb01ed in the usual manner", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2146, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "da21513f-a890-4594-adb1-c83ccdd16530": {"__data__": {"id_": "da21513f-a890-4594-adb1-c83ccdd16530", "embedding": null, "metadata": {"page_label": "100", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7a67226d-9103-4d88-946f-98a131d7c5ab", "node_type": "4", "metadata": {"page_label": "100", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "e7bda56968c364eb1062b78d7c47732b010e9d49d20bf9e5254a403e2fadfadb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "100 Numerical Examples\nto handle sparse or structured matrices ( e.g., use CHOLMOD [35]\nfor sparse Cholesky factorization), and would also scale to very large\nproblems. More broadly, it could also be adapted with minimal work\nto add constraints or otherwise modify the lasso problem, or even\nsolve completely di\ufb00erent problems, like training logistic regression\nor SVMs.\nIt is worth observing that ADMM scales well both horizontally and\nvertically. We could easily have solved much larger problem instances in\nroughly the same amount of time than the one described here by hav-\ning each subsystem solve a larger subproblem (up to the point where\neach machine\u2019s RAM is saturated, which it was not here); by running\nmore subsystems on each machine (though this can lead to perfor-\nmance degradation in key areas like the factorization step); or simply\nby adding more machines to the cluster, which is mostly straightfor-\nward and relatively inexpensive on Amazon EC2.\nUp to a certain problem size, the solver can be implemented by users\nwho are not expert in distributed systems, distributed linear algebra,\nor advanced implementation-level performance enhancements. This is\nin sharp contrast to what is required in many other cases. Solving\nextremely large problem instances requiring hundreds or thousands of\nmachines would require a more sophisticated implementation from a\nsystems perspective, but it is interesting to observe that a basic ver-\nsion can solve rather large problems quickly on standard software and\nhardware. To the best of our knowledge, the example above is one of\nthe largest lasso problems ever solved.\n11.5 Regressor Selection\nIn our last example, we apply ADMM to an instance of the (nonconvex)\nleast squares regressor selection problem described in\u00a79.1, which seeks\nthe best quadratic \ufb01t to a set of labelsb from a combination of no more\nthan c columns ofA (regressors). We use the sameA and b generated for\nthe dense lasso example in\u00a711.1, withm = 1500 examples andn = 5000\nfeatures, but instead of relying on the \u2113\n1 regularization heuristic to\nachieve a sparse solution, we explicitly constrain its cardinality to be\nbelow c = 100.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2159, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "13c5249d-dfce-4c69-9ed4-25161852294e": {"__data__": {"id_": "13c5249d-dfce-4c69-9ed4-25161852294e", "embedding": null, "metadata": {"page_label": "101", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a70caa47-ac2e-4bea-8807-5be978f21f00", "node_type": "4", "metadata": {"page_label": "101", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "499693b39d77768432c6aebcc9a290cdda8cf0d76936b2b2fa154e4d44fa404b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11.5 Regressor Selection 101\nFig. 11.8. Fit versus cardinality for the lasso (dotted line), lasso with posterior least squares\n\ufb01t (dashed line), and regressor selection (solid line).\nThe x-update step has exactly the same expression as in the lasso\nexample, so we use the same method, based on the matrix inversion\nlemma and caching, described in that example. Thez-update step con-\nsists of keeping thec largest magnitude components of x + u and zero-\ning the rest. For the sake of clarity, we performed an intermediate\nsorting of the components, but more e\ufb03cient schemes are possible. In\nany case, the cost of the z-update is negligible compared with that of\nthe x-update.\nConvergence of ADMM for a nonconvex problem such as this one\nis not guaranteed; and even when it does converge, the \ufb01nal result can\ndepend on the choice of\u03c1 and the initial values forz and u. To explore\nthis, we ran 100 ADMM simulations with randomly chosen initial val-\nues and \u03c1 ranging between 0.1 and 100. Indeed, some of them did not\nconverge, or at least, were converging slowly. But most of them con-\nverged, though not to exactly the same points. However, the objective\nvalues obtained by those that converged were reasonably close to each\nother, typically within 5%. The di\ufb00erent values of x found had small", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1291, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7e519b41-ec89-42fe-9403-890916131790": {"__data__": {"id_": "7e519b41-ec89-42fe-9403-890916131790", "embedding": null, "metadata": {"page_label": "102", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "21ddfb01-65c5-4553-ae6f-8922232b66a6", "node_type": "4", "metadata": {"page_label": "102", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "331f709f3b108fe3861e5771724ddc1e21cec612e3cff6a1449df046261f252b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "102 Numerical Examples\nvariations in support (choice of regressors) and value (weights), but the\nlargest weights were consistently assigned to the same regressors.\nWe now compare the use of nonconvex regressor selection with the\nlasso, in terms of obtaining the sparsity-\ufb01t trade-o\ufb00. We obtain this\ncurve for regressor selection by running ADMM for each value of c\nbetween c = 1 andc = 120. For the lasso, we compute the regularization\npath for 300 values of \u03bb; for each x\nlasso found, we then perform a least\nsquares \ufb01t using the sparsity pattern in xlasso to get our \ufb01nal x.F o r\neach cardinality, we plot the best \ufb01t found among all suchx. Figure 11.8\nshows the trade-o\ufb00 curves obtained by regressor selection and the lasso,\nwith and without posterior least squares \ufb01t. We see that while the\nresults are not exactly the same, they are quite similar, and for all\npractical purposes, equivalent. This suggests that regressor selection via\nADMM can be used as well as lasso for obtaining a good cardinality-\ufb01t\ntrade-o\ufb00; it might have an advantage when the desired cardinality is\nknown ahead of time.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1099, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "50059248-7fb1-4ca0-9336-fdc25b56d3d0": {"__data__": {"id_": "50059248-7fb1-4ca0-9336-fdc25b56d3d0", "embedding": null, "metadata": {"page_label": "103", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5f7611bf-9c7c-443b-8723-8bce5ea724d6", "node_type": "4", "metadata": {"page_label": "103", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "b80d7bade8b1db58b76258c16c71948041b3f63846fb0904b9c7180e5206665c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12\nConclusions\nWe have discussed ADMM and illustrated its applicability to dis-\ntributed convex optimization in general and many problems in statis-\ntical machine learning in particular. We argue that ADMM can serve\nas a good general-purpose tool for optimization problems arising in the\nanalysis and processing of modern massive datasets. Much like gradient\ndescent and the conjugate gradient method are standard tools of great\nuse when optimizing smooth functions on a single machine, ADMM\nshould be viewed as an analogous tool in the distributed regime.\nADMM sits at a higher level of abstraction than classical optimiza-\ntion algorithms like Newton\u2019s method. In such algorithms, the base\noperations are low-level, consisting of linear algebra operations and the\ncomputation of gradients and Hessians. In the case of ADMM, the base\noperations include solving small convex optimization problems (which\nin some cases can be done via a simple analytical formula). For exam-\nple, when applying ADMM to a very large model \ufb01tting problem, each\nupdate reduces to a (regularized) model \ufb01tting problem on a smaller\ndataset. These subproblems can be solved using any standard serial\nalgorithm suitable for small to medium sized problems. In this sense,\nADMM builds on existing algorithms for single machines, and so can be\n103", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1319, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e601a6ec-c22e-4ad0-b3d5-ff7de0d0b28f": {"__data__": {"id_": "e601a6ec-c22e-4ad0-b3d5-ff7de0d0b28f", "embedding": null, "metadata": {"page_label": "104", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "845ce1d0-92de-406e-9d14-f4e68eb5b081", "node_type": "4", "metadata": {"page_label": "104", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "4ee9497e8706de92612d5c2079a32a4260266b7fd4224b3b2cb54740ed32ca31", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "104 Conclusions\nviewed as a modular coordination algorithm that \u2018incentivizes\u2019 a set of\nsimpler algorithms to collaborate to solve much larger global problems\ntogether than they could on their own. Alternatively, it can be viewed\nas a simple way of \u2018bootstrapping\u2019 specialized algorithms for small to\nmedium sized problems to work on much larger problems than would\notherwise be possible.\nWe emphasize that for any particular problem, it is likely that\nanother method will perform better than ADMM, or that some vari-\nation on ADMM will substantially improve performance. However, a\nsimple algorithm derived from basic ADMM will often o\ufb00er perfor-\nmance that is at least comparable to very specialized algorithms (even\nin the serial setting), and in most cases, the simple ADMM algorithm\nwill be e\ufb03cient enough to be useful. In a few cases, ADMM-based meth-\nods actually turn out to be state-of-the-art even in the serial regime.\nMoreover, ADMM has the bene\ufb01t of being extremely simple to imple-\nment, and it maps onto several standard distributed programming mod-\nels reasonably well.\nADMM was developed over a generation ago, with its roots stretch-\ning far in advance of the Internet, distributed and cloud computing\nsystems, massive high-dimensional datasets, and the associated large-\nscale applied statistical problems. Despite this, it appears to be well\nsuited to the modern regime, and has the important bene\ufb01t of being\nquite general in its scope and applicability.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1474, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5cd3fe57-80f0-469a-a76b-25659275d9f5": {"__data__": {"id_": "5cd3fe57-80f0-469a-a76b-25659275d9f5", "embedding": null, "metadata": {"page_label": "105", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "991ef4fa-8d15-46b2-94a4-a6bebb870d9c", "node_type": "4", "metadata": {"page_label": "105", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "f4d20f0a655a99a564adaf9e0fb0850231b29548af60f076705ee16ebd412cc2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Acknowledgments\nWe are very grateful to Rob Tibshirani and Trevor Hastie for encour-\naging us to write this review. Thanks also to Alexis Battle, Dimitri\nBertsekas, Danny Bickson, Tom Goldstein, Dimitri Gorinevsky, Daphne\nKoller, Vicente Malave, Stephen Oakley, and Alex Teichman for help-\nful comments and discussions. Yang Wang and Matt Kraning helped in\ndeveloping ADMM for the sharing and exchange problems, and Arezou\nKeshavarz helped work out ADMM for generalized additive models. We\nthank Georgios Giannakis and Alejandro Ribeiro for pointing out some\nvery relevant references that we had missed in an earlier version. We\nthank John Duchi for a very careful reading of the manuscript and for\nsuggestions that greatly improved it.\nSupport for this work was provided in part by AFOSR grant\nFA9550-09-0130 and NASA grant NNX07AEIIA. Neal Parikh was sup-\nported by the Cortlandt and Jean E. Van Rensselaer Engineering\nFellowship from Stanford University and by the National Science Foun-\ndation Graduate Research Fellowship under Grant No. DGE-0645962.\nEric Chu was supported by the Pan Wen-Yuan Foundation Scholarship.\n105", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1126, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0cce7747-d815-4d50-b465-9257b08db87f": {"__data__": {"id_": "0cce7747-d815-4d50-b465-9257b08db87f", "embedding": null, "metadata": {"page_label": "106", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f60cce34-67bf-475c-bdaf-24756070597b", "node_type": "4", "metadata": {"page_label": "106", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "98813c9a6d3b922b7444e2101420dc9742f9b545038a636375461f142f38a377", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A\nConvergence Proof\nThe basic convergence result given in \u00a73.2 can be found in several ref-\nerences, such as [81, 63]. Many of these give more sophisticated results,\nwith more general penalties or inexact minimization. For completeness,\nwe give a proof here.\nWe will show that if f and g are closed, proper, and convex, and\nthe Lagrangian L\n0 has a saddle point, then we have primal residual\nconvergence, meaning thatrk \u2192 0, and objective convergence, meaning\nthat pk \u2192 p\u22c6 , where pk = f(xk)+ g(zk). We will also see that the dual\nresidual sk = \u03c1AT B(zk \u2212 zk\u22121) converges to zero.\nLet (x\u22c6 ,z\u22c6 ,y\u22c6 ) be a saddle point for L0, and de\ufb01ne\nV k =( 1/\u03c1)\u2225yk \u2212 y\u22c6 \u22252\n2 + \u03c1\u2225B(zk \u2212 z\u22c6 )\u22252\n2,\nWe will see that V k is a Lyapunov function for the algorithm, i.e.,a\nnonnegative quantity that decreases in each iteration. (Note thatV k is\nunknown while the algorithm runs, since it depends on the unknown\nvalues z\n\u22c6 and y\u22c6 .)\nWe \ufb01rst outline the main idea. The proof relies on three key inequal-\nities, which we will prove below using basic results from convex analysis\n106", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1057, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "414aed5c-ec53-4922-872c-c145fc79d82c": {"__data__": {"id_": "414aed5c-ec53-4922-872c-c145fc79d82c", "embedding": null, "metadata": {"page_label": "107", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e54ab27f-2295-4789-9317-288ccaf70d3a", "node_type": "4", "metadata": {"page_label": "107", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "4ce0544759dbfd912d5f638cb5c0fa95a7d39064f5bb0526d719af2b1e4f5375", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "107\nalong with simple algebra. The \ufb01rst inequality is\nV k+1 \u2264 V k \u2212 \u03c1\u2225rk+1\u22252\n2 \u2212 \u03c1\u2225B(zk+1 \u2212 zk)\u22252\n2. (A.1)\nThis states that V k decreases in each iteration by an amount that\ndepends on the norm of the residual and on the change in z over one\niteration. Because V k \u2264 V 0, it follows that yk and Bzk are bounded.\nIterating the inequality above gives that\n\u03c1\n\u221e\u2211\nk=0\n(\n\u2225rk+1\u22252\n2 + \u2225B(zk+1 \u2212 zk)\u22252\n2\n)\n\u2264 V 0,\nwhich implies that rk \u2192 0 and B(zk+1 \u2212 zk) \u2192 0a s k \u2192\u221e . Multi-\nplying the second expression by \u03c1AT shows that the dual residual\nsk = \u03c1AT B(zk+1 \u2212 zk) converges to zero. (This shows that the stop-\nping criterion (3.12), which requires the primal and dual residuals to\nbe small, will eventually hold.)\nThe second key inequality is\np\nk+1 \u2212 p\u22c6\n\u2264\u2212 (yk+1)T rk+1 \u2212 \u03c1(B(zk+1 \u2212 zk))T (\u2212rk+1 + B(zk+1 \u2212 z\u22c6 )),\n(A.2)\nand the third inequality is\np\u22c6 \u2212 pk+1 \u2264 y\u22c6T rk+1. (A.3)\nThe righthand side in (A.2) goes to zero as k \u2192\u221e , because B(zk+1 \u2212\nz\u22c6 ) is bounded and both rk+1 and B(zk+1 \u2212 zk) go to zero. The right-\nhand side in (A.3) goes to zero as k \u2192\u221e , since rk goes to zero. Thus\nwe have limk\u2192\u221e pk = p\u22c6 , i.e., objective convergence.\nBefore giving the proofs of the three key inequalities, we derive the\ninequality (3.11) mentioned in our discussion of stopping criterion from\nthe inequality (A.2). We simply observe that\u2212r\nk+1 + B(zk+1 \u2212 zk)=\n\u2212A(xk+1 \u2212 x\u22c6 ); substituting this into (A.2) yields (3.11),\npk+1 \u2212 p\u22c6 \u2264\u2212 (yk+1)T rk+1 +( xk+1 \u2212 x\u22c6 )T sk+1.\nProof of inequality (A.3)\nSince (x\u22c6 ,z\u22c6 ,y\u22c6 ) is a saddle point for L0,w eh a v e\nL0(x\u22c6 ,z\u22c6 ,y\u22c6 ) \u2264 L0(xk+1,zk+1,y\u22c6 ).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1565, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8e067e52-a3f9-4a49-8118-8c6a2f8dd9da": {"__data__": {"id_": "8e067e52-a3f9-4a49-8118-8c6a2f8dd9da", "embedding": null, "metadata": {"page_label": "108", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2afa67eb-ad74-41b0-9eb9-cd5e2c06b3b2", "node_type": "4", "metadata": {"page_label": "108", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "4e22698517fd7265db9429b37c4bc2b02628e51f935f37b352117a7be60a54f4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "108 Convergence Proof\nUsing Ax\u22c6 + Bz\u22c6 = c, the lefthand side is p\u22c6 . With pk+1 = f(xk+1)+\ng(zk+1), this can be written as\np\u22c6 \u2264 pk+1 + y\u22c6T rk+1,\nwhich gives (A.3).\nProof of inequality (A.2)\nBy de\ufb01nition, xk+1 minimizes L\u03c1(x,zk,yk). Since f is closed, proper,\nand convex it is subdi\ufb00erentiable, and so is L\u03c1. The (necessary and\nsu\ufb03cient) optimality condition is\n0 \u2208 \u2202L\u03c1(xk+1,zk,yk)= \u2202f(xk+1)+ AT yk + \u03c1AT (Axk+1 + Bzk \u2212 c).\n(Here we use the basic fact that the subdi\ufb00erential of the sum of a\nsubdi\ufb00erentiable function and a di\ufb00erentiable function with domain\nR\nn is the sum of the subdi\ufb00erential and the gradient; see, e.g., [140,\n\u00a723].)\nSince yk+1 = yk + \u03c1rk+1, we can plug in yk = yk+1 \u2212 \u03c1rk+1 and\nrearrange to obtain\n0 \u2208 \u2202f(xk+1)+ AT (yk+1 \u2212 \u03c1B(zk+1 \u2212 zk)).\nThis implies that xk+1 minimizes\nf(x)+( yk+1 \u2212 \u03c1B(zk+1 \u2212 zk))T Ax.\nA similar argument shows that zk+1 minimizes g(z)+ y(k+1)T Bz.I t\nfollows that\nf(xk+1)+( yk+1 \u2212 \u03c1B(zk+1 \u2212 zk))T Axk+1\n\u2264 f(x\u22c6 )+( yk+1 \u2212 \u03c1B(zk+1 \u2212 zk))T Ax\u22c6\nand that\ng(zk+1)+ y(k+1)T Bzk+1 \u2264 g(z\u22c6 )+ y(k+1)T Bz\u22c6 .\nAdding the two inequalities above, usingAx\u22c6 + Bz\u22c6 = c, and rearrang-\ning, we obtain (A.2).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1128, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6bffc94a-ab80-43fb-8cca-0e93b93b3d9b": {"__data__": {"id_": "6bffc94a-ab80-43fb-8cca-0e93b93b3d9b", "embedding": null, "metadata": {"page_label": "109", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9ef4bb87-a72a-4f6b-957f-8767f3b01866", "node_type": "4", "metadata": {"page_label": "109", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "41eaa05bd67f4ebb56bb0529408a2ccdfdd05581dfd941f2c8cefea16f78359f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "109\nProof of inequality (A.1)\nAdding (A.2) and (A.3), regrouping terms, and multiplying through by\n2 gives\n2(yk+1 \u2212 y\u22c6 )T rk+1 \u2212 2\u03c1(B(zk+1 \u2212 zk))T rk+1\n+2 \u03c1(B(zk+1 \u2212 zk))T (B(zk+1 \u2212 z\u22c6 )) \u2264 0.\n(A.4)\nThe result (A.1) will follow from this inequality after some manipula-\ntion and rewriting.\nWe begin by rewriting the \ufb01rst term. Substituting yk+1 = yk +\n\u03c1rk+1 gives\n2(yk \u2212 y\u22c6 )T rk+1 + \u03c1\u2225rk+1\u22252\n2 + \u03c1\u2225rk+1\u22252\n2,\nand substituting rk+1 =( 1/\u03c1)(yk+1 \u2212 yk) in the \ufb01rst two terms gives\n(2/\u03c1)(yk \u2212 y\u22c6 )T (yk+1 \u2212 yk)+( 1 /\u03c1)\u2225yk+1 \u2212 yk\u22252\n2 + \u03c1\u2225rk+1\u22252\n2.\nSince yk+1 \u2212 yk =( yk+1 \u2212 y\u22c6 ) \u2212 (yk \u2212 y\u22c6 ), this can be written as\n(1/\u03c1)\n(\n\u2225yk+1 \u2212 y\u22c6 \u22252\n2 \u2212\u2225 yk \u2212 y\u22c6 \u22252\n2\n)\n+ \u03c1\u2225rk+1\u22252\n2. (A.5)\nWe now rewrite the remaining terms, i.e.,\n\u03c1\u2225rk+1\u22252\n2 \u22122\u03c1(B(zk+1 \u2212zk))T rk+1 +2\u03c1(B(zk+1 \u2212zk))T (B(zk+1 \u2212z\u22c6 )),\nwhere \u03c1\u2225rk+1\u22252\n2 is taken from (A.5). Substituting\nzk+1 \u2212 z\u22c6 =( zk+1 \u2212 zk)+( zk \u2212 z\u22c6 )\nin the last term gives\n\u03c1\u2225rk+1 \u2212 B(zk+1 \u2212 zk)\u22252\n2 + \u03c1\u2225B(zk+1 \u2212 zk)\u22252\n2\n+2\u03c1(B(zk+1 \u2212 zk))T (B(zk \u2212 z\u22c6 )),\nand substituting\nzk+1 \u2212 zk =( zk+1 \u2212 z\u22c6 ) \u2212 (zk \u2212 z\u22c6 )\nin the last two terms, we get\n\u03c1\u2225rk+1 \u2212 B(zk+1 \u2212 zk)\u22252\n2 + \u03c1\n(\n\u2225B(zk+1 \u2212 z\u22c6 )\u22252\n2 \u2212\u2225 B(zk \u2212 z\u22c6 )\u22252\n2\n)\n.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1132, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f185b236-e9df-4afb-bc78-b2864d943ae7": {"__data__": {"id_": "f185b236-e9df-4afb-bc78-b2864d943ae7", "embedding": null, "metadata": {"page_label": "110", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bbdaf581-ff54-4afb-8449-57a33cb9acb1", "node_type": "4", "metadata": {"page_label": "110", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "22d75d58f8bafbad236aabd7c1d93f986e55f090ce432166cbf821c0fa0bbcdd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "110 Convergence Proof\nWith the previous step, this implies that (A.4) can be written as\nV k \u2212 V k+1 \u2265 \u03c1\u2225rk+1 \u2212 B(zk+1 \u2212 zk)\u22252\n2. (A.6)\nTo show (A.1), it now su\ufb03ces to show that the middle term\n\u22122\u03c1r(k+1)T (B(zk+1 \u2212 zk)) of the expanded right hand side of (A.6)\nis positive. To see this, recall that zk+1 minimizes g(z)+ y(k+1)T Bz\nand zk minimizes g(z)+ ykT Bz, so we can add\ng(zk+1)+ y(k+1)T Bzk+1 \u2264 g(zk)+ y(k+1)T Bzk\nand\ng(zk)+ ykT Bzk \u2264 g(zk+1)+ ykT Bzk+1\nto get that\n(yk+1 \u2212 yk)T (B(zk+1 \u2212 zk)) \u2264 0.\nSubstituting yk+1 \u2212 yk = \u03c1rk+1 gives the result, since \u03c1> 0.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 564, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9237f21f-d5fc-4800-83e9-81a26087d30f": {"__data__": {"id_": "9237f21f-d5fc-4800-83e9-81a26087d30f", "embedding": null, "metadata": {"page_label": "111", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "138a481a-6bc6-403a-81a8-2f5c17190273", "node_type": "4", "metadata": {"page_label": "111", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "00b93b65a5da2f2578711bed800934b929a4a60b9d8c9467b377b71242791824", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "References\n[1] M. V. Afonso, J. M. Bioucas-Dias, and M. A. T. Figueiredo, \u201cFast image recov-\nery using variable splitting and constrained optimization,\u201dIEEE Transactions\non Image Processing, vol. 19, no. 9, pp. 2345\u20132356, 2010.\n[2] M. V. Afonso, J. M. Bioucas-Dias, and M. A. T. Figueiredo, \u201cAn Aug-\nmented Lagrangian Approach to the Constrained Optimization Formulation of\nImaging Inverse Problems,\u201d IEEE Transactions on Image Processing, vol. 20,\npp. 681\u2013695, 2011.\n[3] E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. D. Croz, A. Green-\nbaum, S. Hammarling, A. McKenney, and D. Sorenson,LAPACK: A portable\nlinear algebra library for high-performance computers. IEEE Computing Soci-\nety Press, 1990.\n[4] K. J. Arrow and G. Debreu, \u201cExistence of an equilibrium for a competitive\neconomy,\u201d Econometrica: Journal of the Econometric Society, vol. 22, no. 3,\npp. 265\u2013290, 1954.\n[5] K. J. Arrow, L. Hurwicz, and H. Uzawa, Studies in Linear and Nonlinear\nProgramming. Stanford University Press: Stanford, 1958.\n[6] K. J. Arrow and R. M. Solow, \u201cGradient methods for constrained maxima,\nwith weakened assumptions,\u201d in Studies in Linear and Nonlinear Program-\nming, (K. J. Arrow, L. Hurwicz, and H. Uzawa, eds.), Stanford University\nPress: Stanford, 1958.\n[7] O. Banerjee, L. E. Ghaoui, and A. d\u2019Aspremont, \u201cModel selection through\nsparse maximum likelihood estimation for multivariate Gaussian or binary\ndata,\u201d Journal of Machine Learning Research, vol. 9, pp. 485\u2013516, 2008.\n111", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1484, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "87497604-ff97-4b4d-b3fb-a087170190ad": {"__data__": {"id_": "87497604-ff97-4b4d-b3fb-a087170190ad", "embedding": null, "metadata": {"page_label": "112", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "07f5054d-8320-4945-8af8-26da39e9bbf1", "node_type": "4", "metadata": {"page_label": "112", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "0246eae53ce3d7efd618429b695ad61a4122a084633cfcb4a671e3515638f3a6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "112 References\n[8] P. L. Bartlett, M. I. Jordan, and J. D. McAuli\ufb00e, \u201cConvexity, classi\ufb01cation,\nand risk bounds,\u201d Journal of the American Statistical Association, vol. 101,\nno. 473, pp. 138\u2013156, 2006.\n[9] H. H. Bauschke and J. M. Borwein, \u201cDykstra\u2019s alternating projection algo-\nrithm for two sets,\u201dJournal of Approximation Theory, vol. 79, no. 3, pp. 418\u2013\n443, 1994.\n[10] H. H. Bauschke and J. M. Borwein, \u201cOn projection algorithms for solving\nconvex feasibility problems,\u201d SIAM Review, vol. 38, no. 3, pp. 367\u2013426, 1996.\n[11] A. Beck and M. Teboulle, \u201cA fast iterative shrinkage-thresholding algorithm\nfor linear inverse problems,\u201dSIAM Journal on Imaging Sciences, vol. 2, no. 1,\npp. 183\u2013202, 2009.\n[12] S. Becker, J. Bobin, and E. J. Cand` es, \u201cNESTA: A fast and accurate \ufb01rst-\norder method for sparse recovery,\u201d Available at http://www.acm.caltech.\nedu/\u02dcemmanuel/papers/NESTA.pdf, 2009.\n[13] J. F. Benders, \u201cPartitioning procedures for solving mixed-variables program-\nming problems,\u201d Numerische Mathematik, vol. 4, pp. 238\u2013252, 1962.\n[14] A. Bensoussan, J.-L. Lions, and R. Temam, \u201cSur les m\u00b4 ethodes de\nd\u00b4ecomposition, de d\u00b4ecentralisation et de coordination et applications,\u201d Meth-\nodes Mathematiques de l\u2019Informatique, pp. 133\u2013257, 1976.\n[15] D. P. Bertsekas, Constrained Optimization and Lagrange Multiplier Methods.\nAcademic Press, 1982.\n[16] D. P. Bertsekas, Nonlinear Programming. Athena Scienti\ufb01c, second ed., 1999.\n[17] D. P. Bertsekas and J. N. Tsitsiklis, Parallel and Distributed Computation:\nNumerical Methods. Prentice Hall, 1989.\n[18] J. M. Bioucas-Dias and M. A. T. Figueiredo, \u201cAlternating Direction Algo-\nrithms for Constrained Sparse Regression: Application to Hyperspectral\nUnmixing,\u201d arXiv:1002.4527, 2010.\n[19] J. Borwein and A. Lewis, Convex Analysis and Nonlinear Optimization: The-\nory and Examples. Canadian Mathematical Society, 2000.\n[20] S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge University\nPress, 2004.\n[21] L. M. Bregman, \u201cFinding the common point of convex sets by the method\nof successive projections,\u201d Proceedings of the USSR Academy of Sciences ,\nvol. 162, no. 3, pp. 487\u2013490, 1965.\n[22] L. M. Bregman, \u201cThe relaxation method of \ufb01nding the common point of con-\nvex sets and its application to the solution of problems in convex program-\nming,\u201d USSR Computational Mathematics and Mathematical Physics, vol. 7,\nno. 3, pp. 200\u2013217, 1967.\n[23] H. Br\u00b4ezis, Op\u00b4erateurs Maximaux Monotones et Semi-Groupes de Contractions\ndans les Espaces de Hilbert. North-Holland: Amsterdam, 1973.\n[24] A. M. Bruckstein, D. L. Donoho, and M. Elad, \u201cFrom sparse solutions of\nsystems of equations to sparse modeling of signals and images,\u201dSIAM Review,\nvol. 51, no. 1, pp. 34\u201381, 2009.\n[25] Y. Bu, B. Howe, M. Balazinska, and M. D. Ernst, \u201cHaLoop: E\ufb03cient Iterative\nData Processing on Large Clusters,\u201d Proceedings of the 36th International\nConference on Very Large Databases, 2010.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2914, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9737deaf-25c1-49b0-afbd-67133394a021": {"__data__": {"id_": "9737deaf-25c1-49b0-afbd-67133394a021", "embedding": null, "metadata": {"page_label": "113", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40b9a1cc-cc28-4030-8dcd-b91b00b82793", "node_type": "4", "metadata": {"page_label": "113", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "d1c495689ed9482fc1ab364f20e842852f8174b1c5c64b410970cb7ae7f3e8d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "291295e5-b0f8-456b-a7d3-4e044aba2fe1", "node_type": "1", "metadata": {}, "hash": "f7c1f10846d5d90da18661f2b49ed6447fcbdf2ed17985d240ecde0efda1bfcd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "References 113\n[26] R. H. Byrd, P. Lu, and J. Nocedal, \u201cA Limited Memory Algorithm for Bound\nConstrained Optimization,\u201d SIAM Journal on Scienti\ufb01c and Statistical Com-\nputing, vol. 16, no. 5, pp. 1190\u20131208, 1995.\n[27] E. J. Cand`es and Y. Plan, \u201cNear-ideal model selection by \u21131 minimization,\u201d\nAnnals of Statistics, vol. 37, no. 5A, pp. 2145\u20132177, 2009.\n[28] E. J. Cand`es, J. Romberg, and T. Tao, \u201cRobust uncertainty principles: Exact\nsignal reconstruction from highly incomplete frequency information,\u201d IEEE\nTransactions on Information Theory, vol. 52, no. 2, p. 489, 2006.\n[29] E. J. Cand`es and T. Tao, \u201cNear-optimal signal recovery from random pro-\njections: Universal encoding strategies?,\u201d IEEE Transactions on Information\nTheory, vol. 52, no. 12, pp. 5406\u20135425, 2006.\n[30] Y. Censor and S. A. Zenios, \u201cProximal minimization algorithm with D-\nfunctions,\u201d Journal of Optimization Theory and Applications, vol. 73, no. 3,\npp. 451\u2013464, 1992.\n[31] Y. Censor and S. A. Zenios, Parallel Optimization: Theory, Algorithms, and\nApplications. Oxford University Press, 1997.\n[32] F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A. Wallach, M. Burrows,\nT. Chandra, A. Fikes, and R. E. Gruber, \u201cBigTable: A distributed storage\nsystem for structured data,\u201dACM Transactions on Computer Systems, vol. 26,\nno. 2, pp. 1\u201326, 2008.\n[33] G. Chen and M. Teboulle, \u201cA proximal-based decomposition method for con-\nvex minimization problems,\u201d Mathematical Programming, vol. 64, pp. 81\u2013101,\n1994.\n[34] S. S. Chen, D. L. Donoho, and M. A. Saunders, \u201cAtomic decomposition by\nbasis pursuit,\u201d SIAM Review, vol. 43, pp. 129\u2013159, 2001.\n[35] Y. Chen, T. A. Davis, W. W. Hager, and S. Rajamanickam, \u201cAlgo-\nrithm 887: CHOLMOD, supernodal sparse Cholesky factorization and\nupdate/downdate,\u201d ACM Transactions on Mathematical Software , vol. 35,\nno. 3, p. 22, 2008.\n[36] W. Cheney and A. A. Goldstein, \u201cProximity maps for convex sets,\u201d Proceed-\nings of the American Mathematical Society, vol. 10, no. 3, pp. 448\u2013450, 1959.\n[37] C. T. Chu, S. K. Kim, Y. A. Lin, Y. Y. Yu, G. Bradski, A. Y. Ng, and\nK. Olukotun, \u201cMapReduce for machine learning on multicore,\u201d in Advances\nin Neural Information Processing Systems, 2007.\n[38] J. F. Claerbout and F. Muir, \u201cRobust modeling with erratic data,\u201dGeophysics,\nvol. 38, p. 826, 1973.\n[39] P. L. Combettes, \u201cThe convex feasibility problem in image recovery,\u201dAdvances\nin Imaging and Electron Physics, vol. 95, pp. 155\u2013270, 1996.\n[40] P. L. Combettes and J. C. Pesquet, \u201cA Douglas-Rachford splitting approach\nto nonsmooth convex variational signal recovery,\u201d IEEE Journal on Selected\nTopics in Signal Processing, vol. 1, no. 4, pp. 564\u2013574, 2007.\n[41] P. L. Combettes and J. C. Pesquet, \u201cProximal Splitting Methods in Signal\nProcessing,\u201d arXiv:0912.3522, 2009.\n[42] P. L. Combettes and V. R. Wajs, \u201cSignal recovery by proximal forward-\nbackward splitting,\u201d Multiscale Modeling and Simulation , vol. 4, no.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2908, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "291295e5-b0f8-456b-a7d3-4e044aba2fe1": {"__data__": {"id_": "291295e5-b0f8-456b-a7d3-4e044aba2fe1", "embedding": null, "metadata": {"page_label": "113", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40b9a1cc-cc28-4030-8dcd-b91b00b82793", "node_type": "4", "metadata": {"page_label": "113", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "d1c495689ed9482fc1ab364f20e842852f8174b1c5c64b410970cb7ae7f3e8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9737deaf-25c1-49b0-afbd-67133394a021", "node_type": "1", "metadata": {"page_label": "113", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "fee2b6f54ef021ee54050493f3a05a0806ac53500b27ead36593f555e8575dd9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[39] P. L. Combettes, \u201cThe convex feasibility problem in image recovery,\u201dAdvances\nin Imaging and Electron Physics, vol. 95, pp. 155\u2013270, 1996.\n[40] P. L. Combettes and J. C. Pesquet, \u201cA Douglas-Rachford splitting approach\nto nonsmooth convex variational signal recovery,\u201d IEEE Journal on Selected\nTopics in Signal Processing, vol. 1, no. 4, pp. 564\u2013574, 2007.\n[41] P. L. Combettes and J. C. Pesquet, \u201cProximal Splitting Methods in Signal\nProcessing,\u201d arXiv:0912.3522, 2009.\n[42] P. L. Combettes and V. R. Wajs, \u201cSignal recovery by proximal forward-\nbackward splitting,\u201d Multiscale Modeling and Simulation , vol. 4, no. 4,\npp. 1168\u20131200, 2006.", "mimetype": "text/plain", "start_char_idx": 2290, "end_char_idx": 2932, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "10396245-96c9-40e6-a013-eb2c0c13f725": {"__data__": {"id_": "10396245-96c9-40e6-a013-eb2c0c13f725", "embedding": null, "metadata": {"page_label": "114", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3f07bb6e-22f7-41c6-b8da-d3796324b6ce", "node_type": "4", "metadata": {"page_label": "114", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "7ff1ad148ff1a45d29f33d7c2c9546ddf806aa9dbf2f781ae8834d4b89cc2041", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "114 References\n[43] G. B. Dantzig, Linear Programming and Extensions. RAND Corporation,\n1963.\n[44] G. B. Dantzig and P. Wolfe, \u201cDecomposition principle for linear programs,\u201d\nOperations Research, vol. 8, pp. 101\u2013111, 1960.\n[45] I. Daubechies, M. Defrise, and C. D. Mol, \u201cAn iterative thresholding algorithm\nfor linear inverse problems with a sparsity constraint,\u201d Communications on\nPure and Applied Mathematics, vol. 57, pp. 1413\u20131457, 2004.\n[46] J. Dean and S. Ghemawat, \u201cMapReduce: Simpli\ufb01ed data processing on large\nclusters,\u201d Communications of the ACM, vol. 51, no. 1, pp. 107\u2013113, 2008.\n[47] J. W. Demmel, Applied Numerical Linear Algebra. SIAM: Philadelphia, PA,\n1997.\n[48] A. P. Dempster, \u201cCovariance selection,\u201d Biometrics, vol. 28, no. 1, pp. 157\u2013\n175, 1972.\n[49] D. L. Donoho, \u201cDe-noising by soft-thresholding,\u201dIEEE Transactions on Infor-\nmation Theory, vol. 41, pp. 613\u2013627, 1995.\n[50] D. L. Donoho, \u201cCompressed sensing,\u201d IEEE Transactions on Information\nTheory, vol. 52, no. 4, pp. 1289\u20131306, 2006.\n[51] D. L. Donoho, A. Maleki, and A. Montanari, \u201cMessage-passing algorithms\nfor compressed sensing,\u201d Proceedings of the National Academy of Sciences ,\nvol. 106, no. 45, p. 18914, 2009.\n[52] D. L. Donoho and Y. Tsaig, \u201cFast solution of \u21131-norm minimization problems\nwhen the solution may be sparse,\u201d Tech. Rep., Stanford University, 2006.\n[53] J. Douglas and H. H. Rachford, \u201cOn the numerical solution of heat conduction\nproblems in two and three space variables,\u201d Transactions of the American\nMathematical Society, vol. 82, pp. 421\u2013439, 1956.\n[54] J. C. Duchi, A. Agarwal, and M. J. Wainwright, \u201cDistributed Dual Averaging\nin Networks,\u201d in Advances in Neural Information Processing Systems, 2010.\n[55] J. C. Duchi, S. Gould, and D. Koller, \u201cProjected subgradient methods for\nlearning sparse Gaussians,\u201d in Proceedings of the Conference on Uncertainty\nin Arti\ufb01cial Intelligence, 2008.\n[56] R. L. Dykstra, \u201cAn algorithm for restricted least squares regression,\u201d Journal\nof the American Statistical Association, vol. 78, pp. 837\u2013842, 1983.\n[57] J. Eckstein, Splitting methods for monotone operators with applications to\nparallel optimization. PhD thesis, MIT, 1989.\n[58] J. Eckstein, \u201cNonlinear proximal point algorithms using Bregman func-\ntions, with applications to convex programming,\u201d Mathematics of Operations\nResearch, pp. 202\u2013226, 1993.\n[59] J. Eckstein, \u201cParallel alternating direction multiplier decomposition of convex\nprograms,\u201d Journal of Optimization Theory and Applications, vol. 80, no. 1,\npp. 39\u201362, 1994.\n[60] J. Eckstein, \u201cSome saddle-function splitting methods for convex program-\nming,\u201d Optimization Methods and Software, vol. 4, no. 1, pp. 75\u201383, 1994.\n[61] J. Eckstein, \u201cA practical general approximation criterion for methods of mul-\ntipliers based on Bregman distances,\u201d Mathematical Programming, vol. 96,\nno. 1, pp. 61\u201386, 2003.\n[62] J. Eckstein and D. P. Bertsekas, \u201cAn alternating direction method for linear\nprogramming,\u201d Tech. Rep., MIT, 1990.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2980, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7415086e-15fb-40d2-a53b-24ce9afb4acc": {"__data__": {"id_": "7415086e-15fb-40d2-a53b-24ce9afb4acc", "embedding": null, "metadata": {"page_label": "115", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "66560a34-7c1b-4fa2-ad77-acda932385e7", "node_type": "4", "metadata": {"page_label": "115", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "a321a7a7eb4525debcf2dd148f8bd3fae65c18ae479bb0a78624e80e6337afdf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "References 115\n[63] J. Eckstein and D. P. Bertsekas, \u201cOn the Douglas-Rachford splitting method\nand the proximal point algorithm for maximal monotone operators,\u201d Mathe-\nmatical Programming, vol. 55, pp. 293\u2013318, 1992.\n[64] J. Eckstein and M. C. Ferris, \u201cOperator-splitting methods for monotone\na\ufb03ne variational inequalities, with a parallel application to optimal control,\u201d\nINFORMS Journal on Computing, vol. 10, pp. 218\u2013235, 1998.\n[65] J. Eckstein and M. Fukushima, \u201cSome reformulations and applications of the\nalternating direction method of multipliers,\u201d Large Scale Optimization: State\nof the Art, pp. 119\u2013138, 1993.\n[66] J. Eckstein and B. F. Svaiter, \u201cA family of projective splitting methods for\nthe sum of two maximal monotone operators,\u201d Mathematical Programming,\nvol. 111, no. 1-2, p. 173, 2008.\n[67] J. Eckstein and B. F. Svaiter, \u201cGeneral projective splitting methods for sums\nof maximal monotone operators,\u201dSIAM Journal on Control and Optimization,\nvol. 48, pp. 787\u2013811, 2009.\n[68] E. Esser, \u201cApplications of Lagrangian-based alternating direction methods\nand connections to split Bregman,\u201d CAM report, vol. 9, p. 31, 2009.\n[69] H. Everett, \u201cGeneralized Lagrange multiplier method for solving problems of\noptimum allocation of resources,\u201dOperations Research, vol. 11, no. 3, pp. 399\u2013\n417, 1963.\n[70] M. J. Fadili and J. L. Starck, \u201cMonotone operator splitting for optimization\nproblems in sparse recovery,\u201d IEEE ICIP, 2009.\n[71] A. V. Fiacco and G. P. McCormick, Nonlinear Programming: Sequential\nUnconstrained Minimization Techniques. Society for Industrial and Applied\nMathematics, 1990. First published in 1968 by Research Analysis Corporation.\n[72] M. A. T. Figueiredo and J. M. Bioucas-Dias, \u201cRestoration of Poissonian\nImages Using Alternating Direction Optimization,\u201d IEEE Transactions on\nImage Processing, vol. 19, pp. 3133\u20133145, 2010.\n[73] M. A. T. Figueiredo, R. D. Nowak, and S. J. Wright, \u201cGradient projection\nfor sparse reconstruction: Application to compressed sensing and other inverse\nproblems,\u201d IEEE Journal on Selected Topics in Signal Processing, vol. 1, no. 4,\npp. 586\u2013597, 2007.\n[74] P. A. Forero, A. Cano, and G. B. Giannakis, \u201cConsensus-based distributed\nsupport vector machines,\u201d Journal of Machine Learning Research , vol. 11,\npp. 1663\u20131707, 2010.\n[75] M. Fortin and R. Glowinski, Augmented Lagrangian Methods: Applications to\nthe Numerical Solution of Boundary-Value Problems . North-Holland: Ams-\nterdam, 1983.\n[76] M. Fortin and R. Glowinski, \u201cOn decomposition-coordination methods using\nan augmented Lagrangian,\u201d in Augmented Lagrangian Methods: Applications\nto the Solution of Boundary-Value Problems , (M. Fortin and R. Glowinski,\neds.), North-Holland: Amsterdam, 1983.\n[77] M. Forum, MPI: A Message-Passing Interface Standard, version 2.2 . High-\nPerformance Computing Center: Stuttgart, 2009.\n[78] Y. Freund and R. Schapire, \u201cA decision-theoretic generalization of on-line\nlearning and an application to boosting,\u201d in Computational Learning Theory,\npp. 23\u201337, Springer, 1995.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3019, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "98f4af05-6d23-40bc-b68d-b752447acdee": {"__data__": {"id_": "98f4af05-6d23-40bc-b68d-b752447acdee", "embedding": null, "metadata": {"page_label": "116", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "df21464b-c7be-47a4-9b6e-5fd8293bc972", "node_type": "4", "metadata": {"page_label": "116", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "818ea595fdd04af15d75c01f4742f247a2ef28465858a52c6c13f5df02a3fee6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "116 References\n[79] J. Friedman, T. Hastie, and R. Tibshirani, \u201cSparse inverse covariance estima-\ntion with the graphical lasso,\u201d Biostatistics, vol. 9, no. 3, p. 432, 2008.\n[80] M. Fukushima, \u201cApplication of the alternating direction method of multipli-\ners to separable convex programming problems,\u201dComputational Optimization\nand Applications, vol. 1, pp. 93\u2013111, 1992.\n[81] D. Gabay, \u201cApplications of the method of multipliers to variational inequal-\nities,\u201d in Augmented Lagrangian Methods: Applications to the Solution of\nBoundary-Value Problems, (M. Fortin and R. Glowinski, eds.), North-Holland:\nAmsterdam, 1983.\n[82] D. Gabay and B. Mercier, \u201cA dual algorithm for the solution of nonlinear vari-\national problems via \ufb01nite element approximations,\u201d Computers and Mathe-\nmatics with Applications, vol. 2, pp. 17\u201340, 1976.\n[83] M. Galassi, J. Davies, J. Theiler, B. Gough, G. Jungman, M. Booth, and\nF. Rossi, GNU Scienti\ufb01c Library Reference Manual. Network Theory Ltd.,\nthird ed., 2002.\n[84] A. M. Geo\ufb00rion, \u201cGeneralized Benders decomposition,\u201d Journal of Optimiza-\ntion Theory and Applications, vol. 10, no. 4, pp. 237\u2013260, 1972.\n[85] S. Ghemawat, H. Gobio\ufb00, and S. T. Leung, \u201cThe Google \ufb01le system,\u201d ACM\nSIGOPS Operating Systems Review, vol. 37, no. 5, pp. 29\u201343, 2003.\n[86] R. Glowinski and A. Marrocco, \u201cSur l\u2019approximation, par elements \ufb01nis\nd\u2019ordre un, et la resolution, par penalisation-dualit\u00b4e, d\u2019une classe de problems\nde Dirichlet non lineares,\u201d Revue Fran\u00b8caise d\u2019Automatique, Informatique, et\nRecherche Op\u00b4erationelle, vol. 9, pp. 41\u201376, 1975.\n[87] R. Glowinski and P. L. Tallec, \u201cAugmented Lagrangian methods for the\nsolution of variational problems,\u201d Tech. Rep. 2965, University of Wisconsin-\nMadison, 1987.\n[88] T. Goldstein and S. Osher, \u201cThe split Bregman method for \u2113\n1 regularized\nproblems,\u201d SIAM Journal on Imaging Sciences , vol. 2, no. 2, pp. 323\u2013343,\n2009.\n[89] E. G. Gol\u2019shtein and N. V. Tret\u2019yakov, \u201cModi\ufb01ed Lagrangians in convex pro-\ngramming and their generalizations,\u201d Point-to-Set Maps and Mathematical\nProgramming, pp. 86\u201397, 1979.\n[90] G. H. Golub and C. F. van Loan, Matrix Computations. Johns Hopkins Uni-\nversity Press, third ed., 1996.\n[91] D. Gregor and A. Lumsdaine, \u201cThe Parallel BGL: A generic library for dis-\ntributed graph computations,\u201dParallel Object-Oriented Scienti\ufb01c Computing,\n2005.\n[92] A. Halevy, P. Norvig, and F. Pereira, \u201cThe Unreasonable E\ufb00ectiveness of\nData,\u201d IEEE Intelligent Systems, vol. 24, no. 2, 2009.\n[93] K. B. Hall, S. Gilpin, and G. Mann, \u201cMapReduce/BigTable for distributed\noptimization,\u201d in Neural Information P rocessing Systems: Workshop on\nLearning on Cores, Clusters, and Clouds, 2010.\n[94] T. Hastie and R. Tibshirani, Generalized Additive Models. Chapman & Hall,\n1990.\n[95] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learn-\ning: Data Mining, Inference and Prediction. Springer, second ed., 2009.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2902, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e354cf5e-1685-411c-9d04-65869e4ee66e": {"__data__": {"id_": "e354cf5e-1685-411c-9d04-65869e4ee66e", "embedding": null, "metadata": {"page_label": "117", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4412e606-8da1-4a92-a9dc-61b86050c16c", "node_type": "4", "metadata": {"page_label": "117", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "3f5bbe1203e560a8778c135391dbaa3c5152216d7d3420ebba9aac7f58b512d4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "References 117\n[96] B. S. He, H. Yang, and S. L. Wang, \u201cAlternating direction method with self-\nadaptive penalty parameters for monotone variational inequalities,\u201d Journal\nof Optimization Theory and Applications, vol. 106, no. 2, pp. 337\u2013356, 2000.\n[97] M. R. Hestenes, \u201cMultiplier and gradient methods,\u201d Journal of Optimization\nTheory and Applications, vol. 4, pp. 302\u2013320, 1969.\n[98] M. R. Hestenes, \u201cMultiplier and gradient methods,\u201d in Computing Methods\nin Optimization Problems, (L. A. Zadeh, L. W. Neustadt, and A. V. Balakr-\nishnan, eds.), Academic Press, 1969.\n[99] J.-B. Hiriart-Urruty and C. Lemar\u00b4echal, Fundamentals of Convex Analysis.\nSpringer, 2001.\n[100] P. J. Huber, \u201cRobust estimation of a location parameter,\u201d Annals of Mathe-\nmatical Statistics, vol. 35, pp. 73\u2013101, 1964.\n[101] S.-J. Kim, K. Koh, S. Boyd, and D. Gorinevsky, \u201c \u21131 Trend \ufb01ltering,\u201d SIAM\nReview, vol. 51, no. 2, pp. 339\u2013360, 2009.\n[102] S.-J. Kim, K. Koh, M. Lustig, S. Boyd, and D. Gorinevsky, \u201cAn interior-point\nmethod for large-scale \u21131-regularized least squares,\u201d IEEE Journal of Selected\nTopics in Signal Processing, vol. 1, no. 4, pp. 606\u2013617, 2007.\n[103] K. Koh, S.-J. Kim, and S. Boyd, \u201cAn interior-point method for large-scale\u21131-\nregularized logistic regression,\u201d Journal of Machine Learning Research, vol. 1,\nno. 8, pp. 1519\u20131555, 2007.\n[104] D. Koller and N. Friedman, Probabilistic Graphical Models: Principles and\nTechniques. MIT Press, 2009.\n[105] S. A. Kontogiorgis, Alternating directions methods for the parallel solution of\nlarge-scale block-structured optimization problems. PhD thesis, University of\nWisconsin-Madison, 1994.\n[106] S. A. Kontogiorgis and R. R. Meyer, \u201cA variable-penalty alternating direc-\ntions method for convex optimization,\u201d Mathematical Programming, vol. 83,\npp. 29\u201353, 1998.\n[107] L. S. Lasdon, Optimization Theory for Large Systems. MacMillan, 1970.\n[108] J. Lawrence and J. E. Spingarn, \u201cOn \ufb01xed points of non-expansive piecewise\nisometric mappings,\u201d Proceedings of the London Mathematical Society, vol. 3,\nno. 3, p. 605, 1987.\n[109] C. L. Lawson, R. J. Hanson, D. R. Kincaid, and F. T. Krogh, \u201cBasic linear\nalgebra subprograms for Fortran usage,\u201dACM Transactions on Mathematical\nSoftware, vol. 5, no. 3, pp. 308\u2013323, 1979.\n[110] D. D. Lee and H. S. Seung, \u201cAlgorithms for non-negative matrix factoriza-\ntion,\u201d Advances in Neural Information Processing Systems, vol. 13, 2001.\n[111] J. Lin and M. Schatz, \u201cDesign Patterns for E\ufb03cient Graph Algorithms in\nMapReduce,\u201d in Proceedings of the Eighth Workshop on Mining and Learning\nwith Graphs, pp. 78\u201385, 2010.\n[112] P. L. Lions and B. Mercier, \u201cSplitting algorithms for the sum of two nonlinear\noperators,\u201d SIAM Journal on Numerical Analysis, vol. 16, pp. 964\u2013979, 1979.\n[113] D. C. Liu and J. Nocedal, \u201cOn the Limited Memory Method for Large Scale\nOptimization,\u201d Mathematical Programming B, vol. 45, no. 3, pp. 503\u2013528,\n1989.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2901, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7b563e6a-a32c-43f5-8c21-e393be6d73c1": {"__data__": {"id_": "7b563e6a-a32c-43f5-8c21-e393be6d73c1", "embedding": null, "metadata": {"page_label": "118", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "10ec693d-b0a2-4e34-a299-2843c6c3a32e", "node_type": "4", "metadata": {"page_label": "118", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "1c8831810d0ffdae309e0e526f1a298700626e3c73f465822a9221693a5e9017", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "118 References\n[114] Y. Low, J. Gonzalez, A. Kyrola, D. Bickson, C. Guestrin, and J. M. Hellerstein,\n\u201cGraphLab: A New Parallel Framework for Machine Learning,\u201d inConference\non Uncertainty in Arti\ufb01cial Intelligence, 2010.\n[115] Z. Lu, \u201cSmooth optimization approach for sparse covariance selection,\u201dSIAM\nJournal on Optimization, vol. 19, no. 4, pp. 1807\u20131827, 2009.\n[116] Z. Lu, T. K. Pong, and Y. Zhang, \u201cAn Alternating Direction Method for\nFinding Dantzig Selectors,\u201d arXiv:1011.4604, 2010.\n[117] D. G. Luenberger, Introduction to Linear and Nonlinear Programming .\nAddison-Wesley: Reading, MA, 1973.\n[118] J. Mairal, R. Jenatton, G. Obozinski, and F. Bach, \u201cNetwork \ufb02ow algorithms\nfor structured sparsity,\u201d Advances in Neural Information Processing Systems,\nvol. 24, 2010.\n[119] G. Malewicz, M. H. Austern, A. J. C. Bik, J. C. Dehnert, I. Horn, N. Leiser,\nand G. Czajkowski, \u201cPregel: A system for large-scale graph processing,\u201d in\nProceedings of the 2010 International Conference on Management of Data ,\npp. 135\u2013146, 2010.\n[120] A. F. T. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar, N. A. Smith, and E. P.\nXing, \u201cAn Augmented Lagrangian Approach to Constrained MAP Inference,\u201d\nin International Conference on Machine Learning, 2011.\n[121] G. Mateos, J.-A. Bazerque, and G. B. Giannakis, \u201cDistributed sparse linear\nregression,\u201d IEEE Transactions on Signal Processing, vol. 58, pp. 5262\u20135276,\nOct. 2010.\n[122] P. J. McCullagh and J. A. Nelder, Generalized Linear Models. Chapman &\nHall, 1991.\n[123] N. Meinshausen and P. B\u00a8 uhlmann, \u201cHigh-dimensional graphs and variable\nselection with the lasso,\u201d Annals of Statistics, vol. 34, no. 3, pp. 1436\u20131462,\n2006.\n[124] A. Miele, E. E. Cragg, R. R. Iver, and A. V. Levy, \u201cUse of the augmented\npenalty function in mathematical programming problems, part 1,\u201dJournal of\nOptimization Theory and Applications, vol. 8, pp. 115\u2013130, 1971.\n[125] A. Miele, E. E. Cragg, and A. V. Levy, \u201cUse of the augmented penalty function\nin mathematical programming problems, part 2,\u201d Journal of Optimization\nTheory and Applications, vol. 8, pp. 131\u2013153, 1971.\n[126] A. Miele, P. E. Mosely, A. V. Levy, and G. M. Coggins, \u201cOn the method of\nmultipliers for mathematical programming problems,\u201d Journal of Optimiza-\ntion Theory and Applications, vol. 10, pp. 1\u201333, 1972.\n[127] J.-J. Moreau, \u201cFonctions convexes duales et points proximaux dans un espace\nHilbertien,\u201d Reports of the Paris Academy of Sciences, Series A , vol. 255,\npp. 2897\u20132899, 1962.\n[128] D. Mosk-Aoyama, T. Roughgarden, and D. Shah, \u201cFully distributed algo-\nrithms for convex optimization problems,\u201d Available at http://theory.\nstanford.edu/\u02dctim/papers/distrcvxopt.pdf, 2007.\n[129] I. Necoara and J. A. K. Suykens, \u201cApplication of a smoothing technique\nto decomposition in convex optimization,\u201d IEEE Transactions on Automatic\nControl, vol. 53, no. 11, pp. 2674\u20132679, 2008.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2858, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "24008349-7c43-42b2-baf6-bcf01ffc1048": {"__data__": {"id_": "24008349-7c43-42b2-baf6-bcf01ffc1048", "embedding": null, "metadata": {"page_label": "119", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8758f6df-d987-4bab-95f3-f15fc101735e", "node_type": "4", "metadata": {"page_label": "119", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "a9b64049c1c3f48917ebc258f5e2f8b4a9e70c81e5027f62d103e59673907d8e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "References 119\n[130] A. Nedi\u00b4c and A. Ozdaglar, \u201cDistributed subgradient methods for multi-\nagent optimization,\u201d IEEE Transactions on Automatic Control, vol. 54, no. 1,\npp. 48\u201361, 2009.\n[131] A. Nedi\u00b4c and A. Ozdaglar, \u201cCooperative distributed multi-agent optimiza-\ntion,\u201d in Convex Optimization in Signal P rocessing and Communications,\n(D. P. Palomar and Y. C. Eldar, eds.), Cambridge University Press, 2010.\n[132] Y. Nesterov, \u201cA method of solving a convex programming problem with\nconvergence rate O(1/k2),\u201d Soviet Mathematics Doklady , vol. 27, no. 2,\npp. 372\u2013376, 1983.\n[133] Y. Nesterov, \u201cGradient methods for minimizing composite objective function,\u201d\nCORE Discussion Paper, Catholic University of Louvain , vol. 76, p. 2007,\n2007.\n[134] M. Ng, P. Weiss, and X. Yuang, \u201cSolving Constrained Total-Variation Image\nRestoration and Reconstruction Problems via Alternating Direction Meth-\nods,\u201d ICM Research Report, Available athttp://www.optimization-online.\norg/DB_FILE/2009/10/2434.pdf, 2009.\n[135] J. Nocedal and S. J. Wright, Numerical Optimization. Springer-Verlag, 1999.\n[136] H. Ohlsson, L. Ljung, and S. Boyd, \u201cSegmentation of ARX-models using sum-\nof-norms regularization,\u201d Automatica, vol. 46, pp. 1107\u20131111, 2010.\n[137] D. W. Peaceman and H. H. Rachford, \u201cThe numerical solution of parabolic\nand elliptic di\ufb00erential equations,\u201d Journal of the Society for Industrial and\nApplied Mathematics, vol. 3, pp. 28\u201341, 1955.\n[138] M. J. D. Powell, \u201cA method for nonlinear constraints in minimization prob-\nlems,\u201d in Optimization, (R. Fletcher, ed.), Academic Press, 1969.\n[139] A. Ribeiro, I. Schizas, S. Roumeliotis, and G. Giannakis, \u201cKalman \ufb01ltering in\nwireless sensor networks \u2014 Incorporating communication cost in state esti-\nmation problems,\u201d IEEE Control Systems Magazine, vol. 30, pp. 66\u201386, Apr.\n2010.\n[140] R. T. Rockafellar, Convex Analysis. Princeton University Press, 1970.\n[141] R. T. Rockafellar, \u201cAugmented Lagrangians and applications of the prox-\nimal point algorithm in convex programming,\u201d Mathematics of Operations\nResearch, vol. 1, pp. 97\u2013116, 1976.\n[142] R. T. Rockafellar, \u201cMonotone operators and the proximal point algorithm,\u201d\nSIAM Journal on Control and Optimization, vol. 14, p. 877, 1976.\n[143] R. T. Rockafellar and R. J.-B. Wets, \u201cScenarios and policy aggregation in\noptimization under uncertainty,\u201dMathematics of Operations Research, vol. 16,\nno. 1, pp. 119\u2013147, 1991.\n[144] R. T. Rockafellar and R. J.-B. Wets, Variational Analysis. Springer-Verlag,\n1998.\n[145] L. Rudin, S. J. Osher, and E. Fatemi, \u201cNonlinear total variation based noise\nremoval algorithms,\u201d Physica D, vol. 60, pp. 259\u2013268, 1992.\n[146] A. Ruszczy\u00b4nski, \u201cAn augmented Lagrangian decomposition method for block\ndiagonal linear programming problems,\u201d Operations Research Letters, vol. 8,\nno. 5, pp. 287\u2013294, 1989.\n[147] A. Ruszczy\u00b4nski, \u201cOn convergence of an augmented Lagrangian decomposition\nmethod for sparse convex optimization,\u201dMathematics of Operations Research,\nvol. 20, no. 3, pp. 634\u2013656, 1995.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3007, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c9b1e539-831b-4684-949b-535526589a08": {"__data__": {"id_": "c9b1e539-831b-4684-949b-535526589a08", "embedding": null, "metadata": {"page_label": "120", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f16b09c7-88d7-4b01-9dd8-fa2b76417ae3", "node_type": "4", "metadata": {"page_label": "120", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "6af5c0526af43308df8b103dde865a353bc0f985e328c94700bc361c4bb0236d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "120 References\n[148] K. Scheinberg, S. Ma, and D. Goldfarb, \u201cSparse inverse covariance selection\nvia alternating linearization methods,\u201d in Advances in Neural Information\nProcessing Systems, 2010.\n[149] I. D. Schizas, G. Giannakis, S. Roumeliotis, and A. Ribeiro, \u201cConsensus in ad\nhoc WSNs with noisy links \u2014 part II: Distributed estimation and smoothing of\nrandom signals,\u201d IEEE Transactions on Signal Processing, vol. 56, pp. 1650\u2013\n1666, Apr. 2008.\n[150] I. D. Schizas, A. Ribeiro, and G. B. Giannakis, \u201cConsensus in ad hoc WSNs\nwith noisy links \u2014 part I: Distributed estimation of deterministic signals,\u201d\nIEEE Transactions on Signal Processing, vol. 56, pp. 350\u2013364, Jan. 2008.\n[151] B. Sch\u00a8olkopf and A. J. Smola, Learning with Kernels: Support Vector\nMachines, Regularization, Optimization, and Beyond. MIT Press, 2002.\n[152] N. Z. Shor, Minimization Methods for Non-Di\ufb00erentiable Functions. Springer-\nVerlag, 1985.\n[153] J. E. Spingarn, \u201cApplications of the method of partial inverses to convex pro-\ngramming: decomposition,\u201d Mathematical Programming, vol. 32, pp. 199\u2013223,\n1985.\n[154] G. Steidl and T. Teuber, \u201cRemoving multiplicative noise by Douglas-Rachford\nsplitting methods,\u201d Journal of Mathematical Imaging and Vision , vol. 36,\nno. 2, pp. 168\u2013184, 2010.\n[155] C. H. Teo, S. V. N. Vishwanathan, A. J. Smola, and Q. V. Le, \u201cBundle meth-\nods for regularized risk minimization,\u201dJournal of Machine Learning Research,\nvol. 11, pp. 311\u2013365, 2010.\n[156] R. Tibshirani, \u201cRegression shrinkage and selection via the lasso,\u201d Journal of\nthe Royal Statistical Society, Series B, vol. 58, pp. 267\u2013288, 1996.\n[157] P. Tseng, \u201cApplications of a splitting algorithm to decomposition in convex\nprogramming and variational inequalities.,\u201d SIAM Journal on Control and\nOptimization, vol. 29, pp. 119\u2013138, 1991.\n[158] P. Tseng, \u201cAlternating projection-proximal methods for convex program-\nming and variational inequalities,\u201d SIAM Journal on Optimization , vol. 7,\npp. 951\u2013965, 1997.\n[159] P. Tseng, \u201cA modi\ufb01ed forward-backward splitting method for maximal mono-\ntone mappings,\u201d SIAM Journal on Control and Optimization, vol. 38, p. 431,\n2000.\n[160] J. N. Tsitsiklis, Problems in decentralized decision making and computation.\nPhD thesis, Massachusetts Institute of Technology, 1984.\n[161] J. N. Tsitsiklis, D. P. Bertsekas, and M. Athans, \u201cDistributed asynchronous\ndeterministic and stochastic gradient optimization algorithms,\u201d IEEE Trans-\nactions on Automatic Control, vol. 31, no. 9, pp. 803\u2013812, 1986.\n[162] H. Uzawa, \u201cMarket mechanisms and mathematical programming,\u201dEconomet-\nrica: Journal of the Econometric Society, vol. 28, no. 4, pp. 872\u2013881, 1960.\n[163] H. Uzawa, \u201cWalras\u2019 t\u02c6atonnement in the theory of exchange,\u201d The Review of\nEconomic Studies, vol. 27, no. 3, pp. 182\u2013194, 1960.\n[164] L. G. Valiant, \u201cA bridging model for parallel computation,\u201d Communications\nof the ACM, vol. 33, no. 8, p. 111, 1990.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2904, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "46d0b0cc-61a6-45f9-b645-e3875aabc7b7": {"__data__": {"id_": "46d0b0cc-61a6-45f9-b645-e3875aabc7b7", "embedding": null, "metadata": {"page_label": "121", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "057f1af5-e93b-4dd9-a266-2e699aafde04", "node_type": "4", "metadata": {"page_label": "121", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "3c96486ef08428dfcbbfb9760376faed350df933a47112d9c5d720dc14522930", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "References 121\n[165] V. N. Vapnik, The Nature of Statistical Learning Theory . Springer-Verlag,\n2000.\n[166] J. von Neumann, Functional Operators, Volume 2: The Geometry of Orthogo-\nnal Spaces. Princeton University Press: Annals of Mathematics Studies, 1950.\nReprint of 1933 lecture notes.\n[167] M. J. Wainwright and M. I. Jordan, \u201cGraphical models, exponential fami-\nlies, and variational inference,\u201dFoundations and Trends in Machine Learning,\nvol. 1, no. 1-2, pp. 1\u2013305, 2008.\n[168] L. Walras, \u00b4El\u00b4ements d\u2019\u00b4economie politique pure, ou, Th\u00b4eorie de la richesse\nsociale. F. Rouge, 1896.\n[169] S. L. Wang and L. Z. Liao, \u201cDecomposition method with a variable parameter\nfor a class of monotone variational inequality problems,\u201dJournal of Optimiza-\ntion Theory and Applications, vol. 109, no. 2, pp. 415\u2013429, 2001.\n[170] T. White, Hadoop: The De\ufb01nitive Guide. O\u2019Reilly Press, second ed., 2010.\n[171] J. M. Wooldridge, Introductory Econometrics: A Modern Approach. South\nWestern College Publications, fourth ed., 2009.\n[172] L. Xiao and S. Boyd, \u201cFast linear iterations for distributed averaging,\u201dSystems\n& Control Letters, vol. 53, no. 1, pp. 65\u201378, 2004.\n[173] A. Y. Yang, A. Ganesh, Z. Zhou, S. S. Sastry, and Y. Ma, \u201cA Review of Fast\n\u21131-Minimization Algorithms for Robust Face Recognition,\u201d arXiv:1007.3753,\n2010.\n[174] J. Yang and X. Yuan, \u201cAn inexact alternating direction method for\ntrace norm regularized least squares problem,\u201d Available at http://www.\noptimization-online.org, 2010.\n[175] J. Yang and Y. Zhang, \u201cAlternating direction algorithms for \u21131-problems in\ncompressive sensing,\u201d Preprint, 2009.\n[176] W. Yin, S. Osher, D. Goldfarb, and J. Darbon, \u201cBregman iterative algorithms\nfor \u21131-minimization with applications to compressed sensing,\u201d SIAM Journal\non Imaging Sciences, vol. 1, no. 1, pp. 143\u2013168, 2008.\n[177] M. Yuan and Y. Lin, \u201cModel selection and estimation in regression with\ngrouped variables,\u201d Journal of the Royal Statistical Society: Series B (Sta-\ntistical Methodology), vol. 68, no. 1, pp. 49\u201367, 2006.\n[178] X. M. Yuan, \u201cAlternating direction methods for sparse covariance selec-\ntion,\u201d Preprint, Available at http://www.optimization-online.org/DB_\nFILE/2009/09/2390.pdf, 2009.\n[179] M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, and I. Stoica, \u201cSpark:\nCluster computing with working sets,\u201d in Proceedings of the 2nd USENIX\nConference on Hot Topics in Cloud Computing, 2010.\n[180] T. Zhang, \u201cStatistical behavior and consistency of classi\ufb01cation methods based\non convex risk minimization,\u201d Annals of Statistics, vol. 32, no. 1, pp. 56\u201385,\n2004.\n[181] P. Zhao, G. Rocha, and B. Yu, \u201cThe composite absolute penalties family\nfor grouped and hierarchical variable selection,\u201d Annals of Statistics, vol. 37,\nno. 6A, pp. 3468\u20133497, 2009.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2767, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3a984861-5316-4625-9016-10bec07df664": {"__data__": {"id_": "3a984861-5316-4625-9016-10bec07df664", "embedding": null, "metadata": {"page_label": "122", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8beef72-ce8f-46f3-87f5-7aee3b82fe18", "node_type": "4", "metadata": {"page_label": "122", "file_name": "admm_distr_stats.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\admm_distr_stats.pdf", "file_type": "application/pdf", "file_size": 794236, "creation_date": "2025-11-07", "last_modified_date": "2025-11-07"}, "hash": "d9159708b41efb3fc40b58ef788dbb02c8a70ac9ba8951930a05ae197cf48f41", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "122 References\n[182] H. Zhu, A. Cano, and G. B. Giannakis, \u201cDistributed consensus-based demod-\nulation: algorithms and error analysis,\u201dIEEE Transactions on Wireless Com-\nmunications, vol. 9, no. 6, pp. 2044\u20132054, 2010.\n[183] H. Zhu, G. B. Giannakis, and A. Cano, \u201cDistributed in-network channel decod-\ning,\u201d IEEE Transactions on Signal Processing, vol. 57, no. 10, pp. 3970\u20133983,\n2009.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 385, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9ec96a62-c504-4652-845e-6b70c5b16c29": {"__data__": {"id_": "9ec96a62-c504-4652-845e-6b70c5b16c29", "embedding": null, "metadata": {"page_label": "i", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "15507f04-38db-4377-b787-5f281d10192c", "node_type": "4", "metadata": {"page_label": "i", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "d6154bfbb15a806af808e6b331e07765d114e06f045b451d20f072012e4e87d6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "arXiv:2403.14606v3  [cs.LG]  24 Jun 2025\nThe Elements of\nDifferentiable Programming\nMathieu Blondel\nGoogle DeepMind\nmblondel@google.com\nVincent Roulet\nGoogle DeepMind\nvroulet@google.com\nDraft version 3 (last update: June 24, 2025)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 230, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "41eed3e4-ee8c-45b6-8e65-25dbe683d959": {"__data__": {"id_": "41eed3e4-ee8c-45b6-8e65-25dbe683d959", "embedding": null, "metadata": {"page_label": "ii", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ebae195b-60a4-4ea0-aa2a-6e64e670fe0b", "node_type": "4", "metadata": {"page_label": "ii", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "68734720f3122bd209023dd29c6fd9578c029accf29a1aad2865600aa691c936", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Contents\n1 Introduction 6\n1.1 What is differentiable programming? . . . . . . . . . . . . 6\n1.2 Book goals and scope . . . . . . . . . . . . . . . . . . . . 10\n1.3 Intended audience . . . . . . . . . . . . . . . . . . . . . . 11\n1.4 How to read this book? . . . . . . . . . . . . . . . . . . . 11\n1.5 Related work . . . . . . . . . . . . . . . . . . . . . . . . . 11\nI Fundamentals 13\n2 Differentiation 14\n2.1 Univariate functions . . . . . . . . . . . . . . . . . . . . . 14\n2.1.1 Derivatives . . . . . . . . . . . . . . . . . . . . . . 14\n2.1.2 Calculus rules . . . . . . . . . . . . . . . . . . . . 18\n2.1.3 Leibniz\u2019s notation . . . . . . . . . . . . . . . . . . 20\n2.2 Multivariate functions . . . . . . . . . . . . . . . . . . . . 21\n2.2.1 Directional derivatives . . . . . . . . . . . . . . . . 21\n2.2.2 Gradients . . . . . . . . . . . . . . . . . . . . . . 22\n2.2.3 Jacobians . . . . . . . . . . . . . . . . . . . . . . 26\n2.3 Linear maps . . . . . . . . . . . . . . . . . . . . . . . . . 31\n2.3.1 The need for linear maps . . . . . . . . . . . . . . 31\n2.3.2 Euclidean spaces . . . . . . . . . . . . . . . . . . . 32", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1124, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f28e9cc7-1611-4606-bec7-4ee244b14b28": {"__data__": {"id_": "f28e9cc7-1611-4606-bec7-4ee244b14b28", "embedding": null, "metadata": {"page_label": "iii", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25d02b27-b012-4ed9-94cd-d3402e3f374e", "node_type": "4", "metadata": {"page_label": "iii", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "3d215f93fa7e68c0347f62b5747ea2f45235a965f88a023642c3b3f1fdcaf3ca", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.3.3 Linear maps and their adjoints . . . . . . . . . . . 33\n2.3.4 Jacobian-vector products . . . . . . . . . . . . . . 34\n2.3.5 Vector-Jacobian products . . . . . . . . . . . . . . 36\n2.3.6 Chain rule using linear maps . . . . . . . . . . . . 37\n2.3.7 Functions of multiple inputs (fan-in) . . . . . . . . 38\n2.3.8 Functions with multiple outputs (fan-out) . . . . . 41\n2.3.9 Extensions to non-Euclidean linear spaces . . . . . 41\n2.4 Second-order differentiation . . . . . . . . . . . . . . . . . 43\n2.4.1 Second derivatives . . . . . . . . . . . . . . . . . . 43\n2.4.2 Second directional derivatives . . . . . . . . . . . . 44\n2.4.3 Hessians . . . . . . . . . . . . . . . . . . . . . . . 44\n2.4.4 Hessian-vector products . . . . . . . . . . . . . . . 46\n2.4.5 Second-order Jacobians . . . . . . . . . . . . . . . 46\n2.5 Higher-order differentiation . . . . . . . . . . . . . . . . . 47\n2.5.1 Higher-order derivatives . . . . . . . . . . . . . . . 47\n2.5.2 Higher-order directional derivatives . . . . . . . . . 47\n2.5.3 Higher-order Jacobians . . . . . . . . . . . . . . . 48\n2.5.4 Taylor expansions . . . . . . . . . . . . . . . . . . 49\n2.6 Differential geometry . . . . . . . . . . . . . . . . . . . . 50\n2.6.1 Differentiability on manifolds . . . . . . . . . . . . 50\n2.6.2 Tangent spaces and pushforward operators . . . . . 51\n2.6.3 Cotangent spaces and pullback operators . . . . . 52\n2.7 Generalized derivatives . . . . . . . . . . . . . . . . . . . 55\n2.7.1 Rademacher\u2019s theorem . . . . . . . . . . . . . . . 56\n2.7.2 Clarke derivatives . . . . . . . . . . . . . . . . . . 56\n2.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n3 Probabilistic learning 61\n3.1 Probability distributions . . . . . . . . . . . . . . . . . . . 61\n3.1.1 Discrete probability distributions . . . . . . . . . . 61\n3.1.2 Continuous probability distributions . . . . . . . . 62\n3.2 Maximum likelihood estimation . . . . . . . . . . . . . . . 63\n3.2.1 Negative log-likelihood . . . . . . . . . . . . . . . 63\n3.2.2 Consistency w.r.t. the Kullback-Leibler divergence . 63\n3.3 Probabilistic supervised learning . . . . . . . . . . . . . . 64\n3.3.1 Conditional probability distributions . . . . . . . . 64", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2205, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f5edc9e0-e56a-45a5-bb15-df95d93a907c": {"__data__": {"id_": "f5edc9e0-e56a-45a5-bb15-df95d93a907c", "embedding": null, "metadata": {"page_label": "iv", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e2429523-b149-44ce-847c-49e8a30c4dfb", "node_type": "4", "metadata": {"page_label": "iv", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "47a9c223621963c9c3554b5dd89df251aa02f2a646202a45da65a8149cedb713", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.3.2 Inference . . . . . . . . . . . . . . . . . . . . . . . 64\n3.3.3 Binary classification . . . . . . . . . . . . . . . . . 65\n3.3.4 Multiclass classification . . . . . . . . . . . . . . . 67\n3.3.5 Regression . . . . . . . . . . . . . . . . . . . . . . 69\n3.3.6 Multivariate regression . . . . . . . . . . . . . . . 70\n3.3.7 Integer regression . . . . . . . . . . . . . . . . . . 71\n3.3.8 Loss functions . . . . . . . . . . . . . . . . . . . . 72\n3.4 Exponential family distributions . . . . . . . . . . . . . . . 75\n3.4.1 Definition . . . . . . . . . . . . . . . . . . . . . . 75\n3.4.2 The log-partition function . . . . . . . . . . . . . . 76\n3.4.3 Maximum entropy principle . . . . . . . . . . . . . 78\n3.4.4 Maximum likelihood estimation . . . . . . . . . . . 79\n3.4.5 Probabilistic learning with exponential families . . . 80\n3.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\nII Differentiable programs 83\n4 Parameterized programs 84\n4.1 Representing computer programs . . . . . . . . . . . . . . 84\n4.1.1 Computation chains . . . . . . . . . . . . . . . . . 84\n4.1.2 Directed acylic graphs . . . . . . . . . . . . . . . . 85\n4.1.3 Computer programs as DAGs . . . . . . . . . . . . 87\n4.1.4 Arithmetic circuits . . . . . . . . . . . . . . . . . . 89\n4.2 Feedforward networks . . . . . . . . . . . . . . . . . . . . 90\n4.3 Multilayer perceptrons . . . . . . . . . . . . . . . . . . . . 90\n4.3.1 Combining affine layers and activations . . . . . . . 90\n4.3.2 Link with generalized linear models . . . . . . . . . 91\n4.4 Activation functions . . . . . . . . . . . . . . . . . . . . . 92\n4.4.1 ReLU and softplus . . . . . . . . . . . . . . . . . . 92\n4.4.2 Max pooling and log-sum-exp . . . . . . . . . . . . 92\n4.4.3 Sigmoids: binary step and logistic functions . . . . 94\n4.4.4 Probability mappings: argmax and softargmax . . . 96\n4.5 Normalization layers . . . . . . . . . . . . . . . . . . . . . 97\n4.5.1 Batch normalization . . . . . . . . . . . . . . . . . 98\n4.5.2 Layer normalization . . . . . . . . . . . . . . . . . 99", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2051, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "03c01a7a-4a15-4999-818b-077abefc71a8": {"__data__": {"id_": "03c01a7a-4a15-4999-818b-077abefc71a8", "embedding": null, "metadata": {"page_label": "v", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a3058728-f533-4bec-ac10-f63bc0c64fac", "node_type": "4", "metadata": {"page_label": "v", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "c6c8e4b292abef3efb7494c95f88e44f1dc3b7e2df36a6b68ddd1cdc291b6bb6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.6 Residual neural networks . . . . . . . . . . . . . . . . . . 100\n4.7 Recurrent neural networks . . . . . . . . . . . . . . . . . . 101\n4.7.1 Vector to sequence . . . . . . . . . . . . . . . . . 101\n4.7.2 Sequence to vector . . . . . . . . . . . . . . . . . 103\n4.7.3 Sequence to sequence (aligned) . . . . . . . . . . . 103\n4.7.4 Sequence to sequence (unaligned) . . . . . . . . . 104\n4.8 Transformers . . . . . . . . . . . . . . . . . . . . . . . . . 105\n4.8.1 Attention . . . . . . . . . . . . . . . . . . . . . . 105\n4.8.2 Self-attention . . . . . . . . . . . . . . . . . . . . 106\n4.8.3 Multi-head attention . . . . . . . . . . . . . . . . 107\n4.8.4 Transformer layer . . . . . . . . . . . . . . . . . . 108\n4.8.5 Transformer block . . . . . . . . . . . . . . . . . . 109\n4.8.6 Token encoding . . . . . . . . . . . . . . . . . . . 110\n4.8.7 Positional encoding . . . . . . . . . . . . . . . . . 111\n4.8.8 Decoder-only architectures . . . . . . . . . . . . . 116\n4.8.9 Encoder-only architectures . . . . . . . . . . . . . 119\n4.8.10 Encoder-decoder architectures . . . . . . . . . . . 120\n4.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\n5 Control flows 123\n5.1 Comparison operators . . . . . . . . . . . . . . . . . . . . 123\n5.2 Soft inequality operators . . . . . . . . . . . . . . . . . . 125\n5.2.1 Heuristic definition . . . . . . . . . . . . . . . . . 125\n5.2.2 Stochastic process perspective . . . . . . . . . . . 126\n5.3 Soft equality operators . . . . . . . . . . . . . . . . . . . 129\n5.3.1 Heuristic definition . . . . . . . . . . . . . . . . . 129\n5.3.2 Stochastic process perspective . . . . . . . . . . . 130\n5.3.3 Gaussian process perspective . . . . . . . . . . . . 133\n5.4 Logical operators . . . . . . . . . . . . . . . . . . . . . . 134\n5.5 Continuous extensions of logical operators . . . . . . . . . 135\n5.5.1 Probabilistic continuous extension . . . . . . . . . 135\n5.5.2 Triangular norms and co-norms . . . . . . . . . . . 137\n5.6 If-else statements . . . . . . . . . . . . . . . . . . . . . . 138\n5.6.1 Differentiating through branch variables . . . . . . 139\n5.6.2 Differentiating through predicate variables . . . . . 140\n5.6.3 Continuous relaxations . . . . . . . . . . . . . . . 141", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2238, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "46ba7a57-3691-4487-b5cd-c83820fef529": {"__data__": {"id_": "46ba7a57-3691-4487-b5cd-c83820fef529", "embedding": null, "metadata": {"page_label": "vi", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d7242ee-e0a3-4d27-8c75-32f2e7e1b40d", "node_type": "4", "metadata": {"page_label": "vi", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "8a201ff36427bffdd9cf0226dfe3e914ee102e080ea343f931c901adcf3d49bf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5.7 Else-if statements . . . . . . . . . . . . . . . . . . . . . . 144\n5.7.1 Encoding K branches . . . . . . . . . . . . . . . . 144\n5.7.2 Conditionals . . . . . . . . . . . . . . . . . . . . . 145\n5.7.3 Differentiating through branch variables . . . . . . 146\n5.7.4 Differentiating through predicate variables . . . . . 147\n5.7.5 Continuous relaxations . . . . . . . . . . . . . . . 148\n5.8 For loops . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n5.9 Scan functions . . . . . . . . . . . . . . . . . . . . . . . . 151\n5.10 While loops . . . . . . . . . . . . . . . . . . . . . . . . . 152\n5.10.1 While loops as cyclic graphs . . . . . . . . . . . . 152\n5.10.2 Unrolled while loops . . . . . . . . . . . . . . . . . 154\n5.10.3 Markov chain perspective . . . . . . . . . . . . . . 157\n5.11 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\n6 Data structures 161\n6.1 Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\n6.1.1 Basic operations . . . . . . . . . . . . . . . . . . . 162\n6.1.2 Operations on variable-length lists . . . . . . . . . 163\n6.1.3 Continuous relaxations using soft indexing . . . . . 165\n6.2 Dictionaries . . . . . . . . . . . . . . . . . . . . . . . . . 168\n6.2.1 Basic operations . . . . . . . . . . . . . . . . . . . 168\n6.2.2 Continuous relaxation using kernel regression . . . 170\n6.2.3 Discrete probability distribution perspective . . . . 171\n6.2.4 Link with attention in Transformers . . . . . . . . 172\n6.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\nIII Differentiating through programs 175\n7 Finite differences 176\n7.1 Forward differences . . . . . . . . . . . . . . . . . . . . . 176\n7.2 Backward differences . . . . . . . . . . . . . . . . . . . . 177\n7.3 Central differences . . . . . . . . . . . . . . . . . . . . . . 178\n7.4 Higher-accuracy finite differences . . . . . . . . . . . . . . 179\n7.5 Higher-order finite differences . . . . . . . . . . . . . . . . 180\n7.6 Complex-step derivatives . . . . . . . . . . . . . . . . . . 181", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2035, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "146c09e5-7e89-48ca-8f56-513358b00b97": {"__data__": {"id_": "146c09e5-7e89-48ca-8f56-513358b00b97", "embedding": null, "metadata": {"page_label": "vii", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1bc58610-56f5-4fe7-922f-7d2ad04c23b0", "node_type": "4", "metadata": {"page_label": "vii", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "81c9bc267ecfabb5152a532d2ee96508c33320df6c4791da203324921a54de5e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7.7 Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . 182\n7.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\n8 Automatic differentiation 184\n8.1 Computation chains . . . . . . . . . . . . . . . . . . . . . 184\n8.1.1 Forward-mode . . . . . . . . . . . . . . . . . . . . 185\n8.1.2 Reverse-mode . . . . . . . . . . . . . . . . . . . . 188\n8.1.3 Complexity of computing entire Jacobians . . . . . 192\n8.2 Feedforward networks . . . . . . . . . . . . . . . . . . . . 194\n8.2.1 Computing the adjoint . . . . . . . . . . . . . . . 194\n8.2.2 Computing the gradient . . . . . . . . . . . . . . . 195\n8.3 Computation graphs . . . . . . . . . . . . . . . . . . . . . 197\n8.3.1 Forward mode . . . . . . . . . . . . . . . . . . . . 197\n8.3.2 Reverse mode . . . . . . . . . . . . . . . . . . . . 200\n8.3.3 Complexity, the Baur-Strassen theorem . . . . . . . 204\n8.4 Implementation . . . . . . . . . . . . . . . . . . . . . . . 204\n8.4.1 Primitive functions . . . . . . . . . . . . . . . . . 204\n8.4.2 Closure under function composition . . . . . . . . 205\n8.4.3 Examples of JVPs and VJPs . . . . . . . . . . . . 205\n8.4.4 Automatic linear transposition . . . . . . . . . . . 206\n8.5 Checkpointing . . . . . . . . . . . . . . . . . . . . . . . . 207\n8.5.1 Recursive halving . . . . . . . . . . . . . . . . . . 209\n8.5.2 Dynamic programming . . . . . . . . . . . . . . . 211\n8.5.3 Online checkpointing . . . . . . . . . . . . . . . . 213\n8.6 Reversible layers . . . . . . . . . . . . . . . . . . . . . . . 213\n8.6.1 General case . . . . . . . . . . . . . . . . . . . . . 213\n8.6.2 Case of orthonormal JVPs . . . . . . . . . . . . . 214\n8.7 Randomized forward-mode gradient estimator . . . . . . . 215\n8.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\n9 Second-order automatic differentiation 217\n9.1 Hessian-vector products . . . . . . . . . . . . . . . . . . . 217\n9.1.1 Four possible methods . . . . . . . . . . . . . . . 217\n9.1.2 Complexity . . . . . . . . . . . . . . . . . . . . . . 218\n9.2 Gauss-Newton matrix . . . . . . . . . . . . . . . . . . . . 222\n9.2.1 An approximation of the Hessian . . . . . . . . . . 222", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2158, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2495de7a-5231-44d8-99b8-01bc460a2c66": {"__data__": {"id_": "2495de7a-5231-44d8-99b8-01bc460a2c66", "embedding": null, "metadata": {"page_label": "viii", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f316e2ba-173f-4612-bdcc-2a4c050491f4", "node_type": "4", "metadata": {"page_label": "viii", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "8a8f52ad257cd4397a137e5fbc4dba9599fd7a4c1d1eccef786ccec9bffc0172", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9.2.2 Gauss-Newton chain rule . . . . . . . . . . . . . . 223\n9.2.3 Gauss-Newton vector product . . . . . . . . . . . . 223\n9.2.4 Gauss-Newton matrix factorization . . . . . . . . . 224\n9.2.5 Stochastic setting . . . . . . . . . . . . . . . . . . 225\n9.3 Fisher information matrix . . . . . . . . . . . . . . . . . . 225\n9.3.1 Definition using the score function . . . . . . . . . 225\n9.3.2 Link with the Hessian . . . . . . . . . . . . . . . . 226\n9.3.3 Equivalence with the Gauss-Newton matrix . . . . 226\n9.4 Inverse-Hessian vector product . . . . . . . . . . . . . . . 228\n9.4.1 Definition as a linear map . . . . . . . . . . . . . . 228\n9.4.2 Implementation with matrix-free linear solvers . . . 228\n9.4.3 Complexity . . . . . . . . . . . . . . . . . . . . . . 229\n9.5 Second-order backpropagation . . . . . . . . . . . . . . . 230\n9.5.1 Second-order Jacobian chain rule . . . . . . . . . . 230\n9.5.2 Computation chains . . . . . . . . . . . . . . . . . 232\n9.5.3 Fan-in and fan-out . . . . . . . . . . . . . . . . . 233\n9.6 Block diagonal approximations . . . . . . . . . . . . . . . 234\n9.6.1 Feedforward networks . . . . . . . . . . . . . . . . 234\n9.6.2 Computation graphs . . . . . . . . . . . . . . . . . 236\n9.7 Diagonal approximations . . . . . . . . . . . . . . . . . . 236\n9.7.1 Computation chains . . . . . . . . . . . . . . . . . 237\n9.7.2 Computation graphs . . . . . . . . . . . . . . . . . 238\n9.8 Randomized estimators . . . . . . . . . . . . . . . . . . . 239\n9.8.1 Girard-Hutchinson estimator . . . . . . . . . . . . 239\n9.8.2 Bartlett estimator for the factorization . . . . . . . 240\n9.8.3 Bartlett estimator for the diagonal . . . . . . . . . 241\n9.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 242\n10 Inference in graphical models as differentiation 243\n10.1 Chain rule of probability . . . . . . . . . . . . . . . . . . .243\n10.2 Conditional independence . . . . . . . . . . . . . . . . . . 244\n10.3 Inference problems . . . . . . . . . . . . . . . . . . . . . . 245\n10.3.1 Joint probability distributions . . . . . . . . . . . . 245\n10.3.2 Likelihood . . . . . . . . . . . . . . . . . . . . . . 245\n10.3.3 Maximum a-posteriori inference . . . . . . . . . . .246\n10.3.4 Marginal inference . . . . . . . . . . . . . . . . . . 246", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2274, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f6970b5b-6c43-4a7b-afd9-aa3092ee5321": {"__data__": {"id_": "f6970b5b-6c43-4a7b-afd9-aa3092ee5321", "embedding": null, "metadata": {"page_label": "ix", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b3cf393-5386-442b-9675-dc7e93a304c6", "node_type": "4", "metadata": {"page_label": "ix", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "ba32a103bc5f26d4138c1d446be1ef1ecd77e5ffad6fbb1f8834d83596179dbe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10.3.5 Expectation, convex hull, marginal polytope . . . .247\n10.3.6 Complexity of brute force . . . . . . . . . . . . . .248\n10.4 Markov chains . . . . . . . . . . . . . . . . . . . . . . . . 248\n10.4.1 The Markov property . . . . . . . . . . . . . . . . 249\n10.4.2 Time-homogeneous Markov chains . . . . . . . . . 251\n10.4.3 Higher-order Markov chains . . . . . . . . . . . . .252\n10.5 Bayesian networks . . . . . . . . . . . . . . . . . . . . . . 252\n10.5.1 Expressing variable dependencies using DAGs . . . 252\n10.5.2 Parameterizing Bayesian networks . . . . . . . . . 253\n10.5.3 Ancestral sampling . . . . . . . . . . . . . . . . . 254\n10.6 Markov random fields . . . . . . . . . . . . . . . . . . . . 254\n10.6.1 Expressing factors using undirected graphs . . . . .254\n10.6.2 MRFs as exponential family distributions . . . . . .255\n10.6.3 Conditional random fields . . . . . . . . . . . . . . 257\n10.6.4 Sampling . . . . . . . . . . . . . . . . . . . . . . . 257\n10.7 Inference on chains . . . . . . . . . . . . . . . . . . . . . 257\n10.7.1 The forward-backward algorithm . . . . . . . . . . 258\n10.7.2 The Viterbi algorithm . . . . . . . . . . . . . . . . 259\n10.8 Inference on trees . . . . . . . . . . . . . . . . . . . . . . 261\n10.9 Inference as differentiation . . . . . . . . . . . . . . . . . 262\n10.9.1 Inference as gradient of the log-partition . . . . . .262\n10.9.2 Semirings and softmax operators . . . . . . . . . . 263\n10.9.3 Inference as backpropagation . . . . . . . . . . . . 265\n10.10Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 267\n11 Differentiating through optimization 269\n11.1 Implicit functions . . . . . . . . . . . . . . . . . . . . . . 269\n11.1.1 Optimization problems . . . . . . . . . . . . . . . 270\n11.1.2 Nonlinear equations . . . . . . . . . . . . . . . . . 270\n11.1.3 Application to bilevel optimization . . . . . . . . . 270\n11.2 Envelope theorems . . . . . . . . . . . . . . . . . . . . . . 271\n11.2.1 Danskin\u2019s theorem . . . . . . . . . . . . . . . . . . 272\n11.2.2 Rockafellar\u2019s theorem . . . . . . . . . . . . . . . . 273\n11.3 Implicit function theorem . . . . . . . . . . . . . . . . . . 274\n11.3.1 Univariate functions . . . . . . . . . . . . . . . . . 274\n11.3.2 Multivariate functions . . . . . . . . . . . . . . . . 276", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2285, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4014de6b-3a7b-406a-abf2-a9cdcccd52e2": {"__data__": {"id_": "4014de6b-3a7b-406a-abf2-a9cdcccd52e2", "embedding": null, "metadata": {"page_label": "x", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1dcdd682-3688-41f3-ba4b-1038cba79e00", "node_type": "4", "metadata": {"page_label": "x", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "7c40b12db6912e27136ca6e35ca5a606ad85cee3b27ab547087e0905da551950", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11.3.3 JVP and VJP of implicit functions . . . . . . . . .278\n11.3.4 Proof of the implicit function theorem . . . . . . .279\n11.4 Adjoint state method . . . . . . . . . . . . . . . . . . . . 280\n11.4.1 Differentiating nonlinear equations . . . . . . . . . 280\n11.4.2 Relation with envelope theorems . . . . . . . . . . 281\n11.4.3 Proof using the method of Lagrange multipliers . .281\n11.4.4 Proof using the implicit function theorem . . . . . 282\n11.4.5 Reverse mode as adjoint method with backsubstitution282\n11.5 Inverse function theorem . . . . . . . . . . . . . . . . . . 285\n11.5.1 Differentiating inverse functions . . . . . . . . . . 285\n11.5.2 Link with the implicit function theorem . . . . . . 286\n11.5.3 Proof of inverse function theorem . . . . . . . . . 286\n11.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 287\n12 Differentiating through integration 289\n12.1 Differentiation under the integral sign . . . . . . . . . . . 289\n12.2 Differentiating through expectations . . . . . . . . . . . . 290\n12.2.1 Parameter-independent distributions . . . . . . . . 290\n12.2.2 Parameter-dependent distributions . . . . . . . . . 291\n12.2.3 Application to expected loss functions . . . . . . . 293\n12.2.4 Application to experimental design . . . . . . . . . 294\n12.3 Score function estimators, REINFORCE . . . . . . . . . . 295\n12.3.1 Scalar-valued functions . . . . . . . . . . . . . . . 295\n12.3.2 Variance reduction . . . . . . . . . . . . . . . . . . 298\n12.3.3 Vector-valued functions . . . . . . . . . . . . . . . 299\n12.3.4 Second derivatives . . . . . . . . . . . . . . . . . . 300\n12.4 Path gradient estimators, reparametrization trick . . . . . 301\n12.4.1 Location-scale transforms . . . . . . . . . . . . . . 301\n12.4.2 Differentiable transforms . . . . . . . . . . . . . . 303\n12.4.3 Inverse transforms . . . . . . . . . . . . . . . . . . 304\n12.4.4 Pushforward operators . . . . . . . . . . . . . . . 306\n12.4.5 Change-of-variables theorem . . . . . . . . . . . . 308\n12.5 Stochastic programs . . . . . . . . . . . . . . . . . . . . . 309\n12.5.1 Stochastic computation graphs . . . . . . . . . . . 309\n12.5.2 Examples . . . . . . . . . . . . . . . . . . . . . . 312\n12.5.3 Unbiased gradient estimators . . . . . . . . . . . . 314", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2260, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d1637837-4685-4bdd-aae8-3b0271964031": {"__data__": {"id_": "d1637837-4685-4bdd-aae8-3b0271964031", "embedding": null, "metadata": {"page_label": "xi", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2a62d218-63b5-45f7-b38a-9ea517be099f", "node_type": "4", "metadata": {"page_label": "xi", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "53931af60c59d0a37e4546a469f1635e04396db49d72815d8b3a7b53edc9e5e0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12.5.4 Local vs. global expectations . . . . . . . . . . . . 316\n12.6 Differential equations . . . . . . . . . . . . . . . . . . . . 317\n12.6.1 Parameterized differential equations . . . . . . . . 317\n12.6.2 Continuous adjoint method . . . . . . . . . . . . . 320\n12.6.3 Gradients via the continuous adjoint method . . . .321\n12.6.4 Gradients via reverse-mode on discretization . . . .324\n12.6.5 Reversible discretization schemes . . . . . . . . . . 325\n12.6.6 Proof of the continuous adjoint method . . . . . .328\n12.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 329\nIV Smoothing programs 332\n13 Smoothing by optimization 333\n13.1 Primal approach . . . . . . . . . . . . . . . . . . . . . . . 333\n13.1.1 Infimal convolution . . . . . . . . . . . . . . . . . 334\n13.1.2 Moreau envelope . . . . . . . . . . . . . . . . . . 335\n13.1.3 Vector-valued functions . . . . . . . . . . . . . . . 339\n13.2 Legendre\u2013Fenchel transforms, convex conjugates . . . . . .341\n13.2.1 Definition . . . . . . . . . . . . . . . . . . . . . . 341\n13.2.2 Closed-form examples . . . . . . . . . . . . . . . . 342\n13.2.3 Properties . . . . . . . . . . . . . . . . . . . . . . 344\n13.2.4 Conjugate calculus . . . . . . . . . . . . . . . . . 346\n13.2.5 Fast Legendre transform . . . . . . . . . . . . . . 346\n13.3 Dual approach . . . . . . . . . . . . . . . . . . . . . . . . 347\n13.3.1 Duality between strong convexity and smoothness .347\n13.3.2 Smoothing by dual regularization . . . . . . . . . .348\n13.3.3 Equivalence between primal and dual regularizations350\n13.3.4 Regularization scaling . . . . . . . . . . . . . . . . 351\n13.3.5 Generalized entropies . . . . . . . . . . . . . . . . 352\n13.4 Smoothed ReLU functions . . . . . . . . . . . . . . . . . 356\n13.5 Smoothed max operators . . . . . . . . . . . . . . . . . . 358\n13.5.1 Definition and properties . . . . . . . . . . . . . . 358\n13.5.2 Reduction to root finding . . . . . . . . . . . . . . 359\n13.5.3 The softmax . . . . . . . . . . . . . . . . . . . . . 360\n13.5.4 The sparsemax . . . . . . . . . . . . . . . . . . . . 361", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2080, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c76e899a-a93f-4a23-833c-fdcff790fc9c": {"__data__": {"id_": "c76e899a-a93f-4a23-833c-fdcff790fc9c", "embedding": null, "metadata": {"page_label": "xii", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fcdd957-e322-4f9d-adcc-b1725fdac1f1", "node_type": "4", "metadata": {"page_label": "xii", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "e899810368c8508829434bf472d068e6118ff85e0b804f581d93bdfeb8ed0193", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13.5.5 Recovering smoothed ReLU functions . . . . . . . 364\n13.6 Relaxed step functions (sigmoids) . . . . . . . . . . . . . .364\n13.7 Relaxed argmax operators . . . . . . . . . . . . . . . . . .365\n13.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 369\n14 Smoothing by integration 371\n14.1 Convolution . . . . . . . . . . . . . . . . . . . . . . . . . 371\n14.1.1 Convolution operators . . . . . . . . . . . . . . . . 371\n14.1.2 Convolution with a kernel . . . . . . . . . . . . . .372\n14.1.3 Discrete convolution . . . . . . . . . . . . . . . . . 373\n14.1.4 Differentiation . . . . . . . . . . . . . . . . . . . . 375\n14.1.5 Multidimensional convolution . . . . . . . . . . . . 375\n14.1.6 Link between convolution and infimal convolution .375\n14.1.7 The soft infimal convolution . . . . . . . . . . . . 376\n14.1.8 The soft Moreau envelope . . . . . . . . . . . . . .377\n14.2 Fourier and Laplace transforms . . . . . . . . . . . . . . .378\n14.2.1 Convolution theorem . . . . . . . . . . . . . . . . 378\n14.2.2 Link between Fourier and Legendre transforms . . .378\n14.2.3 The soft Legendre-Fenchel transform . . . . . . . .379\n14.3 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . 382\n14.3.1 Smoothed step function . . . . . . . . . . . . . . . 382\n14.3.2 Smoothed ReLU function . . . . . . . . . . . . . . 383\n14.4 Perturbation of blackbox functions . . . . . . . . . . . . . 385\n14.4.1 Expectation in a location-scale family . . . . . . . 385\n14.4.2 Gradient estimation by reparametrization . . . . . 386\n14.4.3 Gradient estimation by SFE, Stein\u2019s lemma . . . . 387\n14.4.4 Link between reparametrization and SFE . . . . . .388\n14.4.5 Variance reduction and evolution strategies . . . . 389\n14.4.6 Zero-temperature limit . . . . . . . . . . . . . . . 390\n14.5 Gumbel tricks . . . . . . . . . . . . . . . . . . . . . . . . 391\n14.5.1 The Gumbel distribution . . . . . . . . . . . . . . 391\n14.5.2 Perturbed comparison . . . . . . . . . . . . . . . . 393\n14.5.3 Perturbed argmax . . . . . . . . . . . . . . . . . . 394\n14.5.4 Perturbed max . . . . . . . . . . . . . . . . . . . . 395\n14.5.5 Gumbel trick for sampling . . . . . . . . . . . . . .396\n14.5.6 Perturb-and-MAP . . . . . . . . . . . . . . . . . . 396", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2238, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d5d1f626-49b8-4ec0-a839-04aef8c5cc80": {"__data__": {"id_": "d5d1f626-49b8-4ec0-a839-04aef8c5cc80", "embedding": null, "metadata": {"page_label": "xiii", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aeead514-701c-4be7-aa0a-5aa2f0d97e72", "node_type": "4", "metadata": {"page_label": "xiii", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "7fbd3543d4456652126fb77affd8f3bbfc45980203084404c0391a3239d5be5d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14.5.7 Gumbel-softargmax . . . . . . . . . . . . . . . . . 398\n14.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 400\nV Optimizing differentiable programs 402\n15 Optimization basics 403\n15.1 Objective functions . . . . . . . . . . . . . . . . . . . . . 403\n15.2 Oracles . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404\n15.3 Variational perspective of optimization algorithms . . . . .405\n15.4 Classes of functions . . . . . . . . . . . . . . . . . . . . . 405\n15.4.1 Lipschitz functions . . . . . . . . . . . . . . . . . 405\n15.4.2 Smooth functions . . . . . . . . . . . . . . . . . . 406\n15.4.3 Convex functions . . . . . . . . . . . . . . . . . . 408\n15.4.4 Strongly-convex functions . . . . . . . . . . . . . . 410\n15.4.5 Nonconvex functions . . . . . . . . . . . . . . . . 411\n15.5 Performance guarantees . . . . . . . . . . . . . . . . . . . 413\n15.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 416\n16 First-order optimization 417\n16.1 Gradient descent . . . . . . . . . . . . . . . . . . . . . . . 417\n16.1.1 Variational perspective . . . . . . . . . . . . . . . 417\n16.1.2 Convergence for smooth functions . . . . . . . . . 418\n16.1.3 Momentum and accelerated variants . . . . . . . . 420\n16.2 Stochastic gradient descent . . . . . . . . . . . . . . . . . 421\n16.2.1 Stochastic gradients . . . . . . . . . . . . . . . . . 422\n16.2.2 Vanilla SGD . . . . . . . . . . . . . . . . . . . . . 423\n16.2.3 Momentum variants . . . . . . . . . . . . . . . . . 424\n16.2.4 Adaptive variants . . . . . . . . . . . . . . . . . . 425\n16.3 Projected gradient descent . . . . . . . . . . . . . . . . . 425\n16.3.1 Variational perspective . . . . . . . . . . . . . . . 426\n16.3.2 Optimality conditions . . . . . . . . . . . . . . . . 427\n16.3.3 Commonly-used projections . . . . . . . . . . . . . 427\n16.4 Proximal gradient method . . . . . . . . . . . . . . . . . .428\n16.4.1 Variational perspective . . . . . . . . . . . . . . . 429\n16.4.2 Optimality conditions . . . . . . . . . . . . . . . . 429", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2032, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2c96b25b-3f5a-4369-9ae2-100e93c32e32": {"__data__": {"id_": "2c96b25b-3f5a-4369-9ae2-100e93c32e32", "embedding": null, "metadata": {"page_label": "xiv", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c59b9d6-36c8-47bb-b95b-c3f45b5d9ff2", "node_type": "4", "metadata": {"page_label": "xiv", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "4cb2339bc44e1422ffe5553cdbd67875c8fca3d0dbeac3a9298be78a52d32bcd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "16.4.3 Commonly-used proximal operators . . . . . . . . .430\n16.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 430\n17 Second-order optimization 432\n17.1 Newton\u2019s method . . . . . . . . . . . . . . . . . . . . . . 432\n17.1.1 Variational perspective . . . . . . . . . . . . . . . 432\n17.1.2 Regularized Newton method . . . . . . . . . . . . 433\n17.1.3 Approximate direction . . . . . . . . . . . . . . . . 434\n17.1.4 Convergence guarantees . . . . . . . . . . . . . . . 434\n17.1.5 Linesearch . . . . . . . . . . . . . . . . . . . . . . 434\n17.1.6 Geometric interpretation . . . . . . . . . . . . . . 435\n17.1.7 Stochastic Newton\u2019s method . . . . . . . . . . . . 436\n17.2 Gauss-Newton method . . . . . . . . . . . . . . . . . . . 437\n17.2.1 With exact outer function . . . . . . . . . . . . . .438\n17.2.2 With approximate outer function . . . . . . . . . . 439\n17.2.3 Linesearch . . . . . . . . . . . . . . . . . . . . . . 440\n17.2.4 Stochastic Gauss-Newton . . . . . . . . . . . . . . 440\n17.3 Natural gradient descent . . . . . . . . . . . . . . . . . . 441\n17.3.1 Variational perspective . . . . . . . . . . . . . . . 441\n17.3.2 Stochastic natural gradient descent . . . . . . . . .442\n17.4 Quasi-Newton methods . . . . . . . . . . . . . . . . . . . 443\n17.4.1 BFGS . . . . . . . . . . . . . . . . . . . . . . . . 443\n17.4.2 Limited-memory BFGS . . . . . . . . . . . . . . . 444\n17.5 Approximate Hessian diagonal inverse preconditionners . .444\n17.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 444\n18 Duality 446\n18.1 Dual norms . . . . . . . . . . . . . . . . . . . . . . . . . 446\n18.2 Fenchel duality . . . . . . . . . . . . . . . . . . . . . . . .447\n18.3 Bregman divergences . . . . . . . . . . . . . . . . . . . . 450\n18.4 Fenchel-Young loss functions . . . . . . . . . . . . . . . . 453\n18.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 454\nReferences 455", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1914, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d4f095b2-c735-4141-a4fa-aa9798eac5f4": {"__data__": {"id_": "d4f095b2-c735-4141-a4fa-aa9798eac5f4", "embedding": null, "metadata": {"page_label": "1", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bc31d22d-92d8-431d-a2f7-f317658115a5", "node_type": "4", "metadata": {"page_label": "1", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "f1142ab3a9aea955af85307dae30c543767925203a05bbfd2f77ac3d3d03e2db", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Elements of\nDifferentiable Programming\nMathieu Blondel1 and Vincent Roulet1\n1Google DeepMind\nABSTRACT\nArtificial intelligence has recently experienced remarkable\nadvances, fueled by large models, vast datasets, acceler-\nated hardware, and, last but not least, the transformative\npower of differentiable programming. This new programming\nparadigm enables end-to-end differentiation of complex com-\nputer programs (including those with control flows and data\nstructures), making gradient-based optimization of program\nparameters possible.\nAs an emerging paradigm, differentiable programming builds\nupon several areas of computer science and applied mathe-\nmatics, including automatic differentiation, graphical mod-\nels, optimization and statistics. This book presents a com-\nprehensive review of the fundamental concepts useful for\ndifferentiable programming. We adopt two main perspec-\ntives, that of optimization and that of probability, with clear\nanalogies between the two.\nDifferentiable programming is not merely the differentiation\nof programs, but also the thoughtful design of programs\nintended for differentiation. By making programs differen-\ntiable, we inherently introduce probability distributions over\ntheir execution, providing a means to quantify the uncer-\ntainty associated with program outputs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1317, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "23546f82-a25a-4326-9d07-e05529a31672": {"__data__": {"id_": "23546f82-a25a-4326-9d07-e05529a31672", "embedding": null, "metadata": {"page_label": "2", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d14e76bc-e91c-409a-9a11-a609bb01a8dd", "node_type": "4", "metadata": {"page_label": "2", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "411527e9f7c8f9214b978a6bc1acbb225902d382777f0530ac5e2ab20ba5cb73", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Acknowledgements\nWe thank the following people for sending us feedback, suggestions\nand typos: Fabian Pedregosa, Kevin Murphy, Niklas Schmitz, Nidham\nGazagnadou, Bruno De Backer, David L\u00f3pez, Guillaume Gautier, Sam\nDuffield, Logan Bruns, Wojciech Stokowiec, Alex Towell, John Reid,\nSadish Dhakal, Fabian Schaipp, Mahmoud Asem, Simone Scardapane,\n(add your name here!).\n2", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 370, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4978da61-21e6-42ed-8ef6-c9bd449f1f2e": {"__data__": {"id_": "4978da61-21e6-42ed-8ef6-c9bd449f1f2e", "embedding": null, "metadata": {"page_label": "3", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ae82100d-8a93-498e-845a-f1bd93600563", "node_type": "4", "metadata": {"page_label": "3", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "2fc94d3554d1bc3faa36d0c94d75a72bb7b9a8782cd78759e1607e6a5c862e5e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Source code\nWe provide some Python source code to accompany the book on github.\n3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 81, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fe71ffa5-dc19-43ca-a8a6-75d24526d12b": {"__data__": {"id_": "fe71ffa5-dc19-43ca-a8a6-75d24526d12b", "embedding": null, "metadata": {"page_label": "4", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bd522413-28da-4ff5-8131-001f353b7a92", "node_type": "4", "metadata": {"page_label": "4", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "bf7059093398a977e08a9f12d4ba218cdb68cf55f7200655f102f6450622a1a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Notation\nTable 1:Naming conventions\nNotation Description\nX\u2286 RD Input space (e.g., features)\nY\u2286 RM Output space (e.g., classes)\nSk \u2286RDk Output space on layer or statek\nW\u2286 RP Weight space\n\u039b \u2286RQ Hyperparameter space\n\u0398 \u2286RR Distribution parameter space, logit space\nN Number of training samples\nT Number of optimization iterations\nx\u2208X Input vector\ny\u2208Y Target vector\nsk \u2208Sk State vectork\nw\u2208W Network (model) weights\n\u03bb\u2208\u039b Hyperparameters\n\u03b8\u2208\u0398 Distribution parameters, logits\n\u03c0\u2208[0,1] Probability value\n\u03c0\u2208\u25b3M Probability vector\n4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 517, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "81061db1-604d-4a22-bc1a-a362f6d6d8ab": {"__data__": {"id_": "81061db1-604d-4a22-bc1a-a362f6d6d8ab", "embedding": null, "metadata": {"page_label": "5", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5c36486e-83af-438f-bd99-fe99c67fb514", "node_type": "4", "metadata": {"page_label": "5", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "559506f6d59e50ccffb064794640fd2f4c1165e84d61af4cf6a7603e4f470978", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5\nTable 2:Naming conventions (continued)\nNotation Description\nf Network function\nf(\u00b7; x) Network function withxfixed\nL Objective function\n\u2113 Loss function\n\u03ba Kernel function\n\u03d5 Output embedding, sufficient statistic\nstep Heaviside step function\nlogistic\u03c3 Logistic function with temperature\u03c3\nlogistic Shorthand forlogistic1\np\u03b8 Model distribution with parameters\u03b8\n\u03c1 Data distribution overX\u00d7Y\n\u03c1X Data distribution overX\n\u00b5,\u03c32 Mean and variance\nZ Random noise variable", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 460, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "61c975e4-a365-4528-a52c-4ee4c60dda6c": {"__data__": {"id_": "61c975e4-a365-4528-a52c-4ee4c60dda6c", "embedding": null, "metadata": {"page_label": "6", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a9b4e6fe-0ce0-45e1-b1ab-1c665d54d426", "node_type": "4", "metadata": {"page_label": "6", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "752389530952699289832f8b23943c3ad615ad6f008b331bd4e333eb7faadace", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1\nIntroduction\n1.1 What is differentiable programming?\nA computer program is a sequence of elementary instructions for per-\nforming a task. In traditional computer programming, the program is\ntypically manually written by a programmer. However, for certain tasks,\nparticularly those involving intricate patterns and complex decision-\nmaking, such as image recognition or text generation, manually writing\na program is extremely challenging, if not impossible.\nIn contrast, modern neural networks offer a different approach. They\nare constructed by combining parameterized functional blocks and\nare trained directly from data using gradient-based optimization. This\nend-to-end training process, where the network learns both feature\nextraction and task execution simultaneously, allows neural networks to\ntackle complex tasks that were previously considered insurmountable\nfor traditional, hand-coded programs. This new programming paradigm\nhas been referred to as \u201cdifferentiable programming\u201d or \u201csoftware 2.0\u201d.\nWe give an informal definition below.\n6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1051, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d0a5cf6f-db77-4a1e-b23f-37dd6816959f": {"__data__": {"id_": "d0a5cf6f-db77-4a1e-b23f-37dd6816959f", "embedding": null, "metadata": {"page_label": "7", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c59d9ca0-7be7-4ee7-9e65-c3d26bd8bf2c", "node_type": "4", "metadata": {"page_label": "7", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "e4e4d60a3c005e1fb4069f7886486b0c619b4e636db48ba2c3aea05c6b22e6af", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1.1. What is differentiable programming? 7\n    \n  \n    \n  \n...\nUnparameterized program\n...\nParameterized program\nFigure 1.1:Neural networks can be seen as parameterized programs.\nDefinition 1.1(Differentiable programming). Differentiableprogram-\nming is a programming paradigm in which complex computer pro-\ngrams (including those with control flows and data structures) can\nbe differentiated end-to-end automatically, enabling gradient-based\noptimization of parameters in the program.\nNeural networks as parameterized programs\nIn differentiable programming, as in regular computer programming, a\nprogram is defined as the composition of elementary operations, forming\na computation graph. The key difference is that, as illustrated in\nFig. 1.1, the program (such as a neural network) containsparameters\nthat can be adjusted from data and can be differentiated end-to-end,\nusing automatic differentiation(autodiff). Typically, it is assumed\nthat the program defines amathematically valid function(a.k.a.\npure function): the function should return identical values for identical\narguments and should not have any side effects. Moreover, the function\nshould havewell-defined derivatives, ensuring that it can be used\nin a gradient-based optimization algorithm. Therefore, differentiable\nprogramming is not only the art of differentiating through programs\nbut also ofdesigning meaningful differentiable programs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1409, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "813c32fd-8061-46ee-8f56-b997ac751fc6": {"__data__": {"id_": "813c32fd-8061-46ee-8f56-b997ac751fc6", "embedding": null, "metadata": {"page_label": "8", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "72f1d33e-6577-45a6-a546-38613b8d35bb", "node_type": "4", "metadata": {"page_label": "8", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "8cfb47f3822ca2091705fb4dbd4574e805d7db534300c40fc7bdb886a73bab2a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8 Introduction\nWhy do we need derivatives?\nMachine learning typically boils down to optimizing a certain objec-\ntive function, which is the composition of a loss function and a model\n(network) function. Derivative-free optimization is calledzero-order\noptimization. It only assumes that we can evaluate the objective\nfunction that we wish to optimize. Unfortunately, it is known to suffer\nfrom the curse of dimensionality, i.e., it only scales to small di-\nmensional problems, such as less than10 dimensions. Derivative-based\noptimization, on the other hand, is much more efficient and can scale to\nmillions or billions of parameters. Algorithms that use first and second\nderivatives are known asfirst-order and second-order algorithms,\nrespectively.\nWhy is autodiff so useful?\nBefore the autodiff revolution, researchers and practitioners needed\nto manually implement the gradient of the functions they wished to\noptimize. Manually deriving gradients can become very tedious for\ncomplicated functions. Moreover, every time the function is changed\n(for example, for trying out a new idea), the gradient needs to be re-\nderived. Autodiff is a game changer because it allows users to focus on\nquickly and creatively experimenting with functions for their tasks. An\nexample of JAX code (Bradburyet al., 2018) is given in Fig. 1.2.\nDifferentiable programming is not just deep learning\nWhile there is clearly overlap between deep learning and differentiable\nprogramming, their focus is different. Deep learning studies artificial\nneuralnetworkscomposedofmultiplelayers,abletolearn intermediate\nrepresentations of the data. Neural network architectures have been\nproposed with variousinductive biases. For example, convolutional\nneural networks are designed for images and transformers are designed\nfor sequences. On the other hand, differentiable programming studies the\ntechniques for designing complex programs and differentiating through\nthem. It is useful beyond deep learning: for instance in reinforcement\nlearning, probabilistic programming and scientific computing in general.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2079, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4e3c21c4-4203-4c46-8278-17ed9b06c913": {"__data__": {"id_": "4e3c21c4-4203-4c46-8278-17ed9b06c913", "embedding": null, "metadata": {"page_label": "9", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8aef010f-d6b9-478c-8951-a1454a2b7aac", "node_type": "4", "metadata": {"page_label": "9", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "680eb69573246d248e0e3bdfc62b9986787c7c560682a2913549e9df50a28f59", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1.1. What is differentiable programming? 9\nThe autodiff revolution\nimport jax.numpy as jnp\nfrom jax import grad, jit\ndef predict(params, inputs): \n  for W, b in params:\n    outputs = jnp.dot(inputs, W) + b\n    inputs = jnp.tanh(outputs) \n  return outputs\ndef loss_fn(params, inputs, targets):\n  outputs = predict(params, inputs)\n  return jnp.sum((outputs - targets) ** 2)\ngrad_fun = jit(grad(loss_fn))\nFigure 1.2:Thanks to automatic differentiation (autodiff), the user can focus on\nexpressing the forward computation (model), enabling fast experimentation and\nalleviating the need for error-prone manual gradient derivation.\nDifferentiable programming is not just autodiff\nWhile autodiff is a key ingredient of differentiable programming, this\nis not the only one. Differentiable programming is also concerned with\nthe design of principled differentiable operations. In fact, much research\non differentiable programming has been devoted to make classical com-\nputer programming operations compatible with autodiff. As we shall\nsee, many differentiable relaxations can be interpreted in a probabilis-\ntic framework. A core theme of this book is the interplay between\noptimization, probability and differentiation. Differentiation is useful\nfor optimization and conversely, optimization can be used to design\ndifferentiable operators.\nOur vision for differentiable programming\nComputer programming offers powerful tools like control flows, data\nstructures, and standard libraries, enabling users to construct complex\nprograms for solving intricate problems. Our long-term vision is to\nachieve parity between traditional and differentiable programming, em-\npowering programmers to seamlessly express differentiable programs\n(such as neural networks) using the full suite of tools they are accus-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1793, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e9b793d8-48a7-4cea-86c6-9cbd901fca1f": {"__data__": {"id_": "e9b793d8-48a7-4cea-86c6-9cbd901fca1f", "embedding": null, "metadata": {"page_label": "10", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8f4891f0-5e7c-4ac6-9e30-df58d1c69f90", "node_type": "4", "metadata": {"page_label": "10", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "30fdcacf8a8ab7d0348b7d6b7405c3eac3cd6179aadc4924ca698739fbaf1476", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10 Introduction\ntomed to. However, as discussed earlier, differentiable programming is\nnot simply a matter of applying automatic differentiation to existing\ncode. Programs must be designed with differentiability in mind. This\nusually comes to inducing a probability distribution over the program\nor its components. While significant work remains to fully realize this\nambitious goal, we hope this book offers a solid foundation.\nWhere does \u201cdifferentiable programming\u201d come from?\nWhile neural networks and autodiff have existed for several decades,\nthe term \u201cdifferentiable programming\u201d is more recent. Olah (2015) dis-\ncussed anologies between neural network architectures and higher-order\nfunctions in functional programming, and referred to it as a \u201cnew kind\nof programming\u201d. Dalrymple (2016) wrote an essay titled \u201cdifferentiable\nprogramming\u201d, recognizing a paradigm where programs learn details\nthrough differentiation, with the expressiveness of functional program-\nming. Plotkin (2018) gave a keynote talk titled \u201cSome principles of\ndifferential programming languages\u201d at POPL 2018. The \u201cdifferentiable\nprogramming\u201d and \u201csoftware 2.0\u201d terms were popularized among others\nby LeCun (2018) and Karpathy (2017). From this perspective, autodiff\nframeworks can be seen, not merely as libraries, but as domain-specific\nlanguages (DSLs) embedded into an existing programming language,\nsuch as Python. See also Imai (2019) for a review.\n1.2 Book goals and scope\nThe present book aims to provide an comprehensive introduction to\ndifferentiable programming with an emphasis oncore mathematical\ntools.\n\u2022 In Part I, we reviewfundamentals: differentiation and proba-\nbilistic learning.\n\u2022 In Part II, we reviewdifferentiable programs. This includes\nneural networks, sequence networks and control flows.\n\u2022 In Part III, we review how todifferentiate through programs.\nThis includes automatic differentiation, but also differentiating", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1923, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ddb55c98-adde-40f9-a75a-ed989d620e76": {"__data__": {"id_": "ddb55c98-adde-40f9-a75a-ed989d620e76", "embedding": null, "metadata": {"page_label": "11", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f962c606-28d6-43b5-ac10-e20210f42aa8", "node_type": "4", "metadata": {"page_label": "11", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "79b96d454da9cf88530ecf772b37766fd34aa2dba04fbdd58b33a041153f4529", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1.3. Intended audience 11\nthrough optimization and integration (in particular, expectations).\n\u2022 In Part IV, we reviewsmoothing programs. We focus on two\nmain techniques: infimal convolution, which comes from the world\nof optimization and convolution, which comes from the world of\nintegration. We also strive to spell out the connections between\nthem.\n\u2022 In Part V, we reviewoptimizing programs: basic optimiza-\ntion concepts, first-order algorithms, second-order algorithms and\nduality.\nOurgoalistopresentthefundamentaltechniquesusefulfordifferentiable\nprogramming, not to survey how these techniques have been used in\nvarious applications.\n1.3 Intended audience\nThisbookisintendedtobeagraduate-levelintroductiontodifferentiable\nprogramming. Our pedagogical choices are made with the machine\nlearning community in mind. Some familiarity with calculus, linear\nalgebra, probability theory and machine learning is beneficial.\n1.4 How to read this book?\nThis book does not need to be read linearly chapter by chapter. When\nneeded, we indicate at the beginning of a chapter what chapters are\nrecommended to be read as a prerequisite.\n1.5 Related work\nDifferentiable programming builds upon a variety of connected topics.\nWe review in this section relevant textbooks, tutorials and software.\nStandard textbooks on backpropagation and automatic differentia-\ntion (autodiff) are that of Werbos (1994) and Griewank and Walther\n(2008). A tutorial with a focus on machine learning is provided by\nBaydin et al.(2018). Automatic differentiation is also reviewed as part", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1556, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a50fa191-c7cf-402b-bc10-46c4a2f04071": {"__data__": {"id_": "a50fa191-c7cf-402b-bc10-46c4a2f04071", "embedding": null, "metadata": {"page_label": "12", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "adf9f41b-d5e3-4793-b35d-cfe305c0465d", "node_type": "4", "metadata": {"page_label": "12", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "8897bcb012c22622d992b7b900f413501a806316128458a9656e65ef30fe8dbe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12 Introduction\nof more general textbooks, such as those of Deisenrothet al.(2020),\nMurphy (2022) (from a linear algebra perspective) and Murphy (2023)\n(from a functional perspective; autodiff section authored by Roy Frostig).\nThe present book was also influenced by Peyr\u00e9 (2020)\u2019s textbook on data\nscience. The history of reverse-mode autodiff is reviewed by Griewank\n(2012).\nA tutorial on different perspectives of backpropagation is \u201cThere and\nBack Again: A Tale of Slopes and Expectations\u201d (link), by Deisenroth\nand Ong. A tutorial on implicit differentiation is \u201cDeep Implicit Layers -\nNeural ODEs, Deep Equilibirum Models, and Beyond\u201d (link), by Kolter,\nDuvenaud, and Johnson.\nThe standard reference on inference in graphical models and its\nconnection with exponential families is that of Wainwright and Jor-\ndan (2008). Differential programming is also related to probabilistic\nprogramming; see, e.g., Meentet al.(2018).\nA review of smoothing from the infimal convolution perspective is\nprovided by Beck and Teboulle (2012). A standard textbook on convex\noptimization is that of Nesterov (2018). A textbook on first-order\noptimization methods is that of Beck (2017).\nAutodiff implementations that accelerated the autodiff revolution\nin machine learning are Theano (Bergstraet al., 2010) and Autograd\n(Maclaurin et al., 2015). Major modern implementations of autodiff\ninclude Tensorflow (Abadiet al., 2016), JAX (Bradburyet al., 2018),\nand PyTorch (Paszkeet al., 2019). We in particular acknowledge the\nJAX team for influencing our view of autodiff.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1555, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fe7a65fa-04aa-4e4e-b697-9b3d03a744d3": {"__data__": {"id_": "fe7a65fa-04aa-4e4e-b697-9b3d03a744d3", "embedding": null, "metadata": {"page_label": "13", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2463e660-25cd-4e1e-afe3-78d78eafcbde", "node_type": "4", "metadata": {"page_label": "13", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "c848b2f0562a964e3afe2a910816e4772892042e6298d827bc0c1f3ee90214a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Part I\nFundamentals", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 19, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bc112e93-414c-4d61-bb88-7efab9e2277c": {"__data__": {"id_": "bc112e93-414c-4d61-bb88-7efab9e2277c", "embedding": null, "metadata": {"page_label": "14", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0c36437e-2907-4549-bdaa-9f0b901f64ab", "node_type": "4", "metadata": {"page_label": "14", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "01c1593687e4050d26d04c33f54f58670f9c313c6ff647f7d0167f10d90c1403", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2\nDifferentiation\nIn this chapter, we review key differentiation concepts. In particular,\nwe emphasize on the fundamental role played by linear maps.\n2.1 Univariate functions\n2.1.1 Derivatives\nTo study functions, we need to capture their infinitesimal variations\naround points as defined by the notion oflimit.\nDefinition 2.1(Limit). We say thatc\u2208R is thelimitof f: R \u2192R\nas v\u2208R approaches w\u2208R, denoted\nlimv\u2192wf(v) = c,\nif, for any \u03b5 >0, there exists R >0 such that for anyv \u2208R\nsatisfying 0 <|v\u2212w|\u2264 R, we have|f(v) \u2212c|\u2264 \u03b5.\nWe can also writef(v) \u2192c as v \u2192w. Limits are preserved un-\nder additions and multiplications. Namely, iflimv\u2192wf(v) = c and\nlimv\u2192wg(v) = d, then denoting (af + bg)(w) := af(w) + bg(w) for\nany a,b \u2208R and (fg)(w) := f(w)g(w), we have by definition of the\n14", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 775, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b808ee3d-6754-47c3-986c-0ca58d5d2507": {"__data__": {"id_": "b808ee3d-6754-47c3-986c-0ca58d5d2507", "embedding": null, "metadata": {"page_label": "15", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4af66db7-e787-43fa-a725-ba6d67512629", "node_type": "4", "metadata": {"page_label": "15", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "2fdcf452e7c21fcd20252425a05675151b83e0cda4fcf89918aae06836826fe6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.1. Univariate functions 15\nlimit, limv\u2192w(af + bg)(v) = ac+ bd and limv\u2192w(fg)(v) = cd. The\npreservation of the limit under addition and multiplication by a scalar\nis generally referred to as the linearity of the limit, a property that\nmany definitions in the sequel inherit.\nWith the notion of limit, we can already delineate a class of \u201cwell-\nbehaved\u201d functions: functions whose limits at any point equals the value\nof the function at that point. Functions satisfying this property are\ncalled continuous.\nDefinition 2.2(Continuous function). A function f : R \u2192 R is\ncontinuous at a pointw\u2208R if\nlimv\u2192wf(v) = f(w).\nA functionf is said to be continuous if it is continuous at all points\nin its domain.\nAlthough the notion of continuity appears to be a benign assump-\ntion, several functions commonly-used in machine learning, such as the\nHeaviside step function (displayed in the left panel of Fig. 2.2), are not\ncontinuous and require special treatment.\nRemark 2.1(Little o notation). In the following, we will make use\nof Landau\u2019s littleo notation. We write\ng(v) = o(f(v)) as v\u2192w\nif\nlimv\u2192w\n|g(v)|\n|f(v)|= 0.\nThat is, the functionf dominates gin the limitv\u2192w. For example,\nf is continuous atw if and only if\nf(w+ \u03b4) = f(w) + o(1) as \u03b4\u21920.\nWe now explain derivatives. Consider a functionf : R \u2192R. As\nillustrated in Fig. 2.1, its value on an interval[w0,w0 + \u03b4] can be\napproximated by the secant between its valuesf(w0) and f(w0 + \u03b4),\na linear function with slope(f(w0 + \u03b4) \u2212f(w0))/\u03b4. In the limit of an", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1500, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a152016d-1484-4154-9491-705f69feddd8": {"__data__": {"id_": "a152016d-1484-4154-9491-705f69feddd8", "embedding": null, "metadata": {"page_label": "16", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7f94ea95-0283-45d1-8a81-38da77061a02", "node_type": "4", "metadata": {"page_label": "16", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "1a71d8adb1bb748bbb9aa64c47f900aeb8be7162e595eb01c54f8c228f51af1c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "16 Differentiation\nFigure 2.1:A functionf can be locally approximated around a pointw0 by a secant,\na linear functionw\u21a6\u2192aw+bwith slopeaand interceptb, crossingf at w0 with value\nu0 = f(w0) and crossing atw0 + \u03b4 with valueu\u03b4 = f(w0 + \u03b4). Usingu0 = aw0 + b\nand u\u03b4 = a(w0 + \u03b4) + b, we find that its slope isa= (f(w0 + \u03b4) \u2212f(w0))/\u03b4 and the\nintercept isb= f(w0) \u2212aw0. The derivativef\u2032(w) of a functionf at a pointw0 is\nthen defined as the limit of the slopea when \u03b4 \u21920. It is the slope of the tangent\nof f at w0. The valuef(w) of the function atw can then be locally approximated\naround w0 by w\u21a6\u2192f\u2032(w0)w+ f(w0) \u2212f\u2032(w0)w0 = f(w0) + f\u2032(w0)(w\u2212w0).\ninfinitesimal variation\u03b4around w0, the secant converges to thetangent\nof f at w0 and the resulting slope defines the derivative off at w0. The\ndefinition below formalizes this intuition.\nDefinition 2.3(Derivative). The derivativeof f : R \u2192R at w\u2208\nR is defined as\nf\u2032(w) := lim\n\u03b4\u21920\nf(w+ \u03b4) \u2212f(w)\n\u03b4 , (2.1)\nprovided that the limit exists. Iff\u2032(w) is well-defined at a particular\nw, we say that the function f is differentiable at w. If f is\ndifferentiable at any w \u2208 R, we say that it is differentiable\neverywhere or differentiable for short.\nIf f is differentiable at a givenw, then it is necessarilycontinuous\nat w as shown in the following proposition. Non-differentiability of a\ncontinuous function at a given pointw is generally illustrated by a kink,\nas shown in Fig. 2.2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1415, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1fb27db6-3aeb-4de5-870c-2970b0315647": {"__data__": {"id_": "1fb27db6-3aeb-4de5-870c-2970b0315647", "embedding": null, "metadata": {"page_label": "17", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3b49b232-5373-4b58-b0c6-4126c3dcefeb", "node_type": "4", "metadata": {"page_label": "17", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "e1611011beaa08bdb6df662c3bb78d73df4799fe5b1031dfb51b12980f97d7eb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.1. Univariate functions 17\n2\n 1\n 0 1 2\n0.0\n0.5\n1.0\nDiscontinuous at 0\n2\n 1\n 0 1 2\n0.0\n0.5\n1.0\nContinuous\nnon-differentiable at 1 and -1\n2\n 1\n 0 1 2\n0.0\n0.5\n1.0\nDifferentiable everywhere\nFigure 2.2:Illustration of discontinuity and non-differentiability.Left. A discon-\ntinuous function presents a jump in function values at a given point.Center. A\ncontinuous but non-differentiable everywhere function presents kinks at the points of\nnon-differentiability.Right. A differentiable everywhere function is smooth.\nProposition 2.1(Differentiability implies continuity). If f : R \u2192R\nis differentiable atw\u2208R, then it is continuous atw\u2208R.\nProof. In littleonotation, f is differentiable atwif there existsf\u2032(w) \u2208\nR, such that\nf(w+ \u03b4) = f(w) + f\u2032(w)\u03b4+ o(\u03b4) as \u03b4\u21920.\nSince f\u2032(w)\u03b4+ o(\u03b4) = o(1) as \u03b4\u21920, f is continuous atw.\nIn addition to enabling the construction of a linear approximation\nof f in a neighborhood ofw, since it is the slope of the tangent off at\nw, the derivativef\u2032informs us about themonotonicity of f around\nw. Iff\u2032(w) is positive, the function is increasing aroundw. Conversely,\nif f\u2032(w) is negative, the function is decreasing. Such information can be\nused to develop iterative algorithms seeking to minimizef by computing\niterates of the formwt+1 = wt \u2212\u03b3f\u2032(wt) for \u03b3 >0, which move along\ndescent directions off around wt.\nFor several elementary functions such aswn, ew, ln w, cos wor sin w,\ntheir derivatives can be obtained directly by applying the definition of\nthe derivative in Eq. (2.1) as we now illustrate.\nExample 2.1(Derivative of power function). Consider f(w) = wn", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1586, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "42be38a5-6c0c-48f7-9365-42259c6be171": {"__data__": {"id_": "42be38a5-6c0c-48f7-9365-42259c6be171", "embedding": null, "metadata": {"page_label": "18", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b1ffec2-f416-49f1-91a6-6638a3707ebc", "node_type": "4", "metadata": {"page_label": "18", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "910a4183a9ec8a93326ab9cb0ee633ef162cd393eed676c9b2224b4ddebdbdf9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "18 Differentiation\nfor w\u2208R, n\u2208N \\{0}. For any\u03b4\u2208R, we have\nf(w+ \u03b4) \u2212f(w)\n\u03b4 = (w+ \u03b4)n \u2212wn\n\u03b4\n=\n\u2211n\nk=0\n(n\nk\n)\n\u03b4kwn\u2212k \u2212wn\n\u03b4\n=\nn\u2211\nk=1\n(\nn\nk\n)\n\u03b4k\u22121wn\u2212k\n=\n(\nn\n1\n)\nwn\u22121 +\nn\u2211\nk=2\n(\nn\nk\n)\n\u03b4k\u22121wn\u2212k,\nwhere, in the second line, we used the binomial theorem. Since(n\n1\n)\n= nand lim\u03b4\u21920\n\u2211n\nk=2\n(n\nk\n)\n\u03b4k\u22121wn\u2212k = 0, we getf\u2032(w) = nwn\u22121.\nRemark 2.2(Functions on a subsetUof R). For simplicity, we pre-\nsented the definition of the derivative for a function defined on the\nwhole set of real numbersR. If a functionf : U\u2192 R is defined on a\nsubset U\u2286 R of the real numbers, as it is the case forf(w) = \u221aw\ndefined onU= R+, the derivative off at w\u2208U is defined by the\nlimit in Eq. (2.1) provided that the functionf is well defined on a\nneighborhood ofw, that is, there existsr> 0 such thatw+ \u03b4\u2208U\nfor any |\u03b4|\u2264 r. The functionf is then saiddifferentiable ev-\nerywhere or differentiable for short if it is differentiable at any\npoint w in theinterior of U, the set of pointsw \u2208U such that\n{w+ \u03b4: |\u03b4|\u2264 r}\u2286U for rsufficiently small. For points lying at the\nboundary ofU(such asa and b if U= [a,b]), one may define the\nright and left derivatives off at a and b, meaning that the limit is\ntaken by approachinga from the right orb from the left.\n2.1.2 Calculus rules\nFor a givenw \u2208R and two functionsf : R \u2192R and g : R \u2192R, the\nderivative of elementary operations onf and g such as their sums,\nproducts or compositions can easily be derived from the definition of\nthe derivative, under appropriate conditions on the differentiability off\nand g at w. For example, if the derivatives off and g exist atw, then", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1574, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8bdb543e-a7b8-462d-9bf9-d3f6cd8670a9": {"__data__": {"id_": "8bdb543e-a7b8-462d-9bf9-d3f6cd8670a9", "embedding": null, "metadata": {"page_label": "19", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c896ec5f-0a2f-4aea-8f04-6806ba251529", "node_type": "4", "metadata": {"page_label": "19", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "13ec901a4e8868be7a12dcce10a46f22943de99049e1742dd25b97a0d47d3c33", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.1. Univariate functions 19\nthe derivatives of their weighted sum or product exist, and satisfy the\nrules\n\u2200a,b \u2208R, (af + bg)\u2032(w) = af\u2032(w) + bg\u2032(w) (Linearity)\n(fg)\u2032(w) = f\u2032(w)g(w) + f(w)g\u2032(w), (Product rule)\nwhere (fg)(w) := f(w)g(w). The linearity can be verified directly from\nthe linearity of the limits. For the product rule, in littleo notation, we\nhave, as\u03b4\u21920,\n(fg)(w+ \u03b4) = (f(w) + f\u2032(w)\u03b4+ o(\u03b4))(g(w) + g\u2032(w)\u03b4+ o(\u03b4))\n= f(w)g(w) + f\u2032(w)g(w)\u03b4+ f(w)g\u2032(w)\u03b4+ o(\u03b4),\nhence the result.\nIf the derivatives ofgat wand off at g(w) exist, then the derivative\nof the composition(f \u25e6g)(w) := f(g(w)) at w exists and is given by\n(f \u25e6g)\u2032(w) = f\u2032(g(w))g\u2032(w). (Chain rule)\nWe prove this result more generally in Proposition 2.2. As seen in the\nsequel, the linearity and the product rule can be seen as byproducts of\nthe chain rule, making the chain rule the cornerstone of differentiation.\nConsider a function that can be expressed using sums, products or\ncompositions of elementary functions, such asf(w) := ewln w+ cos w2.\nIts derivative can be computed by applying the aforementioned rules\non the decomposition off into elementary operations and functions.\nExample 2.2(Applying rules of differentiation). Consider f(w) :=\newln w+ cos w2. The derivative off at w >0 can be computed\nstep by step as follows, denotingsq(w) := w2,\nf\u2032(w) = (exp \u00b7ln)\u2032(w) + (cos\u25e6sq)\u2032(w) (Linearity)\n(exp \u00b7ln)\u2032(w) = exp\u2032(w) \u00b7ln(w) + exp(w) \u00b7ln\u2032(w) (Product rule)\n(cos \u25e6sq)\u2032(w) = cos\u2032(sq(w)) sq\u2032(w) (Chain rule)\nexp\u2032(w) = exp(w), ln\u2032(w) = 1/w, (Elem. func.)\nsq\u2032(w) = 2w, cos\u2032(w) = \u2212sin(w). (Elem. func.)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1570, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "168b83c4-ebc5-435b-86e3-33a4368ec5b8": {"__data__": {"id_": "168b83c4-ebc5-435b-86e3-33a4368ec5b8", "embedding": null, "metadata": {"page_label": "20", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "69dccb96-3af3-4f1e-b23b-96d9e2f59e6e", "node_type": "4", "metadata": {"page_label": "20", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "3f63d46f7b8405794567a4b43e7177254245a61d16985cf57d9e70f4e797c217", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "20 Differentiation\nWe therefore obtain thatf\u2032(w) = ewln w+ ew/w\u22122wsin w2.\nSuch a process is purely mechanical and lends itself to an automated\nprocedure, which is the main idea of automatic differentiation presented\nin Chapter 8.\n2.1.3 Leibniz\u2019s notation\nThe notion of derivative was first introduced independently by Newton\nand Leibniz in the 18th century (Ball, 1960). The latter considered\nderivatives as the quotient of infinitesimal variations. Namely, denoting\nu= f(w) a variable depending onw through f, Leibniz considered the\nderivative off as the quotient\nf\u2032= du\ndw with f\u2032(w) = du\ndw\n\u23d0\u23d0\u23d0\u23d0\nw\nwhere duand dw denote infinitesimal variations ofuand w respectively\nand the symbol|w denotes the evaluation of the derivative at a given\npoint w. This notation simplifies the statement of the chain rule first\ndiscovered by Leibniz (Rodriguez and Lopez Fernandez, 2010) as we\nhave forv= g(w) and u= f(v)\ndu\ndw = du\ndv \u00b7dv\ndw.\nThis hints that derivatives are multiplied when considering compositions.\nAt evaluation, the chain rule in Leibniz notation recovers the formula\npresented above as\ndu\ndw\n\u23d0\u23d0\u23d0\u23d0\nw\n= du\ndv\n\u23d0\u23d0\u23d0\u23d0\ng(w)\ndv\ndw\n\u23d0\u23d0\u23d0\u23d0\nw\n= f\u2032(g(w))g\u2032(w) = (f \u25e6g)\u2032(w).\nThe ability of Leibniz\u2019s notation to capture the chain rule as a mere\nproduct of quotients made it popular throughout the centuries, espe-\ncially in mechanics (Ball, 1960). The rationale behind Leibniz\u2019s notation,\nthe concept of \u201cinfinitesimal variations\u201d, was questioned by later mathe-\nmaticians for its potential logical issues (Ball, 1960). The notationf\u2032(w)\nfirst introduced by Euler and further popularized by Lagrange (Cajori,\n1993) has then taken over in numerous mathematical textbooks. The\nconcept of infinitesimal variations has been rigorously defined by using", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1737, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c0bf57af-e364-4236-a1ef-153ee4563a70": {"__data__": {"id_": "c0bf57af-e364-4236-a1ef-153ee4563a70", "embedding": null, "metadata": {"page_label": "21", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60c3a714-f332-45f6-937d-819ad4723549", "node_type": "4", "metadata": {"page_label": "21", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "1be4c8ed871c932d9c8d2397cff81dfe84326ac2b801fa2da1c40ebedbfe651f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.2. Multivariate functions 21\nthe set of hyperreal numbers. They extend the set of real numbers by\nconsidering each number as a sum of a non-infinitesimal part and an\ninfinitesimal part (Hewitt, 1948). The formalism of infinitesimal vari-\nations further underlies the development of automatic differentiation\nalgorithms through the concept of dual numbers.\n2.2 Multivariate functions\n2.2.1 Directional derivatives\nLet us now consider a functionf : RP \u2192R with multi-dimensional\ninput w:= (w1,...,w P) \u2208RP. The most important example in machine\nlearning is a function which, to the parametersw \u2208RP of a neural\nnetwork, associates a loss value inR. Variations off need to be defined\nalong specific directions, such as the variationf(w+ \u03b4v)\u2212f(w) of\nf around w \u2208 RP in the direction v \u2208 RP by an amount \u03b4 > 0.\nThis consideration naturally leads to the definition of the directional\nderivative.\nDefinition 2.4(Directional derivative). Thedirectionalderivative\nof f at win thedirection vis given by\n\u2202f(w)[v] := lim\n\u03b4\u21920\nf(w+ \u03b4v) \u2212f(w)\n\u03b4 ,\nprovided that the limit exists.\nWe use the notation[v] to emphasize that, for a given inputw,\nwe can seev\u21a6\u2192\u2202f(w)[v] as a function. This is essential to define the\ndifferentiability off (Definition 2.6) atw and to later define linear\nmaps (Section 2.3).\nOne example of directional derivative consists in computing the\nderivative of a functionf at win any of the canonical directions\nei := (0,..., 0, 1\ued19\ued18\ued17\ued1a\ni\n,0,..., 0).\nThis allows us to define the notion ofpartial derivatives, denoted for\ni\u2208[P]\n\u2202if(w) := \u2202f(w)[ei] = lim\n\u03b4\u21920\nf(w+ \u03b4ei) \u2212f(w)\n\u03b4 .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1577, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "590bbb30-6772-414a-83c4-e58aa98d4753": {"__data__": {"id_": "590bbb30-6772-414a-83c4-e58aa98d4753", "embedding": null, "metadata": {"page_label": "22", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "180fa6d7-f31e-42ee-835f-b6c88ab890a4", "node_type": "4", "metadata": {"page_label": "22", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "cd5c35c0ac7c7815c174b2b424a5d255895ce4e69c9b639d73c9c15a7cb2b649", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "22 Differentiation\nThis is also denoted in Leibniz\u2019s notation as\u2202if(w) = \u2202f(w)\n\u2202wi\nor\u2202if(w) =\n\u2202wif(w). By moving along only theith coordinate of the function,\nthe partial derivative is akin to differentiating the function \u03c9i \u21a6\u2192\nf(w1,...,\u03c9 i,...,w P) around \u03c9i, letting all other coordinates fixed at\ntheir valueswi.\n2.2.2 Gradients\nWe now introduce the gradient vector, which gathers the partial deriva-\ntives. We first recall the definitions of linear map and linear form.\nDefinition 2.5(Linear map, linear form). A functionl: RP \u2192RM\nis alinear mapif for anya1,a2 \u2208R, v1,v2 \u2208RD,\nl[a1v1 + a2v2] = a1l[v1] + a2l[v2].\nA linear map with values inR, l: RP \u2192R, is called alinear form.\nLinearity plays a crucial role in the differentiability of a function.\nDefinition 2.6(Differentiability, single-output case). A functionf :\nRP \u2192R is differentiable at w\u2208RP if its directional derivative\nis defined along any direction, is linear in any directionv, and if\nlim\n\u2225v\u22252\u21920\n|f(w+ v) \u2212f(w) \u2212\u2202f(w)[v]|\n\u2225v\u22252\n= 0.\nWe can now introduce the gradient.\nDefinition 2.7(Gradient). The gradient of a differentiable func-\ntion f : RP \u2192R at a pointw \u2208RP is defined as the vector of\npartial derivatives\n\u2207f(w) :=\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\u22021f(w)\n...\n\u2202Pf(w)\n\uf8f6\n\uf8f7\uf8f7\uf8f8=\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\u2202f(w)[e1]\n...\n\u2202f(w)[eP]\n\uf8f6\n\uf8f7\uf8f7\uf8f8\u2208RP.\nBy linearity, the directional derivative off at win the direction", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1321, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c3d62f1f-5df7-4dd1-986f-e1c628481dbc": {"__data__": {"id_": "c3d62f1f-5df7-4dd1-986f-e1c628481dbc", "embedding": null, "metadata": {"page_label": "23", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "acaff5d2-c796-476a-8f22-40a9fcfc9ac2", "node_type": "4", "metadata": {"page_label": "23", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "6ba2d885041bf50d4a3c0f1c66acecc99332f9bb5e1001c82564fdf304b941e3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.2. Multivariate functions 23\nv= \u2211P\ni=1 viei is then given by\n\u2202f(w)[v] =\nP\u2211\ni=1\nvi\u2202f(w)[ei] = \u27e8v,\u2207f(w)\u27e9\u2208 R.\nHere, \u27e8\u00b7,\u00b7\u27e9denotes the inner product. We provide its definition in\nEuclidean spaces in Section 2.3.2.\nIn the definition above, the fact that the gradient can be used to\ncompute the directional derivative is a mere consequence of the linearity\nof \u2202f(w)[v] w.r.t. v. However, in more abstract cases presented in later\nsections, the gradient is defined directly through this property.\nAs a simple example, any linear function of the form f(w) =\n\u27e8a,w\u27e9= \u2211P\ni=1 aiwi is differentiable as we have(\u27e8a,w+ v\u27e9\u2212\u27e8a,w\u27e9\u2212\n\u27e8a,v\u27e9)/\u2225v\u22252 = 0 for anyvand in particular for\u2225v\u2225\u2192 0. Moreover, its\ngradient is naturally given by\u2207f(w) = a.\nMore generally, to show that a function is differentiable and find its\ngradient, one approach is to approximatef(w+ v) around v= 0. If we\ncan find a vectorgsuch that\nf(w+ v) = f(w) + \u27e8g,v\u27e9+ o(\u2225v\u22252),\nthen f is differentiable atw, since\u27e8g,\u00b7\u27e9is linear. Moreover,gis then\nthe gradient off at w.\nRemark 2.3(Gateaux and Fr\u00e9chet differentiability). Multipledefini-\ntions of differentiability exist. The one presented in Definition 2.6\nis that of Fr\u00e9chet differentiable functions. Alternatively, if\nf : RP \u2192R has well-defined directional derivatives along any\ndirections then the function isGateaux differentiable. Note that\nthe existence of directional derivatives in any directions is not a\nsufficient condition for the function to be differentiable. In other\nwords, any Fr\u00e9chet differentiable function is Gateaux differentiable,\nbut the converse is not true. As a counter-example, one can verify\nthat the functionf(x1,x2) := x3\n1/(x2\n1 +x2\n2) is Gateaux differentiable\nat 0 but not (Fr\u00e9chet) differentiable at0 (because the directional\nderivative at 0 is not linear).\nSome authors also require Gateaux differentiable functions to", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1847, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "12e66299-4608-4caf-8592-fa617027b782": {"__data__": {"id_": "12e66299-4608-4caf-8592-fa617027b782", "embedding": null, "metadata": {"page_label": "24", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "869f0e38-da66-42bf-a449-f40a48963754", "node_type": "4", "metadata": {"page_label": "24", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "65ff3aeb497f9d259771db09fc7909e34a720f140f16404a1b0a11b6c72c3605", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "24 Differentiation\nhave linear directional derivatives along any direction. These are\nstill not Fr\u00e9chet differentiable functions. Indeed, the limit in Defini-\ntion 2.6 is over any vectors tending to0 (potentially in a pathological\nway), while directional derivatives look at such limits uniquely in\nterms of a single direction.\nIntheremainderofthischapter,alldefinitionsofdifferentiability\nare in terms of Fr\u00e9chet differentiability.\nThe next example illustrates how to compute the gradient of the\nlogistic loss and validates its differentiability.\nExample 2.3(Gradient of logistic loss). Consider the logistic loss\n\u2113(\u03b8,y) := \u2212\u27e8y,\u03b8\u27e9+log \u2211M\ni=1 e\u03b8i, that measures the prediction error\nof the logits\u03b8\u2208RM w.r.t. the correct labely\u2208{e1,..., eM}. Let\nus compute the gradient of this loss w.r.t.\u03b8 for fixedy, i.e., we\nwant to compute the gradient off(\u03b8) := \u2113(\u03b8,y). Let us decompose\nf as f = l+ logsumexp with l(\u03b8) := \u27e8\u2212y,\u03b8\u27e9and\nlogsumexp(\u03b8) := log\nM\u2211\ni=1\nexp(\u03b8i),\nthe log-sum-exp function. The functionl is linear so differentiable\nwith gradient \u2207l(\u03b8) = \u2212y. We therefore focus on logsumexp.\nDenoting exp(\u03b8) = ( exp(\u03b81),..., exp(\u03b8M)), using thatexp(x) =\n1 +x+ o(x), log(1 +x) = x+ o(x), and denoting\u2299the elementwise\nproduct, we get\nlogsumexp(\u03b8+ v) = log (\u27e8exp(\u03b8+ v),1\u27e9)\n= log (\u27e8exp(\u03b8) \u2299exp(v),1\u27e9)\n= log (\u27e8exp(\u03b8) \u2299(1 +v+ o(\u2225v\u22252)),1\u27e9)\n= log (\u27e8exp(\u03b8),1\u27e9+ \u27e8exp(\u03b8),v\u27e9+ o(\u2225v\u22252))\n= log (\u27e8exp(\u03b8),1\u27e9) +\n\u27e8 exp(\u03b8)\n\u27e8exp(\u03b8),1\u27e9,v\n\u27e9\n+ o(\u2225v\u22252).\nThe above decomposition oflogsumexp(\u03b8+ v) shows that it is", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1478, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "09ef6012-468e-4d9f-bf3b-62dcf9d41179": {"__data__": {"id_": "09ef6012-468e-4d9f-bf3b-62dcf9d41179", "embedding": null, "metadata": {"page_label": "25", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a61ec00b-17b6-4540-8a98-e5fe77d57d50", "node_type": "4", "metadata": {"page_label": "25", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "f04d0ad25fbb3284c36f72a5cda4b11aab210f675c3c9cccffa5dfa66176468b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.2. Multivariate functions 25\ndifferentiable, and that\u2207logsumexp(\u03b8) = softargmax(\u03b8), where\nsoftargmax(\u03b8) :=\n\uf8eb\n\uf8ede\u03b81 /\n\uf8eb\n\uf8ed\nM\u2211\nj=1\ne\u03b8j\n\uf8f6\n\uf8f8,...,e \u03b8M/\n\uf8eb\n\uf8ed\nM\u2211\nj=1\ne\u03b8j\n\uf8f6\n\uf8f8\n\uf8f6\n\uf8f8.\nOverall, we get that\u2207f(\u03b8) = \u2212y+ softargmax(\u03b8).\nLinearity of gradients\nThe notion of differentiability for multi-input functions naturally inherits\nfrom the linearity of derivatives for single-input functions. For any\nu1,...,u M \u2208R and any multi-input functionsf1,...,f M differentiable\nat w, the functionu1f1 + ... + uMfM is differentiable atw and its\ngradient is\n\u2207(u1f1 + ... + uMfM)(w) = u1\u2207f1(w) + ... + uM\u2207fM(w).\nWhy is the gradient useful?\nWhen f is differentiable, we say thatv is anascent directionof f\nfrom wif\n\u27e8v,\u2207f(w)\u27e9>0.\nConversely, we say thatvis adescent directionof f from wif\n\u27e8v,\u2207f(w)\u27e9<0.\nUsing this definition, the gradient leads to thesteepest ascent direc-\ntion of f from w. To see why, we note that\narg max\nv\u2208RP,\u2225v\u22252\u22641\n\u27e8v,\u2207f(w)\u27e9= arg max\nv\u2208RP,\u2225v\u22252\u22641\n\u2202f(w)[v]\n= \u2207f(w)/\u2225\u2207f(w)\u22252,\nwhere we assumed\u2207f(w) \u0338= 0. The gradient\u2207f(w) is orthogonal to\nthe level set of the function (the set of pointsw sharing the same\nvalue f(w)) and points towards higher values off, as illustrated in\nFig. 2.3. Conversely, the negative gradient\u2212\u2207f(w) points towards lower\nvalues off. This observation motivates the development of optimization\nalgorithms such as gradient descent. It is based on iteratively performing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1381, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cc71cc61-1640-44c3-8acb-0dbba54498f8": {"__data__": {"id_": "cc71cc61-1640-44c3-8acb-0dbba54498f8", "embedding": null, "metadata": {"page_label": "26", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "557d23b3-4b99-42a8-b7c5-69683ef71182", "node_type": "4", "metadata": {"page_label": "26", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "2e8346d493b2a4740402a5704f4a90e83a4c86ac7c5708d9bb8bb8af987cee31", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "26 Differentiation\nFigure 2.3:The gradient of a function\nf : R2 \u2192R at (w1,w2) is the normal\nvector to the tangent space of the level\nset Lf(w1,w2) = {(w\u2032\n1,w\u2032\n2) : f(w\u2032\n1,w\u2032\n2) =\nf(w1,w2)} and points towards points\nwith higher function values.\nFigure 2.4:The directional derivative\nof a parametric curvef : R \u2192R2 at w\nis the tangent to the curve at the point\nf(w) \u2208R2.\nthe update wt+1 := wt \u2212\u03b3\u2207f(wt), for \u03b3 >0. It therefore seeks for\na minimizer off by moving along thesteepest descent direction\naround wt given, up to a multiplicative factor, by\u2212\u2207f(wt). See also\nDefinition 17.1 for more details.\n2.2.3 Jacobians\nLet us now consider a multi-output functionf : RP \u2192RM defined by\nf(w) := (f1(w),...,f M(w)), where fj: RP \u2192R. A typical example\nin machine learning is a neural network. The notion of directional\nderivative can be extended to such function by defining it as the vector\ncomposed of the coordinate-wise directional derivatives:\n\u2202f(w)[v] := lim\n\u03b4\u21920\nf(w+ \u03b4v) \u2212f(w)\n\u03b4 = lim\n\u03b4\u21920\n\uf8eb\n\uf8ec\uf8ec\uf8ed\nf1(w+\u03b4v)\u2212f1(w)\n\u03b4\n...\nfM(w+\u03b4v)\u2212fM(w)\n\u03b4\n\uf8f6\n\uf8f7\uf8f7\uf8f8\u2208RM,\nwhere the limits (provided that they exist) are applied coordinate-wise.\nThe directional derivative off in the directionv\u2208RP is therefore the\nvector that gathers the directional derivative of eachfj, i.e.,\u2202f(w)[v] =\n(\u2202fj(w)[v])M\nj=1. In particular, we can define thepartial derivativesof", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1327, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e8db72d1-59b9-4e22-970f-4d6b02a1731d": {"__data__": {"id_": "e8db72d1-59b9-4e22-970f-4d6b02a1731d", "embedding": null, "metadata": {"page_label": "27", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "536a7f32-9a7d-409a-b6d0-b98867339186", "node_type": "4", "metadata": {"page_label": "27", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "5145ccad5fad4875d3c57c74eafdaca3fd3c3f25144780441384b1c51d12361d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.2. Multivariate functions 27\nf at was the vectors\n\u2202if(w) := \u2202f(w)[ei] =\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\u2202if1(w)\n...\n\u2202ifM(w)\n\uf8f6\n\uf8f7\uf8f7\uf8f8\u2208RM.\nAs for the usual definition of the derivative, the directional derivative\ncan provide a linear approximation of a function around a current input,\nas illustrated in Fig. 2.4 for a parametric curvef : R \u2192R2.\nJust as in the single-output case, differentiability is defined not only\nas the existence of directional derivatives in any direction but also by\nthe linearity in the chosen direction.\nDefinition 2.8(Differentiability, multi-output case). A functionf :\nRP \u2192RM is (Fr\u00e9chet)differentiable at a pointw \u2208RP if its\ndirectional derivative is defined along any direction, is linear in any\ndirection, and,\nlim\n\u2225v\u22252\u21920\n\u2225f(w+ v) \u2212f(w) \u2212\u2202f(w)[v]\u22252\n\u2225v\u22252\n= 0.\nThe partial derivatives of each coordinate\u2019s function are gathered in\nthe Jacobian matrix.\nDefinition 2.9(Jacobian). The Jacobian of a differentiable func-\ntion f : RP \u2192RM at wis defined as the matrix gathering partial\nderivatives of each coordinate\u2019s function provided they exist,\n\u2202f(w) :=\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\u22021f1(w) ... \u2202 Pf1(w)\n... ... ...\n\u22021fM(w) ... \u2202 PfM(w)\n\uf8f6\n\uf8f7\uf8f7\uf8f8\u2208RM\u00d7P.\nThe Jacobian can be represented by stacking columns of partial\nderivatives or rows of gradients,\n\u2202f(w) =\n(\n\u22021f(w),...,\u2202 Pf(w)\n)\n=\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\u2207f1(w)\u22a4\n...\n\u2207fM(w)\u22a4\n\uf8f6\n\uf8f7\uf8f7\uf8f8\u2208RM\u00d7P.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1293, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "70b45da8-9608-4d17-9923-d63e58fa0fd2": {"__data__": {"id_": "70b45da8-9608-4d17-9923-d63e58fa0fd2", "embedding": null, "metadata": {"page_label": "28", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1093b147-40d4-4059-933c-184a597b0c74", "node_type": "4", "metadata": {"page_label": "28", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "e81c7c50764de292d4a153a4d7245552c3407f7e0dd2eb65ce95386f12703b35", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "28 Differentiation\nBy linearity, the directional derivative off at walong any input\ndirection v= \u2211P\ni=1 viei \u2208RP is then given by\n\u2202f(w)[v] =\nP\u2211\ni=1\nvi\u2202if(w) = \u2202f(w)v\u2208RM.\nNotice that we use bold\u2202to indicate the Jacobian, seen as a matrix.\nThe Jacobian matrix naturally generalizes the concepts of derivatives\nand gradients presented earlier. As for the single input case, to show that\na function is differentiable, one approach is to approximatef(w+ v)\naround v= 0. If we find a linear mapl such that\nf(w+ v) = f(w) + l[v] + o(\u2225v\u22252),\nthen f is differentiable atw. Moreover, ifl is represented by matrixJ\nsuch thatl[v] = Jv then J= \u2202f(w).\nAs a simple example, any linear functionf(w) = Awfor A\u2208RM\u00d7P\nis differentiable, since all its coordinate-wise components are single-\noutput linear functions, and the Jacobian off at anyw is given by\n\u2202f(w) = A.\nRemark 2.4(Special cases of the Jacobian). Forsingle-outputfunc-\ntions f : RP \u2192R, i.e.,M = 1, the Jacobian matrix reduces to a\nrow vector identified as thetranspose of the gradient,\n\u2202f(w) = \u2207f(w)\u22a4\u2208R1\u00d7P.\nFor a single-input functionf : R \u2192RM, the Jacobian reduces to a\nsingle column vector of directional derivatives, denoted\n\u2202f(w) = f\u2032(w) :=\n\uf8eb\n\uf8ec\uf8ec\uf8ed\nf\u2032\n1(w)\n...\nf\u2032\nM(w)\n\uf8f6\n\uf8f7\uf8f7\uf8f8\u2208RM\u00d71.\nFor a single-input single-output functionf : R \u2192R, the Jacobian\nreduces to the derivative off, i.e.,\n\u2202f(w) = f\u2032(w) \u2208R.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1343, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aa081668-3de0-431f-b7f3-4e56fd1eb9a9": {"__data__": {"id_": "aa081668-3de0-431f-b7f3-4e56fd1eb9a9", "embedding": null, "metadata": {"page_label": "29", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b7ebd51d-26d0-489c-a0e9-4fb094d04361", "node_type": "4", "metadata": {"page_label": "29", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "2d03f57d32d176713e40cfbd3b6cb05eb316f28e52d501384f824bbabf1cce64", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.2. Multivariate functions 29\nThe next example illustrates the form of the Jacobian matrix for\nthe element-wise application of a differentiable function\u03c3, such as the\nsoftplus activation. In this case, the Jacobian takes a simple diagonal\nmatrix form. As a consequence, the directional derivative associated\nwith this function is simply given by an element-wise product: a full\nmatrix-vector product is not needed, as would suggest Definition 2.9.\nWe will revisit this point in Section 2.3.\nExample 2.4(Jacobian matrix of the softplus activation). Consider\nthe element-wise application of the softplus defined forw\u2208RP by\nf(w) :=\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\u03c3(w1)\n...\n\u03c3(wP)\n\uf8f6\n\uf8f7\uf8f7\uf8f8\u2208RP where \u03c3(w) := log(1 + ew).\nSince \u03c3 is differentiable, each coordinate of this function is differen-\ntiable and the overall function is differentiable. Thejth coordinate of\nf is independent of theith coordinate ofwfor i\u0338= j, so\u2202ifj(w) = 0\nfor i \u0338= j. Fori = j, the result boils down to the derivative of\u03c3\nat wj. That is,\u2202jfj(w) = \u03c3\u2032(wj), where\u03c3\u2032(w) = ew/(1 + ew). The\nJacobian off is therefore a diagonal matrix\n\u2202f(w) = diag(\u03c3\u2032(w1),...,\u03c3 \u2032(wP)) :=\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u03c3\u2032(w1) 0 ... 0\n0 ... ... ...\n... ... ... 0\n0 ... 0 \u03c3\u2032(wP)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n.\nChain rule\nEquipped with a generic definition of differentiability and the associated\nobjects, gradients and Jacobians, we can now generalize the chain rule,\npreviously introduced for single-input single-output functions.\nProposition 2.2(Chain rule). Consider f : RP \u2192 RM and g :\nRM \u2192RR. If f is differentiable at w \u2208RP and g is differen-\ntiable atf(w) \u2208RM, then the compositiong\u25e6f is differentiable", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1585, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "16f4e90f-fb76-4e6b-9ce1-683e850be6d3": {"__data__": {"id_": "16f4e90f-fb76-4e6b-9ce1-683e850be6d3", "embedding": null, "metadata": {"page_label": "30", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6a28a010-75c5-4014-a837-af578a5eea41", "node_type": "4", "metadata": {"page_label": "30", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "22394b12ec9da1ada79cc36c3682da2afa32b81d151fb70784c712826b03b893", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "30 Differentiation\nat w\u2208RP and its Jacobian is given by\n\u2202(g\u25e6f)(w) = \u2202g(f(w))\u2202f(w).\nProof. We progressively approximateg\u25e6f(w+ v) using the differentia-\nbility off at wand g at f(w),\ng(f(w+ v)) = g(f(w) + \u2202f(w)v+ o(\u2225v\u2225))\n= g(f(w)) + \u2202g(f(w))\u2202f(w)v+ o(\u2225v\u2225).\nHence, g\u25e6f is differentiable atwwith Jacobian\u2202g(f(w))\u2202f(w).\nProposition 2.2 can be seen as the cornerstone of any derivative\ncomputations. For example, it can be used to rederive the linearity and\nproduct rules associated to the derivatives of single-input single-outptut\nfunctions.\nWhen g is scalar-valued, combined with Remark 2.4, we obtain a\nsimple expression for\u2207(g\u25e6f).\nProposition 2.3(Chain rule, scalar-valued case). Considerf : RP \u2192\nRM and g: RM \u2192R. The gradient of the composition is given by\n\u2207(g\u25e6f)(w) = \u2202f(w)\u22a4\u2207g(f(w)).\nThis is a very useful identity in machine learning, as we often need\nto compose a vector-valued model function and a scalar-valued loss\nfunction. We illustrate this with linear regression below.\nExample 2.5(Linear regression). Consider a linear regression ofN\ninputs x1,..., xN \u2208 RD onto N targets y1,...,y N \u2208 R, using\na parameter vectorw \u2208RD. The loss is defined as the sum of\nsquared residuals, L(w) := \u2225Xw \u2212y\u22252\n2 = \u2211N\ni=1(\u27e8xi,w\u27e9\u2212 yi)2\nwhere X:= (x1,..., xN)\u22a4\u2208RN\u00d7D and y:= (y1,...,y N)\u22a4\u2208RN.\nThe function L can be decomposed into a linear mapping\nf(w) := Xwand a squared error\u2113(p) := \u2225p\u2212y\u22252\n2, so thatL= \u2113\u25e6f.\nWe can then apply the chain rule in Proposition 2.3 to get\n\u2207L(w) = \u2202f(w)\u22a4\u2207\u2113(f(w))\nprovided thatf and \u2113are differentiable atwand f(w), respectively.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1543, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "957f5aeb-9579-43d4-bad2-4adc476e3a49": {"__data__": {"id_": "957f5aeb-9579-43d4-bad2-4adc476e3a49", "embedding": null, "metadata": {"page_label": "31", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8de3e4ce-4634-4794-b0fb-f57dffb74aa2", "node_type": "4", "metadata": {"page_label": "31", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "201414339513ead40b0107d5406b359aff3782e6f97c562089d05316e6419677", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.3. Linear maps 31\nThe functionf is linear so differentiable with Jacobian\u2202f(w) =\nX. On the other hand the partial derivatives of\u2113 are given by\n\u2202j\u2113(p) = 2(pj\u2212yj) for j \u2208{1,...,N }. Therefore,\u2113is differentiable\nat anypand its gradient is\u2207\u2113(p) = 2(p\u2212y). By combining the\ntwo, we then get the gradient ofL as\n\u2207L(w) = 2X\u22a4(f(w) \u2212y) = 2X\u22a4(Xw\u2212y).\n2.3 Linear maps\nThe Jacobian matrix is useful as a representation of the partial deriva-\ntives. However, the core idea underlying the definition of differentiable\nfunctions, as well as their implementation in an autodiff framework,\nlies in the access to two keylinear maps. These two maps encode in-\nfinitesimal variations alonginput or output directions and are referred\nto, respectively, asJacobian-vector product (JVP) and Vector-\njacobian product(VJP). This section formalizes these notions, in the\ncontext of Euclidean spaces.\n2.3.1 The need for linear maps\nSo far, we have focused on functionsf: RP \u2192RM, that take a vector as\ninputandproduceavectorasoutput.However,functionsthatusematrix\nor even tensor inputs/outputs are common place in neural networks. For\nexample, consider the function of matrices of the formf(W) := Wx,\nwhere x\u2208RD and W \u2208RM\u00d7D. This function takes a matrix as input,\nnot a vector. Of course, a matrixW \u2208RM\u00d7D can always be \u201cflattened\u201d\ninto a vectorw\u2208RMD, by stacking the columns ofW. We denote this\noperation byw= vec(W) and its inverse byW = vec\u22121(w). We can\nthen equivalently writef(W) as \u02dcf(w) = f(vec\u22121(w)) = vec\u22121(w)x,\nso that the previous framework applies. However, we will now see that\nthis would be inefficient.\nIndeed, the resulting Jacobian of\u02dcf at anywconsists in a matrix\nof sizeRM\u00d7MD, which, after some computations, can be observed to\nbe mostly filled with zeros. Getting the directional derivative off at\nW \u2208RM\u00d7D in a directionV \u2208RM\u00d7D would consist in (i) vectorizing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1850, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "30557e0e-3173-465a-a8fe-7d27a34a4401": {"__data__": {"id_": "30557e0e-3173-465a-a8fe-7d27a34a4401", "embedding": null, "metadata": {"page_label": "32", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6f3d497e-d30f-4a9c-96d9-de7c81642a38", "node_type": "4", "metadata": {"page_label": "32", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "37533961a624146a91736a7bbd18642b10d8bc3fa8d029253c0ef7ead340db6a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "32 Differentiation\nV into v= vec(V), (ii) computing the matrix-vector product\u2202\u02dcf(w)v\nat a cost ofM3D2 computations (ignoring the fact that the Jacobian\nhas many zero entries), (iii) re-shaping the result into a matrix.\nOn the other hand, sincef is linear in its matrix input, we can\ninfer that the directional derivative off at any W \u2208RM\u00d7D in any\ndirection V \u2208RM\u00d7D is simply given by the function itself applied on\nV. Namely, we have\u2202f(W)[V] = f(V) = Vx, which is simple to\nimplement and clearly only requiresMD operations. Note that the\ncost would have been the same, had we ignored the non-zero entries of\n\u2202\u02dcf(w). The point here is that by considering the operations associated\nto the differentiation of a function as linear maps rather than using\nthe associated representation as a Jacobian matrix, we can efficiently\nexploit the underlying input or output space structure. To that end,\nwe now recall the main abstractions necessary to extend the previous\ndefinitions in the context of Euclidean spaces.\n2.3.2 Euclidean spaces\nLinear spaces, a.k.a.vector spaces, are spaces equipped with and\nclosed under an addition rule compatible with multiplication by a scalar\n(we limit ourselves to the field of reals). Namely, in a linear spaceE,\nthere exist operations+ and \u00b7, such that for anyu, v\u2208E, anda\u2208R,\nwe haveu+ v\u2208E and a\u00b7u\u2208E.\nEuclideanspaces arelinearspacesequippedwithabasis e1,..., eP \u2208\nE. Any elementv\u2208E can be decomposed asv= \u2211P\ni=1 viei for some\nunique scalarsv1,...,v P \u2208R. A canonical example of Euclidean space\nis the set RP of all vectors of sizeP that we already covered. The\nset of matricesRP1\u00d7P2 of size P1 \u00d7P2 is also naturally a Euclidean\nspace generated by the set of canonical matricesEij \u2208{0,1}P1\u00d7P2 for\ni\u2208[P1],j \u2208[P2] filled with zero except at the(i,j)th entry filled with\none. For example,W \u2208RP1\u00d7P2 can be writtenW = \u2211P1,P2\ni,j=1 WijEij.\nEuclidean spaces are naturally equipped with a notion of inner product.\nDefinition 2.10(Inner product). An inner product on a linear\nspace Eis a function\u27e8\u00b7,\u00b7\u27e9: E\u00d7E\u2192 R that is", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2035, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0cd5b501-ed12-4851-94b9-b0ec33ec350e": {"__data__": {"id_": "0cd5b501-ed12-4851-94b9-b0ec33ec350e", "embedding": null, "metadata": {"page_label": "33", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b4661db2-b657-4f5d-a83f-fbb2b37f1a23", "node_type": "4", "metadata": {"page_label": "33", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "02d31467269af868046188d0e6892fbc24ab3813f6261b6aa691dd61253e3ebd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.3. Linear maps 33\n\u2022 bilinear: x\u21a6\u2192\u27e8x,w\u27e9and y\u21a6\u2192\u27e8v,y\u27e9are linear for anyw,v\u2208\nE,\n\u2022 symmetric: \u27e8w,v\u27e9= \u27e8v,w\u27e9for anyw,v\u2208E,\n\u2022 positive definite:\u27e8w,w\u27e9\u2265 0 for anyw\u2208E, and\u27e8w,w\u27e9= 0\nif and only ifw= 0.\nAn inner product defines a norm\u2225w\u2225:=\n\u221a\n\u27e8w,w\u27e9.\nThe norm induced by an inner product defines a distance\u2225w\u2212v\u2225\nbetween w,v\u2208E, and therefore a notion of convergence.\nFor vectors, whereE = RP, the inner product is the usual one\n\u27e8w,v\u27e9= \u2211P\ni=1 wivi.For matrices, whereE= RP1\u00d7P2 , the inner product\nis the so-called Frobenius inner product. It is defined for anyW,V \u2208\nRP1\u00d7P2 by\n\u27e8W,V\u27e9:= \u27e8vec(W),vec(V)\u27e9=\nP1,P2\u2211\ni,j=1\nWijVij = tr(W\u22a4V),\nwhere tr(Z) := \u2211P\ni=1 Zii is the trace operator defined for square matrices\nZ \u2208RP\u00d7P. For tensors of orderR, which generalize matrices toE=\nRP1\u00d7...\u00d7PR, the inner product is defined similarly forW,V \u2208RP1\u00d7...\u00d7PR\nby\n\u27e8W,V\u27e9:= \u27e8vec(W),vec(V)\u27e9=\nP1,...,PR\u2211\ni1,...,iR=1\nWi1...iRVi1...iR,\nwhere Wi1...iR is the(i1,...,i R)th entry ofW.\n2.3.3 Linear maps and their adjoints\nThe notion of linear map in Definition 2.5 naturally extends to Euclidean\nspaces. Namely, a functionl: E\u2192F from a Euclidean spaceEonto a\nEuclidean spaceFis alinear mapif for anyw,v\u2208E and a,b \u2208R, we\nhave l[aw+ bv] = a\u00b7l[w] + b\u00b7l[v]. WhenE= RP and F= RM, there\nalways exists a matrixA\u2208RM\u00d7P such thatl[v] = Av. Therefore, we\ncan think ofAas the \u201cmaterialization\u201d as a matrix ofl.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1353, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ee9a83e0-a25a-4ec5-9dc9-b3e4761ea42d": {"__data__": {"id_": "ee9a83e0-a25a-4ec5-9dc9-b3e4761ea42d", "embedding": null, "metadata": {"page_label": "34", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a22bb487-4bb7-4be3-ba45-7b85994080a3", "node_type": "4", "metadata": {"page_label": "34", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "e9a0589277f94a4edfd90b0d8987b1d7467639abc5a7a7b0131fd2a4f84bcf20", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "34 Differentiation\nExample 2.6(Linear map). Considerthelinearmap l[v] := (ab\u22a4)v,\nwhere a\u2208RM, b\u2208RP and v\u2208RP. This is a function fromRP\nto RM. We can always materialize the linear map as a matrix\nA := ab\u22a4 \u2208RM\u00d7P and write l[v] = Av. However, applying a\nlinear map on a vectorvoften does not require materializing the\ncorresponding matrix. Following the previous example, we can sim-\nply writel[v] = (b\u22a4v)a, which only requires an inner product and\nan element-wise multiplication. This is more efficient than mate-\nrializing Athen computingAv, which requires an outer product\nand a matrix-vector multiplication.\nWe can define the adjoint operator of a linear map.\nDefinition 2.11(Adjoint operator). Given two Euclidean spacesE\nand Fequipped with inner products\u27e8\u00b7,\u00b7\u27e9E and \u27e8\u00b7,\u00b7\u27e9F, theadjoint\nof a linear mapl: E\u2192F is the unique linear mapl\u2217: F\u2192E such\nthat for anyv\u2208E and u\u2208F,\n\u27e8l[v],u\u27e9F = \u27e8v,l\u2217[u]\u27e9E.\nThe adjoint can be thought as the counterpart of the matrix trans-\npose for linear maps. Whenl[v] = Av, we havel\u2217[u] = A\u22a4usince\n\u27e8l[v],u\u27e9F = \u27e8Av,u\u27e9F = \u27e8v,A\u22a4u\u27e9E= \u27e8v,l\u2217[u]\u27e9E.\nExample 2.7(Adjoint linear map). Using the linear mapl[v] from\nthe previous example, we have for allu\u2208RM and v\u2208RP,\n\u27e8ab\u22a4v,u\u27e9= \u27e8v,ba\u22a4u\u27e9.\nTherefore, the adjoint linear map isl\u2217[u] = ( ba\u22a4)u. This is a\nfunction from RM to RP. It can be materialized as the matrix\nA\u22a4 = ba\u22a4 \u2208RP\u00d7M. Applying l\u2217[u] can be done efficiently as\nl\u2217[u] = (a\u22a4u)b.\n2.3.4 Jacobian-vector products\nWe now define the directional derivative using linear maps, leading\nto the notion of Jacobian-vector product (JVP). This can be used to", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1565, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ebc15953-cd6b-484e-a055-d6a398350d27": {"__data__": {"id_": "ebc15953-cd6b-484e-a055-d6a398350d27", "embedding": null, "metadata": {"page_label": "35", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8bc7d0bb-11ca-4e39-8897-1aa693a5d9ec", "node_type": "4", "metadata": {"page_label": "35", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "bf327d993b90485e423f4de61df654257dee4cc3a1c6fee8bb6ea14a9e1bb508", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.3. Linear maps 35\nfacilitate the treatment of functions on tensors or for further extensions\nto infinite-dimensional spaces. In the following,Eand Fdenote two\nEuclidean spaces equipped with inner products\u27e8\u00b7,\u00b7\u27e9E and \u27e8\u00b7,\u00b7\u27e9F. We\nstart by defining differentiability in general Euclidean spaces.\nDefinition 2.12(Differentiability in Euclidean spaces). Afunction f :\nE \u2192Fis differentiable at a point w \u2208E if the directional\nderivativealong v\u2208E\n\u2202f(w)[v] := lim\n\u03b4\u21920\nf(w+ \u03b4v) \u2212f(w)\n\u03b4\nis well-defined for anyv\u2208E, linear invand if\nlim\n\u2225v\u2225F\u21920\n\u2225f(w+ v) \u2212f(w) \u2212\u2202f(w)[v]\u2225F\n\u2225v\u2225F\n= 0.\nWe can now formally define the Jacobian-vector product.\nDefinition 2.13(Jacobian-vector product). Foradifferentiablefunc-\ntion f : E\u2192F , thelinear map\u2202f(w) : E\u2192F , mappingv to\n\u2202f(w)[v], is called theJacobian-vector product(JVP). From\nthis perspective, the function\u2202f is a function fromEto a linear\nmap fromEto F. That is, we have\n\u2202f: E\u2192 (E\u2192F ).\nWe emphasize again that the directional derivative\u2202f(w)[v] \u2208R is\na value, while the JVPv\u21a6\u2192f(w)[v] is a function. Strictly speaking,v\ncan belong to any Euclidean spaceEand does not need to be limited\nto a vector, as the JVP acronym would suggest. We adopt the name\nJVP, as it is now standard.\nRecovering the gradient\nPreviously, we saw that for differentiable functions with vector input\nand scalar output, the directional derivative is equal to the inner prod-\nuct between the direction and the gradient. The same applies when\nconsidering differentiable functions from a Euclidean space with single", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1513, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "624f3045-0292-490b-93d6-87c7104acfd5": {"__data__": {"id_": "624f3045-0292-490b-93d6-87c7104acfd5", "embedding": null, "metadata": {"page_label": "36", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1751a2c0-07c9-4f6a-8525-243780e2ceeb", "node_type": "4", "metadata": {"page_label": "36", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "79e41ce2de99e682685b306b56a573d029a087235a5ab44a47f307138d0a1a99", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "36 Differentiation\noutputs, except that the gradient is now an element of the input space\nand the inner product is the one associated with the input space.\nProposition 2.4(Gradient). If a function f : E \u2192R is differen-\ntiable atw\u2208E, then there exists\u2207f(w) \u2208E, called thegradient\nof f at wsuch that the directional derivative off at walong any\ninput directionv\u2208E is given by\n\u2202f(w)[v] = \u27e8\u2207f(w),v\u27e9E.\nIn Euclidean spaces, the existence of the gradient can simply be\nshown by decomposing the partial derivative along a basis ofE. Such a\ndefinition generalizes to infinite-dimensional (e.g., Hilbert spaces) spaces\nas discussed in Section 2.3.9.\n2.3.5 Vector-Jacobian products\nConsider a function f: RP \u2192RM. Instead of variations off along\nan input direction v\u2208RP, we may also consider the variations off\nalong anoutput direction u\u2208RM, namely, computing the gradient\n\u2207\u27e8u,f\u27e9(w) of the scalar-valued function\n\u27e8u,f\u27e9(w) := \u27e8u,f(w)\u27e9\u2208 R.\nEquivalently, we may compute the gradients\u2207fj(w) of each coordinate\nfunction fj := \u27e8ej,f\u27e9at w, where ej is the jth canonical vector in\nRM. The infinitesimal variations off at walong any output direction\nu= \u2211M\nj=1 ujej \u2208RM are given by\n\u2207\u27e8u,f\u27e9(w) =\nM\u2211\nj=1\nuj\u2207fj(w) = \u2202f(w)\u22a4u\u2208RP,\nwhere \u2202f(w)\u22a4\u2208RP\u00d7M is the Jacobian\u2019s transpose. Using the definition\nof derivative as a limit, we may also write fori\u2208[P]\n\u2207i\u27e8u,f\u27e9(w) = [\u2202f(w)\u22a4u]i = lim\n\u03b4\u21920\n\u27e8u,f(w+ \u03b4ei) \u2212f(w)\u27e9\n\u03b4 ,\nwhere ei is theith canonical vector inRP.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1423, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bd860081-b3ed-4936-a7a6-4a888869bd99": {"__data__": {"id_": "bd860081-b3ed-4936-a7a6-4a888869bd99", "embedding": null, "metadata": {"page_label": "37", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "13d27492-0bd7-4cdd-8fba-c815d36bf260", "node_type": "4", "metadata": {"page_label": "37", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "dce9fe1135d34f2cbc371fda1f86e9b5466ca279ed77c8e1615e1ee3eea92da3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.3. Linear maps 37\nFor generic Euclidean spacesEand F, the counterpart of the trans-\npose is the adjoint operator, leading to the notion of vector-Jacobian\nproduct.\nProposition 2.5(Vector-Jacobian product). If a functionf : E\u2192\nFis differentiable atw\u2208E, then its infinitesimal variation along an\noutputdirectionu\u2208F isgivenbythe adjointmap \u2202f(w)\u2217: F\u2192\nE of the JVP, called thevector-Jacobian product (VJP). It\nsatisfies\n\u2207\u27e8u,f\u27e9F(w) = \u2202f(w)\u2217[u],\nwhere we denoted\u27e8u,f\u27e9F(w) := \u27e8u,f(w)\u27e9F. The function\u2202f(\u00b7)\u2217\nis a function fromEto a linear map fromFto E. That is, we have\n\u2202f(\u00b7)\u2217: E\u2192 (F\u2192E ).\nProof. The chain rule presented in Proposition 2.2 naturally generalizes\nto Euclidean spaces (see Proposition 2.6). Since\u27e8u,\u00b7\u27e9F is linear, its\ndirectional derivative is itself. Therefore, the directional derivative of\n\u27e8u,f\u27e9F is\n\u2202(\u27e8u,f\u27e9F)(w)[v] = \u27e8u,\u2202f (w)[v]\u27e9F\n= \u27e8\u2202f(w)\u2217[u],v\u27e9E.\nAs this is true for anyv\u2208E, \u2202f(w)\u2217[u] is the gradient of\u27e8u,f\u27e9F per\nProposition 2.4.\nWe illustrate the JVP and VJP linear maps in Fig. 2.5.\n2.3.6 Chain rule using linear maps\nThe chain rule presented before in terms of Jacobian matrices can\nreadily be formulated to take advantage of the implementations of the\nJVP and VJP as linear maps.\nProposition 2.6(Chain rule, general case). Consider f : E \u2192 F\nand g : F\u2192G , where E, Fand Gare Euclidean spaces. Iff is\ndifferentiable atw\u2208E and g is differentiable atf(w) \u2208F, then", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1380, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2db0f05b-01c6-40f6-b7fd-926c6609a282": {"__data__": {"id_": "2db0f05b-01c6-40f6-b7fd-926c6609a282", "embedding": null, "metadata": {"page_label": "38", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "603b96d6-97da-459e-ba51-3ccdb83a3663", "node_type": "4", "metadata": {"page_label": "38", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "575622994bf911f74aa6cc8e80abb238a033cf2b2b3cb018f9b44ed7d8f83262", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "38 Differentiation\nFigure 2.5: Jacobian-vector product (JVP)v \u21a6\u2192\u2202f(w)[v] and vector-Jacobian\nproduct (VJP)u\u21a6\u2192\u2202f(w)\u2217[u], seen as linear maps.\nthe compositiong\u25e6f is differentiable atw\u2208E. Its JVP is given\nfor allv\u2208E by\n\u2202(g\u25e6f)(w)[v] = \u2202g(f(w))[\u2202f(w)[v]]\nand its VJP is given for allu\u2208G by\n\u2202(g\u25e6f)(w)\u2217[u] = \u2202f(w)\u2217[\u2202g(f(w))\u2217[u]].\nThe proof follows the one of Proposition 2.2. This is illustrated in\nFig. 2.6. When the last function is scalar-valued, which is often the case\nin machine learning, we obtain the following simplified result.\nProposition 2.7(Chain rule, scalar case). Consider f : E\u2192F and\ng: F\u2192 R, the gradient of the composition is given by\n\u2207(g\u25e6f)(w) = \u2202f(w)\u2217[\u2207g(f(w))].\n2.3.7 Functions of multiple inputs (fan-in)\nOftentimes, the inputs of a function do not belong to only one Euclidean\nspace but to a product of them. An example isf(x,W) := Wx, which\nis defined on E := RD \u00d7RM\u00d7D. In such a case, it is convenient to\ngeneralize the notion of partial derivatives to handle blocks of inputs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 996, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e7cc1021-af56-488b-881a-431b808a657d": {"__data__": {"id_": "e7cc1021-af56-488b-881a-431b808a657d", "embedding": null, "metadata": {"page_label": "39", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d590e653-6e6d-4ed0-80e8-d9aef585bec7", "node_type": "4", "metadata": {"page_label": "39", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "d0ce50185644fbebc8c725ae1ba5ed844305c502453d9a80c551f4de451c5d98", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.3. Linear maps 39\nFigure 2.6:Chain rule using JVP and VJP linear maps.\nConsider a functionf(w1,..., wS) defined onE:= E1 \u00d7... \u00d7ES,\nwhere wi \u2208Ei. We denote the partial derivative with respect to the\nith input wi along vi \u2208Ei as \u2202if(w1,..., wS)[vi]. Equipped with this\nnotation, we can analyze how JVPs or VJPs are decomposed along\nseveral inputs.\nProposition 2.8(Multiple inputs). Consider a differentiable func-\ntion of the formf(w) = f(w1,..., wS) with signaturef: E\u2192F ,\nwhere w := (w1,..., wS) \u2208E and E:= E1 \u00d7\u00b7\u00b7\u00b7\u00d7E S. Then the\nJVP with the input directionv= (v1,..., vS) \u2208E is given by\n\u2202f(w)[v] = \u2202f(w1,..., wS)[v1,..., vS] \u2208F\n=\nS\u2211\ni=1\n\u2202if(w1,..., wS)[vi].\nThe VJP with the output directionu\u2208F is given by\n\u2202f(w)\u2217[u] = \u2202f(w1,..., wS)\u2217[u] \u2208E\n= (\u22021f(w1,..., wS)\u2217[u],...,\u2202 Sf(w1,..., wS)\u2217[u]).\nExample 2.8(Matrix-vector product). Consider f(x,W) := Wx,\nwhere W \u2208 RM\u00d7D and x \u2208 RD. This corresponds to setting\nE:= E1 \u00d7E2 := RD \u00d7RM\u00d7D and F:= RM. For the JVP, letting", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 963, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2adc12c8-f956-4ca8-8ecb-6328660d645c": {"__data__": {"id_": "2adc12c8-f956-4ca8-8ecb-6328660d645c", "embedding": null, "metadata": {"page_label": "40", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e7d2da3a-6b4f-4bd7-9f8d-6712343642f6", "node_type": "4", "metadata": {"page_label": "40", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "a67efbcd9f32803158e9104d07d13640ce02098f68d1be514bcc2fbb0a77c984", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "40 Differentiation\nv\u2208E1 and V \u2208E2, we obtain\n\u2202f(x,W)[v,V] = Wv + Vx \u2208F.\nWe can also access the individual JVPs as\n\u22021f(x,W)[v] = Wv \u2208F,\n\u22022f(x,W)[V] = Vx \u2208F.\nFor the VJP, lettingu\u2208F, we obtain\n\u2202f(x,W)\u2217[u] = (W\u22a4u,ux\u22a4) \u2208E.\nWe can access the individual VJPs by\n\u22021f(x,W)\u2217[u] = W\u22a4u\u2208E1,\n\u22022f(x,W)\u2217[u] = ux\u22a4\u2208E2.\nRemark 2.5(Nested inputs). It is sometimes convenient to group\ninputs into meaningful parts. For instance, if the input is naturally\nbroken down into two parts x = ( x1,x2), where x1 is a text\npart and x2 is an image part, and the network parameters are\nnaturally grouped into three layersw= (w1,w2,w3), we can write\nf(x,w) = f((x1,x2),(w1,w2,w3)). This is mostly a convenience\nand we can again reduce it to a function of a single input, thanks\nto the linear map perspective in Euclidean spaces.\nRemark 2.6(Hiding away inputs). It will often be convenient to\nignore inputs when differentiating. We use the semicolon for this\npurpose. For instance, a function of the formL(w; x,y) (notice\nthe semicolon) has signatureL: W\u2192 R because we treatxand\ny as constants. Therefore, the gradient is\u2207L(w; x,y) \u2208W. On\nthe other hand, the functionL(w,x,y) (notice the comma) has\nsignature L: W\u00d7X\u00d7Y \u2192 R so its gradient is\u2207L(w,x,y) \u2208\nW\u00d7X\u00d7Y . If we need to access partial gradients, we use indexing,\ne.g., \u22071L(w,x,y) \u2208W or \u2207wL(w,x,y) \u2208W when there is no\nambiguity.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1349, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "45e28d1b-2679-4b2c-b6f1-682563c936a3": {"__data__": {"id_": "45e28d1b-2679-4b2c-b6f1-682563c936a3", "embedding": null, "metadata": {"page_label": "41", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "01908d4d-c8d2-42e1-8049-f2640eecdd99", "node_type": "4", "metadata": {"page_label": "41", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "f27d91bb68d3bc68cb33906246ec2e74b1684924804c66a2ae4b4ae507539306", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.3. Linear maps 41\n2.3.8 Functions with multiple outputs (fan-out)\nSimilarly, it is often convenient to deal with functions that have multiple\noutputs.\nProposition 2.9(Multiple outputs). Consider a differentiable func-\ntion of the form f(w) := ( f1(w),...,f T(w)), with signatures\nf: E \u2192 Fand fi: E \u2192 Fi, where F := F1 \u00d7\u00b7\u00b7\u00b7\u00d7F T. Then\nthe JVP with the input directionv\u2208E is given by\n\u2202f(w)[v] = (\u2202f1(w)[v],...,\u2202f T(w)[v]) \u2208F.\nThe VJP with the output directionu= (u1,..., uT) \u2208F is\n\u2202f(w)\u2217[u] = \u2202f(w)\u2217[u1,..., uT] \u2208E\n=\nT\u2211\ni=1\n\u2202fi(w)\u2217[ui].\nCombined with the chain rule, we obtain that the Jacobian of\nh(w) := g(f(w)) = g(f1(w),...,f T(w))\nis \u2202h(w) = \u2211T\ni=1 \u2202ig(f(w)) \u25e6\u2202fi(w) and therefore the JVP is\n\u2202h(w)[v] =\nT\u2211\ni=1\n\u2202ig(f(w))[\u2202fi(w)[v]].\n2.3.9 Extensions to non-Euclidean linear spaces\nSo far, we focused on Euclidean spaces, i.e., linear spaces with a finite\nbasis. However, the notions studied earlier can be generalized to more\ngeneric spaces.\nFor example,directional derivatives(see Definition 2.12) can\nbe defined in any linear space equipped with a norm and complete\nwith respect to this norm. Such spaces are calledBanach spaces.\nCompleteness is a technical assumption that requires that any Cauchy\nsequence converges (a Cauchy sequence is a sequence whose elements\nbecome arbitrarily close to each other as the sequence progresses). A\nfunction f : E\u2192F defined from a Banach spaceEonto a Banach space", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1405, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f3925c94-975f-4a1e-8161-ab3a08b4c9da": {"__data__": {"id_": "f3925c94-975f-4a1e-8161-ab3a08b4c9da", "embedding": null, "metadata": {"page_label": "42", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ef346a62-a45b-49fd-979f-c43f84a93d6c", "node_type": "4", "metadata": {"page_label": "42", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "768a11f46b1251db232307556c7db67e6fd258a679009d2091acddb158ae5844", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "42 Differentiation\nFis then calledGateaux differentiableif its directional derivative is\ndefined along any direction (where limits are defined w.r.t. the norm in\nF). Some authors also require the directional derivative to be linear to\ndefine a Gateaux differentiable function.\nFr\u00e9chet differentiabilitycan also naturally be generalized to\nBanach spaces. The only difference is that, in generic Banach spaces,\nthe linear maplsatisfying Definition 2.12 must be continuous, i.e., there\nmust existC >0, such thatl[v] \u2264C\u2225v\u2225, where\u2225\u00b7\u2225 is the norm in the\nBanach spaceE.\nThe definitions of gradient and VJPs require in addition a notion of\ninner product. They can be defined inHilbert spaces, that is, linear\nspaces equipped with an inner product and complete with respect to\nthe norm induced by the inner product (they could also be defined\nin a Banach space by considering operations in the dual space, see,\ne.g. (Clarkeet al., 2008)). The existence of the gradient is ensured by\nRiesz\u2019s representation theoremwhich states that any continuous\nlinear form in a Hilbert space can be represented by the inner product\nwith a vector. Since for a differentiable functionf : E\u2192 R, the JVP\n\u2202f(w) : E\u2192 R is a linear form, Riesz\u2019s representation theorem ensures\nthe existence of the gradient as the elementg\u2208E such that\u2202f(w)v=\n\u27e8g,v\u27e9for anyv\u2208E. The VJP is also well-defined as the adjoint of the\nJVP w.r.t. the inner product of the Hilbert space.\nAs an example, the space of squared integrable functions onR is a\nHilbert space equipped with the inner product\u27e8a,b\u27e9:=\n\u222b\na(x)b(x)dx.\nHere, we cannot find a finite number of functions that can express all\npossible functions onR. Therefore, this space is not a mere Euclidean\nspace. Nevertheless, we can consider functions on this Hilbert space\n(called functionals to distinguish them from the elements of the space).\nThe associated directional derivatives and gradients, can be defined\nand are called respectively,functional derivativeand functional\ngradient, see, e.g., Frigyiket al.(2008) and references therein.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2043, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "986c666b-ebb2-4bc5-bb6d-8ca33d636af7": {"__data__": {"id_": "986c666b-ebb2-4bc5-bb6d-8ca33d636af7", "embedding": null, "metadata": {"page_label": "43", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "35489d90-8530-4200-af4a-c7cd129d5f74", "node_type": "4", "metadata": {"page_label": "43", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "3a39b877e7c87781571fa9aa770578d7611887149cab6a68378c18fe5d07cf21", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.4. Second-order differentiation 43\nFigure 2.7:Points at which the second derivative is small are points along which\nthe function is well approximated by its tangent line. On the other hand, points with\nlarge second derivative tend to be badly approximated by the tangent line.\n2.4 Second-order differentiation\n2.4.1 Second derivatives\nFor a single-input, single-output differentiable functionf : R \u2192R,\nits derivative at any point is itself a functionf\u2032 : R \u2192R. We may\nthen consider the derivative of the derivative at any point: thesecond\nderivative.\nDefinition 2.14(Second derivative). Thesecondderivative f(2)(w)\nof a differentiable functionf : R \u2192R at w \u2208R is defined as the\nderivative off\u2032at w, that is,\nf(2)(w) := lim\n\u03b4\u21920\nf\u2032(w+ \u03b4) \u2212f\u2032(w)\n\u03b4 ,\nprovided that the limit is well-defined. If the second derivative of\na functionf is well-defined atw, the function is said to betwice\ndifferentiable at w. The second derivative is also denotedf\u2032\u2032.\nIf f has a small second derivative at a givenw, the derivative around\nw is almost constant. That is, the function behaves like a line around\nw, as illustrated in Fig. 2.7. Hence, the second derivative is usually\ninterpreted as thecurvatureof the function at a given point.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1218, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9ddbb48b-24cf-49cd-a599-e0fe678a95f2": {"__data__": {"id_": "9ddbb48b-24cf-49cd-a599-e0fe678a95f2", "embedding": null, "metadata": {"page_label": "44", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ef7f518-da59-46e5-9185-1c660a27b9ca", "node_type": "4", "metadata": {"page_label": "44", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "77fe15c70a84f9d508eaabdb7017a1f9a44f26e267ce523100a6a381a8a22321", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "44 Differentiation\n2.4.2 Second directional derivatives\nFor a multi-input functionf : RP \u2192R, we saw that the directional\nderivative encodes infinitesimal variations off along a given direction.\nTo analyze the second derivative, the curvature of the function at a\ngiven pointw, we can consider the variations along a pair of directions,\nas defined below.\nDefinition 2.15(Second directional derivative). Theseconddirec-\ntional derivativeof f : RP \u2192R at w \u2208RP along v,v\u2032 \u2208RP\nis defined as the directional derivative ofw\u21a6\u2192\u2202f(w)[v] along v\u2032,\nthat is,\n\u22022f(w)[v,v\u2032] := lim\n\u03b4\u21920\n\u2202f(w+ \u03b4v\u2032)[v] \u2212\u2202f(w)[v]\n\u03b4 ,\nprovided that\u2202f(w)[v] is well-defined aroundwand that the limit\nexists.\nOf particular interest are the variations of a function around the\ncanonical directions: thesecond partial derivatives, defined as\n\u22022\nijf(w) := \u22022f(w)[ei,ej]\nfor ei, ej the ith and jth canonical directions inRP, respectively. In\nLeibniz notation, the second partial derivatives are denoted\n\u22022\nijf(w) = \u22022f(w)\n\u2202wi\u2202wj\n.\n2.4.3 Hessians\nFor a multi-input function, twice differentiability is simply defined as\nthe differentiability of any directional derivative\u2202f(w)[v] w.r.t. w.\nDefinition 2.16(Twice differentiability). A functionf : RP \u2192R is\ntwice differentiable atw\u2208RP if it is differentiable and\u2202f : RP \u2192\n(RP \u2192R) is also differentiable atw.\nAs a result, the second directional derivative is a bilinear form.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1378, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0394dec8-a084-423c-a812-26295f9af42a": {"__data__": {"id_": "0394dec8-a084-423c-a812-26295f9af42a", "embedding": null, "metadata": {"page_label": "45", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5007ad2a-5776-4f0a-8215-66598905fb4b", "node_type": "4", "metadata": {"page_label": "45", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "98c2f631e01be341092c558d8e3cae0d45200cc2f65156ebff9ef83889635462", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.4. Second-order differentiation 45\nDefinition 2.17(Bilinear map, bilinear form). A functionb: RP \u00d7\nRP \u2192RM is abilinear mapif b[v,\u00b7] : RP \u2192R is linear for anyv\nand b[\u00b7,v\u2032] is linear for anyv\u2032. That is,\nb[v,v\u2032] =\nP\u2211\ni=1\nvib[ei,v\u2032] =\nP\u2211\ni=1\nP\u2211\nj=1\nviv\u2032\njb[ei,ej],\nfor v= \u2211P\ni=1 viei and v\u2032= \u2211P\ni=1 v\u2032\niei. A bilinear map with values\nin R, b: RP \u00d7RP \u2192R, is called abilinear form.\nThe second partial derivatives are gathered in theHessian and the\nsecond directional derivatives can be computed from it.\nDefinition 2.18(Hessian). The Hessian of a twice differentiable\nfunction f : RP \u2192R at wis theP\u00d7P matrix gathering all second\npartial derivatives,\n\u22072f(w) :=\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\u220211f(w) ... \u2202 1Pf(w)\n... ... ...\n\u2202P1f(w) ... \u2202 PP f(w)\n\uf8f6\n\uf8f7\uf8f7\uf8f8\u2208RP\u00d7P,\nprovided that all second partial derivatives are well-defined.\nThe second directional derivative atwis bilinear in any direc-\ntions v= \u2211P\ni=1 viei and v\u2032= \u2211P\ni=1 v\u2032\niei. Therefore,\n\u22022f(w)[v,v\u2032] =\nP\u2211\ni,j=1\nviv\u2032\nj\u22022f(w)[ei,ej] = \u27e8v,\u22072f(w)v\u2032\u27e9.\nGiven the gradient off, the Hessian is equivalent to the transpose\nof the Jacobian of the gradient. By slightly generalizing the notation\u2207\nto denote the transpose of the Jacobian of a function (which matches\nits definition for single-output functions), we have that the Hessian can\nbe expressed as\u22072f(w) = \u2207(\u2207f)(w), which justifies its notation.\nSimilarly as for the differentiability of a functionf, twice differen-\ntiability off at wis equivalent to having the second partial derivatives\nnot only defined but also continuous in a neighborhood ofw. Remark-\nably, by requiring twice differentiability, i.e., continuous second partial\nderivatives, the Hessian is guaranteed to be symmetric (Schwarz, 1873).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1675, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d6a62002-712b-410e-b7fb-2e792d4d4260": {"__data__": {"id_": "d6a62002-712b-410e-b7fb-2e792d4d4260", "embedding": null, "metadata": {"page_label": "46", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cf876de5-32bd-406d-9907-9f5b44863f4f", "node_type": "4", "metadata": {"page_label": "46", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "c02c4f9b68276507d8fa5a07bcdb0ed6e0b138a8899d23b28c96d8ee05929e79", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "46 Differentiation\nProposition 2.10(Symmetry of the Hessian). Ifafunction f : RP \u2192\nR is twice differentiable atw, then its Hessian\u22072f(w) is symmetric,\nthat is,\u22022\nijf(w) = \u22022\njif(w) for anyi,j \u2208{1,...P }.\nThe symmetry of the Hessian means that it can alternatively be\nwritten as\u22072f(w) = (\u22022\njif(w))P\ni,j=1 = \u2202(\u2207f)(w), i.e., the Jacobian of\nthe gradient off.\n2.4.4 Hessian-vector products\nSimilarly to the Jacobian, we can exploit the formal definition of the\nHessian as a bilinear form to extend its definition to Euclidean spaces.\nIn particular, we can define the notion of Hessian-vector product.\nDefinition 2.19(Hessian-vector product). If a functionf : E\u2192 R\ndefined on a Euclidean spaceEwith inner product\u27e8\u00b7,\u00b7\u27e9, is twice\ndifferentiable at w \u2208E , then for anyv \u2208E , there exists v \u21a6\u2192\n\u22072f(w)[v], called theHessian-vector product(HVP) off at w\nalong v, such that for anyv\u2032\u2208E,\n\u22022f(w)[v,v\u2032] = \u27e8v\u2032,\u22072f(w)[v]\u27e9.\nInparticularfor E= RP,theHVPis \u22072f(w)[v] = (\u22022f(w)[v,ei])P\ni=1.\nFrom an autodiff point of view, the HVP can be implemented in\nfour different ways, as explained in Section 9.1.\n2.4.5 Second-order Jacobians\nThe previous definitions naturally extend to multi-output functions\nf : E\u2192F , wheref := (f1,...,f M), fj: E\u2192F j and F:= F1\u00d7\u00b7\u00b7\u00b7\u00d7FM.\nThe second directional derivative is defined by gathering the second\nderivatives of each coordinate\u2019s function. That is, forw,v,v\u2032\u2208E,\n\u2202f(w)[v,v\u2032] = (\u2202fj(w)[v,v\u2032])M\nj=1 \u2208F.\nThe functionf is twice differentiable if and only if all its coordinates are\ntwice differentiable. The second directional derivative is then abilinear", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1565, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "55acb479-9866-4b9a-926a-6b28c59e2d4a": {"__data__": {"id_": "55acb479-9866-4b9a-926a-6b28c59e2d4a", "embedding": null, "metadata": {"page_label": "47", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "22cfa8ed-f7a4-4c30-9880-cb2f7d833635", "node_type": "4", "metadata": {"page_label": "47", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "4e7f09969d3e43d74fd75cb7ada0566f51ae6e21b44b4c7c33fc90c2594ee0a6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.5. Higher-order differentiation 47\nmap. We can then compute second directional derivatives as\n\u22022f(w)[v,v\u2032] =\nP\u2211\ni,j=1\nviv\u2032\nj\u22022f(w)[ei,ej] = (\u27e8v,\u22072fj(w)v\u2032\u27e9)M\nj=1.\nWhen E= RP and Fj = R, so thatF= RM, the bilinear map can be\nmaterialized as a tensor\n\u22022f(w) = (\u22022f(w)[ei,ej])P\ni,j=1 \u2208RM\u00d7P\u00d7P,\nthe \u201csecond-order Jacobian\u201d off. However, similarly to the Hessian,\nit is usually more convenient to apply the bilinear map to prescribed\nvectors vand v\u2032than to materialize the second partial derivatives as a\ntensor.\n2.5 Higher-order differentiation\n2.5.1 Higher-order derivatives\nDerivatives can be extended to any order. Formally, thenth derivative\ncan be defined inductively as follows for a single-input, single-output\nfunction.\nDefinition 2.20(nth order derivative). The nth derivative f(n) of a\nfunction f : R \u2192R at w\u2208R is defined as\nf(n)(w) := (f(n\u22121))\u2032(w) = lim\n\u03b4\u21920\nf(n\u22121)(w+ \u03b4) \u2212f(n\u22121)(w)\n\u03b4\nprovided thatfn\u22121 is differentiable aroundw and that the limit\nexists. In such a case, the function is said to bentimes differentiable\nat w.\n2.5.2 Higher-order directional derivatives\nFor a multi-input functionf, we can naturally extend the notion of\ndirectional derivative as follows.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1176, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "14a637ff-0e83-4d4e-ab69-84fe36e1201d": {"__data__": {"id_": "14a637ff-0e83-4d4e-ab69-84fe36e1201d", "embedding": null, "metadata": {"page_label": "48", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "758942b9-e579-4cf0-abf5-0911dd34133f", "node_type": "4", "metadata": {"page_label": "48", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "ad0023fae6a09059424dd2184e3db6e848272961faab1a8f3a9467daed9d150c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "48 Differentiation\nDefinition 2.21(nth order directional derivative). Thenth directional\nderivative off : RP \u2192R at w\u2208RP along v1,..., vn is defined as\n\u2202nf(w)[v1,..., vn]\n= \u2202(\u2202n\u22121f(w)[v1,..., vn\u22121])[vn]\n= lim\n\u03b4\u21920\n\u2202f(w+ \u03b4vn)[v1,..., vn\u22121] \u2212\u2202f(w)[v1,..., vn\u22121]\n\u03b4\nA multi-input function f is n-times differentiable if it is n\u22121\ndifferentiable and itsn\u22121 directional derivative along any direction\nis differentiable. As a consequence thenth directional derivative is a\nmultilinear form.\nDefinition 2.22(Multilinear map, multilinear form). A function c :\n\u2297n\ni=1RP \u2192RM is amultilinear mapif it is linear in each coor-\ndinate given all others fixed, that is, ifvj \u21a6\u2192c[v1,..., vj,..., vn]\nis linear invj for anyj \u2208[n]. It is amultilinear formif it has\nvalues inR.\nThe nth order directional derivative is then given by\n\u2202nf(w)[v1,..., vn] =\nP\u2211\ni1,...,in=1\nv1,i1 ...v n,in\u2202nf(w)[ei1 ,..., ein].\nThe nth order partial derivatives can be materialized as annth order\ntensor\n\u2207nf(w) = (\u2202nf(w)[ei1 ,..., ein])P\ni1,...,in=1 \u2208RP\u00d7...\u00d7P.\n2.5.3 Higher-order Jacobians\nAllabovedefinitionsextenddirectlytothecaseofmulti-outputfunctions\nf : E\u2192F , whereF:= F1 \u00d7\u00b7\u00b7\u00b7\u00d7F M. Thenth directional derivatives\nare then\n\u2202nf(w)[v1,..., vn] = (\u2202nfj(w)[v1,..., vn])M\nj=1.\nThe functionf is thenn times differentiable if it isn\u22121 differentiable\nand itsn\u22121 directional derivative along any direction is differentiable.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1375, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "28a21f32-3532-47a0-b0f9-42c678e51f56": {"__data__": {"id_": "28a21f32-3532-47a0-b0f9-42c678e51f56", "embedding": null, "metadata": {"page_label": "49", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a96f86ba-2083-4595-88dd-be386ea2d8d0", "node_type": "4", "metadata": {"page_label": "49", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "da1140b7dd2478ec06e773c4465473c9674cb8a68a785b40bb21f3c9e06954af", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.5. Higher-order differentiation 49\nAs a consequence, thenth directional derivative is amultilinear map.\nThe nth directional derivative can be decomposed into partial derivatives\nas\n\u2202nf(w)[v1,..., vn] =\nP\u2211\ni1,...,in=1\nv1,i1 ...v n,in\u2202nf(w)[ei1 ,..., ein].\nWhen E= RP and F= RM, thenth order partial derivatives can be\nmaterialized by ann+ 1th order tensor\n\u2202nf(w) = (\u2202nfj(w)[ei1 ,..., ein])M,P,...,P\nj=1,i1,...,in=1 \u2208RM\u00d7P\u00d7...\u00d7P.\n2.5.4 Taylor expansions\nWith Landau\u2019s little o notation, we have seen that if a function is\ndifferentiable, it is approximated by a linear function inv,\nf(w+ v) = f(w) + \u27e8\u2207f(w),v\u27e9+ o(\u2225v\u22252).\nSuch an expansion of the function up to its first derivative is called the\nfirst-order Taylor expansionof f around w.\nIf the functionf is twice differentiable, we can approximate it by a\nquadratic inv, leading to thesecond-order Taylor expansionof f\naround w,\nf(w+ v) = f(w) + \u27e8\u2207f(w),v\u27e9+ 1\n2\u27e8v,\u22072f(w)v\u27e9+ o(\u2225v\u22252\n2).\nCompared to the first-order Taylor approximation, it is naturally more\naccurate around w, as reflected by the fact that \u2225v\u22253\n2 \u2264 \u2225v\u22252\n2 for\n\u2225v\u22252 \u22641.\nMore generally, we can build thenth order Taylor expansionof\na n times differentiable functionf: RP \u2192RM around w\u2208RP by\nf(w+ v) = f(w) + \u2202f(w)[v] + 1\n2\u22022f(w)[v,v] + ...\n+ 1\nn!\u2202nf(w)[v,..., v\ued19 \ued18\ued17\ued1a\nntimes\n] + o(\u2225v\u2225n\n2 ).\nNote that, using the change of variablew\u2032= w+ v \u21d0\u21d2v= w\u2032\u2212w,\nit is often convenient to write thenth Taylor expansion off(w\u2032) around", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1430, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f3e50ea8-4c49-4575-8c72-3a2521da661d": {"__data__": {"id_": "f3e50ea8-4c49-4575-8c72-3a2521da661d", "embedding": null, "metadata": {"page_label": "50", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "504aad8e-0f9e-4734-b857-341512dc0290", "node_type": "4", "metadata": {"page_label": "50", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "2720590e56bce3149367440ea39278d86bb7f72c701f744dd2dfc753ed37daf7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "50 Differentiation\nwas\nf(w\u2032) = f(w) +\nn\u2211\nj=1\n1\nj!\u2202jf(w)[w\u2032\u2212w,..., w\u2032\u2212w\ued19 \ued18\ued17 \ued1a\njtimes\n] + o(\u2225w\u2032\u2212w\u2225n\n2 ).\nTaylor expansions will prove useful in Chapter 7 for computing deriva-\ntives by finite differences.\n2.6 Differential geometry\nIn this chapter, we progressively generalized the notion of derivative\nfrom real numbers to vectors and variables living in a linear space\n(a.k.a. vector space), either finite dimensional or infinite dimensional.\nWe can further generalize these notions by considering a local notion\nof linearity. This is formalized by smooth manifolds indifferential\ngeometry, whose terminology is commonly adopted in the automatic\ndifferentiation literature and software. In this section, we give a brief\noverview of derivatives on smooth manifolds (simply referred to as\nmanifolds), and refer to Boumal (2023) for a complete introduction.\n2.6.1 Differentiability on manifolds\nEssentially, a manifold is a set that can be locally approximated by\na Euclidean space. The most common example is a sphere like the\nEarth. Seen from the Moon, the Earth is not a plane, but locally, at a\nhuman level, it can be seen as a flat surface. Euclidean spaces are also\ntrivial examples of manifolds. A formal characterization of the sphere\nas a manifold is presented in Example 2.9. For now, we may think of\na \u201cmanifold\u201d as some set (e.g., the sphere) contained in some ambient\nEuclidean space; note however that manifolds can be defined generally\nwithout being contained in a Euclidean space (Boumal, 2023, Chapter\n8). Differentiability in manifolds is simply inherited from the notion of\ndifferentiability in the ambient Euclidean space.\nDefinition 2.23(Differentiability of restricted functions). LetMand\nNbe manifolds. A functionf : M\u2192N defined fromM\u2286E to\nN \u2286F, with Eand FEuclidean spaces, is differentiable iff is", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1818, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "254eae49-7748-4606-b0cc-bf1abc648c73": {"__data__": {"id_": "254eae49-7748-4606-b0cc-bf1abc648c73", "embedding": null, "metadata": {"page_label": "51", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3c52983a-6a98-4ef5-abc1-efd71cb6ec04", "node_type": "4", "metadata": {"page_label": "51", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "71fe9dff07eebd0fb1b6cc8d80a4e8520b48f9477ecd7abf34b833129f043394", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.6. Differential geometry 51\nthe restriction of a differentiable function\u00aff : E\u2192F , so thatf\ncoincides with \u00aff on M.\nOur objective is to formalize the directional derivatives and gradients\nfor functions defined on manifolds. This formalization leads to the\ndefinitions of tangent spaces and cotangent spaces, and the associated\ngeneralizations of JVP and VJP operators as pushforward and pullback\noperators, respectively.\n2.6.2 Tangent spaces and pushforward operators\nTo generalize the notion of directional derivatives of a functionf, the\none property we want to preserve is the chain rule. Rather than starting\nfrom the variations off at a given point along a direction, we start with\nthe variations off along curves. Namely, on a manifold like the sphere\nSP in RP, we can look at curves\u03b1: R \u2192SP passing throughw\u2208SP\nat time0, that is,\u03b1(0) = w. For single-input functions like\u03b1, we denote\nfor simplicity\u03b1\u2032(0) := (\u03b1\u2032\n1(0),...,\u03b1 \u2032\nP(0)). The directional derivative of\na functionf must typically serve to define the derivative off \u25e6\u03b1 at\n0, such that(f \u25e6\u03b1)\u2032(0) = \u2202f(w)[\u03b1\u2032(0)]. In the case of the sphere, as\nillustrated in Fig. 2.8, the derivative\u03b1\u2032(0) of a curve\u03b1 passing through\na pointwis alwaystangent to the sphere atw. The tangent plane to\nthe sphere atwthen captures all possible relevant vectors to pass to the\nJVP we are building. To define the directional derivative of a function\nf on a manifold, we therefore restrict ourselves to an operator defined\non thetangent spaceTwM, whose definition below is simplified for\nour purposes.\nDefinition 2.24(Tangent space). The tangent spaceof a mani-\nfold Mat w\u2208M is defined as\nTwM:= {v= \u03b1\u2032(0) for any\u03b1: R \u2192Mdifferentiable s.t.\u03b1(0) = w}.\nIn the case of the sphere in Fig. 2.8, the tangent space is a plane,\nthat is, a Euclidean space. This property is generally true: tangent\nspaces are Euclidean spaces, enabling us to define directional derivatives\nas linear maps. Now, iff is differentiable and goes from a manifoldM\nto a manifoldN, thenf \u25e6\u03b1 is a differentiable curve inN. Therefore,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2035, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "49657dc5-9a37-4393-8fcf-72fa9300e991": {"__data__": {"id_": "49657dc5-9a37-4393-8fcf-72fa9300e991", "embedding": null, "metadata": {"page_label": "52", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3f98685c-888c-4bc6-99e3-c56494c5b4f5", "node_type": "4", "metadata": {"page_label": "52", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "1d2c65bc23b767ac66920a3322731e088c55079b982c18af97beea6b7f6b3466", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "52 Differentiation\nFigure 2.8: A differentiable functionf defined from a sphereMto a sphereN\ndefines a push-forward operator that maps tangent vectors (derivatives of functions\non the sphere passing throughw) in the tangent spaceTwMto tangent vectors of\nNat f(w) in the tangent spaceTf(w)N.\n(f\u25e6\u03b1)\u2032(0) is the derivative of a curve passing throughf(w) at 0 and is\ntangent toNat f(w). Hence, the directional derivative off : M\u2192N\nat wcan be defined as a function from the tangent spaceTwMof M\nat wonto the tangent spaceTf(w)Nof Nat f(w). Overall, we built\nthe directional derivative (JVP) by considering how a composition off\nwith any curve\u03b1 pushes forward the derivative of\u03b1 into the derivative\nof f \u25e6\u03b1. The resulting JVP is called apushforward operator in\ndifferentiable geometry.\nDefinition 2.25(Pushforward operator). Given two manifoldsM\nand N, thepushforward operatorof a differentiable function\nf : M\u2192N at w\u2208M is the linear map\u2202f(w) : TwM\u2192T f(w)N\ndefined by\n\u2202f(w)[v] := (f \u25e6\u03b1)\u2032(0),\nfor any v \u2208TwMsuch that v = \u03b1\u2032(0), where \u03b1 : R \u2192M is a\ndiffererentiable curve passing throughwat 0, i.e.,\u03b1(0) = w.\n2.6.3 Cotangent spaces and pullback operators\nTo generalize the JVP, we composedf : M\u2192N with any single-input\nfunction \u03b1: R \u2192M giving values on the manifold. The derivative of\nany such\u03b1 is then pushed forward fromTwMto Tf(w)Nby the action\nof f. To define the VJP, we take a symmetric approach. We consider all\nsingle-output differentiable functions\u03b2 : N\u2192 R defined ony\u2208N with", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1475, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4b61695e-1cf7-4df0-ae20-58f1736ca80a": {"__data__": {"id_": "4b61695e-1cf7-4df0-ae20-58f1736ca80a", "embedding": null, "metadata": {"page_label": "53", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8447d23d-fc15-49a7-9c56-d6f6ec10d158", "node_type": "4", "metadata": {"page_label": "53", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "7315e12da253a8b46fbdea33c40c9a94f97af20126387667f17ceb8303db58bc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.6. Differential geometry 53\ny= f(w) for somew\u2208M. We then want to pull back the derivatives\nof \u03b2 when precomposing it byf. Therefore, the space on which the\nVJP acts is the space of directional derivatives of any\u03b2 : N\u2192 R at y,\ndefining thecotangent space.\nDefinition 2.26(Cotangent space). The cotangent space of a\nmanifold Nat y\u2208N is defined as\nT\u2217\nyN= {u= \u2202\u03b2(y) for any\u03b2 : N\u2192 R differentiable}\n= {u: TyN\u2192 R for any linear mapu},\nNote that elements of the cotangent space are linear mappings, not\nvectors. This distinction is important to define the pullback operator\nas an operator on functions as done in measure theory. From a linear\nalgebra viewpoint, the cotangent space is exactly thedual spaceof\nTyN, that is, the set of linear maps fromTyNto R, calledlinear forms.\nAs TyNis a Euclidean space, its dual spaceT\u2217\nyNis also a Euclidean\nspace. Thepullbackoperator is then defined as the operator that gives\naccess to directional derivatives of\u03b2\u25e6f given the directional derivative\nof \u03b2 at f(w).\nDefinition 2.27(Pullback operator). Given two manifoldsMand\nN, thepullback operatorof a differentiable functionf : M\u2192N\nat w\u2208M is the linear map\u2202f(w)\u22c6 : T\u2217\nf(w)N\u2192T \u2217\nwMdefined by\n\u2202f(w)\u22c6u:= \u2202(\u03b2\u25e6f)(w),\nfor anyu\u2208Tf(w)N\u2217such that\u2202\u03b2(f(w)) = u, for a differentiable\nfunction \u03b2 : N\u2192 R.\nContrary to the pushforward operator that acts on vectors, the\npullback operator acts on linear forms. Hence, the slight difference in\nnotation between\u2202f(w)\u22c6 and \u2202f(w)\u2217, the adjoint operator of\u2202f(w).\nTo properly define the adjoint operator \u2202f(w)\u2217, we need a notion\nof inner product. Since tangent spaces are Euclidean spaces, we can\ndefine an inner product\u27e8\u00b7,\u00b7\u27e9w for eachTwMand w\u2208M, makingM\na Riemannian manifold. Equipped with these inner products, the\ncotangent space can be identified with the tangent space, and we can\ndefine gradients.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1816, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e1611f22-9e94-4a05-bb43-d4ad915e1708": {"__data__": {"id_": "e1611f22-9e94-4a05-bb43-d4ad915e1708", "embedding": null, "metadata": {"page_label": "54", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "81494ac2-ec6d-4057-8c61-9b7436e7ad26", "node_type": "4", "metadata": {"page_label": "54", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "889ba255dde269567b32c9bfc2abb9452c6f89bee5c2a88f306500ce905730b8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "54 Differentiation\nFunction f M\u2192N\nPush-forward \u2202f(w) TwM\u2192T f(w)N\nPullback \u2202f(w)\u22c6 T\u2217\nf(w)N\u2192T \u2217\nwM\nAdjoint of pushforward \u2202f(w)\u2217 Tf(w)N\u2192T wM\nTable 2.1:For a differentiable functionf defined from a manifoldMonto a manifold\nN, the JVP is generalized with the notion of pushforward\u2202f(w). The counterpart\nof the pushforward is the pullback operation\u2202f(w)\u22c6 that acts on linear forms in the\ntangent spaces. For Riemannian manifolds, the pullback operation can be identified\nwith the adjoint operator\u2202f(w)\u2217of the pushforward operator as any linear form is\nrepresented by a vector.\nDefinition 2.28(Gradients in Riemannian manifolds). Let Mbe a\nRiemannian manifold equipped with inner products\u27e8\u00b7,\u00b7\u27e9w. For any\ncotangent vectoru\u2208T \u2217\nwM, withw\u2208M, there exists a unique\ntangent vectoru\u2208TwMsuch that\n\u2200v\u2208TwM, u[v] = \u27e8u,v\u27e9w.\nIn particular for any differentiable functionf : M\u2192 R, we can\ndefine thegradient of f as the unique tangent vector\u2207f(w) \u2208\nTwMsuch that\n\u2200v\u2208TwM, \u2202f(w)[v] = \u27e8\u2207f(w),v\u27e9.\nTherefore, rather than pulling back directional derivatives, we can\npull back gradients. The corresponding operator is then naturally the\nadjoint\u2202f(w)\u2217of the pushforward operator. Namely, given two Rieman-\nnian manifoldsMand N, and a differentiable functionf : M\u2192N ,\nwe have\n(\u2202f(w)\u22c6u) [v] = \u27e8\u2202f(w)\u2217[u],v\u27e9for anyv\u2208TwM\nfor u= \u27e8\u00b7,u\u27e9\u2208T \u2217\nf(w)Nrepresented byu\u2208Tf(w)N.\nExample 2.9(The sphere as a manifold). The sphereSP in RP is\ndefined as the set of pointsw\u2208RP, satisfyingc(w) := \u27e8w,w\u27e9\u22121 =\n0, with JVP\u2202c(w)[v] = 2\u27e8w,v\u27e9.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1487, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3060fbc1-4e87-4961-ba77-3e61b3797c9e": {"__data__": {"id_": "3060fbc1-4e87-4961-ba77-3e61b3797c9e", "embedding": null, "metadata": {"page_label": "55", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f906dfbb-3fc4-4f79-9960-662e8d7d420b", "node_type": "4", "metadata": {"page_label": "55", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "ce81a59304f1e5c851ce725e489feba883df1c79be6ff576527ceaeadae6f965", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.7. Generalized derivatives 55\nFor anyv= (v1,...,v P\u22121) \u2208RP\u22121 close enough to a pointwon\nthe sphere, we can define\u03c81(v) :=\n\u221a\n1 \u2212\u27e8v,v\u27e9such that\u03c8(v) :=\n(v1,...,v P\u22121,\u03c81(v)) satisfies \u27e8\u03c8(v),\u03c8(v)\u27e9= 1, that isc(\u03c8(w)) =\n1. With the help of the mapping\u03c8\u22121 from a neighborhood ofw\nin the sphere toRP\u22121, we can locally see the sphere as a space of\ndimension P \u22121.\nThe tangent space can be naturally characterized in terms of\nthe constraining functionc. Namely, the curve\u03b1: R \u2192S such that\n\u03b1(0) = wsatisfies for any\u03b4\u2208R, c(\u03b1(\u03b4)) = 0. Hence, differentiating\nthe implicit equation, we have\n(c\u25e6\u03b1)\u2032(0) = \u2202c(w)[\u03b1\u2032(0)].\nThat is,\u03b1\u2032(0) is in the null space of\u2202c(w), denoted\nNull(\u2202c(w)) := {v\u2208RP : \u2202c(w)[v] = 0}.\nThe tangent space ofSat wis then\nTwM= Null(2\u27e8w,\u00b7\u27e9)\n= {v\u2208RP : \u27e8w,v\u27e9= 0}\nWe naturally recover that the tangent space is a Euclidean space\nof dimensionP \u22121, defined as the set of points orthogonal tow.\n2.7 Generalized derivatives\nWhile we largely focus on differentiable functions in this book, it is\nimportant to characterize non-differentiable functions. We distinguish\nhere two cases: continuous functions and non-continuous functions. For\nthe former case, there exist generalizations of the notion of directional\nderivative, gradient and Jacobian, presented below. For non-continuous\nfunctions, even if derivatives exist almost everywhere, they may be\nuninformative. For example, piecewise-constant functions, encountered\nin e.g. control flows (Chapter 5), are almost everywhere differentiable\nbut with zero derivatives. In such cases, surrogate functions can be\ndefined to ensure the differentiability of a program (Part IV).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1620, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dceb3e93-456b-4ffe-9c05-b66ee1b4541b": {"__data__": {"id_": "dceb3e93-456b-4ffe-9c05-b66ee1b4541b", "embedding": null, "metadata": {"page_label": "56", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "acaab71d-b86b-4efa-aa4f-89b766603b61", "node_type": "4", "metadata": {"page_label": "56", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "405b436168cbd4b74a9c67d6132675504258cb4af6e2530fbcb90714f1973c1f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "56 Differentiation\n2.7.1 Rademacher\u2019s theorem\nWe first recall the definition of (locally) Lipschitz continuous function.\nDefinition 2.29((Locally) Lipschitz continuous function). Afunction\nf : E\u2192F , is Lipschitz continuous if there existsC \u22650 such that\nfor anyx,y\u2208E,\n\u2225f(x) \u2212f(y)\u2225\u2264 C\u2225x\u2212y\u2225.\nA function f : E \u2192Fis locally Lipschitz continuous if for any\nx\u2208E, there exists a neighborhoodUof xsuch thatf restricted\nto Uis Lipschitz continuous.\nRademacher\u2019s theorem (Rademacher, 1919) then ensures thatf is\ndifferentiable almost everywhere.\nProposition 2.11(Rademacher\u2019s theorem). LetEandFdenoteEu-\nclidean spaces. Iff : E\u2192F is locally Lipschitz-continuous, thenf\nis almost everywhere differentiable, that is, the set of points inE\nat whichf is not differentiable is of (Lebesgue) measure zero.\nSee also Morrey Jr (2009) for a standard proof.\n2.7.2 Clarke derivatives\nRademacher\u2019s theorem hints that the definitions of directional deriva-\ntives, gradients and Jacobians may be generalized to locally Lipschitz\ncontinuous functions. This is what Clarke (1975) did in his seminal\nwork, which laid the foundation ofnonsmooth analysis. The first\nbuilding block is a notion of generalized directional derivative.\nDefinition 2.30(Clarke generalized directional derivative). TheClarke\ngeneralized directional derivativeof a locally Lipschitz con-\ntinuous function f : E \u2192R at w \u2208 Ein the direction v \u2208 E\nis\n\u2202Cf(w)[v] := lim sup\nu\u2192w\n\u03b4\u21980\nf(u+ \u03b4v) \u2212f(u)\n\u03b4 ,\nprovided that the limit exists, where\u03b4\u21980 means that\u03b4approaches", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1504, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "06fc6a2b-3fbb-4364-8cc8-3800f45b3bb1": {"__data__": {"id_": "06fc6a2b-3fbb-4364-8cc8-3800f45b3bb1", "embedding": null, "metadata": {"page_label": "57", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d2d95939-b5a1-4964-ab45-6ef117f429d1", "node_type": "4", "metadata": {"page_label": "57", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "ecab5535c1df42d2535d79a78080c20a86acf56a5ed62789a2d23caba6a21343", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.7. Generalized derivatives 57\n0 by non-negative values, and where the limit superior is defined as\nlim sup\nx\u2192a\nf(x) := lim\n\u03b5\u21920\nsup{f(x) : x\u2208B(a,\u03b5) \\{a}}\nfor B(a,\u03b5) := {x \u2208E : \u2225x\u2212a\u2225\u2264 \u03b5}the ball centered ata of\nradius \u03b5.\nThere are two differences with the usual definition of a directional\nderivative: (i) we consider slopes of the function in a neighborhood of\nthe point rather than at the given point, (ii) we take a limit superior\nrather than a usual limit. The first point is rather natural in the light\nof Rademacher\u2019s theorem: we can properly characterize variations on\npoints where the function is differentiable, therefore we may take the\nlimits of these slopes as a candidate slope for the point of interest. The\nsecond point is more technical but essential: it allows us to characterize\nthe directional derivative as the supremum of some linear forms (Clarke\net al., 2008). These linear forms in turn define a set of generalized\ngradients (Clarkeet al., 2008, Chapter 2).\nDefinition 2.31(Clarke generalized gradient). A Clarke general-\nized gradientof a locally Lipschitz functionf : E\u2192 R at w\u2208E\nis a pointg\u2208E such that\u2200v\u2208E\n\u2202f(w)[v] \u2265\u27e8g,v\u27e9.\nThe set of Clarke generalized gradients is called theClarke subd-\nifferential of f at w.\nDefinition 2.30 and Definition 2.31 can be used in non-Euclidean\nspaces, such as Banach or Hilbert spaces (Clarke et al., 2008). In\nEuclidean spaces, the Clarke generalized gradients can be characterized\nmore simply thanks to Rademacher\u2019s theorem (Clarkeet al., 2008,\nTheorem 8.1). Namely, as shown below, they can be defined as a convex\ncombination of limits of gradients off evaluated at a sequence inE\\ \u2126\nthat converges tow.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1667, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "32f9d7ad-f5b7-41e4-9d4e-98c32f33b141": {"__data__": {"id_": "32f9d7ad-f5b7-41e4-9d4e-98c32f33b141", "embedding": null, "metadata": {"page_label": "58", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a18084ec-e602-4289-a815-065d3a9463c8", "node_type": "4", "metadata": {"page_label": "58", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "6cb811b4ece43596c852d37abf6e7085f2a5553f93c3c1ec71d8ca607752e25e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "58 Differentiation\nProposition 2.12(Characterization of Clarke generalized gradients).\nLet f : E\u2192 R be a locally Lipschitz continuous and denote\u2126 the\nset of points at whichf is not differentiable (Proposition 2.11). An\nelement g\u2208E is a Clarke generalized gradient off at w\u2208E if and\nonly if\ng\u2208conv\n({\nlim\nn\u2192+\u221e\n\u2207f(vn): ( vn)+\u221e\nn=1 s.t. vn \u2208E\\ \u2126, vn \u2192\nn\u2192+\u221e\nw\n})\n.\nIn the above, the convex hull of a setS \u2286E , the set of convex\ncombinations of elements ofS, is denoted\nconv(S) := {\u03bb1s1 + ... + \u03bbmsm : m\u2208N, \u03bbi \u22650,\nm\u2211\ni=1\n\u03bbi = 1,si \u2208S}.\nThe Jacobian of a functionf : E\u2192F between two Euclidean spaces\ncan be generalized similarly (Clarkeet al., 2008, Section 3.3).\nDefinition 2.32(Clarke generalized Jacobian). Let f : E\u2192F be a\nlocally Lipschitz continuous and denote\u2126 the set of points at which\nf is not differentiable (Proposition 2.11). AClarke generalized\nJacobian of f at w\u2208E is an elementJ of\nconv\n({\nlim\nn\u2192+\u221e\n\u2202f(vn): ( vn)+\u221e\nn=1 s.t. vn \u2208E\\ \u2126, vn \u2192\nn\u2192+\u221e\nw\n})\n.\nFor a continuously differentiable functionf : E\u2192F or f : E\u2192 R,\nthere is a unique generalized gradient, recovering the usual gradi-\nent (Clarke et al., 2008, Proposition 3.1, page 78). The chain rule\ncan be generalized to these objects (Clarkeet al., 2008). Recently, Bolte\nand Pauwels (2020) and Bolteet al.(2022) further generalized Clarke\ngradients through the definition of conservative gradients to define\nautomatic differentiation schemes for nonsmooth functions.\n2.8 Summary\n\u2022 The usual definition of derivatives of real-valued univariate\nfunctions extends to multivariate functionsf: RP \u2192R through\nthe notion ofdirectional derivative\u2202f(w)[v] at w \u2208RP in\nthe directionv\u2208RP.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1640, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1b48f23e-eaf2-4d85-b657-cd09cb974b41": {"__data__": {"id_": "1b48f23e-eaf2-4d85-b657-cd09cb974b41", "embedding": null, "metadata": {"page_label": "59", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c416c2a7-e4fb-43d6-a561-4bdfc099c90d", "node_type": "4", "metadata": {"page_label": "59", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "a0df9975bd8f494e8fbb784a0dc4113efe01a38ae6c76e5ab7b23aadde548d80", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.8. Summary 59\n\u2022 To take advantage of the representation ofw= \u2211P\nj=1 wjej using\nthe canonical bases{e1,..., eP}, the definition ofdifferentiable\nfunctions requires thelinearity of the directional derivative w.r.t.\nthe directionv.\n\u2022 This requirement gives rise to the notion ofgradient \u2207f(w) \u2208\nRP, the vector that gathers the partial derivatives and further\ndefines thesteepest ascent directionat w.\n\u2022 For vector-input vector-output functionsf: RP \u2192RM, the di-\nrectional derivative leads to the definition ofJacobian matrix\n\u2202f(w) \u2208RM\u00d7P, the matrix which gathers all partial derivatives\n(notice that we use bold\u2202). Thechain ruleis then theproduct\nof Jacobian matrices.\n\u2022 These notions can be extended to general Euclidean spaces, such as\nthe spaces of matrices or tensors. For functions of the formf: E\u2192\nR, the gradient is \u2207f(w) \u2208 E. More generally, for functions\nof the form f: E \u2192F, the Jacobian \u2202f(w) can be seen as a\nlinear map(notice the non-bold\u2202). The directional derivative\nat w\u2208E naturally defines a linear mapl[v] = \u2202f(w)[v], where\n\u2202f(w): E\u2192F is called theJacobian vector product(JVP)\nand captures the infinitesimal variation atw\u2208E along theinput\ndirection v\u2208E.\n\u2022 Its adjoint \u2202f(w)\u2217: F\u2192E defines another linear mapl[u] =\n\u2202f(w)\u2217[u] called the vector Jacobian product (VJP) and\ncaptures the infinitesimal variation atw\u2208E along theoutput\ndirection u\u2208F. The chain ruleis then thecomposition of\nthese linear maps.\n\u2022 For the particular case when we compose a scalar-valued function\n\u2113 (such as a loss function) with a vector-valued functionf (such\nas a network function), the gradient is given by\u2207(\u2113\u25e6f)(w) =\n\u2202f(w)\u2217\u2207\u2113(f(w)). This is why being able to apply the adjoint to\na gradient, which as we shall see can be done with reverse-mode\nautodiff, is so pervasive in machine learning.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1783, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7fd2281c-8330-48f1-baa1-2c29423e5511": {"__data__": {"id_": "7fd2281c-8330-48f1-baa1-2c29423e5511", "embedding": null, "metadata": {"page_label": "60", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0d775c92-a4e1-48ca-98aa-a483248ffa96", "node_type": "4", "metadata": {"page_label": "60", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "b2b18f3f4d81a7b41a0c695b2fc0350e180debd30e19bf1c178b781497710b8d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "60 Differentiation\n\u2022 The definitions of JVP and VJP operators can further be general-\nized in the context of differentiable geometry. In that framework,\nthe JVP amounts to thepushforward operator that acts on\ntangent vectors. The VJP amounts to thepullback operator\nthat acts oncotangent vectors.\n\u2022 We also saw that theHessian matrixof a functionf(w) from\nRP to R is denoted\u22072f(w) \u2208RP\u00d7P. It is symmetric if the second\npartial derivatives are continuous. Seen as linear map, the Hessian\nleads to the notion ofHessian-vector product(HVP), which\nwe saw can be reduced to the JVP or the VJP of\u2207f(w).\n\u2022 The main take-away message of this chapter is that computing the\ndirectional derivative or the gradient of compositions of functions\ndoes notrequire computing intermediate Jacobians but only to\nevaluate linear maps (JVPs or VJPs) associated with these inter-\nmediate functions. The goal of automatic differentiation, presented\nin Chapter 8, is precisely to provide an efficient implementation\nof these maps forcomputation chainsor more generally for\ncomputation graphs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1067, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b5795ca1-c647-4be5-ab7a-ecd46fe3a38c": {"__data__": {"id_": "b5795ca1-c647-4be5-ab7a-ecd46fe3a38c", "embedding": null, "metadata": {"page_label": "61", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ff9acfb5-0f0f-41ff-aa0a-707ad5be359c", "node_type": "4", "metadata": {"page_label": "61", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "85590b310da3d4245f2e06ac58cc9a04e24f3eb77c878c5a78a61ae59d82237f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3\nProbabilistic learning\nIn this chapter, we review how to perform probabilistic learning. We\nalso introduce exponential family distributions, as they play a key role\nin this book.\n3.1 Probability distributions\n3.1.1 Discrete probability distributions\nA discrete probability distribution over a set Y is specified by its\nprobability mass function(PMF) p: Y\u2192 [0,1]. The probability of\ny\u2208Y is then defined by\nP(Y = y) := p(y),\nwhere Y denotes a random variable. WhenY follows a distributionp,\nwe writeY \u223cp (with some abuse of notation, we use the same letterp\nto denote the distribution and the PMF). Theexpectation of \u03d5(Y),\nwhere Y \u223cp and \u03d5: Y\u2192 RM, is then\nE[\u03d5(Y)] =\n\u2211\ny\u2208Y\np(y)\u03d5(y),\n61", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 684, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c36fd0bf-5302-4023-9adc-5f0b61a53fa7": {"__data__": {"id_": "c36fd0bf-5302-4023-9adc-5f0b61a53fa7", "embedding": null, "metadata": {"page_label": "62", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "55492c51-b24b-4279-8a54-9c01271b75f9", "node_type": "4", "metadata": {"page_label": "62", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "f8f7fc2b118a511cae5c46d0815b05fd02fb196e027cd576ea0ed72ce440ae11", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "62 Probabilistic learning\nits variance(for one-dimensional variables) is\nV[\u03d5(Y)] = E[(\u03d5(Y) \u2212E[\u03d5(Y)])2] =\n\u2211\ny\u2208Y\np(y)(\u03d5(y) \u2212E[\u03d5(Y)])2\nand itsmode is\narg max\ny\u2208Y\np(y).\nTheKullback-Leibler(KL) divergence (also known as relative entropy)\nbetween two discrete distributions overY, with associated PMFsp and\nq, is the statistical \u201cdistance\u201d defined by\nKL(p,q) :=\n\u2211\ny\u2208Y\np(y) log\n(p(y)\nq(y)\n)\n= EY\u223cplog\n(p(Y)\nq(Y)\n)\n.\n3.1.2 Continuous probability distributions\nA continuous probability distribution overYis specified by itsproba-\nbility density function(PDF) p: Y\u2192 R+. The probability ofA\u2286Y\nis then\nP(Y \u2208A) =\n\u222b\nA\np(y)dy.\nThe definitions of expectation, variance and KL divergence are defined\nanalogously to the discrete setting, simply replacing\u2211\ny\u2208Y with\n\u222b\nY.\nSpecifically, the expectation of\u03d5(Y) is\nE[\u03d5(Y)] =\n\u222b\nY\np(y)\u03d5(y)dy,\nthe variance is\nV[\u03d5(Y)] = E[(\u03d5(Y) \u2212E[\u03d5(Y)])2] =\n\u222b\nY\np(y)(\u03d5(y) \u2212E[\u03d5(Y)])2dy\nand the KL divergence is\nKL(p,q) :=\n\u222b\nY\np(y) log\n(p(y)\nq(y)\n)\ndy= EY\u223cplog\n(p(Y)\nq(Y)\n)\n.\nThe mode is defined as the arg maximum of the PDF.\nWhen Y= R, we can also define thecumulative distribution\nfunction (CDF)\nP(Y \u2264b) =\n\u222bb\n\u2212\u221e\np(y)dy.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1128, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1e0d01fd-019f-4dc2-a995-902f1048ff4c": {"__data__": {"id_": "1e0d01fd-019f-4dc2-a995-902f1048ff4c", "embedding": null, "metadata": {"page_label": "63", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bc915d77-e2df-440d-8d5f-ed10ad2a2d4d", "node_type": "4", "metadata": {"page_label": "63", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "3195b7f9ad72aef65bb146b3d6d4b45d9a9013ac8fbf86386c9ac078eb8fad16", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.2. Maximum likelihood estimation 63\nThe probability ofY lying in the semi-closed interval(a,b] is then\nP(a<Y \u2264b) = P(Y \u2264b) \u2212P(Y \u2264a).\n3.2 Maximum likelihood estimation\n3.2.1 Negative log-likelihood\nWe saw that a probability distribution overYis specified byp(y), which\nis called the probability mass function (PMF) for discrete variables\nor the probability density function (PDF) for continuous variables. In\npractice, the true distributionpgenerating the data is unknown and we\nwish to approximate it with a distributionp\u03bb, with parameters\u03bb\u2208\u039b.\nGiven a finite set of i.i.d. observationsy1,..., yN, how do we fit\u03bb\u2208\u039b\nto the data? This can be done by maximizing thelikelihood of the\ndata, i.e., we seek to solve\n\u02c6\u03bbN := arg max\n\u03bb\u2208\u039b\nN\u220f\ni=1\np\u03bb(yi).\nThis is known asmaximum likelihood estimation(MLE). Because\nthe log function is monotonically increasing, this is equivalent to mini-\nmizing thenegative log-likelihood, i.e., we have\n\u02c6\u03bbN = arg min\n\u03bb\u2208\u039b\n\u2212\nN\u2211\ni=1\nlog p\u03bb(yi).\nExample 3.1(MLE for the normal distribution). Suppose we setp\u03bb\nto the normal distribution with parameters\u03bb= (\u00b5,\u03c3), i.e.,\np\u03bb(y) := 1\n\u03c3\n\u221a\n2\u03c0exp\n(\n\u22121\n2\n(y\u2212\u00b5\n\u03c3\n)2)\n.\nThen, given observationsy1,...,y N, the MLE estimators for\u00b5and\n\u03c32 are the sample mean and the sample variance, respectively.\n3.2.2 Consistency w.r.t. the Kullback-Leibler divergence\nIt is well-known that the MLE estimator isconsistent, in the sense of\nthe Kullback-Leibler divergence. That is, denoting the true distribution", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1452, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7fa30983-9c67-4321-befd-ff90a04122b8": {"__data__": {"id_": "7fa30983-9c67-4321-befd-ff90a04122b8", "embedding": null, "metadata": {"page_label": "64", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f27ef110-576c-468b-a6ff-9f3ff0b79902", "node_type": "4", "metadata": {"page_label": "64", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "5a94927ef526c219ebbf65e72a24d9bad21263afa37a98b1528e3711fbc3934b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "64 Probabilistic learning\np and\n\u03bb\u221e:= arg min\n\u03bb\u2208\u039b\nKL(p,p\u03bb) = EY\u223cplog\n(p(Y)\np\u03bb(Y)\n)\n,\nthen \u02c6\u03bbN \u2192\u03bb\u221ein expectation over the observations, asN \u2192\u221e. This\ncan be seen by using\nKL(p,p\u03bb) \u2248 1\nN\nN\u2211\ni=1\nlog\n(p(yi)\np\u03bb(yi)\n)\n= 1\nN\nN\u2211\ni=1\nlog p(yi) \u2212log p\u03bb(yi)\nand the law of large numbers.\n3.3 Probabilistic supervised learning\n3.3.1 Conditional probability distributions\nMany times in machine learning, instead of a probabilityP(Y = y) for\nsome y\u2208Y, we wish to define a conditional probabilityP(Y = y|X =\nx), for some inputx\u2208X . This can be achieved by reduction to an\nunconditional probability distribution,\nP(Y = y|X = x) := p\u03bb(y)\nwhere\n\u03bb:= f(x,w)\nand f is amodel functionwith model parametersw\u2208W. That is,\nrather than being a deterministic function fromXto Y, f is a function\nfrom Xto \u039b, the set of permissibledistribution parametersof the\noutput distribution associated with the input.\nWe emphasize that\u03bbcould be a single parameter or a collection of\nparameters. For instance, in the Bernoulli distribution,\u03bb= \u03c0, while in\nthe univariate normal distribution,\u03bb= (\u00b5,\u03c3).\nIn Section 3.4 and throughout this book, we will also use the notation\np\u03b8 instead ofp\u03bb when \u03b8are the canonical parameters of an exponential\nfamily distribution.\n3.3.2 Inference\nThe main advantage of this probabilistic approach is that our prediction\nmodel is much richer than if we just learned a function fromXto Y.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1373, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "80ddb8ea-84aa-447a-ae87-d87b0ed33ec0": {"__data__": {"id_": "80ddb8ea-84aa-447a-ae87-d87b0ed33ec0", "embedding": null, "metadata": {"page_label": "65", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b1e923b2-a931-410d-9a5c-ec139dde008f", "node_type": "4", "metadata": {"page_label": "65", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "efa6abbb9e7875b89effa1a0ae6e95166bfc14f6037a29370abc9f630f3c14b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.3. Probabilistic supervised learning 65\nWe now have access to the whole distribution over possible outcomes in\nYand can compute various statistics:\n\u2022 Probability: P(Y = y|X = x) or P(Y \u2208A|X = x),\n\u2022 Expectation: E[\u03d5(Y)|X = x] for some function\u03d5,\n\u2022 Variance:V[\u03d5(Y)|X = x],\n\u2022 Mode: arg maxy\u2208Yp\u03bb(y).\nWe now review probability distributions useful for binary classification,\nmulticlass classification, regression, multivariate regression, and integer\nregression. In the following, to make the notation more lightweight, we\nomit the dependence onx.\n3.3.3 Binary classification\nFor binary outcomes, where Y= {0,1}, we can use aBernoulli\ndistribution with parameter\n\u03bb:= \u03c0\u2208[0,1].\nWhen a random variableY is distributed according to a Bernoulli\ndistribution with parameter\u03c0, we write\nY \u223cBernoulli(\u03c0).\nThe PMF of this distribution is\np\u03c0(y) :=\n\uf8f1\n\uf8f2\n\uf8f3\n\u03c0 if y= 1\n1 \u2212\u03c0 if y= 0\n.\nThe Bernoulli distribution is abinomial distributionwith a single\ntrial. Sincey\u2208{0,1}, the PMF can be rewritten as\np\u03c0(y) = \u03c0y(1 \u2212\u03c0)1\u2212y.\nThe mean is\nE[Y] = \u03c0= P(Y = 1)\nand the variance is\nV[Y] = \u03c0(1 \u2212\u03c0) = P(Y = 1)P(Y = 0).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1086, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0579e9f4-e3d2-445c-99f5-24635594b890": {"__data__": {"id_": "0579e9f4-e3d2-445c-99f5-24635594b890", "embedding": null, "metadata": {"page_label": "66", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1452b476-5ebc-4dba-918a-36fa434eb4f7", "node_type": "4", "metadata": {"page_label": "66", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "4144c0913e30e1b622fcec5f0ad255f2c39bb7309166db2bd5e8aa0ef3ec2f2f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "66 Probabilistic learning\n0 1\ny\n0.0\n0.5\n1.0\n0.2\n0.8\nPMF\n0 1\ny\n0.0\n0.5\n1.0\n0.2\n1.0\nCDF\n3\n 0 3\n0.0\n0.5\n1.0\nMean function A\u2032( )\n5\n 0 5\n0\n2\n4\nLoss L( , y)\ny = 1\ny = 0\nFigure 3.1:The Bernoulli distribution, whose PMF and CDF are here illustrated\nwith parameter\u03c0 = 0.8. Its mean function is\u03c0 = A\u2032(\u03b8) = logistic(\u03b8) = 1\n1+exp(\u2212\u03b8) ,\nwhere \u03b8 is for instance the output of a neural network. The negative log-likelihood\nleads to thelogistic loss, L(\u03b8,y) = softplus(\u03b8) \u2212\u03b8y = log(1 + exp(\u03b8)) \u2212\u03b8y. The\nloss curve is shown fory\u2208{0,1}.\nParameterization using a sigmoid\nSince the parameter\u03c0 of a Bernoulli distribution needs to belong to\n[0,1], we typically use asigmoid function(Section 4.4.3), such as a\nlogistic functionas the output layer:\n\u03c0:= f(x,w) := logistic(g(x,w)),\nwhere g: X\u00d7W\u2192 R is for example a neural network and\nlogistic(a) := 1\n1 + exp(\u2212a) \u2208(0,1).\nWhen g is linear inw, this is known asbinary logistic regression.\nRemark 3.1(Link with the logistic distribution). The logistic distri-\nbution with mean and scale parameters\u00b5 and \u03c3 is acontinuous\nprobability distribution with PDF\np\u00b5,\u03c3(u) := p0,1\n(u\u2212\u00b5\n\u03c3\n)\nwhere\np0,1(z) := exp(\u2212z)\n(1 + exp(\u2212z))2 .\nIf a random variable U follows a logistic distribution with pa-\nrameters \u00b5 and \u03c3, we write U \u223c Logistic(\u00b5,\u03c3). The CDF of", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1264, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5a8cf1fc-890c-49af-affe-c8916fbfb7e6": {"__data__": {"id_": "5a8cf1fc-890c-49af-affe-c8916fbfb7e6", "embedding": null, "metadata": {"page_label": "67", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45f89314-f1a8-4e77-b0a7-79e40b747f09", "node_type": "4", "metadata": {"page_label": "67", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "2a2c5c01efcf103705648a67ac1a15f7aac6eded2fee38cb8b3d7c99d19592cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.3. Probabilistic supervised learning 67\nU \u223cLogistic(\u00b5,\u03c3) is\nP(U \u2264u) =\n\u222bu\n\u2212\u221e\np\u00b5,\u03c3(u)du= logistic\n(u\u2212\u00b5\n\u03c3\n)\n.\nTherefore, if\nU \u223cLogistic(\u00b5,\u03c3)\nand\nY \u223cBernoulli\n(\nlogistic\n(u\u2212\u00b5\n\u03c3\n))\n,\nthen\nP(Y = 1) = P(U \u2264u).\nHere, U can be interpreted as a latent continuous variable andu\nas a threshold.\n3.3.4 Multiclass classification\nForcategorical outcomeswith M possible choices, whereY= [M],\nwe can use acategorical distributionwith parameters\n\u03bb:= \u03c0\u2208\u25b3M,\nwhere we define the probability simplex\n\u25b3M := {\u03c0\u2208RM\n+ : \u27e8\u03c0,1\u27e9= 1},\ni.e., the set of valid discrete probability distributions. WhenY follows\na categorical distribution with parameter\u03c0, we write\nY \u223cCategorical(\u03c0).\nThe PMF of the categorical distribution is\np\u03c0(y) := \u27e8\u03c0,\u03d5(y)\u27e9= \u03c0y,\nwhere\n\u03d5(y) := ey\nis the standard basis vector for the coordinatey\u2208[M].\nSince Y is a categorical variable, it does not make sense to compute\nthe expectation ofY but we can compute that of\u03d5(Y) = eY,\nEY\u223cp\u03c0[\u03d5(Y)] = \u03c0.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 931, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "37c21c61-d9d3-45a3-8cb6-3d951eabc55d": {"__data__": {"id_": "37c21c61-d9d3-45a3-8cb6-3d951eabc55d", "embedding": null, "metadata": {"page_label": "68", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b41ca5b-78df-46e3-9b66-eec35a6e5696", "node_type": "4", "metadata": {"page_label": "68", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "fded8bcfcdd69bf922423a500b9edcd3720a4c008bfc114910df2997352781fc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "68 Probabilistic learning\n1 2 3\ny\n0.0\n0.5\n1.0\n0.3\n0.6\n0.1\nPMF\n1 2 3\ny\n0.0\n0.5\n1.0\n0.3\n0.9\n1.0\nCDF\n3\n 0 3\ns\n0.0\n0.5\n1.0\nA( ), y\n5\n 0 5\ns\n0\n2\n4\n6\nLoss L( , y)\ny = e1\ny = e2\ny = e3\nFigure 3.2: The categorical distribution, whose PMF and CDF are here il-\nlustrated with parameter\u03c0 = (0.3,0.6,0.1). Its mean function is\u03c0 = \u2207A(\u03b8) =\nsoftargmax(\u03b8), where\u03b8\u2208RM is for instance the output of a neural network. Here,\nfor illustration purpose, we choose to set\u03b8 = (s,1,0) and vary onlys. Since the\nmean function\u2207A(\u03b8) belongs toR3, we choose to display\u27e8\u2207A(\u03b8),ei\u27e9= \u2207A(\u03b8)i,\nfor i \u2208{1,2,3}. The negative log-likelihood leads to thelogistic loss, L(\u03b8,y) =\nlogsumexp(\u03b8) \u2212\u27e8\u03b8,y\u27e9. The loss curve is shown fory \u2208 {e1,e2,e3}, again with\n\u03b8= (s,1,0) and varyings.\nTherefore, as was also the case for the Bernoulli distribution, the mean\nand the probability distribution (represented by the vector\u03c0) are the\nsame in this case.\nParameterization using a softargmax\nSince the parameter vector\u03c0 of a categorical distribution needs to\nbelong to\u25b3M, we typically use a softargmax as the output layer:\n\u03c0:= f(x,w) := softargmax(g(x,w)),\nwhere g: X\u00d7W\u2192 RM is for example a neural network and\nsoftargmax(u) := exp(u)\u2211\nj exp(uj) \u2208relint(\u25b3M).\nTheoutputofthesoftargmaxisintherelativeinteriorof \u25b3M,relint(\u25b3M) =\n\u25b3M \u2229RM\n>0. That is, the produced probabilities are always strictly posi-\ntive. The categorical distribution is amultinomial distributionwith\na single trial. Wheng is linear inw, this is therefore known asmulti-\nclass or multinomial logistic regression, though strictly speaking a\nmultinomial distribution could use more than one trial.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1603, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3ac16900-04f5-43d0-998b-c250211bb3ee": {"__data__": {"id_": "3ac16900-04f5-43d0-998b-c250211bb3ee", "embedding": null, "metadata": {"page_label": "69", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bcd1d024-9d52-4dae-82d9-b775c32a2ea1", "node_type": "4", "metadata": {"page_label": "69", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "2d18f079b00f2fea9d790f21740773206837192bb9818647f74e381a57af4d47", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.3. Probabilistic supervised learning 69\n3.3.5 Regression\nFor real outcomes, whereY= R, we can use, among other choices, a\nnormal distributionwith parameters\n\u03bb:= (\u00b5,\u03c3),\nwhere\u00b5\u2208R isthemeanparameterand \u03c3\u2208R+ isthestandarddeviation\nparameter. WhenY followsa normaldistributionwith parameters (\u00b5,\u03c3),\nwe write\nY \u223cNormal(\u00b5,\u03c3).\nThe PDF is\np\u00b5,\u03c3(y) := 1\n\u03c3\n\u221a\n2\u03c0exp\n(\n\u22121\n2\n(y\u2212\u00b5)2\n\u03c32\n)\n.\nThe expectation is\nEY\u223cp\u00b5,\u03c3[Y] = \u00b5.\nOne advantage of the probabilistic perspective is that we are not limited\nto predicting the mean. We can also compute the CDF\nP(Y \u2264y) = 1\n2\n[\n1 + erf\n(y\u2212\u00b5\n\u03c3\n\u221a\n2\n)]\n,\nwhere we used theerror function\nerf(z) := 2\u221a\u03c0\n\u222bz\n0\ne\u2212t2\ndt.\nThis function is available in most scientific computing libraries, such as\nSciPy (Virtanenet al., 2020). We can also write\nP(Y \u2264y) = \u03a6\n(y\u2212\u00b5\n\u03c3\n)\nwhere\n\u03a6(z) := 1\n2\n[\n1 + erf\n( z\u221a\n2\n)]\n(3.1)\nis the CDF of the standard Gaussian distribution (with zero mean and\nunit variance). From the CDF, we also easily obtain\nP(a<Y \u2264b) = 1\n2\n[\nerf\n(b\u2212\u00b5\n\u03c3\n\u221a\n2\n)\n\u2212erf\n(a\u2212\u00b5\n\u03c3\n\u221a\n2\n)]\n.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1001, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a68e83bd-6689-40d4-94c6-f575dde9551b": {"__data__": {"id_": "a68e83bd-6689-40d4-94c6-f575dde9551b", "embedding": null, "metadata": {"page_label": "70", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b45a8597-f74b-4a7c-b3e7-56db07efbb41", "node_type": "4", "metadata": {"page_label": "70", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "6385f0626ce3e7e2036c315113a0ea7bf43eacd80af5449b9feebdd727d95f2e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "70 Probabilistic learning\n5\n 0 5\ny\n0.0\n0.2\n0.4\nPDF\n= 2\n= 0\n= 2\n5\n 0 5\ny\n0.0\n0.5\n1.0\nCDF (Y y)\n= 2\n= 0\n= 2\n5\n 0 5\n=\n5\n0\n5\nMean function A\u2032( )\n5\n 0 5\n0\n20\n40\nLoss L( , y)\ny = 2\ny = 0\ny = 2\nFigure 3.3:The Gaussian distribution, with mean parameter\u00b5 and variance\n\u03c32 = 1. Its mean function is\u00b5= A\u2032(\u03b8) = \u03b8, where\u03b8 is for instance the output of a\nneural network. The negative log-likelihood leads to thesquared loss, L(\u03b8,y) =\n(y\u2212\u03b8)2. The loss curve is shown fory\u2208{\u22122,0,2}.\nParameterization\nTypically, in regression, the mean is output by a model, while the\nstandard deviation\u03c3 is kept fixed (typically set to1). Since\u00b5 is uncon-\nstrained, we can simply set\n\u00b5:= f(x,w) \u2208R,\nwhere f: X\u00d7W\u2192 R is for example a neural network. That is, the\noutput off is the mean of the distribution,\nEY\u223cp\u00b5,1 [Y] = \u00b5= f(x,w).\nWe can also use\u00b5 to predict P(Y \u2264y) or P(a < Y\u2264b), as shown\nabove.\n3.3.6 Multivariate regression\nMore generally, formultivariate outcomes, whereY= RM, we can\nuse amultivariate normal distributionwith parameters\n\u03bb:= (\u00b5,\u03a3),\nwhere \u00b5\u2208RM is the mean and\u03a3 \u2208RM\u00d7M is the covariance matrix.\nWhen Y follows a multivariate normal distribution with parameters\n(\u00b5,\u03a3), we write\nY \u223cNormal(\u00b5,\u03c3).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1175, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4d845ca7-5573-4db0-b141-edf6e5d16bbe": {"__data__": {"id_": "4d845ca7-5573-4db0-b141-edf6e5d16bbe", "embedding": null, "metadata": {"page_label": "71", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5e423553-067d-492a-9b4d-6fab9610a934", "node_type": "4", "metadata": {"page_label": "71", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "32ae2a8994395bc9a448f9559572a02e0205dd248e1b4e6868abd2d5e0b6f7b4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.3. Probabilistic supervised learning 71\nThe PDF is\np\u00b5,\u03a3(y) := 1\u221a\n2\u03c0M|\u03a3|\nexp\n(\n\u22121\n2\u27e8y\u2212\u00b5,\u03a3\u22121(y\u2212\u00b5)\u27e9\n)\n.\nUsing a diagonal covariance matrix is equivalent to usingM independent\nnormal distributions for eachYj, forj \u2208[M]. The expectation is\nEY\u223cp\u00b5,\u03a3[Y] = \u00b5.\nParameterization\nTypically, in multivariate regression, the mean is output by a model,\nwhile the covariance matrix is kept fixed (typically set to the identity\nmatrix). Since\u00b5is again unconstrained, we can simply set\n\u00b5:= f(x,w) \u2208RM.\nMore generally, we can parametrize the functionf so as to output both\nthe mean\u00b5and the covariance matrix\u03a3, i.e.,\n(\u00b5,\u03a3) := f(x,w) \u2208RM \u00d7RM\u00d7M.\nThe functionf must be designed such that\u03a3 is symmetric and positive\nsemi-definite. This is easy to achieve for instance by parametrizing\n\u03a3 = SS\u22a4for some matrixS.\n3.3.7 Integer regression\nForinteger outcomes, whereY= N, we can use, among other choices,\na Poisson distributionwith mean parameter\u03bb> 0. WhenY follows\na Poisson distribution with parameter\u03bb, we write\nY \u223cPoisson(\u03bb).\nThe PMF is\nP(Y = y) = p\u03bb(y) := \u03bbyexp(\u2212\u03bb)\ny! .\nIt is the probability ofy events occuring in an interval of time. The\nPoisson distribution is frequently used when there is a large number of\npossible events, each of which is rare.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1230, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "29de0072-9fb1-422c-8397-0097cccd5c16": {"__data__": {"id_": "29de0072-9fb1-422c-8397-0097cccd5c16", "embedding": null, "metadata": {"page_label": "72", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "77ed86e7-d1c0-4679-b11c-61a720a686cb", "node_type": "4", "metadata": {"page_label": "72", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "b2c898149e129ca3bd5875b10a3b84b2194793e9bbf685da78687709c90a0ea0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "72 Probabilistic learning\n0 10 20\ny\n0.0\n0.2\nPMF (Y = y)\n= 1\n= 4\n= 10\n0 10 20\ny\n0.0\n0.5\n1.0\nCDF (Y y)\n= 1\n= 4\n= 10\n2.5\n 0.0 2.5\n0\n10\n20\nMean function A\u2032( )\n2.5\n 0.0 2.5\n0\n20\n40\nLoss L( , y)\ny = 1\ny = 4\ny = 10\nFigure 3.4: The Poisson distribution, with mean parameter\u03bb. For the PMF\nand the CDF, the lines between markers are shown for visual aid: the Poisson\ndistribution does not assign probability mass to non-integer values. Its mean function\nis \u03bb = A\u2032(\u03b8) = exp(\u03b8), where \u03b8 is for instance the output of a neural network.\nThe negative log-likelihood leads to thePoisson loss, L(\u03b8,y) = \u2212log p\u03bb(y) =\n\u2212y\u03b8+ exp(\u03b8) + log(y!), which is a convex function of\u03b8. The loss curve is shown for\ny\u2208{1,4,10}.\nThe CDF is\nP(Y \u2264y) =\ny\u2211\ni=0\nP(Y = y).\nThe Poisson distribution implies that theindex of dispersion(the\nratio between variance and mean) is1, since\nE[Y] = V[Y] = \u03bb.\nWhen this assumption is inappropriate, one can use generalized Poisson\ndistributions (Satterthwaite, 1942).\nParameterization using an exponential\nSince the parameter\u03bb of a Poisson distribution needs to be strictly\npositive, we typically use an exponential function as output layer:\n\u03bb:= f(x,w) := exp(g(x,w)) >0,\nwhere g: X\u00d7W\u2192 R.\n3.3.8 Loss functions\nWe now discuss how to learn the model parametersw \u2208 Wfrom\ninput-output pairs(x1,y1),..., (xN,yN).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1305, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f52cbded-985e-4363-afd1-9073a592fcd0": {"__data__": {"id_": "f52cbded-985e-4363-afd1-9073a592fcd0", "embedding": null, "metadata": {"page_label": "73", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b2807ba0-b90d-44c2-b9c7-3188e9e9ead7", "node_type": "4", "metadata": {"page_label": "73", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "76e14a10e626f71aacc7d7410e25872326317127acb25865e45ff6bdb67a0ff7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.3. Probabilistic supervised learning 73\nDeterministic vs. probabilistic approaches\nIn a deterministic approach, if we used a mappingf: X\u00d7W\u2192Y , we\ncould formulate an objective function of the form\nL(w) := 1\nN\nN\u2211\ni=1\n\u2113(f(xi,w),yi),\nwhere \u2113: Y\u00d7Y\u2192 R is a loss function. Unfortunately,L(w) would be\ntypically discontinuousif Yis a discrete output space (as is the case\nin classification), making optimization difficult.\nIn contrast, in the probabilistic approach, we use a mappingf: X\u00d7\nW\u2192 \u039b to distribution parameters, and we can formulate an objective\nof the form\nL(w) := 1\nN\nN\u2211\ni=1\n\u2113(f(xi,w),yi),\nwhere \u2113: \u039b \u00d7Y\u2192 R. This is typically acontinuousobjective, since\n\u039b is typically a continuous set andp\u03bb varies continuously w.r.t. \u03bb\neven ifp\u03bb is a distribution over a discrete setY. In other words, the\nprobabilistic approach is not only powerful for the inference it allows\nus to do (probability, expectation, variance, mode), but also because it\nallows to formulate a continuous and typically differentiable objective\nfunction!\nNegative log-likelihood\nIn the conditional setting briefly reviewed in Section 3.3.1, we can use\nmaximum likelihood estimation (MLE) to estimate the model parame-\ntersw\u2208W off.Givenasetofinput-outputpairs (x1,y1),..., (xN,yN),\nwe choose the model parameters that maximize thelikelihood of the\ndata,\n\u02c6wN := arg max\nw\u2208W\nN\u220f\ni=1\np\u03bbi(yi),\nwhere\n\u03bbi := f(xi,w).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1377, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb4de0c0-5ce3-400b-b5b7-26592e0b8d9a": {"__data__": {"id_": "bb4de0c0-5ce3-400b-b5b7-26592e0b8d9a", "embedding": null, "metadata": {"page_label": "74", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "59ae4b91-05d4-49e7-a8de-e5c79b75f98a", "node_type": "4", "metadata": {"page_label": "74", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "ab3338d2ed76d4110c897b4fbc3fa5cf6012dd68fd166ed87dbd98e3159b2957", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "74 Probabilistic learning\nAgain, this is equivalent to minimizing thenegative log-likelihood,\n\u02c6wN = arg min\nw\u2208W\n\u2212\nN\u2211\ni=1\nlog p\u03bbi(yi).\nIn the notation above, this corresponds to defining the loss function\n\u2113(\u03bbi,yi) := \u2212log p\u03bbi(yi).\nRecovering well-known loss functions\nInterestingly, MLE allows us to recover several popular loss functions.\n\u2022 FortheBernoullidistributionwithparameter \u03bbi = \u03c0i = logistic(g(xi,w)),\nwe have\n\u2212log p\u03bbi(yi) = \u2212[yilog \u03c0i + (1 \u2212yi) log(1\u2212\u03c0i)] ,\nwhich is thebinary logistic lossfunction.\n\u2022 For the categorical distribution with parameters \u03bbi = \u03c0i =\nsoftargmax(g(xi,w)), we have\n\u2212log p\u03bbi(yi) = log\nM\u2211\nj=1\nexp(\u03c0i,j) \u2212\u03c0i,yi\n= logsumexp(\u03c0i) \u2212\u27e8\u03c0i,eyi\u27e9,\nwhich is themulticlass logistic lossfunction, also known as\ncross-entropy loss.\n\u2022 For the normal distribution with mean\u03bbi = \u00b5i = f(xi,w) and\nfixed variance\u03c32\ni, we have\n\u2212log p\u03bbi(yi) = 1\n\u03c32\ni\n(yi \u2212\u00b5i)2 + 1\n2 log \u03c32\ni + 1\n2 log(2\u03c0),\nwhich is, up to constant and with unit variance, thesquared loss\nfunction.\n\u2022 For the Poisson distribution with mean\u03bbi = exp(\u03b8i), where\u03b8i :=\ng(xi,w), we have\n\u2212log p\u03bbi(yi) = \u2212yilog(\u03bbi) + \u03bbi + log(yi!)\n= \u2212yi\u03b8i + exp(\u03b8i) + log(yi!)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1128, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5f6f3059-ed46-48c7-ae2c-13f53c878683": {"__data__": {"id_": "5f6f3059-ed46-48c7-ae2c-13f53c878683", "embedding": null, "metadata": {"page_label": "75", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "614df1ea-acb8-4f1f-9310-0a5351b8f3a6", "node_type": "4", "metadata": {"page_label": "75", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "433e153f3a8828d4b80adbf4ab3f019c1fc08cd2c0d8655a84bde0cc6ad25f11", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.4. Exponential family distributions 75\nwhich is thePoisson lossfunction. The loss function is convex\nw.r.t. \u03bbi and \u03b8i for yi \u22650.\n3.4 Exponential family distributions\n3.4.1 Definition\nThe exponential family is a class of probability distributions, whose\nPMF or PDF can be written in the form\np\u03b8(y) = h(y) exp [\u27e8\u03b8,\u03d5(y)\u27e9]\nexp(A(\u03b8))\n= h(y) exp [\u27e8\u03b8,\u03d5(y)\u27e9\u2212A(\u03b8)] ,\nwhere \u03b8are thenatural or canonical parametersof the distribution.\nThe functionh is known as the base measure. The function\u03d5 is the\nsufficient statistic: it holds all the information aboutyand is used\nto embed y in a vector space. The functionA is the log-partition\nor log-normalizer (see below for a details). All the distributions we\nreviewed in Section 3.3 belong to the exponential family. With some\nabuse of notation, we usep\u03bb for the distribution inoriginal formand\np\u03b8 for the distribution inexponential family form. As we will see,\nwe can go from\u03b8to \u03bband vice-versa. We illustrate how to rewrite a\ndistribution in exponential family form below.\nExample 3.2(Bernoulli distribution). The PMF of the Bernoulli\ndistribution with parameter\u03bb= \u03c0 equals\np\u03bb(y) := \u03c0y(1 \u2212\u03c0)1\u2212y\n= exp(log(\u03c0y(1 \u2212\u03c0)1\u2212y))\n= exp(ylog(\u03c0) + (1\u2212y) log(1\u2212\u03c0))\n= exp(log(\u03c0/(1 \u2212\u03c0))y+ log(1 \u2212\u03c0))\n= exp(\u03b8y\u2212log(1 + exp(\u03b8)))\n= exp(\u03b8y\u2212softplus(\u03b8))\n=: p\u03b8(y).\nTherefore, Bernoulli distributions belong to the exponential family,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1348, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8ab5cd7a-9be0-4bfa-bc2b-fa5852c23ea0": {"__data__": {"id_": "8ab5cd7a-9be0-4bfa-bc2b-fa5852c23ea0", "embedding": null, "metadata": {"page_label": "76", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "30dcaae6-7c4e-46da-afaf-d0bbadf89987", "node_type": "4", "metadata": {"page_label": "76", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "f802afe489e3b81742e0140ea1b3e76109b9022557472a81cbd02b854486fb96", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "76 Probabilistic learning\nwith natural parameter\u03b8= logit(\u03c0) := log(\u03c0/(1 + \u03c0)). Conversely,\nwe have\u03c0= logistic(\u03b8) = 1\n1+exp(\u2212\u03b8) .\nWe rewrite the previously-described distributions in exponential\nfamily form in Table 3.1. This list is non-exhaustive: there are many\nmore distributions in the exponential family! (Barndorff-Nielsen, 2014)\n3.4.2 The log-partition function\nThe log-partition function A is the logarithm of the distribution\u2019s\nnormalization factor. That is,\nA(\u03b8) := log\n\u2211\ny\u2208Y\nh(y) exp [\u27e8\u03b8,\u03d5(y)\u27e9]\nfor discrete random variables and\nA(\u03b8) := log\n\u222b\nY\nh(y) exp [\u27e8\u03b8,\u03d5(y)\u27e9] dy\nfor continuous random variables. We denote the set of valid parameters\n\u0398 := {\u03b8\u2208RM: A(\u03b8) <+\u221e}\u2286 RM.\nWe can conveniently rewriteA(\u03b8) for discrete random variables as\nA(\u03b8) = logsumexp(B(\u03b8)) := log\n\u2211\ny\u2208Y\n[B(\u03b8)]y,\nand similarly for continuous variables. Here, we defined theaffine map\nB(\u03b8) := (\u27e8\u03b8,\u03d5(y)\u27e9+ logh(y))y\u2208Y.\nSince A(\u03b8) is the composition oflogsumexp, a convex function, and of\nB, an affine map, we immediately obtain the following proposition.\nProposition 3.1(Convexity of the log-partition). A(\u03b8) is a convex\nfunction.\nA major property of the log-partition function is that its gradient\ncoincides with the expectation of\u03d5(Y) according top\u03b8.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1223, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "05466cc6-b42b-4ee2-a36a-09a121c43850": {"__data__": {"id_": "05466cc6-b42b-4ee2-a36a-09a121c43850", "embedding": null, "metadata": {"page_label": "77", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5dd6a315-7ca6-4496-ad7c-2eff081c209e", "node_type": "4", "metadata": {"page_label": "77", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "8d790f21c2a9f821bcd8f4d7a14e5269951ca32c44d6173174a66486dfde433b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.4. Exponential family distributions 77\nTable 3.1:Examples of distributions in the exponential family.\nBernoulli Categorical\nY { 0,1} [M]\n\u03bb \u03c0= logistic(\u03b8) \u03c0= softargmax(\u03b8)\n\u03b8 logit(\u03c0) log \u03c0+ exp(A(\u03b8))\n\u03d5(y) y ey\nA(\u03b8) softplus( \u03b8) logsumexp( \u03b8)\nh(y) 1 1\nNormal (location only) Normal (location-scale)\nY R R\n\u03bb \u00b5= \u03b8\u03c3 (\u00b5,\u03c32) = (\u2212\u03b81\n2\u03b82\n, \u22121\n2\u03b82\n)\n\u03b8 \u00b5\n\u03c3 ( \u00b5\n\u03c32 , \u22121\n2\u03c32 )\n\u03d5(y) y\n\u03c3 (y,y2)\nA(\u03b8) \u03b82\n2 = \u00b52\n2\u03c32\n\u2212\u03b82\n1\n4\u03b82\n\u22121\n2 log(\u22122\u03b82) = \u00b52\n\u03c32 + log\u03c3\nh(y)\nexp( \u2212y2\n2\u03c32 )\u221a\n2\u03c0\u03c3\n1\u221a\n2\u03c0\nMultivariate normal Poisson\nY RM N\n\u03bb (\u00b5,\u03a3) = (\u22121\n2 \u03b8\u22121\n2 \u03b81,\u22121\n2 \u03b8\u22121\n2 ) \u03bb= exp(\u03b8)\n\u03b8 (\u03a3\u22121\u00b5,\u22121\n2 \u03a3\u22121) log \u03bb\n\u03d5(y) ( y,yy\u22a4) y\nA(\u03b8) \u22121\n4 \u03b8\u22a4\n1 \u03b8\u22121\n2 \u22121\n2 log |\u22122\u03b82| exp(\u03b8)\n= 1\n2 \u00b5\u22a4\u03a3\u22121\u00b5+ 1\n2 log |\u03a3|\nh(y) (2 \u03c0)\u2212M/2 1/y!", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 684, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "76a71639-4d25-4409-941f-54820b4353ba": {"__data__": {"id_": "76a71639-4d25-4409-941f-54820b4353ba", "embedding": null, "metadata": {"page_label": "78", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "769dee6e-4071-4a68-9c2b-ff0e351b281d", "node_type": "4", "metadata": {"page_label": "78", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "a6793f2be1015f5a017521c0364971b0cd07dc023f2183e06cf9df1540c4772e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "78 Probabilistic learning\nProposition 3.2(Gradient of the log-partition).\n\u00b5(\u03b8) := \u2207A(\u03b8) = EY\u223cp\u03b8[\u03d5(Y)] \u2208M.\nProof. The result follows directly from\n\u2207A(\u03b8) = \u2202B(\u03b8)\u2217\u2207logsumexp(B(\u03b8)) = (\u03d5(y))y\u2208Y softmax(B(\u03b8)).\nThe gradient\u2207A(\u03b8) is therefore often called themean function.\nThe set of achievable means\u00b5(\u03b8) is defined by\nM:= conv(\u03d5(Y)) := {Ep[\u03d5(Y)]: p\u2208P(Y)},\nwhere conv(S) is the convex hull ofS and P(Y) is the set of valid\nprobability distributions overY.\nSimilarly, the Hessian\u22072A(\u03b8) coincides with the covariance matrix\nof \u03d5(Y) according top\u03b8 (Wainwright and Jordan, 2008, Chapter 3).\nWhen the exponential family isminimal, which means that the\nparameters \u03b8uniquely identify the distribution, it is known that\u2207A\nis a one-to-one mapping from\u0398 to M. That is,\u00b5(\u03b8) = \u2207A(\u03b8) and\n\u03b8= (\u2207A)\u22121(\u00b5(\u03b8)).\n3.4.3 Maximum entropy principle\nSuppose we observe the empirical mean\u02c6\u00b5 = 1\nN\n\u2211n\ni=1 \u03d5(yi) \u2208M of\nsome observationsy1,..., yN. How do we find a probability distribution\nachieving this mean? Clearly, such a distribution may not be unique.\nOne way to choose among all possible distributions is by using the\nmaximum entropy principle. Let us define the Shannon entropy by\nH(p) := \u2212\n\u2211\ny\u2208Y\np(y) logp(y)\nfor discrete variables and by\nH(p) := \u2212\n\u222b\nY\np(y) logp(y)dy", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1240, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "14236b03-0bd5-498c-8ac5-7cec66f8f19c": {"__data__": {"id_": "14236b03-0bd5-498c-8ac5-7cec66f8f19c", "embedding": null, "metadata": {"page_label": "79", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cabbca11-7bd4-4788-8231-41180c3d2191", "node_type": "4", "metadata": {"page_label": "79", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "9695db800ca7fe8af175fde4bd938224393d5d24e264b0da7054ab4f23e48b59", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.4. Exponential family distributions 79\nfor continuous variables. This captures the level of \u201cuncertainty\u201d in\np, i.e., it is maximized when the distribution is uniform. Then, the\nmaximum entropy distributionsatisfying the first-order moment\ncondition (i.e., whose expectation matches the empirical mean) is\np\u22c6 := arg max\np\u2208P(Y)\nH(p) s.t. EY\u223cp[\u03d5(Y)] = \u02c6\u00b5.\nIt can be shown that the maximum entropy distribution is necessarily\nin the exponential family with sufficient statistics defined by\u03d5 and its\ncanonical parameters\u03b8 coincide with theLagrange multipliersof\nthe above constraint (Wainwright and Jordan, 2008, Section 3.1).\n3.4.4 Maximum likelihood estimation\nSimilarly as in Section 3.2, to fit the parameters\u03b8\u2208\u0398 of an exponential\nfamily distribution to some i.i.d. observationsy1,..., yN, we can use\nthe MLE principle, i.e.,\n\u02c6\u03b8N = arg max\n\u03b8\u2208\u0398\nN\u220f\ni=1\np\u03b8(yi) = arg min\n\u03b8\u2208\u0398\n\u2212\nN\u2211\ni=1\nlog p\u03b8(yi).\nFortunately, for exponential family distributions, the log probabil-\nity/density enjoys a particularly simple form.\nProposition 3.3(Negative log-likelihood). Thenegativelog-likelihood\nof an exponential family distribution is\n\u2212log p\u03b8(y) = A(\u03b8) \u2212\u27e8\u03b8,\u03d5(y)\u27e9\u2212log h(y).\nIts gradient is\n\u2212\u2207\u03b8log p\u03b8(y) = \u2207A(\u03b8) \u2212\u03d5(y) = EY\u223cp\u03b8[\u03d5(Y)] \u2212\u03d5(y)\nand its Hessian is\n\u2212\u22072\n\u03b8log p\u03b8(y) = \u22072A(\u03b8),\nwhich isindependent of y.\nIt follows from Proposition 3.1 that\u03b8 \u21a6\u2192\u2212log p\u03b8(y) is convex.\nInterestingly, we see that the gradient is theresidual between the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1419, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "11b0ab22-4c2f-4e2b-a463-9a1d2521e86b": {"__data__": {"id_": "11b0ab22-4c2f-4e2b-a463-9a1d2521e86b", "embedding": null, "metadata": {"page_label": "80", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "42bbd4ff-4afd-4a72-aab7-22644c0498ef", "node_type": "4", "metadata": {"page_label": "80", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "f2f1ba032bfe7d4c612fef3125e70e119490ff4052d35fd2470e74aa2116977b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "80 Probabilistic learning\nexpectation of \u03d5(Y) according to the model and the observed\u03d5(y).\nTherefore, the negative log-likelihood of an exponential family distribu-\ntion can be seen as performing first moment matching.\n3.4.5 Probabilistic learning with exponential families\nIn the supervised probabilistic learning setting, we wish to estimate a\nconditional distribution of the formpw(y|x). Given a model function\nf, such as a neural network, a common approach for defining such a\nconditional distribution is by reduction to the unconditional setting,\npw(y|x) := p\u03b8(y) where \u03b8:= f(x,w).\nIn other words, the role off is to produce the parameters ofp\u03b8 given\nsome inputx. It is a function fromX\u00d7W to \u0398. Note thatf must be\ndesigned such that it produces an output in\n\u0398 := {\u03b8\u2208RM: A(\u03b8) <+\u221e}.\nMany times,\u0398 will be the entireRM but this is not always the case.\nFor instance, as we previously discussed, for a multivariate normal\ndistribution, where\u03b8= (\u00b5,\u03a3) = f(x,w), we need to ensure that\u03a3 is\na positive semidefinite matrix.\nTraining\nGiven input-output pairs{(xi,yi)}N\ni=1, we then seek to find the param-\neters wof f(x,w) by minimizing the negative log-likelihood\narg min\nw\u2208W\n\u2212\nN\u2211\ni=1\nlog p\u03b8i(yi) = arg min\nw\u2208W\nN\u2211\ni=1\nA(\u03b8i) \u2212\u27e8\u03b8i,\u03d5(yi)\u27e9\nwhere \u03b8i := f(xi,w). While\u2212log p\u03b8(y) is a convex function of\u03b8 for\nexponential family distributions, we emphasize that\u2212log pf(x,w)(y) is\ntypically a nonconvex function ofw, whenf is anonlinear function,\nsuch as a neural network.\nInference\nOnce we foundwby minimizing the objective function above, there are\nseveral possible strategies to perform inference for a new inputx.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1602, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3d469b8f-a26b-41b0-9e26-6b7e4b6a89e6": {"__data__": {"id_": "3d469b8f-a26b-41b0-9e26-6b7e4b6a89e6", "embedding": null, "metadata": {"page_label": "81", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b420f8ab-1fa2-47dc-8b7a-3f40fc7ded0a", "node_type": "4", "metadata": {"page_label": "81", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "af8c514cd81b0d76bb3411274a180ee0f16ec5a685337ddc71ae4b3fce33e1af", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.5. Summary 81\n\u2022 Expectation. When the goal is to compute the expectation of\n\u03d5(Y), we can use\u2207A(f(x,w)). That is, we compute the distribu-\ntion parameters associated withxby \u03b8= f(x,w) and then we\ncompute the mean by\u00b5= \u2207A(\u03b8). Whenf is linear inw, the\ncomposition \u2207A\u25e6f is called ageneralized linear model.\n\u2022 Probability.When the goal is to compute the probability of a\ncertain y, we can compute the distribution parameters associated\nwith xby \u03b8= f(x,w) and then we can computeP(Y = y|X =\nx) = p\u03b8(y). In the particular case of the categorical distribution\n(of which the Bernoulli distribution is a special case), we point\nout again that the mean and the probability vector coincide:\n\u00b5= p= \u2207A(\u03b8) = softargmax(\u03b8) \u2208\u25b3M.\n\u2022 Other statistics.When the goal is to compute other quantities,\nsuch as the variance or the CDF, we can convert the natural pa-\nrameters \u03b8to the original distribution parameters\u03bb(see Table 3.1\nfor examples). Then, we can use established formulas for the\ndistribution in original form, to compute the desired quantities.\n3.5 Summary\n\u2022 We revieweddiscrete and continuousprobability distributions.\n\u2022 We saw how to fit distribution parameters to data using the\nmaximum likelihood estimation(MLE) principle and saw its\nconnection with theKullback-Leibler divergence.\n\u2022 Instead of designing a model function from the input spaceXto\nthe output spaceY, we saw that we can performprobabilistic\nsupervised learningby designing a model function fromXto\ndistribution parameters\u039b.\n\u2022 Leveraging the so-obtained parametricconditional distribution\nthen allowed us to compute, not only output probabilities, but\nalso various statistics such as the mean and the variance of the\noutputs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1683, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ccd51dc1-643b-4b1b-a589-88eb22bd18db": {"__data__": {"id_": "ccd51dc1-643b-4b1b-a589-88eb22bd18db", "embedding": null, "metadata": {"page_label": "82", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d851f174-6c48-4c92-8d21-01fd0d60047f", "node_type": "4", "metadata": {"page_label": "82", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "32b2bde04973be9a94edff5be5568b9a16b8c94143cdf2e1e30d98ffa3fbc930", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "82 Probabilistic learning\n\u2022 We reviewed theexponential family, a principled generalization\nof numerous distributions, which we saw is tightly connected with\nthe maximum entropy principle.\n\u2022 Importantly, the approaches described in this chapter produce per-\nfectly validcomputation graphs, meaning that we can combine\nthem with neural networks and we can use automatic differentia-\ntion, to compute their derivatives.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 416, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bddff48e-9caf-4123-bb01-690c5c8b01ed": {"__data__": {"id_": "bddff48e-9caf-4123-bb01-690c5c8b01ed", "embedding": null, "metadata": {"page_label": "83", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4e83c8be-7093-44af-a08f-9ccddcf8e5b4", "node_type": "4", "metadata": {"page_label": "83", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "eee34c832af95424e2e0d23d67fc4a2ea59d4cad44681286dd45c8dc33804c8e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Part II\nDifferentiable programs", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 31, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6064286d-8683-4b91-b499-10d4ee9271c4": {"__data__": {"id_": "6064286d-8683-4b91-b499-10d4ee9271c4", "embedding": null, "metadata": {"page_label": "84", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "97cda3fb-7eef-4871-9303-223ec37599df", "node_type": "4", "metadata": {"page_label": "84", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "5e7c82c49db4acf7e2840807fb0890d09f307c65566324c9d72420ced333bbcf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4\nParameterized programs\nNeural networks can be thought of as parameterized programs: programs\nwith learnable parameters. In this chapter, we begin by reviewing how to\nrepresent programs mathematically. We then review several key neural\nnetwork architectures and components.\n4.1 Representing computer programs\n4.1.1 Computation chains\nTo begin with, we consider simple programs that apply asequence of\nfunctions f1,...,f K to an inputs0 \u2208S0. We call such programscom-\nputation chains. For example, an image may go through a sequence of\ntransformations such as cropping, rotation, normalization, and so on. In\nneural networks, the transformations are typically parameterized, and\nthe parameters are learned, leading to feedforward networks, presented\nin Section 4.2. Another example of sequence of functions is a for loop,\npresented in Section 5.8.\n84", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 850, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e689b712-e50e-4f59-9b21-324e5ee38131": {"__data__": {"id_": "e689b712-e50e-4f59-9b21-324e5ee38131", "embedding": null, "metadata": {"page_label": "85", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ef39870b-c6cf-48c3-9dbe-d52fabb4f8dc", "node_type": "4", "metadata": {"page_label": "85", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "d2225bf7ba167d87fce784aefd8ea30f42cfd58711214c3ccc7e8018a0b70725", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.1. Representing computer programs 85\n......\nFigure 4.1:A computation chain is a sequence of function compositions. In the\ngraph above, each intermediate node represents a single function. The first node\nrepresents the input, the last node the output. Edges represent the dependencies of\nthe functions with respect to previous outputs or to the initial input.\nFormally, a computation chain can be written as\ns0 \u2208S0\ns1 := f1(s0) \u2208S1\n...\nsK := fK(sK\u22121) \u2208SK\nf(s0) := sK. (4.1)\nHere, s0 is theinput, sk \u2208Sk is an intermediatestate of the program,\nand sK \u2208SK is the finaloutput. Of course, the domain (input space)\nof fk must be compatible with the image (output space) offk\u22121. That\nis, we should havefk: Sk\u22121 \u2192Sk. We can write a computation chain\nequivalently as\nf(s0) = (fK \u25e6\u00b7\u00b7\u00b7\u25e6 f2 \u25e6f1)(s0)\n= fK(...f 2(f1(s0))).\nA computation chain can be represented by a directed graph, shown\nin Fig. 4.1. The edges in the chain define atotal order. The order is\ntotal, since two nodes are necessarily linked to each other by a path.\n4.1.2 Directed acylic graphs\nIn generic programs, intermediate functions may depend, not only on\nthe previous function output, but on the outputs of several different\nfunctions. Such dependencies are best expressed using graphs.\nA directed graphG= (V,E) is defined by a set ofvertices or\nnodes Vand a set ofedges Edefining directed dependencies between", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1371, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "65e9d45a-e732-4a1f-bdef-2a99e08616df": {"__data__": {"id_": "65e9d45a-e732-4a1f-bdef-2a99e08616df", "embedding": null, "metadata": {"page_label": "86", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "13c72ea9-01bd-48f0-bae0-70e041f91303", "node_type": "4", "metadata": {"page_label": "86", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "653cc1674747ca129f434bc733d6c54681d9019b0df0f27f078bec2f60b96795", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "86 Parameterized programs\nFigure 4.2: Example of a directed acyclic graph. Here the nodes are V =\n{0,1,2,3,4}, the edges areE= {(0,1),(0,2),(0,3),(1,3),(2,3),(1,4),(3,4)}. Parents\nof the node3 are pa(3) = {0,1,2}. Children of node1 are ch(1) = {3,4}. There is\na unique root,0, and a unique leaf,4; 0 \u21923 \u21924 is a path from0 to 4. This is an\nacyclic graph since there is no cycle (i.e., a path from a node to itself). We can order\nnodes 0 and 3 as 0 \u22643 since there is no path from3 to 0. Similarly, we can order1\nand 2 as 1 \u22642 since there is no path from2 to 1. Two possible topological orders of\nthe nodes are(0,1,2,3,4) and (0,2,1,3,4).\nvertices. An edge(i,j) \u2208E is an ordered pair of verticesi \u2208V and\nj \u2208V. It is also denotedi \u2192j, to indicate thatj depends on i. For\nrepresenting inputs and outputs, it will be convenient to useincoming\nhalf-edges \u2192j and outgoing half-edgesi\u2192.\nIn a graphG= (V,E), theparents of a vertexj is the set of nodes\npointing toj, denotedpa(j) := {i: i\u2192j}. Thechildren of a vertexi\nis the set of nodesi is pointing to, that is,ch(i) := {j: i\u2192j}. Vertices\nwithout parents are calledroots and vertices without children are called\nleaves.\nA path from i to j is defined by a sequence of verticesj1,...,j m,\npotentially empty, such thati\u2192j1 \u2192... \u2192jm \u2192j. Anacyclic graph\nis a graph such that there exists no vertexi with a path fromi to i. A\ndirected acyclic graph(DAG) is a graph that is both directed and\nacyclic.\nThe edges of a DAG define apartial orderof the vertices, denoted\ni \u2aafj if there exists a path fromi to j. The order is partial, since\ntwo vertices may not necessarily be linked to each other by a path.\nNevertheless, we can define a total order called atopological order:\nany order such thati\u2264j if and only if there is no path fromj to i.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1771, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2addd8ea-b729-4902-9a4f-3747e8fbde68": {"__data__": {"id_": "2addd8ea-b729-4902-9a4f-3747e8fbde68", "embedding": null, "metadata": {"page_label": "87", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4e1c2e1a-91be-46a1-84c5-f4e2539b1bb5", "node_type": "4", "metadata": {"page_label": "87", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "1c25d44c87f1c58629b76d85f21bb18e53690436811d7bf195a1a88ff9d910cd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.1. Representing computer programs 87\nFigure 4.3: Representation of f(x1,x2) = x2ex1\n\u221ax1 + x2ex1 as a DAG, with\nfunctions and variables as nodes. Edges indicate function and variable dependencies.\nThe functionf is decomposed as8 elementary functions in topological order.\n4.1.3 Computer programs as DAGs\nWe assume that a program defines a mathematically valid function (a.k.a.\npure function): the program should return identical values for identical\narguments and should not have any side effects. We also assume that the\nprogram halts, i.e., that it terminates in afinite number of steps. As\nsuch a program is made of a finite number of intermediate functions and\nintermediate variables, the dependencies between functions and variables\ncan be expressed using a directed acyclic graph (DAG). Without loss of\ngenerality, we make the following simplifying assumptions:\n1. There is a single inputs0 \u2208S0.\n2. There is a single outputsK \u2208SK.\n3. Each intermediate functionfk in the program outputs a single\nvariable sk \u2208Sk.\nWe number the nodes asV:= {0,1,...,K }. Node0 is the root, corre-\nsponding to the inputs0 \u2208S0. NodeK is the leaf, corresponding to the\nfinal outputsK \u2208SK. Because of the third assumption above, apart\nfrom s0, each variablesk is inbijection with a functionfk. Therefore,\nnode 0 represents the inputs0, and nodes1,...,K represent both a\nfunction fk and an output variablesk.\nEdges in the DAG represent dependencies. The parentsi1,...,i pk :=\npa(k) of nodek, wherepk := |pa(k)|, indicate the variablesspa(k) :=\nsi1 ,..., sipk that the functionfk needs to perform its computation. Put", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1599, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "015fa942-fcba-4baf-9034-abde3cd25834": {"__data__": {"id_": "015fa942-fcba-4baf-9034-abde3cd25834", "embedding": null, "metadata": {"page_label": "88", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "365ae8ec-3416-4258-a94c-262e762ac13d", "node_type": "4", "metadata": {"page_label": "88", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "d6633ea4f8ca63bb973f078b9a8c491df58f1cf1da9800f85cd5ad3a4f0287a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "88 Parameterized programs\nAlgorithm 4.1Executing a program\nFunctions:f1,...,f K in topological order\nInput: input s0 \u2208S0\n1: for k:= 1,...,K do\n2: Retrieve parent nodes(i1,...,i pk) := pa(k)\n3: Compute sk := fk(spa(k)) := fk(si1 ,..., sipk)\n4: Output: f(s0) := sK\ndifferently, the parentsi1,...,i pk indicate the functionsfi1 ,...,f ipk that\nneed to be evaluated, prior to evaluatingfk. An example of computation\ngraph in our formalism is presented in Fig. 4.3.\nExecuting a program\nTo execute a program, we need to ensure that we evaluate the intermedi-\nate functions in the correct order. Therefore, we assume that the nodes\n0,1,...,K are in a topological order (if this is not the case, we need\nto perform a topological sort first). We can then execute a program by\nevaluating fork\u2208[K]\nsk := fk(spa(k)) := fk(si1 ,..., sipk) \u2208Sk.\nNote that we can either viewfk as a single-input function ofspa(k),\nwhich is a tuple of elements, or as a multi-input function ofsi1 ,..., sipk.\nThe two views are essentially equivalent.\nThe procedure for executing a program is summarized in Algo-\nrithm 4.1.\nDealing with multiple program inputs or outputs\nWhen a program has multiple inputs, we can always group them into\ns0 \u2208S0 as s0 = (s0,1,..., s0,N0 ) with S0 = (S0,1 \u00d7\u00b7\u00b7\u00b7\u00d7S 0,N0 ), since\nlater functions can always filter out what elements ofs0 they need.\nLikewise, if an intermediate functionfk has multiple outputs, we can\nalways group them as a single outputsk = (sk,1,..., sk,Nk) with Sk =\n(Sk,1 \u00d7\u00b7\u00b7\u00b7\u00d7S k,Nk), since later functions can filter out the elements of\nsk that they need.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1572, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3c44a254-aed9-43af-a7b9-8569e1f91a8b": {"__data__": {"id_": "3c44a254-aed9-43af-a7b9-8569e1f91a8b", "embedding": null, "metadata": {"page_label": "89", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "67ce87c6-2b05-4724-baa2-e4465c0b1b3e", "node_type": "4", "metadata": {"page_label": "89", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "2ec86bb5ccf359ab6545b09f75fe61d380fb585a4147b2d615c345d896f1d2f6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.1. Representing computer programs 89\nFigure 4.4:Two possible representations of a program.Left: Functions and output\nvariables are represented by the same nodes.Right: functions and variables are\nrepresented by a disjoint set of nodes.\nAlternative representation: bipartite graphs\nIn our formalism, because a functionfk always has a single outputsk, a\nnode kcan be seen as representing both the variablesk and the function\nfk. Alternatively, as shown in Fig. 4.4, we can represent variables and\nfunctions as separate nodes, that is, using abipartite graph. This\nformalism is akin tofactor graphs(Freyet al., 1997; Loeliger, 2004)\nused in probabilistic modeling, but with directed edges. One advantage\nof this formalism is that it allows functions to explicitly have multiple\noutputs. We focus on our formalism for simplicity.\n4.1.4 Arithmetic circuits\nArithmetic circuits are one of the simplest examples of computation\ngraph, originating fromcomputational complexity theory. Formally,\nan arithmetic circuit over a fieldF, such as the realsR, is a directed\nacyclic graph (DAG) whose root nodes are elements ofF and whose\nfunctions fk are either + or \u00d7. The latter are often called gates.\nContrary to the general computation graph case, because eachfk is\neither + or \u00d7, it is important to allow the graph to have several root\nnodes. Root nodes can be either variables or constants, and should\nbelong toF.\nArithmetic circuits can be used to computepolynomials. There\nare potentially multiple arithmetic circuits for representing a given\npolynomial. One important question is then to find the most efficient\narithmetic circuit for computing a given polynomial. To compare arith-\nmetic circuits representing the same polynomial, an intuitive notion of\ncomplexity is thecircuit size, as defined below.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1798, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1b23afaf-d168-4332-a082-e8dddce986f4": {"__data__": {"id_": "1b23afaf-d168-4332-a082-e8dddce986f4", "embedding": null, "metadata": {"page_label": "90", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "97ebb57b-dfd3-475f-9533-61f427719cfa", "node_type": "4", "metadata": {"page_label": "90", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "0abe6b72b01d70a00d1af8d3e93d5c336314931e1f06342895509f7ecdd1ce80", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "90 Parameterized programs\nDefinition 4.1(Circuit and polynomial sizes). The size S(C) of a\ncircuit C is the number of edges in the directed acyclic graph\nrepresenting C. The sizeS(f) of a polynomialf is the smallestS(C)\namong allCrepresenting f.\nFor more information on arithmetic circuits, we refer the reader to\nthe monograph of Chenet al.(2011).\n4.2 Feedforward networks\nA feedforward network can be seen as a computation chain withpa-\nrameterized functions fk,\ns0 := x\ns1 := f1(s0,w1)\ns2 := f2(s1,w2)\n...\nsK := fK(sK\u22121,wK),\nfor a given inputx\u2208X and learnable parametersw1,..., wK \u2208\nW1 \u00d7\u00b7\u00b7\u00b7\u00d7W K. Each functionfk is called alayerand eachsk \u2208Sk\ncan be seen as anintermediate representationof the inputx. The\ndimensionality ofSk is known as thewidth (or number of hidden units)\nof layerk. A feedforward network defines a functionsK =: f(x,w) from\nX\u00d7W to SK, wherew:= (w1,..., wK) \u2208W := W1 \u00d7... \u00d7WK.\nGiven such a parameterized program, we can learn the parameters\nby adjustingwto fit some data. For instance, given a dataset of(xi,yi)\npairs, we may minimize the squared loss\u2225yi\u2212f(xi,w)\u22252\n2 on average over\nthe data, w.r.t.w. The minimization of such a loss requires accessing\nits gradients with respect tow.\n4.3 Multilayer perceptrons\n4.3.1 Combining affine layers and activations\nIn the previous section, we did not specify how to parametrize the\nfeedforward network. A typical parametrization, called the multilayer", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1416, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7639b672-c2dc-4a9f-9bb6-2bf69e12099a": {"__data__": {"id_": "7639b672-c2dc-4a9f-9bb6-2bf69e12099a", "embedding": null, "metadata": {"page_label": "91", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d5e68fae-a5ea-4609-a1c9-eca90254087f", "node_type": "4", "metadata": {"page_label": "91", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "c655176b836c25c2f84e4c51ac6459908516323b67336f5866aff8183c8cfe54", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.3. Multilayer perceptrons 91\nperceptron (MLP), usesfully-connected (also calleddense) layers of\nthe form\nsk = fk(sk\u22121,wk) := ak(Wksk\u22121 + bk),\nwhere we defined the tuplewk := (Wk,bk) and where we assumed that\nWk and bk are a matrix and vector of appropriate size. We can further\ndecompose the layer into two functions. The functions\u21a6\u2192Wks+ bk\nis called an affine layer. The functionv \u21a6\u2192ak(v) is a parameter-free\nnonlinearity, often called anactivation function(see Section 4.4).\nThe value\u03b1k := Wksk\u22121 + bk is often called thepre-activation value\nand the valueak(\u03b1k) is theactivation value.\nMore generally, we may replace the matrix-vector productWksk\u22121\nby any parametrized linear function ofsk\u22121. For example,convolu-\ntional layersuse the convolution of an inputsk\u22121 with some filters\nWk, seen as a linear map.\nRemark 4.1(Dealing with multiple inputs). Sometimes, it is neces-\nsary to deal with networks of multiple inputs. For example, sup-\npose we want to design a functiong(x1,x2,wg), where x1 \u2208X1\nand x2 \u2208X2. A simple way to do so is to use the concatenation\nx:= x1 \u2295x2 \u2208X2 \u2295X2 as input to a networkf(x,wf). Alterna-\ntively, instead of concatenatingx1 and x2 at the input layer, they\ncan be concatenated after having been through one or more hidden\nlayers.\n4.3.2 Link with generalized linear models\nWhen the depth isK = 1 (only one layer), the output of an MLP is\ns1 = a1(W1x+ b1).\nThis is called ageneralized linear model(GLM); see Section 3.4.\nTherefore, MLPs include GLMs as a special case. In particular, when\na1 is the softargmax (see Section 4.4), we obtain (multiclass) logistic\nregression. For general depthK, the output of an MLP is\nsK = aK(WKsK\u22121 + bK).\nThis can be seen as a GLM on top oflearned representationsK\u22121\nof the inputx. This is the main appeal of MLPs: they learn the feature", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1800, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6024d058-6e00-4a6d-9be0-36929fd94629": {"__data__": {"id_": "6024d058-6e00-4a6d-9be0-36929fd94629", "embedding": null, "metadata": {"page_label": "92", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "86c1e73a-1941-45c8-ad8b-19fc1e7ba195", "node_type": "4", "metadata": {"page_label": "92", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "7a90bfea4c05f9fafb4c57e743be766577c221eae063157da41d7ac9db2f47c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "92 Parameterized programs\nrepresentation and the output model at the same time! We will see that\nMLPs can also be used as subcomponents in other architectures.\n4.4 Activation functions\nAs we saw in Section 4.3, feedforward networks typically use an ac-\ntivation functionak at each layer. In this section, we present various\nnonlinearities from scalar to scalar or from vector to scalar. We also\npresent probability mappings that can be used as such activations.\n4.4.1 ReLU and softplus\nMany activations arescalar-to-scalar functions, but they can also\nbe applied to vectors in an element-wise fashion. TheReLU (rectified\nlinear unit) is a popular nonlinearity defined as thenon-negative part\nof its input\nrelu(u) := max(u,0) =\n\uf8f1\n\uf8f2\n\uf8f3\nu, u \u22650\n0, u< 0\n.\nIt is a piecewise linear function and includes a kink atu= 0. A multilayer\nperceptron with ReLU activations is called arectifier neural network.\nThe layers take the form\nsk = relu(Aksk\u22121 + bk),\nwhere the ReLU is applied element-wise. The ReLU can be replaced\nwith a smooth approximation (i.e., without kinks), called thesoftplus\nsoftplus(u) := log(1 + eu).\nUnlike the ReLU, it is always strictly positive. Other smoothed variants\nof the ReLU are possible, see Section 13.4.\n4.4.2 Max pooling and log-sum-exp\nMany activations arevector-to-scalar functions: they reduce vec-\ntors to a scalar value. This scalar value can be seen as a statistic,\n\u201csummarizing\u201d the vector.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1419, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "84881dad-2ed6-4848-9f96-ddf3798175ce": {"__data__": {"id_": "84881dad-2ed6-4848-9f96-ddf3798175ce", "embedding": null, "metadata": {"page_label": "93", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c85bb471-d654-49c6-8deb-11d545d8fa1f", "node_type": "4", "metadata": {"page_label": "93", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "c3120eabb8ca76accdd95cf82d0e5b4ce5fa6925c5583b4658072255668d7ac6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.4. Activation functions 93\n0\n1\n2\n3\n4\n5\nu2\n012345\nu1\n0\n1\n2\n3\n4\n5\n6\nm\nax(u1,u2)\nmax\n0\n1\n2\n3\n4\n5\nu2\n012345\nu1\n0\n2\n4\n6\nsoftm\nax(u1,u2)\nsoftmax\nFigure 4.5:The maximum operator is a piecewise linear function. The log-sum-exp\n(a.k.a. softmax) is a smoothed approximation.\nMax pooling\nAn example of vector-to-scalar reduction is the maximum value, also\nknown asmax pooling. Given a vectoru\u2208RM, it is defined as\nmax(u) := max\nj\u2208[M]\nuj.\nLog-sum-exp as a soft maximum\nAnother example of vector-to-scalar reduction is thelog-sum-exp,\nlogsumexp(u) := softmax(u) := log\nM\u2211\nj=1\neuj.\nAs illustrated in Fig. 4.5, it is known to behave like asoft maximum.\nThe log-sum-exp can be seen as a generalization of the softplus, as we\nhave for allu\u2208R\nlogsumexp((u,0)) = softplus(u).\nA numerically stable implementation of the log-sum-exp is given by\nlogsumexp(u) = logsumexp(u\u2212c1) + c,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 861, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "571ef21b-821c-4eeb-8cb0-adef7773b524": {"__data__": {"id_": "571ef21b-821c-4eeb-8cb0-adef7773b524", "embedding": null, "metadata": {"page_label": "94", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0ce6612b-71b1-4463-83b2-590fa497db15", "node_type": "4", "metadata": {"page_label": "94", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "cc7d0c27244700ad5b720ff27fb921f9ed89df0f472135194981dcf3d337713a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "94 Parameterized programs\nwhere c:= maxj\u2208[M] uj.\nMore generally, we can introduce a temperature parameter\u03b3 >0\nlogsumexp\u03b3(u) = \u03b3\u00b7logsumexp(u/\u03b3).\nIt can be shown that for allu\u2208RM,\nmax(u) \u2264logsumexp\u03b3(u) \u2264max(u) + \u03b3\u00b7log(M).\nTherefore, logsumexp\u03b3(u) \u2192max(u) as \u03b3 \u21920. Other definitions of\nsoft maximum are possible; see Section 13.5.\nLog-sum-exp as a log-domain sum\nBesides its use as a soft maximum, the log-sum-exp often arises for\ncomputing sums in the log domain. Indeed, suppose we want to compute\ns:= \u2211M\nj=1 ui, whereui >0. If we define\u02dcui := log ui and \u02dcs:= log s, we\nthen have\n\u02dcs= log\nM\u2211\nj=1\nexp(\u02dcui).\nWritten differently, we have the identity\nlog\n(M\u2211\ni=1\nui\n)\n= logsumexp(log(u)).\nWe can therefore see the log-sum-exp as the sum counterpart of the\nidentity for products\nlog\n\uf8eb\n\uf8ed\nM\u220f\nj=1\nui\n\uf8f6\n\uf8f8=\nM\u2211\ni=1\nlog(ui).\nAs an example, we use the log-sum-exp to perform the forward-backward\nalgorithm in the log-domain in Section 10.7.1.\n4.4.3 Sigmoids: binary step and logistic functions\nOftentimes, we want to map a real value to a number in[0,1], that can\nrepresent the probability of an event. For that purpose, we generally\nuse sigmoids. A sigmoid is a function with a characteristic \u201cS\u201d-shaped\ncurve. These functions arescalar-to-scalar probability mappings: they\nare used to squash real values to[0,1].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1300, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cee5b5c9-8bc6-40b8-9202-211f128b9280": {"__data__": {"id_": "cee5b5c9-8bc6-40b8-9202-211f128b9280", "embedding": null, "metadata": {"page_label": "95", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ab767c0-571e-4650-aee7-c658dc20b3f7", "node_type": "4", "metadata": {"page_label": "95", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "501ecfa11d40be0f89bc566142ca880936874498463a8edd35ed1c41241e1d29", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.4. Activation functions 95\nBinary step function\nAn example is thebinary step function, also known asHeaviside\nstep function,\nstep(u) :=\n\uf8f1\n\uf8f2\n\uf8f3\n1, u \u22650\n0, u< 0\n.\nIt is a mapping fromR to {0,1}. Unfortunately, it has a discontinuity: a\njump in its graph atu= 0. Moreover, because the function is constant\nat all other points, it has zero derivative at these points, which makes it\ndifficult to use as part of a neural network trained with backpropagation.\nLogistic function\nA better sigmoid is thelogistic function, which is a mapping fromR\nto (0,1) and is defined as\nlogistic(u) := 1\n1 + e\u2212u\n= eu\n1 + eu\n= 1\n2 + 1\n2 tanh\n(u\n2\n)\n.\nItmaps (\u2212\u221e,0) to(0,0.5),[0,+\u221e) to[0.5,1) anditsatisfies logistic(0) =\n0.5. It can therefore be seen as mapping from real values to probability\nvalues. The logistic can be seen as a differentiable approximation to the\ndiscontinuous binary step functionstep(u). The logistic function can\nbe shown to be the derivative of softplus, i.e., for allu\u2208R\nsoftplus\u2032(u) = logistic(u).\nTwo important properties of the logistic function are that for allu\u2208R\nlogistic(\u2212u) = 1 \u2212logistic(u)\nand\nlogistic\u2032(u) = logistic(u) \u00b7logistic(\u2212u)\n= logistic(u) \u00b7(1 \u2212logistic(u)).\nOther sigmoids are possible; see Section 13.6.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1228, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5f4f41e5-4b82-48b1-b78c-3e9fb16c6e67": {"__data__": {"id_": "5f4f41e5-4b82-48b1-b78c-3e9fb16c6e67", "embedding": null, "metadata": {"page_label": "96", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a280a15a-39a7-4362-af67-477e2cdf9175", "node_type": "4", "metadata": {"page_label": "96", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "ac16e9371d46f5612ffffeb6cd806dcd917cf4f409f524b51dfccd77a40710c5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "96 Parameterized programs\n4.4.4 Probability mappings: argmax and softargmax\nIt is often useful to transform a real vector into a vector of probabilities.\nThis is a mapping fromRM to the probability simplex, defined by\n\u25b3M :=\n\uf8f1\n\uf8f2\n\uf8f3\u03c0\u2208RM: \u2200j \u2208[M], \u03c0j \u22650,\nM\u2211\nj=1\n\u03c0j = 1\n\uf8fc\n\uf8fd\n\uf8fe.\nTwo examples of suchvector-to-vectorprobability mappings are the\nargmax and the softargmax.\nArgmax\nThe argmax operator is defined by\nargmax(u) := \u03d5\n(\narg max\nj\u2208[M]\nuj\n)\n\u2208{e1,..., eM},\nwhere \u03d5(j) denotes the one-hot encoding of an integerj \u2208[M], that is,\n\u03d5(j) := (0,..., 0, 1\ued19\ued18\ued17\ued1a\nj\n,0,..., 0) = ej \u2208{0,1}M.\nThis mapping puts all the probability mass onto a single coordinate (in\ncase of ties, we pick a single coordinate arbitrarily). Unfortunately, this\nmapping is a discontinuous function.\nSoftargmax\nAs a differentiable everywhere relaxation, we can use thesoftargmax\ndefined by\nsoftargmax(u) := exp(u)\n\u2211M\nj=1 exp(uj) \u2208relint(\u25b3M).\nThis operator is commonly known in the literature assoftmax but this\nis a misnomer: this operator really defines a differentiable relaxation\nof the argmax. The output of the softargmax belongs to the relative\ninterior of the probability simplexrelint(\u25b3M) = {\u03c0 \u2208\u25b3M: \u03c0 > 0},\nmeaning that it can never reach the borders of the simplex. If we denote", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1250, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a688b663-9e5c-4989-b1db-09b70d6e5893": {"__data__": {"id_": "a688b663-9e5c-4989-b1db-09b70d6e5893", "embedding": null, "metadata": {"page_label": "97", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "31ad2038-8762-47b8-9cd4-f0a45f1f3ad7", "node_type": "4", "metadata": {"page_label": "97", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "5e21b616f773b1a5510db06f80c8825f7f284fd6efe15c8cc98ecc35066513f9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.5. Normalization layers 97\n\u03c0= softargmax(u), this means that\u03c0j \u2208(0,1), that is,\u03c0j can never\nbe exactly0 or 1. Thesoftargmax is the gradient of log-sum-exp,\n\u2207logsumexp(u) = softargmax(u).\nThe softargmax can be seen as a generalization of the logistic function,\nas we have for allu\u2208R\n[softargmax((u,0))]1 = logistic(u).\nRemark 4.2(Degrees of freedom and invertibility of softargmax). The\nsoftargmax operator satisfies the property for allu\u2208RM and c\u2208R\n\u03c0:= softargmax(u) = softargmax(u+ c1).\nThis means that the softargmax operator hasM \u22121 degrees of\nfreedom and is a non-invertible function. However, due to the\nabove property, without loss of generality, we can imposeu\u22a41 =\u2211M\ni=1 ui = 0 (if this is not the case, we simply doui \u2190ui \u2212\u00afu,\nwhere \u00afu:= 1\nM\n\u2211M\nj=1 uj). Using this constraint together with\nlog \u03c0i = ui \u2212log\nM\u2211\nj=1\nexp(uj),\nwe then obtain\nM\u2211\ni=1\nlog \u03c0i = \u2212Mlog\nM\u2211\nj=1\nexp(uj)\nso that\nui = [softargmax\u22121(\u03c0)]i = log \u03c0i \u2212 1\nM\nM\u2211\nj=1\nlog \u03c0j.\n4.5 Normalization layers\nIntermediate states in neural networks, such as activations, can often\nattain a wide range of different values, potentially making it difficult for\ngradient descent to converge. To remedy this issue, we can introduce", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1188, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "daedae2f-2fd7-4b3d-a6f6-1e4cc4b7c59c": {"__data__": {"id_": "daedae2f-2fd7-4b3d-a6f6-1e4cc4b7c59c", "embedding": null, "metadata": {"page_label": "98", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "040cb028-0e9f-43e7-b69a-952857ba6040", "node_type": "4", "metadata": {"page_label": "98", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "e007d978583d14edc659010ec6858090e775ad2379654a899163d4dc5521a6e1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "98 Parameterized programs\nargmax\nsparseargmax softargmax\nsigmoid\n[1, 0, 0]\n[0, 1, 0]\n[0, 0, 1]\nFigure 4.6:The argmax, the softargmax and the sparseargmax (see Section 13.7) are\nmapping fromRM to the probability simplex\u25b3M := {\u03c0\u2208RM\n+ : \u27e8\u03c0,1\u27e9= 1}. More\nprecisely, the argmax maps to vertices of the probability simplex, the set of one-hot\nvectors {e1,..., eM}, the softargmax maps to the relative interiorrelint(\u25b3M) =\n{\u03c0\u2208\u25b3M: \u03c0>0}and the sparseargmax maps to the entire probability simplex\u25b3M,\nincluding sparse probability vectors. Sigmoids applied element-wise return vectors in\nthe hypercube[0,1]M, whose vertices are the2M vectors in{0,1}M.\nnormalization layers at suitable locations in the network. In this section,\nwe present the two most popular ones: batch normalization and layer\nnormalization.\n4.5.1 Batch normalization\nSuppose we are given a batch of intermediate variables (such as activa-\ntions or states)s1,..., sB \u2208RD, wheresi := (si,1,...,s i,D), obtained\nby applying some function toB samples drawn from the training set\n(we omit the dependency on the layer indexk for clarity). In batch\nnormalization (Ioffe and Szegedy, 2015), we normalize the values by\ncalculating thestandard score(a.k.a. z-score) across batch samples,\n\u00b5j := 1\nB\nB\u2211\ni=1\nsi,j \u2200j \u2208[D]\n\u03c32\nj := 1\nB\nB\u2211\ni=1\n(si,j \u2212\u00b5j)2 \u2200j \u2208[D]\n\u02c6si,j := si,j \u2212\u00b5j\n\u03c3j\ni\u2208[B],j \u2208[D].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1338, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ce583192-0484-4e10-868d-c445c8e2517b": {"__data__": {"id_": "ce583192-0484-4e10-868d-c445c8e2517b", "embedding": null, "metadata": {"page_label": "99", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ab952f3-e453-4e7f-a756-8e734b4d1d75", "node_type": "4", "metadata": {"page_label": "99", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "3417777f40295b318e51e31ea52c0025cac1e201c438a823818bfa7e4f71c118", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.5. Normalization layers 99\nHere, the means\u00b5j and the standard deviations\u03c3j are computed for\neach featurej \u2208[D] across samplesi\u2208[B] in the batch. In practice,\nwe often add a small value\u03b5 >0 to \u03c3j to avoid division by zero or\nnumerical instabilities. Moreover, we often rescale the values as\n\u02dcsi,j := \u03b2j + \u03b3j\u02c6si,j i\u2208[B],j \u2208[D],\nwhere the means \u03b2 := ( \u03b21,...,\u03b2 D) and standard deviations \u03b3 :=\n(\u03b31,...,\u03b3 D) are learnable parameters.\nOne issue with batch normalization is that the means\u00b5j and stan-\ndard deviations\u03c3j cannot be computed at inference time, as there is no\nnotion of training batch (a single sample would lead to a variance of0).\nTo address this issue, we can estimate means\u02c6\u00b5j and standard deviations\n\u02c6\u03c3j across the whole training set during the course of training, usually\nusing arunning average. A pratical batch normalization implemen-\ntation therefore needs to maintainD mean and standard deviation\nstatistics, so as to be able to use them at inference time.\n4.5.2 Layer normalization\nAs an alternative, in layer normalization (Baet al., 2016), we instead\nstandardize the values by summing across features,\n\u00b5i := 1\nD\nD\u2211\nj=1\nsi,j \u2200i\u2208[B]\n\u03c32\ni := 1\nD\nD\u2211\nj=1\n(si,j \u2212\u00b5i)2 \u2200i\u2208[B]\n\u02c6si,j := si,j \u2212\u00b5i\n\u03c3i\ni\u2208[B],j \u2208[D].\nThis time, the means\u00b5i and the standard deviations\u03c3i are computed\nfor each samplei\u2208[B] across featuresj \u2208[D] (as before, we often add\na small value\u03b5to \u03c3i). Similarly to batch normalization, we often rescale\nthe values as\n\u02dcsi,j := \u03b2j + \u03b3j\u02c6si,j i\u2208[B],j \u2208[D],\nwhere the means \u03b2 := ( \u03b21,...,\u03b2 D) and standard deviations \u03b3 :=\n(\u03b31,...,\u03b3 D) are learnable parameters. A key advantage of layer nor-\nmalization compared to batch normalization is that it is well defined", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1683, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "93547004-12f5-442e-a393-2a8b02f99aaf": {"__data__": {"id_": "93547004-12f5-442e-a393-2a8b02f99aaf", "embedding": null, "metadata": {"page_label": "100", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "003de270-69d6-448c-baa2-6b7e46152a54", "node_type": "4", "metadata": {"page_label": "100", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "3eee3b29a20ec1140799221eee82e4193a8b4996458524d04c53cc0045fdf92d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "100 Parameterized programs\nat inference time, since it is applied on a per-sample basis and does\nnot rely on the notion of training batch. As a result, we can view layer\nnormalization as a function that to anysi \u2208RD (regardless of whether\nit is part of a batch or not) associates\n\u02dcsi := LayerNorm(si).\n4.6 Residual neural networks\nWe now discuss another feedforward network parametrization: residual\nneural networks. Consider a feedforward network withK + 1 layers\nf1, ... , fK, fK+1. Surely, as long asfK+1 can exactly represent the\nidentity function, the set of functions that this feedforward network\ncan express should be a superset of the functions thatf1, ... , fK can\nexpress. In other words, depth should in theory not hurt the expressive\npower of feedforward networks. Unfortunately, the assumption that each\nfk can exactly represent the identity function may not hold in practice.\nThis means that deeper networks can sometimes be more difficult to\ntrain than shallower ones, making the accuracy saturate or degrade as\na function of depth.\nThe key idea of residual neural networks (Heet al., 2016) is to design\nlayers fk, calledresidual blocks, that make it easier to represent the\nidentity function. Formally, a residual block takes the form\nsk = fk(sk\u22121,wk) := sk\u22121 + hk(sk\u22121,wk).\nThe functionhk is calledresidual, since it models the differencesk \u2212\nsk\u22121. The addition withsk\u22121 is often called askip connection. As\nlong as it is easy to adjustwk so thathk(sk\u22121,wk) = 0, fk can freely\nbecome the identity function. For instance, if we use\nhk(sk\u22121,wk) := Ckak(Wksk\u22121 + bk) + dk,\nwhere wk := (Wk,bk,Ck,dk), it suffices to setCk and dk to a zero\nmatrix and vector. Residual blocks are known to remedy the so-called\nvanishing gradient problem.\nMany papers and software packages include an additional activation\nand instead define the residual block as\nsk = fk(sk\u22121,wk) := ak(sk\u22121 + hk(sk\u22121,wk)),", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1901, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "267790ea-e84b-4f31-acfb-c44038774e76": {"__data__": {"id_": "267790ea-e84b-4f31-acfb-c44038774e76", "embedding": null, "metadata": {"page_label": "101", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "74f297da-b32f-4645-8dfa-1dfd40bc2a55", "node_type": "4", "metadata": {"page_label": "101", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "8b6ac7de388215e02a6bddca8e4f7c960ba356eb74a934e5cb6a82f413df4e9e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.7. Recurrent neural networks 101\nwhere ak is typically chosen to be the ReLU activation. Whether to\ninclude this additional activation or not is essentially a modelling choice.\nIn practice, residual blocks may also include additional operations such\nas batch norm and convolutional layers.\n4.7 Recurrent neural networks\nRecurrent neural networks (RNNs) are a class of neural networks that\noperate on sequences of vectors, either as input, output or both. Their\nactual parametrization depends on the setup but the core idea is to\nmaintain astate vectorthat is updated from step to step by a recursive\nfunction that usesshared parametersacross steps. Unrolling this\nrecursion defines a valid computational graph, as we will see in Chapter 8.\nWe distinguish between the following setups illustrated in Fig. 4.7:\n\u2022 Vector to sequence (one to many):\nfd: RD \u00d7RP \u2192RL\u00d7M\n\u2022 Sequence to vector (many to one):\nfe: RL\u00d7D \u00d7RP \u2192RM\n\u2022 Sequence to sequence (many to many, aligned):\nfa: RL\u00d7D \u00d7RP \u2192RL\u00d7M\n\u2022 Sequence to sequence (many to many, unaligned):\nfu: RL\u00d7D \u00d7RP \u2192RL\u2032\u00d7M\nwhere L stands for length. Note that we use the same number of\nparameters P for each setup for notational convenience, but this of\ncourse does not need to be the case. Throughout this section, we use\nthe notationp1:L := (p1,..., pL) for a sequence ofL vectors.\n4.7.1 Vector to sequence\nIn this setting, we define adecoder function p1:L = fd(x,w) from an\ninput vectorx\u2208RD and parametersw\u2208RP to anoutput sequence\np1:L \u2208RL\u00d7M. This is for instance useful for image caption generation,\nwhere a sentence (a sequence of word embeddings) is generated from", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1601, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ac4471b6-e7e4-4406-8dff-68e58a4e498e": {"__data__": {"id_": "ac4471b6-e7e4-4406-8dff-68e58a4e498e", "embedding": null, "metadata": {"page_label": "102", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4e88aa98-6890-4e4e-b229-6a1c70407791", "node_type": "4", "metadata": {"page_label": "102", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "a94f8e2a3cadb9bc3460d08fec6e71984925c9b18f0cdfb79dd5bcd43a1f684c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "102 Parameterized programs\n(a) One to many (decoder)\n (b) Many to one (encoder)\n(c) Sequence to sequence aligned\n (d) Sequence to sequence unaligned\nFigure 4.7:Recurrent neural network architectures", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 198, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e3a5f1fd-539c-4d4e-9731-cece5e1cd013": {"__data__": {"id_": "e3a5f1fd-539c-4d4e-9731-cece5e1cd013", "embedding": null, "metadata": {"page_label": "103", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "250a65c9-cec0-4c1d-aefb-0779d5e80d4a", "node_type": "4", "metadata": {"page_label": "103", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "397306eff04a4721264a62234cd8e7cf6dfbd467fcaa669529461b1cf7b1f55c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.7. Recurrent neural networks 103\nan image (a vector of pixels). Formally, we may definep1:L := fd(x,w)\nthrough the recursion\nzl := g(x,zl\u22121,wg) l\u2208[L]\npl := h(zl,wh) l\u2208[L].\nwhere w:= (wg,wh,z0). The goal ofgis to update the currentdecoder\nstate zl given the inputx, and the previous decoder statezl\u22121. The\ngoal ofh is to generate the outputpl given the current decoder state\nzl. Importantly, the parameters ofg and h are shared across steps.\nTypically,g and hare parametrized using one-hidden-layer MLPs. Note\nthat g has multiple inputs; we discuss how to deal with such cases in\nSection 4.3.\n4.7.2 Sequence to vector\nIn this setting, we define anencoder function p= fe(x1:L,w) from an\ninput sequencex1:L \u2208RL\u00d7D and parametersw\u2208RP to anoutput\nvector p\u2208RM. This is for instance useful for sequence classification,\nsuch as sentiment analysis. Formally, we may definep:= fe(x1:L,w)\nusing the recursion\nsl := \u03b3(xl,sl\u22121,wg) l\u2208[L]\np= pooling(s1:L)\nwhere w:= (wg,s0). The goal of\u03b3 is similar asg, except that it updates\nencoder statesand does not take previous predictions as input. The\npooling function is typically parameter-less. Its goal is to reduce a\nsequence to a vector. Examples include using the last state, the average\nof states and the coordinate-wise maximum of states.\n4.7.3 Sequence to sequence (aligned)\nIn this setting, we define a functionp1:L = fa(x1:L,w) from an in-\nput sequencex1:L \u2208RL\u00d7D and parametersw\u2208RP to anoutput\nsequence p1:L \u2208RL\u00d7M, which we assume to be of thesame length.\nAn example of application is part-of-speech tagging, where the goal is\nto assign each wordxl to a part-of-speech (noun, verb, adjective, etc).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1638, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a0418dd5-d840-41c2-9687-9f8c6cb020a1": {"__data__": {"id_": "a0418dd5-d840-41c2-9687-9f8c6cb020a1", "embedding": null, "metadata": {"page_label": "104", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e60f9c84-8c93-4125-a5c6-114a305d7829", "node_type": "4", "metadata": {"page_label": "104", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "5325c3dc1d20bffdae191e8639327f5b9c64ccd10db6b295099e69759c348fb9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "104 Parameterized programs\nFormally, we may definep1:L = fa(x1:L,w) as\nsl := \u03b3(xl,sl\u22121,w\u03b3) l\u2208[L]\npl = h(sl,wh) l\u2208[L]\nwhere w:= (w\u03b3,wh,s0). The function\u03b3 and h are similar as before.\n4.7.4 Sequence to sequence (unaligned)\nIn this setting, we define a functionp1:L\u2032 = fu(x1:L,w) from aninput\nsequence x1:L \u2208RL\u00d7D and parametersw\u2208RP to an output sequence\np1:L\u2032 \u2208RL\u2032\u00d7M, which potentially has adifferent length. An example\nof application is machine translation, where the sentences in the source\nand target languages do not necessarily have the same length. Typically,\np1:L\u2032 = fu(x1:L,w) is defined as the following two steps\nc:= fe(x1:L,we)\np1:L\u2032 := fd(c,wd)\nwhere w := ( we,wd), and where we reused the previously-defined\nencoder fe and decoderfd. Putting the two steps together, we obtain\nsl := \u03b3(xl,sl\u22121,w\u03b3) l\u2208[L]\nc= pooling(s1:L)\nzl := g(c,pl\u22121,zl\u22121,wg) l\u2208[L\u2032]\npl := h(zl,wh) l\u2208[L\u2032].\nThis architecture is aptly named theencoder-decoder architecture.\nNote that we denoted the length of the target sequence asL\u2032. However,\nin practice, the target length can be input dependent and is often not\nknown ahead of time. To deal with this issue, the vocabulary (of sizeD\nis our notation) is typically augmented with an \u201cend of sequence\u201d (EOS)\ntoken so that, at inference time, we know when to stop generating the\noutput sequence. One disadvantage of this encoder-decoder architecture,\nhowever,isthatalltheinformationabouttheinputsequenceiscontained\nin thecontext vectorc, which can therefore become abottleneck.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1501, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b7b752a4-fc6d-423c-94f1-4066d5c3cbc4": {"__data__": {"id_": "b7b752a4-fc6d-423c-94f1-4066d5c3cbc4", "embedding": null, "metadata": {"page_label": "105", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e3e2c7cb-9915-4558-ab7c-bf8620bb8f6b", "node_type": "4", "metadata": {"page_label": "105", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "0acdde1d3d23271b636e8d341e573bccc6bc187011c10b98d5a0ef920690835e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.8. Transformers 105\n4.8 Transformers\nTransformers (Vaswaniet al., 2017) are one of the most successful recent\ndevelopments in deep learning. In this section, we review Transformers,\ncomponent by component.\n4.8.1 Attention\nThe goal of an attention layer is to map a sequence of inputsv1,..., vL \u2208\nRDv to a sequence of outputsu1,..., uL \u2208RDv, of same dimension.\nA natural idea is to define each output element ui using a linear\ncombination of the input elements,\nui :=\nL\u2211\nj=1\nai,jvj \u2208RDv.\nWe typically use a convex combination: we assume that the combination\nweights are such thatai := ( ai,1,...,a i,L) \u2208\u25b3L. This ensures that\nincreasing a coefficientai,j is made at the expense of decreasing another\ncoefficient ai,j\u2032, forj \u0338= j\u2032. Let us form the matricesV \u2208RL\u00d7Dv and\nU \u2208RL\u00d7Dv by stackingv1,..., vL and u1,..., uL, seen as row vectors.\nLet us also form theattention matrixA\u2208\u25b3L\u00d7L gathering the entries\nai,j, where\u25b3L\u00d7L denotes the set of row-wise stochasticL\u00d7Lmatrices.\nThen, we can rewrite attention succinctly as\nU = AV \u2208RL\u00d7Dv.\nFollowing Section 6.2, we can view attention as asoft dictionary\nlookup. From this perspective, the matrixV \u2208RL\u00d7Dv plays the role\nof dictionary values, and the attention matrix can be defined as\nA:= softargmax(QK\u22a4) \u2208\u25b3L\u00d7L,\nwhere Q \u2208 RL\u00d7Dk and K \u2208 RL\u00d7Dk play the roles of queries and\ndictionary keys, respectively. Intuitively,QK\u22a4 \u2208RL\u00d7L can be seen\nas a similarity matrix containing the similarities between queries and\ndictionary keys. Putting everything together, we can define attention as\nAttention(Q,K,V) := softargmax\n(\nQK\u22a4\n)\nV \u2208RL\u00d7Dv,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1568, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6a8537b7-e4a8-4b8e-98b9-93b5c3ff4b86": {"__data__": {"id_": "6a8537b7-e4a8-4b8e-98b9-93b5c3ff4b86", "embedding": null, "metadata": {"page_label": "106", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c8dada01-b453-4cc4-921c-ecacaef9e95d", "node_type": "4", "metadata": {"page_label": "106", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "baffdffface3f55a132eeeb92b00027f3ea507618798322d4703533d675e62fe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "106 Parameterized programs\nIn masked attention, we additionally incorporate a maskM \u2208RL\u00d7L,\nAttention(Q,K,V; M) := softargmax\n(\n(QK\u22a4) \u25e6M\n)\nV \u2208RL\u00d7Dv,\nwhere \u25e6denotes the Hadamard product (element-wise multiplication).\nThe mask can be used to force some attention weightsai,j to be zero,\nby setting the corresponding mask entrymi,j to \u2212\u221e. For instance, in\ndecoder-only architectures (Section 4.8.8), the mask will prove useful\nto definecausal attentionfor autoregressive models.\nRemark 4.3(Scaled attention). Practicalimplementationsoftenuse\nscaled attention, where a factor of 1\u221aDk\nis used within the soft-\nargmax, in order to reduce the variance (Vaswaniet al., 2017,\nFootnote 4). We omit this detail for clarity.\n4.8.2 Self-attention\nSupposewearegivenasequenceoffeaturevectors x1,..., xL \u2208RD,that\nwe gather as a matrixX\u2208RL\u00d7D. To define the attention weightsai,j, a\nnatural idea is to consider the similarity betweenxi and xj, as measured\nby the inner product\u27e8xi,xj\u27e9. To ensure thatai := (ai,1,...,a i,L) \u2208\u25b3L,\nwe can then define\nai,j := exp(\u27e8xi,xj\u27e9)\n\u2211L\nj\u2032=1 exp(\u27e8xi,xj\u27e9) \u2208(0,1).\nIn matrix notation, this can be written\nA:= softargmax(XX\u22a4) \u2208\u25b3L\u00d7L,\nwhere softargmax is applied in a row-wise fashion. The matrixXX\u22a4\u2208\nRL\u00d7L is theGram matrixassociated with the row vectorsx1,..., xL.\nIn the notation of the previous section, this corresponds to using\nAttention(Q,K,V) with Q = K = V = X. In other words, the\nelements of the sequence \u201cpay attention\u201d to each other. This is called\nself-attention.\nSo far, the formulation we described is parameter-free. To give\nmore expressive power to the self-attention layer, Vaswaniet al.(2017)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1620, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "20389d11-730b-4f3c-90d1-2f545a706d9b": {"__data__": {"id_": "20389d11-730b-4f3c-90d1-2f545a706d9b", "embedding": null, "metadata": {"page_label": "107", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ae626a43-3f20-47e7-8aae-eca0c0137fe8", "node_type": "4", "metadata": {"page_label": "107", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "efabf563e8a92cb99d56bb5f727db7fae1fd58a24fa922783c37ebfb108e43c6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.8. Transformers 107\nproposed instead to defineQ, K and V by projectingX as\nQ:= XWQ \u2208RL\u00d7Dk\nK:= XWK \u2208RL\u00d7Dk\nV := XWV \u2208RL\u00d7Dv,\nusing the learned weight matrices\nWQ \u2208RD\u00d7Dk\nWK \u2208RD\u00d7Dk\nWV \u2208RD\u00d7Dv.\nImportantly, the size of the weight matrices is independent of the length\nL of the sequences. This allows self-attention to work with sequence of\narbitrary length.\n4.8.3 Multi-head attention\nIn order to be able capture multiple patterns of attention, Vaswaniet al.\n(2017) found it beneficial to defineH attention heads\nYi := Attention(Qi,Ki,Vi; M) \u2208RL\u00d7Dv,\nwhere\nQi := XWQ\ni \u2208RL\u00d7Dk\nKi := XWK\ni \u2208RL\u00d7Dk\nVi := XWV\ni \u2208RL\u00d7Dv,\nusing the learned weight matrices\nWQ\ni \u2208RD\u00d7Dk\nWK\ni \u2208RD\u00d7Dk\nWV\ni \u2208RD\u00d7Dv.\nLet us denote the concatenation of theH attention heads by\nConcat(Y1,..., YH) \u2208RL\u00d7HDv.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 766, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "132c2436-b060-45cf-8656-a983307dac69": {"__data__": {"id_": "132c2436-b060-45cf-8656-a983307dac69", "embedding": null, "metadata": {"page_label": "108", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6be54b0e-7ab1-4bca-88a2-09782cf597eb", "node_type": "4", "metadata": {"page_label": "108", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "6d50ca4255677e1a826f78f72b0359cd81d72381000499554e384ec6be82274b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "108 Parameterized programs\nAlgorithm 4.2Multi-head attention withH attention heads\nInput: X\u2208RL\u00d7D, optional maskM \u2208RL\u00d7L\nParameters: {WQ\ni }H\ni=1, {WK\ni }H\ni=1, {WV\ni }H\ni=1, WO\n1: for i:= 1,...,H do\n2: Qi := XWQ\ni \u2208RL\u00d7Dk\n3: Ki := XWK\ni \u2208RL\u00d7Dk\n4: Vi := XWV\ni \u2208RL\u00d7Dv\n5: Yi := Attention(Qi,Ki,Vi; M) \u2208RL\u00d7Dv\n6: Y := Concat(Y1,..., YH)WO\nOutput: MultiheadAttention(X; M) := Y \u2208RL\u00d7D\nVaswaniet al.(2017) then define multi-head attention as\nMultiheadAttention(X; M) := Concat(Y1,..., YH)WO\n=\nH\u2211\ni=1\nAttention(XWQ\ni ,XWK\ni ,XWV\ni )WO\ni\n\u2208RL\u00d7D\nwhere WO \u2208RHDv\u00d7D is a learned matrix. We summarize the procedure\nin Algorithm 4.2. We use a for loop for the sake of clarity; a GPU-\nfriendly implementation would instead be based on a single matrix\nmultiplication.\n4.8.4 Transformer layer\nTo improve training efficiency, we can introduce residual connections\nand a first layer normalization (Section 4.5.2), to define\nZ:= LayerNorm1(MultiheadAttention(X; M) + X) \u2208RL\u00d7D.\nTo improve expressiveness, a Transformer layer furthers uses an MLP\nand a second layer normalization,\nX\u2190LayerNorm2(MLP(Z) + Z) \u2208RL\u00d7D.\nThe subscripts inLayerNorm are used to emphasize that the two layers\nuse their own parameters. In addition, normalization is applied in an", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1224, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5a571a73-2a7c-4fc2-9b64-cb2f50d1c5d5": {"__data__": {"id_": "5a571a73-2a7c-4fc2-9b64-cb2f50d1c5d5", "embedding": null, "metadata": {"page_label": "109", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0068025e-e003-4fdb-b264-48aa9b734964", "node_type": "4", "metadata": {"page_label": "109", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "f315a2299096c8103c5d9c7746904edee1654992303e5ed6bd6940352abcf0e2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.8. Transformers 109\nAlgorithm 4.3Transformer\u2019s MLP block\nInput: Z\u2208RL\u00d7D\nParameters: W1 \u2208RD\u00d7D1 ,W2 \u2208RD1\u00d7D\n1: Z\u2190ZW1 \u2208RL\u00d7D1\n2: Z\u2190\u03c3(Z) \u2208RL\u00d7D1\n3: Z\u2190ZW2 \u2208RL\u00d7D\nOutput: Z\u2208RL\u00d7D\nelement-wise (token-wise) fashion. The MLP block typically uses a single\nhidden layer with an activation function\u03c3, such as a GELU (Gaussian\nerror linear unit); see Algorithm 4.3. Because of the use of residual\nconnections, the input and output dimensions of the MLP block must\nbe the same.\nRemark 4.4(Post-normalization vs. pre-normalization). Ourdescrip-\ntion of the Transformer layer follows the original formulation\n(Vaswaniet al., 2017), which uses post-normalization. Some imple-\nmentations instead rely on pre-normalization, that is,\nZ:= MultiheadAttention(LayerNorm1(X); M) + X\u2208RL\u00d7D\nX\u2190MLP(LayerNorm2(Z)) + Z\u2208RL\u00d7D.\n4.8.5 Transformer block\nA Transformer layer can be iteratedK times (each time with different\nparameters) to define a Transformer block of depthK. Keeping the\ndependency on parameters implicit, we can see a Transformer, with an\noptional maskM \u2208RL\u00d7L, as a function\nX\u21a6\u2192Transformer(X; M) = Transformer(x1,..., xL; M)\nfrom RL\u00d7D to RL\u00d7D. The Transformer takes a sequenceX \u2208RL\u00d7D\nand uses the inter-dependencies between sequence elements to produce\na representation of that sequence. We summarize the procedure in\nAlgorithm 4.4. TheMultiheadAttention, LayerNorm and MLP blocks\nare indexed byk to emphasize that their parameters are different for\neach iteration.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1444, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b7876197-3dd3-412d-b3e0-380f2c932421": {"__data__": {"id_": "b7876197-3dd3-412d-b3e0-380f2c932421", "embedding": null, "metadata": {"page_label": "110", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2a0cfdca-432d-4333-a58f-8cf82428ea06", "node_type": "4", "metadata": {"page_label": "110", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "03cdff778deb16b14712fcebd77d69bd9a514fffc0a3b2b6dff63fb7eeebc5dc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "110 Parameterized programs\nAlgorithm 4.4Transformer block of depthK\nInput: X\u2208RL\u00d7D optional maskM \u2208RL\u00d7L\nParameters: Multi-head attention parameters, MLP parameters\nand layer norm parameters (each depthk uses different parameters)\n1: for k:= 1,...,K do\n2: Y := MultiheadAttentionk(X; M) \u2208RL\u00d7D\n3: Z:= LayerNormk,1(Y + X) \u2208RL\u00d7D \u25b7 Residual connection\n4: X\u2190LayerNormk,2(MLPk(Z) + Z) \u2208RL\u00d7D \u25b7 MLP layer\nOutput: X\u2208RL\u00d7D\nNumber of parameters and computational complexity\nThe Transformer layer can be seen as a function fromRL\u00d7D to RL\u00d7D.\nTo offer the same function signature, a standard multi-layer perceptron\nwould have neededO(N2D2) parameters and a forward pass through\nthe network would have had a time complexity ofO(N2D2) as well. In\ncontrast, a Transformer layer hasO(D2) parameters. Self-attention has\na complexity ofO(N2D) and the final MLP layer has a complexity of\nO(ND2).\n4.8.6 Token encoding\nText can be represented as a sequencex1,...,x L of discrete sym-\nbols, often called tokens. Here, each tokenxi \u2208[M], whereM is the\nvocabulary size, can correspond to words, subwords, or even individ-\nual characters, depending on the tokenization procedure. To obtain\na sequence of continuous vectors x1,..., xL, where xi \u2208RD, it is\ncommon to use anembedding layer. This layer transformsxi \u2208[M]\ninto xi \u2208RD using the linear projection\nxi := WEexi,\nwhere ej \u2208RM is the one-hot encoding ofj \u2208[M] and WE \u2208RD\u00d7M\nis a learnable embedding matrix. The columnWE\n:,j \u2208RD can be in-\nterpreted as the continuous representation of tokenj \u2208[V]. Usually,\nthe vocabulary includes special tokens such as \u201cBOS\u201d (beginning of", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1598, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "177fc440-88eb-4b23-9d62-5ed5680a3143": {"__data__": {"id_": "177fc440-88eb-4b23-9d62-5ed5680a3143", "embedding": null, "metadata": {"page_label": "111", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d135c14-20c1-4295-8581-8af2fb6aff76", "node_type": "4", "metadata": {"page_label": "111", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "db2ebe1a69f1e4b5bdf28a949b566179c6c120f21e4afc01d91ae86c98b24654", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.8. Transformers 111\nsequence), \u201cEOS\u201d (end of sequence) and \u201cPAD\u201d (padding token, to\nensure that the sequence is of lengthL).\n4.8.7 Positional encoding\nDue to their parameterization, Transformers areequivariant with\nrespect to permutations: permuting the input sequence and applying the\nTransformer is equivalent to applying the Transformer and permuting\nthe output sequence. Formally, for any permutation matrixPof size\nL\u00d7L and input sequencex1,..., xL seen as a matrixX, we have\nTransformer(PX) = PTransformer(X).\nThis means that Transformers treat an ordered sequence as amulti-set\n(a modification of the concept of a set that allows for multiple instances\nof its elements). To leverage order information in a Transformer, several\napproaches are possible (Dufteret al., 2022): adding positional encoding\n(either absolute or relative), modifying the attention matrix and pre-\nprocessing the input sequence with an RNN. To add positional encoding,\none typically adds a vectorpi \u2208RD representing the positioni\u2208[L] to\nthe corresponding elementxi \u2208RD,\nxi \u2190xi + pi\nAn ideal positional encoding should work with any sequence length.\nLearned positional encoding\nSimilarly to the token encoding, a simple way to define a positional en-\ncoding is to use an embedding layer. Each positioni\u2208[L] is transformed\ninto a position vectorpi \u2208RD by\npi := WPei\nwhere ei \u2208RL is the one-hot encoding ofi\u2208[L] and WP \u2208RD\u00d7L is a\nlearnable embedding matrix. The columnWP\n:,i \u2208RD can be interpreted\nas the continuous representation of positioni\u2208[L].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1526, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8ddb5efc-2977-4657-812a-07cfb19fdb1c": {"__data__": {"id_": "8ddb5efc-2977-4657-812a-07cfb19fdb1c", "embedding": null, "metadata": {"page_label": "112", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ae8ae80a-9450-4928-aed0-21944aeb8281", "node_type": "4", "metadata": {"page_label": "112", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "05391ef34a0dddec285686bd759bf38d2a86e678f411cc0c5916a7680b43ef98", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "112 Parameterized programs\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n22\n24\n26\n28\n30\n32\n34\n36\n38\n40\n42\n44\n46\n48\nDimension j\n0\n4\n8\n12\n16\n20\n24\n28\n32\n36\n40\n44\n48\n52\n56\n60\n64\n68\n72\n76\n80\n84\n88\n92\n96 Position i\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nFigure 4.8:Heatmap of a positional encoding matrixP \u2208RL\u00d7D, withL= 100,\nD= 50, andN = 30. The values belong to the range[\u22121,1].\nSinusoidal positional encoding\nInstead of learning the positional encoding, we can define it in a heuristic\nmanner. For instance, Vaswaniet al.(2017) proposed thesinusoidal\npositional encodingpi := (pi,1,..., pi,D), defined by\npi,j :=\n\uf8f1\n\uf8f2\n\uf8f3\nsin(\u03c9j \u00b7i) if j is even\ncos(\u03c9j\u22121 \u00b7i) if j is odd\n\u2208[\u22121,1],\nwhere\n\u03c9j := 1\nN\nj\nD\n= N\u2212j\nD\nand whereN is a constant, set toN := 10000 by the authors. We can\ngather the values in a position encoding matrixP \u2208RL\u00d7D, as depicted\nin Fig. 4.8.\nOn first sight, sinusoidal positional encoding may seem a bit mys-\nterious. To gain some intuition, it is useful to compare it to a discrete", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 973, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "388556aa-902b-4591-adb1-70b268285804": {"__data__": {"id_": "388556aa-902b-4591-adb1-70b268285804", "embedding": null, "metadata": {"page_label": "113", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "74511e02-97eb-47eb-9152-384ddcb7ffc4", "node_type": "4", "metadata": {"page_label": "113", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "1cf35912d9293510f04176aad5de26fe4de5072382611dfecb151533543ae509", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.8. Transformers 113\nbinary encodingof integers,\n7 \u2194111\n6 \u2194110\n5 \u2194101\n4 \u2194100\n3 \u2194011\n2 \u2194010\n1 \u2194001\n0 \u2194000.\nWe see that the bits alternate more frequently between0 and 1 as we\ngo from right to left. The sinusoidal positional encoding achieves a\nsimilar behavior, but is continuous. Indeed, each coordinatej \u2208[D] is\nassociated with a sine wave (whenj is even) or a cosine wave (when\nj is odd). Thewavelengthor period of the wave associated withj\nis 2\u03c0\n\u03c9j\nand itsfrequency is \u03c9j\n2\u03c0. Since\u03c9j is a decreasing function ofj,\nwe see that the waves oscillate more frequently whenj is small. This\nbehavior is illustrated in Fig. 4.9.\nStacking sine and cosine waves has been used for creating random\nFourier features (Rahimi and Recht, 2007; Sutherland and Schneider,\n2015). These use waves of random frequencies, while sinusoidal positional\nencoding uses waves of increasing frequency. These approaches therefore\nmainly differ in the way we choose wave frequencies.\nRecovering relative positional information\nAn important property of the sinusoidal positional encoding is that\npi+\u03b4 can be represented as a linear function ofpi, for any offset\u03b4\n(Vaswaniet al., 2017; Zhanget al., 2021). The model should therefore\nbe able to easily learn to attend by relative positions. Indeed, using the\ntrigonometric identities for angle sums\ncos(\u03b1+ \u03b2) = cos(\u03b1) cos(\u03b2) \u2212sin(\u03b1) sin(\u03b2)\nsin(\u03b1+ \u03b2) = sin(\u03b1) cos(\u03b2) + cos(\u03b1) sin(\u03b2),", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1401, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "53e93d68-a103-4e36-a0c1-f00814c889fe": {"__data__": {"id_": "53e93d68-a103-4e36-a0c1-f00814c889fe", "embedding": null, "metadata": {"page_label": "114", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "688377e7-6469-47cd-aba0-04ab8b5bd2af", "node_type": "4", "metadata": {"page_label": "114", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "bfb1673ccc00224e25a99e3089f0a381e4b3361b1dc063082db06730f093e6d9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "114 Parameterized programs\n1\n 0 1\nDimension j = 5\n0\n25\n50\n75\n100\n125\n150\n175\n200Position i\n1\n 0 1\nj = 4\n1\n 0 1\nj = 3\n1\n 0 1\nj = 2\n1\n 0 1\nj = 1\n1\n 0 1\nj = 0\nFigure 4.9: Using a sinusoidal positional encoding, each coordinatej \u2208[D] is\nassociated with a sine or cosine wave. In analogy with a binary encoding, in which\nlower bits alternate between0 and 1 more frequently than higher bits, the wave\nassociated withj oscillates more frequently whenj is small.\nwe have forj even\n(\npi+\u03b4,j+1\npi+\u03b4,j\n)\n=\n(\ncos(\u03c9j(i+ \u03b4))\nsin(\u03c9j(i+ \u03b4))\n)\n=\n(\ncos(\u03c9j \u00b7i) cos(\u03c9j \u00b7\u03b4) \u2212sin(\u03c9j \u00b7i) sin(\u03c9j \u00b7\u03b4)\nsin(\u03c9j \u00b7i) cos(\u03c9j \u00b7\u03b4) + cos(\u03c9j \u00b7i) sin(\u03c9j \u00b7\u03b4)\n)\n=\n(\ncos(\u03c9j \u00b7\u03b4) \u2212sin(\u03c9j \u00b7\u03b4)\nsin(\u03c9j \u00b7\u03b4) cos( \u03c9j \u00b7\u03b4)\n)(\ncos(\u03c9j \u00b7i)\nsin(\u03c9j \u00b7i)\n)\n=\n(\ncos(\u03c9j \u00b7\u03b4) \u2212sin(\u03c9j \u00b7\u03b4)\nsin(\u03c9j \u00b7\u03b4) cos( \u03c9j \u00b7\u03b4)\n)(\npi,j+1\npi,j\n)\nTherefore, (pi,j+1,pi,j) can be linearly projected to(pi+\u03b4,j+1,pi+\u03b4,j), by\napplying arotation matrix. This allows a Transformer to easily learn\nto attend by relative positions.\nRotary positional encoding (RoPE)\nAnother popular approach for taking into account absolute positional\ninformation in self-attention is RoPE (Suet al., 2024), which stands for\nrotary positional encoding. For simplicity, we briefly explain the idea\nwith a single attention head andDv = Dk = D. Suppose we have a\nsequence x1,..., xL \u2208RD already encoded with token encoding. We", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1319, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bdd2cfe3-b1fa-455f-a323-d11f777119b8": {"__data__": {"id_": "bdd2cfe3-b1fa-455f-a323-d11f777119b8", "embedding": null, "metadata": {"page_label": "115", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b43ac4f8-4158-4435-a615-d10bbae5470a", "node_type": "4", "metadata": {"page_label": "115", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "ff9f2c6f9fe61e81008123aca6e810b99d1081db4a2987a4e176acbcda08b052", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.8. Transformers 115\nthen define the query and key vectors as\nqm := RD(\u03b8,m)WQxm m\u2208[L]\nkn := RD(\u03b8,n)WKxn n\u2208[L],\nwhere RD(\u03b8,m) \u2208RD\u00d7D is a rotation matrix. The main intuition is\nthat we are rotating the2-dimensional vector by an angle which is a\nmultiple of the positionm. In contrast to the learned and sinusoidal\npositional encodings, the weight matricesWQ and WK are applied\nbefore applying RoPE, not after.\nWhen D= 2, RD(\u03b8,m) = R2(\u03b8,m) is defined as the rotation matrix\nR2(\u03b8,m) :=\n(\ncos(m\u03b8) \u2212sin(m\u03b8)\nsin(m\u03b8) cos( m\u03b8)\n)\nfor some angle\u03b8\u2208R and positionm\u2208[L]. Using Euler\u2019s formula\neim\u03b8 = cos(m\u03b8) + isin(m\u03b8),\nwe note thatv\u2032:= R2(\u03b8,m)vfor v= (v1,v2) \u2208R2 and v\u2032= (v\u2032\n1,v\u2032\n2) \u2208\nR2 is equivalent toz\u2032:= zeim\u03b8 for z:= v1+iv2 \u2208C and z\u2032= v\u2032\n1+iv\u2032\n2 \u2208C.\nSee Suet al.(2024) for a detailed derivation of RoPE\u2019s formula.\nWhen D> 2, assumingD is even,RD(\u03b8,m) is defined as\nRD(\u03b8,m) :=\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nR2(\u03b81,m)\nR2(\u03b82,m)\n...\nR2(\u03b8D/2,m)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n,\nwhere \u03b8j := 10000\u22122(j\u22121)/D. In practice, we never materializeRD(\u03b8,m)\nas a matrix, since it would be very sparse, but rather view it as a linear\nmap applied to an arbitrary vectorv\u2208RD,\nRD(\u03b8,m)v=\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nv1\nv2\nv3\nv4\n...\nvD\u22121\nvD\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\u2297\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\ncos(m\u03b81)\ncos(m\u03b81)\ncos(m\u03b82)\ncos(m\u03b82)\n...\ncos(m\u03b8D/2)\ncos(m\u03b8D/2)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n+\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u2212v2\nv1\n\u2212v4\nv3\n...\n\u2212vD\nvD\u22121\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\u2297\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nsin(m\u03b81)\nsin(m\u03b81)\nsin(m\u03b82)\nsin(m\u03b82)\n...\nsin(m\u03b8D/2)\nsin(m\u03b8D/2)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1417, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7af4a5f6-01f5-4b01-bfdb-864a1287f2f0": {"__data__": {"id_": "7af4a5f6-01f5-4b01-bfdb-864a1287f2f0", "embedding": null, "metadata": {"page_label": "116", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8a6a125-77e5-412c-ac7c-e950fa6c434c", "node_type": "4", "metadata": {"page_label": "116", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "97dda247b3e64402f49b47a074d30efb8341bd8726a297d6dd4e72528a466477", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "116 Parameterized programs\nOnce we computedQand K, we can useAttention(Q,K,V; M) as\nusual. For multi-head attention, we simply apply the same approach\nfor each attention head.\nAs analyzed by Suet al. (2024), RoPE satisfies several valuable\nproperties. In particular, it is flexible w.r.t. the sequence lengthL and\ndespite being an absolute encoding, it manages to also capture relative\ndistance between tokens. It therefore has the merits of both absolute\nand relative positional encodings.\n4.8.8 Decoder-only architectures\nDefining a language model\nTransformers were originally developed to create encoder-decoder ar-\nchitectures for machine translation (Vaswaniet al., 2017). However,\nin the context of language modelling, Transformers are now routinely\nused for decoder-onlyautoregressive architectures. Suppose we are\ngiven a sequence of discrete symbolsx1,...,x L \u2208[M] (if a sequence\nhas less thanL elements, we can use padding symbols). The goal of\n(unconditional) language models is to create a function producing the\njoint probability,\n(x1,...,x L) \u21a6\u2192P(X1 = x1,...,X L = xL).\nUsing the chain rule of probability (Section 10.1), without loss of\ngenerality, we can write\nP(X1 = x1,...,X L = xL) =\nL\u220f\nk=1\nP(Xk = xk |X1 = x1,...,X k\u22121 = Xk\u22121).\nIf the maximum length L is very large, it is more pratical to only\nconsider a context window of sizeN,\nP(X1 = x1,...,X L = xL) :=\nL\u220f\nk=1\nP(Xk = xk |Xk\u2212N = xk\u2212N,...,X k\u22121 = xk\u22121).\nThisamountstodefininga higher-orderMarkovchain (Section10.4.3).\nUsing this approach, creating a language model then boils down to\ndefining a function\n(xk\u2212N,...,x k\u22121,xk) \u21a6\u2192P(Xk = xk |Xk\u2212N = xk\u2212N,...,X k\u22121 = xk\u22121).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1640, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8d0cf81b-7a9f-446f-9ac0-0121ab8a563c": {"__data__": {"id_": "8d0cf81b-7a9f-446f-9ac0-0121ab8a563c", "embedding": null, "metadata": {"page_label": "117", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb9deeaf-0a2e-4308-a3d3-d1f84da6052a", "node_type": "4", "metadata": {"page_label": "117", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "cadd23047b410e9d6bcccfbd2f1542296560f39870de73bde416a199e259db20", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.8. Transformers 117\nLetusassumethatthesequenceofdiscretesymbols x1,...,x L \u2208[M] has\nbeen mapped to a sequence of continuous vectorsx1,..., xL \u2208RD using\ntoken encoding (Section 4.8.6) and positional encoding (Section 4.8.7).\nUsing a causal Transformer block, we can obtain a representation of\nthe current context,\n(xk\u2212N,..., xk\u22121) \u2190Transformer(xk\u2212N,..., xk\u22121; MN).\nTo ensure that the Transformer only relies on past tokens to predict\nthe current token, the mask matrixMN is set to atriangular matrix\nof sizeN \u00d7N, with the elements of the lower part set to1, and the\nelements of the upper part set to\u2212\u221e. To reduce this to logits inRM,\nwe usually use the last element of the contextxk\u22121 \u2208RD and apply a\nlinear model, to obtain\n\u03b8k := WDxk\u22121 \u2208RM,\nwhere WD \u2208RM\u00d7D is a learned \u201cdisembedding\u201d matrix. To obtain a\nvalid probability distribution, we apply a soft-argmax on the logits\u03b8k,\n\u03c0k := softargmax(\u03b8k) \u2208\u25b3M.\nFinally, we can now define\nP(Xk = xk |Xk\u2212N = xk\u2212N,...,X k\u22121 = xk\u22121) := [\u03c0k]xk.\nRemark 4.5(Weight tying). Instead of learning a separate disem-\nbedding matrix WD \u2208RM\u00d7D, a frequently used technique is to\nset WD := (WE)\u22a4, whereWE \u2208RD\u00d7M is the embedding matrix\nfrom Section 4.8.6. This weight tying reduces the number of pa-\nrameters to learn and is shown to work well in practice (Press and\nWolf, 2016).\nTraining\nLet us gather the Transformer parameters (multi-head attention, MLP,\nlayer norm) asw\u2208RP. The decoder-only Transformer with a context\nwindow of sizeN then defines a parametric probability distribution\npw(x1,...,x L) :=\nL\u220f\nk=1\npw(xk |xk\u2212N,...,x k\u22121).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1563, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e9e45f97-1bcc-444d-8fbb-65a2a8f6861f": {"__data__": {"id_": "e9e45f97-1bcc-444d-8fbb-65a2a8f6861f", "embedding": null, "metadata": {"page_label": "118", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c5816d1-9597-4aa6-a87a-0b82f79a41a0", "node_type": "4", "metadata": {"page_label": "118", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "9afc89cf9ecb9e55cd34f6495f0ddc4a12dce4c7dbd415e932196c6994e913dd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "118 Parameterized programs\nGiven a corpus of sequencesD, we usually seek the parametersw\u2208RP\nby maximizing the log-likelihood,\nEx\u223cD[log pw(x1,...,x L)] = Ex\u223cD\n[L\u2211\nk=1\nlog pw(xk |xk\u2212N,...,x k\u22121)\n]\n.\nThis amounts to using a logistic (cross-entropy) loss in a token-wise fash-\nion. Importantly, the contextxk\u2212N,...,x k\u22121 used to predict the next\ntoken xk always comes from the data, not from the tokens generated by\nthe model. This is calledteacher forcingand makes Transformer train-\ning highly parallelizable. The objective is usually solved approximately\nusing stochastic gradient algorithms. To maximize GPU utilization,\nsequences of variable lengths are usually packed together in order to\nform batches.\nSampling\nA decoder-only Transformer with a context window of sizeN can be\nseen as forming a higher-order Markov chain, which is a special case\nof Bayesian network. To generate a sequence from the model, we can\ntherefore useancestral sampling(Section 10.5.3),\nX0 := x0\nX1 \u223cpw(\u00b7| X0)\nX2 \u223cpw(\u00b7| X0,X1)\n...\nXk \u223cpw(\u00b7| Xk\u2212N,...,X k\u22121),\nwhere x0 denotes the beginning-of-sequence (BOS) token. The sampling\nstops when the end-of-sequence (EOS) token is generated or when a\nmaximum length is reached. The generated sequences are i.i.d.\nBecause sampling happens one token at a time, it is highly sequential.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1302, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5262af32-5f69-4ca9-ab95-e8717ef7aed8": {"__data__": {"id_": "5262af32-5f69-4ca9-ab95-e8717ef7aed8", "embedding": null, "metadata": {"page_label": "119", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "effe889b-e92f-4ab8-a494-425454bae218", "node_type": "4", "metadata": {"page_label": "119", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "72cc069e9f9f231df8feb211654184e25d602a8f9555e4a09d9c2bcfafc69948", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.8. Transformers 119\nThe Transformer block is called sequentially as\nTransformer(X0; M1)\nTransformer(X0,X1; M2)\n...\nTransformer(Xk\u22121,...X k\u2212N; MN).\nTo avoid repeating the same computations again and again, assuming\nthat a causal mask is used, the past key and value matrices used in\nmulti-head attention (Section 4.8.3) are usually stored in the so-called\nKV cache.\n4.8.9 Encoder-only architectures\nEncoder-only Transformers can be used for learning to represent se-\nquences. The most prominent example is BERT (Devlinet al., 2019),\nwhich stands for bidirectional encoder representations from transformers.\nBERT uses Algorithm 4.4 without causal mask (hence \u201cbidirectional\u201d\nin the acronym). Discrete sequence elements are transformed to vec-\ntors using token encoding, positional encoding and potentially segment\nencoding. For reasons that will become clear below, the first token of\nevery sequence is always a special classification token[CLS]. Training\nis broken down into two phases: pretraining and finetuning.\nPretraining\nSince pre-training is performed on an unlabeled corpus, it is necessary\nto synthetically generate prediction tasks.\nIn masked prediction, a percentage (typically 15%) of the input\ntokens are randomly masked. The model is then trained to predict\nthe original vocabulary ID of the masked tokens, given the context\nprovided by the unmasked tokens. This forces the model to learn a\nrich, bidirectional representation of the language. The masking strategy\ninvolves:\n\u2022 80% of the time, replacing the chosen token with[MASK],\n\u2022 10% of the time, replacing the chosen token with a random token\nfrom the vocabulary,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1633, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3b64a0d7-82fb-403b-aef6-606b3325bb29": {"__data__": {"id_": "3b64a0d7-82fb-403b-aef6-606b3325bb29", "embedding": null, "metadata": {"page_label": "120", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "41d4b649-8cd9-425e-af6d-2e8e7e1b377a", "node_type": "4", "metadata": {"page_label": "120", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "206842466e92e78d922cc9d3302a3b0fb4d3e06cf99ebf1b98788b934cf19131", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "120 Parameterized programs\n\u2022 10% of the time, keeping the chosen token unchanged.\nIn next sentence prediction, the model is trained to understand the\nrelationship between two sentences. For each training example, BERT\nis given two sentences,A and B. 50% of the time, B is the actual next\nsentence that follows A in the original document (labeled IsNext). The\nother 50% of the time,B is a random sentence from the corpus (labeled\nNotNext). A special[CLS] token is prepended to the input sequence,\nand its final hidden state is used to predict whether sentenceB follows\nsentence A. The[SEP] token separates the two sentences.\nFinetuning\nAfter pre-training on a massive corpus, the pretrained BERT model can\nbe finetuned for various downstream NLP tasks (e.g., text classification,\nquestion answering, named entity recognition) by adding a small, task-\nspecific output layer on top of the pre-trained Transformer encoder.\nThe entire model, including the pretrained weights, is then finetuned on\nthe labeled data for the specific task. For classification tasks, the final\nhidden state corresponding to the class token[CLS] is used. For tagging\ntasks (token-level classification) such as named entity recognition (NER)\nor part-of-speech (POS) tagging, the final hidden state of each token\nis used. For question answering tasks, which usually involve finding a\nspan (start and end token) within a given text, this is treated as two\ntoken-level classification problems: one for the start token and one for\nthe end token.\n4.8.10 Encoder-decoder architectures\nThe encoder-decoder architecture was the original architecture proposed\nin the seminal Transformer paper (Vaswaniet al., 2017). It can be used\nfor sequence-to-sequence tasks, such as machine translation, summa-\nrization and question answering. We denote the input sequence byX\nand the output sequence byY.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1856, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1ede5beb-ab92-44c9-8c56-e70706fa9294": {"__data__": {"id_": "1ede5beb-ab92-44c9-8c56-e70706fa9294", "embedding": null, "metadata": {"page_label": "121", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ce0863d-1182-44b3-ba89-e9b3dd6a74ec", "node_type": "4", "metadata": {"page_label": "121", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "4e16fa03a24f9d63860aaa76b581218892dbf9cbc660429e9d713cdd97e112bb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.8. Transformers 121\nEncoder\nThe role of the encoder is to produce a rich representationHenc of\nthe input sequenceX, which serves as a context for the subsequent\ndecoding phase. As in encoder-only architectures, this is achieved by\nusing Algorithm 4.4 without causal mask. The multi-head self-attention\nlayer allows the encoder to weigh the importance of different tokens in\nthe input sequence relative to each other. For each token in the input, it\ncomputes a representation that incorporates information from all other\ntokens in the same sequence. The encoder block uses its own parameters\nfor each sub-component (multi-head attention, layer norm, MLP). In\nparticular, multi-head attention uses parameters{WQ\ni }H\ni=1, {WK\ni }H\ni=1,\n{WV\ni }H\ni=1 and WO.\nDecoder\nThe role of the decoder is to generate the output sequenceY one token at\na time, based on the encoded representationHenc from the encoder and\nthe previously generated tokensY1:t\u22121. The decoder differs in the way\nattention is applied. First, we apply causal multi-head self-attention to\nobtain a representationHdec corresponding to the previously generated\ntokens Y1:t\u22121. Second, we apply multi-headcross-attention. The key\ndifference with multi-head self-attention is that the key, query and value\nmatrices are defined as\nQdec\ni := HdecW\u2032Q\ni\nKenc\ni := HencW\u2032K\ni\nVenc\ni := HencW\u2032V\ni ,\nwhere {W\u2032Q\ni }H\ni=1, {W\u2032K\ni }H\ni=1 and {W\u2032V\ni }H\ni=1 are the weight matrices\nused for the decoder. Subsequently, MLP and layer norm layers are\napplied, similarly as before.\nDifferences with decoder-only architectures\nAn encoder-decoder architecture provides a dedicated component for\nunderstanding the input and another for generating the output. It allows", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1705, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "144603d2-4edc-4ac6-ad47-0707cdd019e4": {"__data__": {"id_": "144603d2-4edc-4ac6-ad47-0707cdd019e4", "embedding": null, "metadata": {"page_label": "122", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "017644d3-1264-45bb-a8cc-3707e157c066", "node_type": "4", "metadata": {"page_label": "122", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "ce1d76e48857a99e87c4af6379266669fe42ebb1d92a2ee2e738059016adcd5a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "122 Parameterized programs\nfor a bidirectional understanding of the input. It is therefore best suited\nfor sequence-to-sequence (seq2seq) tasks where the input and output\nsequences may have different domains or modalities.\nDecoder-only models, on the other hand, are streamlined for gen-\nerating text by continuously predicting the next token, leveraging a\nsingle, powerful autoregressive mechanism. That being said, due to\ntheir simplicity, decoder-only architectures are now increasinglh being\nused even for seq2seq tasks, by prepending the input (prompt) to the\ncontext. This means that the input is subject to the causal mask, but\nthe performance remains remarkably strong.\n4.9 Summary\n\u2022 Programs can be mathematically represented as a directed acyclic\ngraph.\n\u2022 Neural networks are parameterized programs.\n\u2022 Feedfoward networks are parameterized computation chains.\n\u2022 Multilayerperceptrons(MLPs),residualneuralnetworks(ResNets)\nandconvolutionalneuralnetwork(CNNs)areallparticularparametriza-\ntions of feedforward networks.\n\u2022 Transformer blocks are designed to process sequences of variable-\nlength, but they are equivariant to permutations and therefore\nrequire positional encoding, in order to leverage positional infor-\nmation.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1233, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bd849597-5990-49aa-aaf9-b35a05453887": {"__data__": {"id_": "bd849597-5990-49aa-aaf9-b35a05453887", "embedding": null, "metadata": {"page_label": "123", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "723daee9-6cc2-4035-9dff-5e8faa2c90e9", "node_type": "4", "metadata": {"page_label": "123", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "a5d0743842373629176672902f03d3e4d51c96e658ed6fc0119bf337c233cdde", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5\nControl flows\nControl flows, such as conditionals or loops, are an essential part of\ncomputer programming, as they allow us to express complex programs.\nIt is therefore natural to ask whether these constructs can be included\nin a differentiable program. This is what we study in this chapter.\n5.1 Comparison operators\nControl flows rely oncomparison operators, a.k.a.relational oper-\nators. Formally, we can define a comparison operator\u03c0 = op(u1,u2)\nas a function from u1 \u2208R and u2 \u2208R to \u03c0 \u2208 {0,1}. The binary\n(Boolean) output\u03c0 can then be used within a conditional statement\n(see Section 5.6, Section 5.7) to decide whether to execute one branch\nor another. We define the following operators, illustrated in Fig. 5.1:\n\u2022 greater than:\ngt(u1,u2) :=\n\uf8f1\n\uf8f2\n\uf8f3\n1 if u1 \u2265u2\n0 otherwise\n= step(u1 \u2212u2)\n123", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 798, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b21800fb-f7d8-4b4d-baa6-b70c06a421c9": {"__data__": {"id_": "b21800fb-f7d8-4b4d-baa6-b70c06a421c9", "embedding": null, "metadata": {"page_label": "124", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "46dc5006-f6e9-4704-9b13-1adab09c3c38", "node_type": "4", "metadata": {"page_label": "124", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "a59cb997a1f5aaba9bd7044d1bfeb0a5c8dd4bb027f24bad4370276438e33694", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "124 Control flows\n\u2022 less than:\nlt(u1,u2) :=\n\uf8f1\n\uf8f2\n\uf8f3\n1 if u1 \u2264u2\n0 otherwise\n= 1 \u2212gt(u1,u2)\n= step(u2 \u2212u1)\n\u2022 equal:\neq(u1,u2) :=\n\uf8f1\n\uf8f2\n\uf8f3\n1 if |u1 \u2212u2|= 0\n0 otherwise\n= gt(u2,u1) \u00b7gt(u1,u2)\n= step(u2 \u2212u1) \u00b7step(u1 \u2212u2)\n\u2022 not equal:\nneq(u1,u2) :=\n\uf8f1\n\uf8f2\n\uf8f3\n1 if |u1 \u2212u2|>0\n0 otherwise\n= 1 \u2212eq(u1,u2)\n= 1 \u2212step(u2 \u2212u2) \u00b7step(u1 \u2212u2),\nwhere step: R \u2192{0,1}is theHeaviside step function\nstep(u) :=\n\uf8f1\n\uf8f2\n\uf8f3\n1 if u\u22650\n0 otherwise\n.\nThe Heaviside step function is piecewise constant. Atu= 0, the function\nis discontinuous. Atu\u0338= 0, it is continuous and hasnull derivative.\nSince the comparison operators we presented are all expressed in terms\nof thestep function, they are all continuous and differentiable almost\neverywhere, with null derivative. Therefore, while their derivatives are\nwell-defined almost everywhere, they areuninformative and prevent\ngradient backpropagation.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 858, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "862835bb-a602-4b05-aca2-24810d11f42e": {"__data__": {"id_": "862835bb-a602-4b05-aca2-24810d11f42e", "embedding": null, "metadata": {"page_label": "125", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "28ac059f-52d7-4cf7-89eb-25746df08244", "node_type": "4", "metadata": {"page_label": "125", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "2e0aafc43a66e8294a7d75bf716216d1d030d0c258a1016b1ae3fa5c3dc19521", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5.2. Soft inequality operators 125\n0 2 4\nu1\n0\n2\n4u2\nu1 greater than u2\n0 2 4\nu1\n0\n2\n4u2\nu1 equal to u2\n0.0\n0.5\n1.0\nValue\n0.0\n0.5\n1.0\nValue\n0 2 4\nu1\n0\n2\n4u2\nu1 greater than u2\n0 2 4\nu1\n0\n2\n4u2\nu1 equal to u2\n0.0\n0.5\n1.0\nValue\n0.0\n0.5\n1.0\nValue\nSmoothed operators with logistic\nFigure 5.1:The greater than and equal to operators are discontinuous functions,\nleading to black or white pictures. They can be smoothed with appropriate approxi-\nmations of the Heaviside step function.\n5.2 Soft inequality operators\n5.2.1 Heuristic definition\nTo obtain acontinuous relaxationof inequality operators, we can\nheuristically replace the step function in the expression of \u201cgreater\nthan\u201d and \u201cless than\u201d by a sigmoid functionsigmoid\u03c3, where\u03c3 >0 is a\nscaling parameter. Such a sigmoid function should satisfy the following\nproperties:\n\u2022 sigmoid\u03c3(\u2212u) = 1 \u2212sigmoid\u03c3(u),\n\u2022 limu\u2192\u221esigmoid\u03c3(u) = 1,\n\u2022 limu\u2192\u2212\u221esigmoid\u03c3(u) = 0,\n\u2022 sigmoid\u03c3(0) = 1\n2 .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 927, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3cd32ed3-23fd-46ba-a0be-c12d1c67ff6b": {"__data__": {"id_": "3cd32ed3-23fd-46ba-a0be-c12d1c67ff6b", "embedding": null, "metadata": {"page_label": "126", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "009dc4cc-327b-4f69-9585-8c643b6ba683", "node_type": "4", "metadata": {"page_label": "126", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "912764d727f606f18b43a002a0350dcb3473d4619975677334aea1629e9449e9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "126 Control flows\nTwo examples of sigmoids satisfying the aforementioned properties are\nthe logistic function (the CDF of the standard logistic distribution)\nsigmoid\u03c3(u) := logistic\u03c3(u) := 1\n1 + e\u2212u/\u03c3 \u2208(0,1)\nand the standard Gaussian\u2019s CDF, defined in Eq. (3.1),\nsigmoid\u03c3(u) := \u03a6(u/\u03c3).\nWe may then define the soft \u201cgreater than\u201d\ngt(\u00b51,\u00b52) = step(\u00b51 \u2212\u00b52)\n\u2248sigmoid\u03c3(\u00b51 \u2212\u00b52)\n=: gt\u03c3(\u00b51,\u00b52)\nand the soft \u201cless than\u201d\nlt(\u00b51,\u00b52) = step(\u00b52 \u2212\u00b51)\n\u2248sigmoid\u03c3(\u00b52 \u2212\u00b51)\n=: lt\u03c3(\u00b51,\u00b52)\n= 1 \u2212sigmoid\u03c3(\u00b51 \u2212\u00b52)\n= 1 \u2212gt\u03c3(\u00b51 \u2212\u00b52).\nIn the limit, we have thatsigmoid\u03c3(\u00b51 \u2212\u00b52) \u21921 when \u00b51 \u2212\u00b52 \u2192\u221e.\nIn the limit,sigmoid\u03c3 therefore outputs a value of1 if \u00b51 and \u00b52 are\ninfinitelyapart.BesidesthelogisticfunctionandthestandardGaussian\u2019s\nCDF, other sigmoid functions are possible, as discussed in Section 13.6.\nIn particular, with sparse sigmoids, there exists a finite value\u03c4 such\nthat \u00b51 \u2212\u00b52 \u2265\u03c4 =\u21d2sigmoid\u03c3(\u00b51 \u2212\u00b52) = 1.\n5.2.2 Stochastic process perspective\nWhen the sigmoid used to replace the step function is the logistic\nfunction or the standard Gaussian\u2019s CDF, we can revisit the previous\nheuristic definition ofgt\u03c3(\u00b51,\u00b52) and lt\u03c3(\u00b51,\u00b52) from a more formal\nperspective. Indeed, to real values\u00b51 \u2208R and \u00b52 \u2208R, we can associate\nrandom variables\nU1 \u223cp\u00b51,\u03c31 and U2 \u223cp\u00b52,\u03c32 ,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1243, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c576d618-a7f5-4368-8cb6-9c86e5f53142": {"__data__": {"id_": "c576d618-a7f5-4368-8cb6-9c86e5f53142", "embedding": null, "metadata": {"page_label": "127", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "84a2f68d-335c-4f75-9586-1b7083af95ca", "node_type": "4", "metadata": {"page_label": "127", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "d54790e7637936d595d3ddd13db4b3565aa09287d1cc50a9789b3932238c071d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5.2. Soft inequality operators 127\nthereby forming astochastic process(we assume that\u03c31 and \u03c32 are\nfixed). Alternatively, we can also write\n(U1,U2) \u223cp\u00b51,\u03c31 \u2297p\u00b52,\u03c32 ,\nwhere for two distributionsp1 and p2, we denotep1 \u2297p2 their outer\nproduct (p1 \u2297p2)(u1,u2) := p1(u1)p2(u2). We can then define\ngt\u03c3(\u00b51,\u00b52) = E[gt(U1,U2)]\n= E[step(U1 \u2212U2)]\n= P(U1 \u2212U2 \u22650)\n= P(U2 \u2212U1 \u22640)\n= FU2\u2212U1 (0),\nwhere FX is the cumulative distribution function(CDF) of the\nrandom variable X, and \u03c3 is a function of\u03c31 and \u03c32. Similarly, we\nobtain\nlt\u03c3(\u00b51,\u00b52) = E[lt(U1,U2)]\n= E[step(U2 \u2212U1)]\n= P(U1 \u2212U2 \u22640)\n= FU1\u2212U2 (0).\nWe see that the soft inequality operators are based on the CDF of the\ndifference between U1 and U2.\nFor location-scale family distributions (Section 12.4.1), from a per-\nturbation perspective, we can also define noise variablesZ1 \u223cp0,1 and\nZ2 \u223cp0,1 such thatU1 = \u00b51 +\u03c31Z1 and U2 = \u00b52 +\u03c32Z2 (Section 12.4.1).\nWe then have\ngt\u03c3(\u00b51,\u00b52) = E[gt(\u00b51 + \u03c31Z1,\u00b52 + \u03c32Z2)]\nlt\u03c3(\u00b51,\u00b52) = E[lt(\u00b51 + \u03c31Z1,\u00b52 + \u03c32Z2)] .\nGaussian case\nWhen U1 \u223cNormal(\u00b51,\u03c32\n1) and U2 \u223cNormal(\u00b52,\u03c32\n2), we have\nU1 \u2212U2 \u223cNormal(\u00b51 \u2212\u00b52,\u03c32\n1 + \u03c32\n2). (5.1)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1103, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "53dda6e0-5bd9-443f-aeab-6a79b84ce0e3": {"__data__": {"id_": "53dda6e0-5bd9-443f-aeab-6a79b84ce0e3", "embedding": null, "metadata": {"page_label": "128", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "17afc4d4-c3f9-450b-ad51-80711b343dbb", "node_type": "4", "metadata": {"page_label": "128", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "75f228b78c0cd6b38bc4021efcdc8fd12799d8119c8eec47ee432217d938ad38", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "128 Control flows\nDenoting \u03a6 the standard Gaussian\u2019s CDF, we then obtain\ngt\u03c3(\u00b51,\u00b52) = FU2\u2212U1 (0)\n= \u03a6\n(0 \u2212(\u00b52 \u2212\u00b51)\n\u03c3\n)\n= \u03a6\n(\u00b51 \u2212\u00b52\n\u03c3\n)\nlt\u03c3(\u00b51,\u00b52) = FU1\u2212U2 (0)\n= \u03a6\n(0 \u2212(\u00b51 \u2212\u00b52)\n\u03c3\n)\n= \u03a6\n(\u00b52 \u2212\u00b51\n\u03c3\n)\n,\nwhere \u03c3:=\n\u221a\n\u03c32\n1 + \u03c32\n2. The corresponding distribution forZ1 and Z2 is\nGaussian noise.\nLogistic case\nWhen U1 \u223cGumbel(\u00b51,\u03c3) and U2 \u223cGumbel(\u00b52,\u03c3), we have\nU1 \u2212U2 \u223cLogistic(\u00b51 \u2212\u00b52,\u03c3). (5.2)\nWe then obtain (see also Proposition 14.3)\ngt\u03c3(\u00b51,\u00b52) = logistic\n(\u00b51 \u2212\u00b52\n\u03c3\n)\nlt\u03c3(\u00b51,\u00b52) = logistic\n(\u00b52 \u2212\u00b51\n\u03c3\n)\n.\nThe corresponding distribution forZ1 and Z2 is Gumbel noise.\nRecovering hard inequality operators\nWe easily recover the \u201chard\u201d inequality operator by\ngt(\u00b51,\u00b52) = E[gt(U1,U2)] ,\nwhere Ui \u223c\u03b4\u00b5i and where\u03b4\u00b5i is thedelta distributionthat assigns a\nprobability of1 to \u00b5i.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 763, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1f095d4b-1562-4aec-aa14-7219a4854076": {"__data__": {"id_": "1f095d4b-1562-4aec-aa14-7219a4854076", "embedding": null, "metadata": {"page_label": "129", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "002dca2b-5dc4-4eda-8fd7-26af71a50609", "node_type": "4", "metadata": {"page_label": "129", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "224f1f79351c785bfa3edc2e67b284a233266f5ce92a9c3f90e615fe46956ef2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5.3. Soft equality operators 129\n5.3 Soft equality operators\n5.3.1 Heuristic definition\nThe equality operatoreq(\u00b51,\u00b52) can be seen as an extreme kind of\nsimilarity functionbetween numbers, that can only output the values\n0 or 1. To define soft equality operators, a natural idea is therefore\nto replace the equality operator by a more general similarity function.\nA similarity function should achieve its maximum at\u00b51 = \u00b52 and it\nshould decrease as\u00b51 and \u00b52 move apart. A common family of similarity\nfunctions are kernels. Briefly, a kernelk(\u00b51,\u00b52) can be seen as the\ninner product\nk(\u00b51,\u00b52) := \u27e8\u03d5(\u00b51),\u03d5(\u00b52)\u27e9\nbetween the embbedings\u03d5(\u00b51) and \u03d5(\u00b52) of \u00b51 and \u00b52 in some (po-\ntentially infinite-dimensional) spaceH, a reproducing kernel Hilbert\nspace to be precise; see Sch\u00f6lkopf and Smola (2002) and Shawe-Taylor\nand Cristianini (2004) for an in-depth review of kernels. To obtain a\nsimilarity measure between0 and 1 approximating the equality operator,\nwe can normalize to obtain\neq(\u00b51,\u00b52) \u2248 k(\u00b51,\u00b52)\u221a\nk(\u00b51,\u00b51)k(\u00b52,\u00b52)\n= \u27e8\u03d5(\u00b51),\u03d5(\u00b52)\u27e9\n\u2225\u03d5(\u00b51)\u2225\u2225\u03d5(\u00b52)\u2225,\nwhere \u2225\u03d5(\u00b5)\u2225:=\n\u221a\n\u27e8\u03d5(\u00b5),\u03d5(\u00b5)\u27e9=\n\u221a\n\u03ba(\u00b5,\u00b5). This is thecosine simi-\nlarity between \u03d5(\u00b51) and \u03d5(\u00b52).\nA particular kind of kernel areisotropic kernels of the form\nk(\u00b51,\u00b52) := \u03ba(\u00b51 \u2212\u00b52),\nthat depend only on the difference between inputs. When the kernel has\na scale parameter\u03c3 >0, we use the notation\u03ba\u03c3. We can then define a\nsoft equality operator as\neq(\u00b51,\u00b52) \u2248eq\u03c3(\u00b51,\u00b52) := \u03ba\u03c3(\u00b51 \u2212\u00b52)\n\u03ba\u03c3(0) .\nSeveral isotropic kernels can be chosen such as theGaussian kernel\n\u03ba\u03c3(t) := exp\n(\n\u2212t2\n2\u03c32\n)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1528, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4fca823b-3065-4e03-ab9c-8622fe850f41": {"__data__": {"id_": "4fca823b-3065-4e03-ab9c-8622fe850f41", "embedding": null, "metadata": {"page_label": "130", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b0ba0db-1592-43fc-a473-689847f3a2d3", "node_type": "4", "metadata": {"page_label": "130", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "919ef285d906bf996f25ff8e611d716a8fed481153837ea1c38ea43b39cc6a61", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "130 Control flows\n2\n 0 2\n0.00\n0.25\n0.50\n0.75\n1.00\nSoft equal zero\n2\n 0 2\n0.00\n0.25\n0.50\n0.75\n1.00\nSoft greater than zero\nHard\nLogistic\nGaussian\nFigure 5.2:Softequalityandsoftgreaterthanoperatorscanbedefinedasnormalized\nkernels (PDF) and as CDF functions, respectively.\nor thelogistic kernel\n\u03ba\u03c3(t) := sech2\n( t\n2\u03c3\n)\n,\nwhere we defined the hyperbolic secant\nsech(u) := 2/(exp(u) + exp(\u2212u)).\nAs their names suggest, these kernels arise naturally from a probabilistic\nperspective, that we present below.\nThesoftequalityoperatorsobtainedwiththesekernelsareillustrated\nin Fig. 5.2. Intuitively, we replaced a bar located at\u00b51 = \u00b52 with a\nbump function. The soft equality operator obtained with the logistic\nkernel coincides with the expression Petersenet al.(2021) arrive at (see\ntheir Eq. 9), in a different manner.\n5.3.2 Stochastic process perspective\nWe again adopt the stochastic process perspective, in which we associate\nrandom variables\nU1 \u223cp\u00b51,\u03c31 and U2 \u223cp\u00b52,\u03c32\nto real values\u00b51 \u2208R and \u00b52 \u2208R. However, to handle the equality\noperator, we cannot simply use the expectation ofeq(U1,U2) since\nE[eq(U1,U2)] = P(U1 = U2) = 0,\nU1 and U2 being independent continuous variables. While we cannot\nuse the probability ofU1 = U2, or equivalently ofU1 \u2212U2 = 0, we can", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1256, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8fc9ca66-984a-424c-bd1b-97fd672fdefe": {"__data__": {"id_": "8fc9ca66-984a-424c-bd1b-97fd672fdefe", "embedding": null, "metadata": {"page_label": "131", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "890da9df-2f34-4a22-9326-1866503e1392", "node_type": "4", "metadata": {"page_label": "131", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "6d13a97639e5bbca0190db0c0aebcc1d440d87fb794207a4457e6825218ad9c0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5.3. Soft equality operators 131\nconsider using the probability density function (PDF)fU1\u2212U2 of U1 \u2212U2\nevaluated at0. To ensure that the maximum is achieved at0 with value\n1, we can normalize the PDF to define\neq\u03c3(\u00b51,\u00b52) = fU1\u2212U2 (0)\nf0(0) .\nIt is well-known that the PDF of the sum of two random variables is\nthe convolutionof their respectives PDFs. We therefore have\nfU1\u2212U2 (t) = (fU1 \u2217f\u2212U2 )(t)\n=\n\u222b\u221e\n\u2212\u221e\nfU1 (\u03c4)f\u2212U2 (t\u2212\u03c4)d\u03c4.\nIn particular, witht = 0, iffX is the PDF of a location-scale family\ndistributed random variable, we obtain\nfU1\u2212U2 (0) = (fU1 \u2217f\u2212U2 )(0)\n=\n\u222b\u221e\n\u2212\u221e\nfU1 (\u03c4)f\u2212U2 (\u2212\u03c4)d\u03c4\n=\n\u222b\u221e\n\u2212\u221e\nfU1 (\u03c4)fU2 (\u03c4)d\u03c4\n:= \u27e8fU1 ,fU2 \u27e9\n:= \u03ba(\u00b51,\u00b52).\nWe indeed recover an inner product and therefore a kernel.\nCDF and PDF of absolute difference\nWhile P(U1 = U2) = 0, we can also consider usingP(|U1 \u2212U2|\u2264 \u03b5) =\nF|U1\u2212U2|(\u03b5) as an alternative notion of soft equality. For any random\nvariable X, we have\nF|X|(x) = P(|X|\u2264 x)\n= P(\u2212x\u2264X \u2264x)\n= P(X \u2264x) \u2212P(X \u2264\u2212x)\n= FX(x) \u2212FX(\u2212x).\nTherefore,\nP(|U1 \u2212U2|\u2264 \u03b5) = FU1\u2212U2 (\u03b5) \u2212FU1\u2212U2 (\u03b5).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1015, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7d1d2b34-57f9-4f05-8f4b-f687a615ce9d": {"__data__": {"id_": "7d1d2b34-57f9-4f05-8f4b-f687a615ce9d", "embedding": null, "metadata": {"page_label": "132", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e42f5fe4-746e-439b-8104-ee4e3a42cb75", "node_type": "4", "metadata": {"page_label": "132", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "1fcd9bc9bbd1bb25157561ec213342e51f2387f3e2036205320e15eb82beca34", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "132 Control flows\nWe can also derive the PDF of|X|as\nf|X|(x) = F\u2032\nX(x) \u2212F\u2032\nX(\u2212x)\n= fX(x) + fX(\u2212x)\nand in particular\nf|X|(0) = 2fX(0).\nTherefore\nf|U1\u2212U2|(0) = 2fU1\u2212U2 (0),\nfurther justifying using the PDF ofU1 \u2212U2 evaluated at0. WhenX\nfollows a normal distribution,|X|follows the so-called folded normal\ndistribution.\nGaussian case\nWhen U1 \u223cNormal(\u00b51,\u03c32\n1) and U2 \u223cNormal(\u00b52,\u03c32\n2), we obtain from\nEq. (5.1)\nfU1\u2212U2 (t) = 1\u221a\n2\u03c0exp\n(\n\u2212(t\u2212(\u00b51 \u2212\u00b52))2\n2(\u03c32\n1 + \u03c32\n2)\n)\nso that\neq\u03c3(\u00b51,\u00b52) = exp\n(\n(\u00b51 \u2212\u00b52)2\n2(\u03c32\n1 + \u03c32\n2)\n)\n\u2208[0,1].\nWe indeed recover\u03ba\u03c3(\u00b51 \u2212\u00b52)/\u03ba\u03c3(0), where\u03ba\u03c3 is the Gaussian kernel\nwith \u03c3=\n\u221a\n\u03c32\n1 + \u03c32\n2. For the CDF of the absolute difference, we obtain\nP(|U1 \u2212U2|\u2264 \u03b5) = \u03a6\n(\u03b5\u2212(\u00b51 \u2212\u00b52)\n\u03c3\n)\n\u2212\u03a6\n(\u2212\u03b5\u2212(\u00b51 \u2212\u00b52)\n\u03c3\n)\n.\nLogistic case\nWhen U1 \u223cGumbel(\u00b51,\u03c3) and U2 \u223cGumbel(\u00b52,\u03c3), recalling that\nsech(u) := 2/(exp(u) + exp(\u2212u)),\nwe obtain from Eq. (5.2)\nfU1\u2212U2 (t) = 1\n4\u03c3sech2\n(t\u2212(\u00b51 \u2212\u00b52)\n2\u03c3\n)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 890, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8ae1e5b1-d0a8-4152-8e8d-c0f637051206": {"__data__": {"id_": "8ae1e5b1-d0a8-4152-8e8d-c0f637051206", "embedding": null, "metadata": {"page_label": "133", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "790b9be9-7901-4e0a-b87a-0af38770246c", "node_type": "4", "metadata": {"page_label": "133", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "f52cdf58ac706872bfe5be7caa0b173283b93a19c62f65e03ecf05d09a4f4de8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5.3. Soft equality operators 133\nso that\neq\u03c3(\u00b51,\u00b52) = sech2\n(\u00b51 \u2212\u00b52\n2\u03c3\n)\n\u2208[0,1].\nWe indeed recover\u03ba\u03c3(\u00b51 \u2212\u00b52)/\u03ba\u03c3(0), where\u03ba\u03c3 is the logistic kernel\nwith \u03c3= \u03c31 = \u03c32.\n5.3.3 Gaussian process perspective\nThe previous approach relied on mapping\u00b51 and \u00b52 to twoindepen-\ndent random variablesU1 \u223cp\u00b51,\u03c31 and U2 \u223cp\u00b52,\u03c32 (we assume that\u03c31\nand \u03c32 are fixed). Instead, we can consider mapping\u00b51 and \u00b52 to two\ndependent random variablesU1 and U2, whose covariance depends on\nthe similarity between\u00b51 and \u00b52. We can do so by using aGaussian\nprocess (Hida and Hitsuda, 1976).\nA Gaussian process onR is a stochastic process{U\u00b5 : \u00b5\u2208R}indexed\nby \u00b5\u2208R such that any subset ofK random variables(U\u00b51 ,...,U \u00b5K)\nassociated with (\u00b51,...,\u00b5 K) \u2208R is a multivariate Gaussian random\nvariable. The Gaussian process is characterized by the mean function\n\u00b5\u21a6\u2192E[U\u00b5], and its covariance function(\u00b5i,\u00b5j) \u21a6\u2192Cov(U\u00b5i,U\u00b5j). For\nthe mean function, we may simply chooseE[U\u00b5] = \u00b5. For the covariance\nfunction, we need to ensure that the variance of any combination of\nrandom variables in the Gaussian process is non-negative. This property\nis satisfied by kernel functions. We can therefore define\nCov(U\u00b5i,U\u00b5j) := k(\u00b51,\u00b52),\nfor some kernelk. Equipped with such a mapping from real numbers\nto random variables, we need a measure of similarity between random\nvariables. A natural choice is theircorrelation\ncorr(U\u00b5i,U\u00b5j) := Cov(U\u00b5i,U\u00b5j)\u221a\nVar(U\u00b5i) Var(U\u00b5j)\n\u2208[0,1].\nWe therefore obtain\ncorr(U\u00b5i,U\u00b5j) = k(\u00b51,\u00b52)\u221a\nk(\u00b51,\u00b51)k(\u00b52,\u00b52)\n= \u27e8\u03d5(\u00b51),\u03d5(\u00b52)\u27e9\n\u2225\u03d5(\u00b51)\u2225\u2225\u03d5(\u00b52)\u2225,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1513, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d9ed022f-53ee-414d-8906-8544035c8f2a": {"__data__": {"id_": "d9ed022f-53ee-414d-8906-8544035c8f2a", "embedding": null, "metadata": {"page_label": "134", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9b46e1b9-b032-4b6a-a9ef-0a4a5c4e138b", "node_type": "4", "metadata": {"page_label": "134", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "c9747de857881094977aa1334c96486c1251ceb6133b309be1521c81dc0568f4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "134 Control flows\nwhich coincides with thecosine similaritymeasure we saw before. In\nthe particular caseK = 2 and whenk\u03c3(\u00b51,\u00b52) = \u03ba(\u00b51 \u2212\u00b52), we then\nrecover the previous heuristically-defined soft equality operator\neq\u03c3(\u00b51,\u00b52) = corr(U\u00b51 ,U\u00b52 ) = \u03ba(\u00b51 \u2212\u00b52)\n\u03ba(0) .\n5.4 Logical operators\nLogical operators can be used to perform Boolean algebra. Formally,\nwe can define them as functions from{0,1}\u00d7{0,1}to {0,1}. Theand\n(logical conjunction a.k.a. logical product),or (logical disjunction a.k.a.\nlogical addition) andnot (logical negation a.k.a. logical complement)\noperators, for example, are defined by\nand(\u03c0,\u03c0\u2032) :=\n\uf8f1\n\uf8f2\n\uf8f3\n1 if \u03c0= \u03c0\u2032= 1\n0 otherwise\nor(\u03c0,\u03c0\u2032) :=\n\uf8f1\n\uf8f2\n\uf8f3\n1 if 1 \u2208{\u03c0,\u03c0\u2032}\n0 otherwise\nnot(\u03c0) :=\n\uf8f1\n\uf8f2\n\uf8f3\n0 if \u03c0= 1\n1 if \u03c0= 0\n.\nClassical properties of these operators include\n\u2022 Commutativity:\nand(\u03c0,\u03c0\u2032) = and(\u03c0\u2032,\u03c0)\nor(\u03c0,\u03c0\u2032) = or(\u03c0\u2032,\u03c0)\n\u2022 Associativity:\nand(\u03c0,and(\u03c0\u2032,\u03c0\u2032\u2032)) = and(and(\u03c0,\u03c0\u2032),\u03c0\u2032\u2032)\nor(\u03c0,or(\u03c0\u2032,\u03c0\u2032\u2032)) = or(or(\u03c0,\u03c0\u2032),\u03c0\u2032\u2032)\n\u2022 Distributivity ofand over or:\nand(\u03c0,or(\u03c0\u2032,\u03c0\u2032\u2032)) = or(and(\u03c0,\u03c0\u2032),and(\u03c0,\u03c0\u2032\u2032))", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1006, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aba59c68-d4eb-4938-b212-a5e7ef22ed01": {"__data__": {"id_": "aba59c68-d4eb-4938-b212-a5e7ef22ed01", "embedding": null, "metadata": {"page_label": "135", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c02fee95-bfcb-48ef-9542-9fe18b63c0bf", "node_type": "4", "metadata": {"page_label": "135", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "590b37dfeeaee0417151a288fef9339b6037e13ef88b8c4f0c6db9f4320ae410", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5.5. Continuous extensions of logical operators 135\n\u2022 Neutral element:\nand(\u03c0,1) = \u03c0\nor(\u03c0,0) = \u03c0\n\u2022 De Morgan\u2019s laws:\nnot(or(\u03c0,\u03c0\u2032)) = and(not(\u03c0),not(\u03c0\u2032))\nnot(and(\u03c0,\u03c0\u2032)) = or(not(\u03c0),not(\u03c0\u2032)).\nMore generally, for a binary vector\u03c0 = ( \u03c01,...,\u03c0 K) \u2208{0,1}K,\nwe can define all (universal quantification, \u2200) and any (existential\nquantification, \u2203) operators, which are functions from{0,1}K to {0,1},\nas\nall(\u03c0) :=\n\uf8f1\n\uf8f2\n\uf8f3\n1 if \u03c01 = \u00b7\u00b7\u00b7 = \u03c0K = 1\n0 otherwise\nand\nany(\u03c0) :=\n\uf8f1\n\uf8f2\n\uf8f3\n1 if 1 \u2208{\u03c01,...,\u03c0 K}\n0 otherwise\n.\n5.5 Continuous extensions of logical operators\n5.5.1 Probabilistic continuous extension\nWe can equivalently write theand, or and not operators as\nand(\u03c0,\u03c0\u2032) = \u03c0\u00b7\u03c0\u2032\nor(\u03c0,\u03c0\u2032) = \u03c0+ \u03c0\u2032\u2212\u03c0\u00b7\u03c0\u2032\nnot(\u03c0) = 1 \u2212\u03c0.\nThese areextensions of the previous definitions: we can use them as\nfunctions from[0,1]\u00d7[0,1] \u2192[0,1], as illustrated in Fig. 5.3. This means\nthat we can use the soft comparison operators defined in Section 5.2 to\nobtain \u03c0,\u03c0\u2032\u2208[0,1]. Likewise, we can define continuous extensions of", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 980, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3f6ab1e3-d371-4d13-b768-5b8d7128a4d6": {"__data__": {"id_": "3f6ab1e3-d371-4d13-b768-5b8d7128a4d6", "embedding": null, "metadata": {"page_label": "136", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18587f29-849f-440c-81bf-40c704fa1caf", "node_type": "4", "metadata": {"page_label": "136", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "ca1fe929bd0e0e502a447bb22c7b6ba4a40a4ff43d3f50eabb0db2160c453f5e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "136 Control flows\n0.0 0.5 1.0\n0.0\n0.5\n1.0\n\u2032\nAnd operator\n0.0 0.5 1.0\n0.0\n0.5\n1.0\n\u2032\nOr operator\n0.0\n0.5\n1.0\nValue\n0.0\n0.5\n1.0\nValue\nFigure 5.3:The Booleanand and or operators are functions from{0,1}\u00d7{0,1}\nto {0,1}(corners in the figure) but their continuous extensionsand(\u03c0,\u03c0\u2032) := \u03c0\u00b7\u03c0\u2032\nas well asor(\u03c0,\u03c0\u2032) := \u03c0+ \u03c0\u2032\u2212\u03c0\u00b7\u03c0\u2032define a function from[0,1] \u00d7[0,1] to [0,1].\nall and any, which are functions from[0,1]K to [0,1], as\nall(\u03c0) =\nK\u220f\ni=1\n\u03c0i\nany(\u03c0) = 1 \u2212\nK\u220f\ni=1\n(1 \u2212\u03c0i).\nFrom a probabilistic perspective, if we letY and Y\u2032 to be two\nindependent random variables distributed according toBernoulli dis-\ntributions with parameter\u03c0 and \u03c0\u2032, then\nand(\u03c0,\u03c0\u2032) = P(Y = 1 \u2229Y\u2032= 1) = P(Y = 1) \u00b7P(Y\u2032= 1)\nor(\u03c0,\u03c0\u2032) = P(Y = 1 \u222aY\u2032= 1)\n= P(Y = 1) + P(Y\u2032= 1) \u2212P(Y = 1 \u2229Y\u2032= 1)\n= P(Y = 1) + P(Y\u2032= 1) \u2212P(Y = 1)P(Y\u2032= 1)\nnot(\u03c0) = P(Y \u0338= 1) = 1 \u2212P(Y = 1).\nIn probability theory, these correspond to the product rule of two\nindependent variables, the addition rule, and the complement rule.\nLikewise, if we letY = (Y1,...,Y K) \u2208{0,1}K be a random variable\ndistributed according to amultivariate Bernoulli distributionwith", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1090, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f033922e-74ef-4000-a409-6996d1d88f06": {"__data__": {"id_": "f033922e-74ef-4000-a409-6996d1d88f06", "embedding": null, "metadata": {"page_label": "137", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6561a6c6-c5c0-46d4-83c9-1ba9a82a44ef", "node_type": "4", "metadata": {"page_label": "137", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "7652190e0388f783e3c6829bb868970982095d18e2a335c0c3655c2306373e47", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5.5. Continuous extensions of logical operators 137\nparameters \u03c0= (\u03c01,...,\u03c0 K), then\nall(\u03c0) = P(Y1 = 1 \u2229\u00b7\u00b7\u00b7\u2229 YK = 1)\n=\nK\u220f\ni=1\nP(Yi = 1)\nany(\u03c0) = P(Y1 = 1 \u222a\u00b7\u00b7\u00b7\u222a YK = 1)\n= 1 \u2212P(\u00ac(Y1 = 1 \u222a\u00b7\u00b7\u00b7\u222a YK = 1))\n= 1 \u2212P(Y1 \u0338= 1 \u2229\u00b7\u00b7\u00b7\u2229 YK \u0338= 1)\n= 1 \u2212\nK\u220f\ni=1\n(1 \u2212P(Yi = 1)).\nThese are the chain rule of probability and the addition rule of proba-\nbility forK independent variables.\n5.5.2 Triangular norms and co-norms\nMore generally, in thefuzzy logicliterature (Klir and Yuan, 1995;\nJayaram and Baczynski, 2008), the concepts of triangular norms and\nco-norms have been introduced to provide continuous relaxations of the\nand and or operators, respectively.\nDefinition 5.1(Triangular norms and conorms). A triangular norm,\na.k.a. t-norm, is a function from[0,1] \u00d7[0,1] to [0,1] which is\ncommutative, associative, neutral w.r.t.1 and is monotone, meaning\nthat t(\u03c0,\u03c0\u2032) \u2264t(\u03c4,\u03c4 \u2032) for all \u03c0 \u2264\u03c4 and \u03c0\u2032 \u2264\u03c4\u2032. A triangular\nconorm, a.k.a. t-conorm, is defined similarly but is neutral w.r.t.0.\nThe previously-defined probabilistic extensions ofand and or are\nexamples of triangle norm and conorm. More examples are given in\nTable 5.1. Thanks to the associative property of these operators, we can\ngeneralize them to vectors\u03c0\u2208[0,1]K to define continuous extensions\nof theall and anyoperators, as shown in Table 5.2. For more examples\nand analysis, see for instance van Krieken (2024, Chapters 2 and 3).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1375, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2a49708c-915f-401e-a755-f54fb14684a0": {"__data__": {"id_": "2a49708c-915f-401e-a755-f54fb14684a0", "embedding": null, "metadata": {"page_label": "138", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f5138277-6011-4b6e-8fe9-783515b62aa6", "node_type": "4", "metadata": {"page_label": "138", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "e32eb9bf62ee80b7a3af1c94255bb267fb0516e7bb9942ca857425b874d516cf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "138 Control flows\n0.0 0.5 1.0\n0.0\n0.5\n1.0\n\u2032\nAnd operator\n0.0 0.5 1.0\n0.0\n0.5\n1.0\n\u2032\nOr operator\n0.0\n0.5\n1.0\nValue\n0.0\n0.5\n1.0\nValue\nExtremum T-Norm\n0.0 0.5 1.0\n0.0\n0.5\n1.0\n\u2032\nAnd operator\n0.0 0.5 1.0\n0.0\n0.5\n1.0\n\u2032\nOr operator\n0.0\n0.5\n1.0\nValue\n0.0\n0.5\n1.0\nValue\nukasiewicz T-Norm\nFigure 5.4: Alternative relaxations of the Booleanand and or operators using\ntriangular norms (t-norms).\nTable 5.1: Examples of triangular norms and conorms, which are continuous\nrelaxations of theand and or operators, respectively. More instances can be obtained\nby smoothing out the min and max operators.\nt-norm (relaxed and) t-conorm (relaxed or)\nProbabilistic \u03c0\u00b7\u03c0\u2032 \u03c0+ \u03c0\u2032\u2212\u03c0\u00b7\u03c0\u2032\nExtremum min(\u03c0,\u03c0\u2032) max( \u03c0,\u03c0\u2032)\n\u0141ukasiewicz max(\u03c0+ \u03c0\u2032\u22121,0) min( \u03c0+ \u03c0\u2032,1)\n5.6 If-else statements\nAn if-else statement executes different code depending on a condition.\nFormally, we can define theifelse: {0,1}\u00d7V\u00d7V\u2192V function by\nifelse(\u03c0,v1,v0) :=\n\uf8f1\n\uf8f2\n\uf8f3\nv1 if \u03c0= 1\nv0 if \u03c0= 0\n(5.3)\n= \u03c0\u00b7v1 + (1 \u2212\u03c0) \u00b7v0.\nThe \u03c0variable is called thepredicate. It is abinary(Boolean) variable,\nmaking the function ifelse undefined if \u03c0 \u0338\u2208 {0,1}. The function is\ntherefore discontinuous and nondifferentiable w.r.t.\u03c0\u2208{0,1}. On the\nother hand, v0 \u2208V and v1 \u2208V, which correspond to the false and\ntrue branches, can becontinuous variables. If\u03c0 = 1, the function\nis linear w.r.t. v1 and constant w.r.t. v0. Conversely, if\u03c0 = 0 , the\nfunction is linear w.r.t.v0 and constant w.r.t.v1. We now discuss how\nto differentiate throughifelse.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1463, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5bb5fcb3-4010-45c0-9df1-7ed8d4e7e565": {"__data__": {"id_": "5bb5fcb3-4010-45c0-9df1-7ed8d4e7e565", "embedding": null, "metadata": {"page_label": "139", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b7d9d557-8d2a-4d96-82c9-afff8c5ade86", "node_type": "4", "metadata": {"page_label": "139", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "c12fffd7d142b472ae3ee715dc6a7f3f57c358d675a169a36f36694870c13fab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5.6. If-else statements 139\nTable 5.2:Continuous extensions of theall and any operators.\nAll (\u2200) Any ( \u2203)\nProbabilistic \u220fK\ni=1 \u03c0i 1 \u2212\u220fK\ni=1(1 \u2212\u03c0i)\nExtremum min(\u03c01,...,\u03c0 K) max( \u03c01,...,\u03c0 K)\n\u0141ukasiewicz max(\u2211K\ni=1 \u03c0i \u2212(K\u22121),0) min( \u2211K\ni=1 \u03c0i,1)\n5.6.1 Differentiating through branch variables\nFor \u03c0 \u2208{0,1}fixed, ifelse(\u03c0,v1,v0) is a valid function w.r.t.v1 \u2208V\nand v0 \u2208V, and can therefore be used as a node in a computational\ngraph (Section 8.3). Due to the linearity w.r.t.v1 and v0, we obtain\nthat the Jacobians w.r.t.v1 and v0 are\n\u2202v0 ifelse(\u03c0,v1,v0) :=\n\uf8f1\n\uf8f2\n\uf8f3\n0 if \u03c0= 1\nI if \u03c0= 0\n= (1 \u2212\u03c0) \u00b7I\nand\n\u2202v1 ifelse(\u03c0,v1,v0) :=\n\uf8f1\n\uf8f2\n\uf8f3\nI if \u03c0= 1\n0 if \u03c0= 0\n= \u03c0\u00b7I,\nwhere I is the identity matrix of appropriate size. Most of the time,\nif-else statements are composed with other functions. Letg1 : U1 \u2192V\nand g0 : U0 \u2192V be differentiable functions. We then definev1 := g1(u1)\nand v0 := g0(u0), where u1 \u2208U1 and u0 \u2208U0. The composition of\nifelse, g1 and g0 is then the functionf: {0,1}\u00d7U1 \u00d7U0 \u2192V defined by\nf(\u03c0,u1,u0) := ifelse(\u03c0,g1(u1),g0(u0))\n= \u03c0\u00b7g1(u1) + (1\u2212\u03c0) \u00b7g0(u0).\nWe obtain that the Jacobians are\n\u2202u1 f(\u03c0,u1,u0) = \u03c0\u00b7\u2202g1(u1)\nand\n\u2202u0 f(\u03c0,u1,u0) = (1 \u2212\u03c0)\u2202g0(u0).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1152, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d2cfe20f-de2d-4da0-ae51-d9ac9da68d30": {"__data__": {"id_": "d2cfe20f-de2d-4da0-ae51-d9ac9da68d30", "embedding": null, "metadata": {"page_label": "140", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "690ca45c-89b9-4488-97bf-9944b4d1cd75", "node_type": "4", "metadata": {"page_label": "140", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "adb8f4d8fd6a74388b6aab7dd08e582de5b24a573d664b5ab6b810b5d17f1cb3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "140 Control flows\nAs long as g1 and g0 are differentiable functions, we can therefore\ndifferentiate through the branch variablesu1 and u0 without any issue.\nMore problematic is the predicate variable\u03c0, as we now discuss.\n5.6.2 Differentiating through predicate variables\nThe predicate variable\u03c0is binary and therefore cannot be differentiated\ndirectly. However,\u03c0 can be the output of a comparison operator. For\nexample, suppose we want to express the functionfh: R\u00d7U1 \u00d7U0 \u2192V\ndefined by\nfh(p,u1,u0) :=\n\uf8f1\n\uf8f2\n\uf8f3\ng1(u1) if p\u22650\ng0(u0) otherwise\n.\nUsing our notation, this can be rewritten as\nfh(p,u1,u0) := ifelse(gt(p,0),g1(u1),g0(u0))\n= ifelse(step(p),g1(u1),g0(u0))\n= step(p)g1(u1) + (1\u2212step(p))g0(u0).\nThe Heaviside step function has adiscontinuityat p = 0, but it is\ncontinuous and differentiable with derivativestep\u2032(p) = 0 for allp\u0338= 0.\nThe functionfh therefore hasnull derivativew.r.t. p\u0338= 0,\n\u2202pfh(p,u1,u0) = \u22021fh(p,u1,u0)\n= step\u2032(p)(g1(u1) \u2212g0(u0))\n= 0.\nIn other words, whilefh has well-definedderivatives w.r.t.pfor p\u0338= 0,\nthe derivatives areuninformative. As another example, let us now\nconsider the function\ngh(u1,u0) := fh(t(u1),u1,u0),\nfor some differentiable functiont. This time,u1 influences both the\npredicate and the true branch. Then, using Proposition 2.8, we obtain\n\u2202u1 gh(u1,u0) = \u2202t(u1)\u22021fh(t(u1),u1,u0) + \u22022fh(t(u1),u1,u0)\n= \u22022fh(t(u1),u1,u0).\nIn other words, the derivatives of the predicatet(u1) do not influence\nthe derivatives ofgh.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1454, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f6e0e3fc-4289-4ead-a47c-278ffc9a441a": {"__data__": {"id_": "f6e0e3fc-4289-4ead-a47c-278ffc9a441a", "embedding": null, "metadata": {"page_label": "141", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d40722f0-1784-4a73-ab8c-386fb074542d", "node_type": "4", "metadata": {"page_label": "141", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "c135561553b235a92a35e10452919b577dfc753a72002b67e8c044f7e4e06141", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5.6. If-else statements 141\n5.6.3 Continuous relaxations\nFortunately, we recall that\nifelse(\u03c0,v1,v0) = \u03c0\u00b7v1 + (1 \u2212\u03c0) \u00b7v0.\nThis function is perfectly well-defined, even if\u03c0 \u2208[0,1], instead of\n\u03c0\u2208{0,1}. That is, this definition is anextension of Eq. (5.3) from the\ndiscrete set{0,1}to the continuous unit segment[0,1]. We saw that\ngt(a,b) \u2248gt\u03c3(a,b) := sigmoid\u03c3(a\u2212b) \u2208[0,1],\nwhere we usesigmoid\u03c3 to denote a differentiable S-shaped function\nmapping R to [0,1]. For instance, we can use the logistic function or\nthe standard Gaussian\u2019s CDF. If we now define\nfs(p,u1,u0) := ifelse(gt\u03c3(p),g1(u1),g0(u0))\n= ifelse(sigmoid\u03c3(p),g1(u1),g0(u0))\n= sigmoid\u03c3(p)g1(u1) + (1\u2212sigmoid\u03c3(p))g0(u0), (5.4)\nthe Jacobian becomes\n\u2202pfs(p,u1,u0) = sigmoid\u2032\n\u03c3(p)(g1(u1) \u2212g0(u0)).\nIf sigmoid\u03c3 = logistic(\u00b7/\u03c3) or sigmoid\u03c3 = \u03a6(\u00b7/\u03c3), the Jacobian isnon-\nnull everywhere, allowing gradients to backpropagate through the\ncomputational graph. This is an example of smoothing as studied in\nPart IV.\nProbabilistic perspective\nFrom a probabilistic perspective, we can view Eq. (5.4) as the expecta-\ntion ofgi(ui), wherei\u2208{0,1}is a binary random variable distributed ac-\ncording to aBernoulli distributionwith parameter\u03c0= sigmoid\u03c3(p):\nfs(p,u1,u0) = Ei\u223cBernoulli(sigmoid\u03c3(p)) [gi(ui)] .\nTaking the expectation over the two possibles branches makes the func-\ntion differentiable with respect top, sincesigmoid\u03c3(p) is differentiable.\nOf course, this comes at the cost of evaluating both branches, instead", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1462, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "da5dbcaf-8de6-4eff-81d2-530e7de7cbd9": {"__data__": {"id_": "da5dbcaf-8de6-4eff-81d2-530e7de7cbd9", "embedding": null, "metadata": {"page_label": "142", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1e0b2afb-eeae-4dec-9eab-a1103bdc0daa", "node_type": "4", "metadata": {"page_label": "142", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "17575aa7ca77fe6a5c8748b2ee9f46b56d9276f32c16b4bca170ec4bf391b479", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "142 Control flows\nifelse\n  \nifelse\n  \nSoft comparisonoperator\nHard comparisonoperator\nFigure 5.5:Computation graphs of programs using if-else statements with either\nhard or soft comparison operators. By using a hard comparison operator (step\nfunction, left panel) the predicate\u03c0 is a discrete variable (represented by a dashed\nline). Depending on the value (0 or 1) of the predicate\u03c0, only one branch (red or blue)\ncontributes to the output. Derivatives along a path of continuous variables (dense\nlines) can be computed. However, discrete variables such as the predicate prevent the\npropagation of meaningful derivatives. By using a soft comparison operator (sigmoid,\nright panel), the predicate is a continuous variable and derivatives with respect to\nthe inputp can be taken. In this case both branches (corresponding tog0 and g1)\ncontribute to the output and therefore need to be evaluated.\nof a single one. The probabilistic perspective suggests that we can also\ncompute the variance if needed as\nVi\u223cBernoulli(sigmoid\u03c3(p)) [gi(ui)]\n=Ei\u223cBernoulli(sigmoid\u03c3(p))\n[\n(fs(p,u1,u0) \u2212gi(ui))2\n]\n.\nThe probabilistic viewpoint also suggests different scales at which a\nsmoothing can be defined as illustrated in Fig. 5.6.\nAnother perspective (Petersenet al., 2021) is based on thelogistic\ndistribution. Indeed, ifP is a random variable following a logistic\ndistribution with meanp and scale1, we saw in Remark 3.1 that the\nCDF isP(P \u22640) = logistic(\u2212p) = 1 \u2212logistic(p) and therefore\nfs(p,u1,u0) = ifelse(logistic(p),g1(u1),g0(u0))\n= logistic(p)g1(u1) + (1\u2212logistic(p))g0(u0)\n= P(P >0) \u00b7g1(u1) + P(P \u22640) \u00b7g0(u0).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1605, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "799976c1-893b-4004-acf9-f1b644ff1780": {"__data__": {"id_": "799976c1-893b-4004-acf9-f1b644ff1780", "embedding": null, "metadata": {"page_label": "143", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b9668b4c-d4c3-4905-9cbe-e3a39df81b69", "node_type": "4", "metadata": {"page_label": "143", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "2c2f727731587af1348c7dae67bb15c22e7b49a627b1fa57cf91d0f2444f431c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5.6. If-else statements 143\nRemark 5.1(Global versus local smoothing). Considerthefunction\nf(x,y,z ) :=\n\uf8f1\n\uf8f2\n\uf8f3\ny if a\u2264x\u2264b\nz otherwise\n.\nThe derivatives w.r.t.y and z are well-defined. The derivative w.r.t.\nxon the other hand is not well-defined since it involves comparison\noperators and the logical operatorand. Using our notation, we can\nrewrite the function as\nf(x,y,z ) = ifelse(and(gt(x,a),lt(x,b)),y,z ).\nA local smoothing approach consists in replacinggt and lt by gt\u03c3\nand lt\u03c3 locally in the program:\nfloc\n\u03c3 (x,y,z ) := ifelse(and(gt\u03c3(x,a),lt\u03c3(x,b)),y,z )\n= \u03c0a\u03c0by+ (1 \u2212\u03c0a\u03c0b)z\nwhere\n\u03c0a := sigmoid\u03c3(x\u2212a)\n\u03c0b := sigmoid\u03c3(b\u2212x),\nfor any sigmoid functionsigmoid\u03c3. A global smoothing approach\ninstead uses the expectation of the entire program\nfglob\n\u03c3 (x,y,z ) := EZ[ifelse(and(gt(x+ \u03c3Z,a),lt(x+ \u03c3Z,b)),y,z )]\n= ifelse(\u03c0,y,z )\nwhere\n\u03c0:= EZ[and(gt(x+ \u03c3Z,a),lt(x+ \u03c3Z,b))]\n= P(a\u2264x+ \u03c3Z \u2264b)\n= sigmoid\u03c3(b\u2212x) \u2212sigmoid\u03c3(a\u2212x)\n= \u03c0b \u2212\u03c0a,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 924, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a0aee713-5f5d-4ad7-a51c-de8221ffe1b1": {"__data__": {"id_": "a0aee713-5f5d-4ad7-a51c-de8221ffe1b1", "embedding": null, "metadata": {"page_label": "144", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "92748345-b2ba-4f27-b35c-7146a901ae29", "node_type": "4", "metadata": {"page_label": "144", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "81952d5922ef324702720833bbf1b24e2e9bf5c60ac16e7540114915fad2a948", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "144 Control flows\n2\n 0 2\n0\n1\n= 1.0\n2\n 0 2\n0\n1\n= 0.5\n2\n 0 2\n0\n1\n= 0.1\nOrignal function Locally smoothed Globally smoothed\nFigure 5.6:Global versus local smoothing approaches on a gate functionf(x) := 1\nif x \u2208 [\u22121,1], and f(x) := 0 otherwise. In our notation, we can writef(x) =\nifelse(and(gt(x,\u22121),lt(x,1)),1,0). A local approach smoothes outgt andlt separately.\nA global approach uses the expectation of the whole program, see Remark 5.1. We\nobserve that, though the approaches differ for large\u03c3, they quickly coincide for\nsmaller \u03c3.\nfor sigmoid\u03c3 the CDF of\u03c3Z. We therefore obtain\nfglob\n\u03c3 (x,y,z ) = (\u03c0b \u2212\u03c0a)y+ (1 \u2212(\u03c0b \u2212\u03c0a))z.\nThe difference stems from the fact that the local approach smoothes\nout a \u2264x and x \u2264b independently (treating1X\u2265a and 1X\u2264b as\nindependent random variables), while the global approah smoothes\nout a \u2264x \u2264b simultaenously. In practice, both approaches ap-\nproximate the original function well as\u03c3 \u21920 and coincide for\u03c3\nsufficiently small as illustrated in Fig. 5.6.\n5.7 Else-if statements\nIn the previous section, we focused on if-else statements: conditionals\nwith only two branches. We now generalize our study to conditionals\nincluding else-if statements, that haveK branches.\n5.7.1 Encoding K branches\nFor conditionals with only2 branches, we encoded the branch that\nthe conditional needs to take using the binary variable\u03c0\u2208{0,1}. For", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1360, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9b416f9c-869c-4d53-9324-fa4ae5f79d93": {"__data__": {"id_": "9b416f9c-869c-4d53-9324-fa4ae5f79d93", "embedding": null, "metadata": {"page_label": "145", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c28a698-8d74-45b1-b3a7-7b1abc45f04c", "node_type": "4", "metadata": {"page_label": "145", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "141092ab6ba7d255998f1151698be985e30885ff744e3ac5e95237847f651a83", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5.7. Else-if statements 145\nconditionals withK branches, we need a way to encode which of theK\nbranches the conditional needs to take. To do so, we can use a vector\n\u03c0 \u2208{e1,..., eK}, where ei denotes the standard basis vector (a.k.a.\none-hot vector)\nei := (0,..., 1\ued19\ued18\ued17\ued1a\ni\n,..., 0),\na vector with a single one in the coordinatei and K \u22121 zeros. The\nvector ei is the encoding of acategorical variablei\u2208[K].\nCombining booleans\nTo form, such a vector\u03c0\u2208{e1,..., eK}, we can combine the previously-\ndefined comparison and logical operators to define\u03c0 = (\u03c01,...,\u03c0 K).\nHowever, we need to ensure that only one\u03c0i is non-zero. We give an\nexample in Example 5.1.\nArgmax and argmin operators\nAnother way to form\u03c0is to use theargmax and argmin operators\nargmax(p) := arg max\n\u03c0\u2208{e1,...,eK}\n\u27e8\u03c0,p\u27e9\nargmin(p) := arg min\n\u03c0\u2208{e1,...,eK}\n\u27e8\u03c0,p\u27e9= argmax(\u2212p).\nThey can be seen as a natural generalization of the greater than and\nless than operators. In case of ties, we break them arbitrarily.\n5.7.2 Conditionals\nWe can now express a conditional statement as the function\ncond: {e1,..., eK}\u00d7V K \u2192V defined by\ncond(\u03c0,v1,..., vK) :=\n\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f3\nv1 if \u03c0= e1\n...\nvK if \u03c0= eK\n(5.5)\n=\nK\u2211\ni=1\n\u03c0ivi.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1166, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b6e25c4c-118f-491a-b493-9b125e507782": {"__data__": {"id_": "b6e25c4c-118f-491a-b493-9b125e507782", "embedding": null, "metadata": {"page_label": "146", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1541be20-43de-4eaa-937a-ee2dfcc7efd2", "node_type": "4", "metadata": {"page_label": "146", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "a9ac5ff699da23116d06bf62f270d5deb8921ddd0bf63557c3f41ea089815184", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "146 Control flows\nSimilarly as for theifelse function, thecond function is discontinuous\nand nondifferentiable w.r.t.\u03c0\u2208{e1,..., eK}. However, given\u03c0= ei\nfixed for somei, the function is linear invi and constant invj for j \u0338= i.\nWe illustrate how to express a simple example, using this formalism.\nExample 5.1(Soft-thresholding operator). Thesoft-thresholdingop-\nerator (see also Section 16.4) is a commonly-used operator to pro-\nmote sparsity. It is defined by\nSoftThreshold(u,\u03bb) :=\n\uf8f1\n\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3\n0 if |u|\u2264 \u03bb\nu\u2212\u03bb if u\u2265\u03bb\nu+ \u03bb if u\u2264\u2212\u03bb\n.\nTo express it in our formalism, we can define\u03c0\u2208{e1,e2,e3}using\ncomparison operators as\n\u03c0:= (lt(|u|,\u03bb),gt(u,\u03bb),lt(u,\u2212\u03bb))\n= (step(\u03bb\u2212|u|),step(u\u2212\u03bb),step(\u2212u\u2212\u03bb)).\nEquivalently, we can also define\u03c0using an argmax operator as\n\u03c0:= argmax((\u03bb\u2212|u|,u \u2212\u03bb,\u2212u\u2212\u03bb)).\nIn case of ties, which happens at|u|= \u03bb, we keep only one non-zero\ncoordinate in\u03c0. We can then rewrite the operator as\nSoftThreshold(u,\u03bb) = cond(\u03c0,0,u \u2212\u03bb,u + \u03bb).\nAs we will see, replacingargmax with softargmax induces a cat-\negorical distribution over the three possible branches. The mean\nvalue can be seen as a smoothed out version of the operator, and we\ncan also compute the standard deviation, as illustrated in Fig. 5.7.\n5.7.3 Differentiating through branch variables\nFor \u03c0 fixed, cond(\u03c0,v1,..., vK) is a valid function w.r.t.vi, and can\ntherefore again be used as a node in a computational graph. Due to the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1395, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4f7ab63b-c073-4024-af37-a2bd3c172d9f": {"__data__": {"id_": "4f7ab63b-c073-4024-af37-a2bd3c172d9f", "embedding": null, "metadata": {"page_label": "147", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "74d54fe9-1693-4481-bf87-bc93e2581a93", "node_type": "4", "metadata": {"page_label": "147", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "53808ae1b406eb3a13063b6db45ad9cb67b65f1ddc99c4ed81a29fe5232dc68d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5.7. Else-if statements 147\n4\n 2\n 0 2 4\n6\n4\n2\n0\n2\n4\n6 Mean\nStandard deviation\nHard\nFigure 5.7:A conditional with three branches: the soft-thresholding operator (see\nExample 5.1). It is a piecewise linear function (dotted black line). Using a softargmax,\nwe can induce a categorical probability distribution over the three branches. The\nexpected value (blue line) can be seen as a smoothed out version of the operator.\nThe induced distribution allows us to also compute the standard deviation.\nlinearity w.r.t.vi, we obtain that the Jacobian w.r.t.vi is\n\u2202vicond(\u03c0,v1,..., vK) :=\n\uf8f1\n\uf8f2\n\uf8f3\nI if \u03c0= ei\n0 if \u03c0\u0338= ei\n.\nLet gi: Ui \u2192V be a differentiable function andui \u2208Ui. If we define\nthe composition\nf(\u03c0,u1,..., uK) := cond(\u03c0,g1(u1),...,g K(uK)),\nwe then obtain that the Jacobian w.r.t.ui is\n\u2202uif(\u03c0,u1,..., uK) :=\n\uf8f1\n\uf8f2\n\uf8f3\n\u2202gi(ui) if \u03c0= ei\n0 if \u03c0\u0338= ei\n.\nAs long as thegi functions are differentiable, we can therefore differen-\ntiate through the branch variablesui for \u03c0fixed.\n5.7.4 Differentiating through predicate variables\nAs we saw,\u03c0 can be obtained by combining comparison and logical\noperators, or it can be obtained by argmax and argmin operators.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1144, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "450273d4-a380-4a7c-81fd-792056ba233b": {"__data__": {"id_": "450273d4-a380-4a7c-81fd-792056ba233b", "embedding": null, "metadata": {"page_label": "148", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "acedcf90-dfa9-40b5-85a4-1524d2890e31", "node_type": "4", "metadata": {"page_label": "148", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "0d7ed175d48eb7cd09f9b47aed59ce1ae8677d6a094bef13ff3ec0b712f2e5a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "148 Control flows\nWe illustrate here why these operators are problematic. For example,\nsuppose we want to express the function\nfa(p,u1,..., uK) :=\n\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f3\nv1 if p= e1\n...\nvK if p= eK\n.\nIn our notation, this can be expressed as\nfa(p,u1,..., uK) := cond(argmax(p),g1(u1),...,g K(uK)),\nAs for the ifelse case, the Jacobian w.r.t.pis null almost everywhere,\n\u2202pfa(p,u1,..., uK) = 0.\n5.7.5 Continuous relaxations\nSimilarly to the Heaviside step function, the argmax and argmin func-\ntions are piecewise constant, with discontinuities in case of ties. Their\nJacobian are zero almost everywhere, and undefined in case of ties.\nTherefore, while their Jacobian is well-defined almost everywhere, they\nare uninformative and prevent gradient backpropagation. We can replace\nthe argmax with asoftargmax\nsoftargmax(p) := exp(p)\n\u2211K\ni=1 exp(pi) \u2208\u25b3K\nand similarly\nsoftargmin(p) := softargmax(\u2212p) \u2208\u25b3K.\nOther relaxations of the argmax are possible, as discussed in Section 13.7.\nSee also Section 14.5.3 for the perturbation perspective.\nFortunately, the definition\ncond(\u03c0,v1,..., vK) =\nK\u2211\ni=1\n\u03c0ivi\nis perfectly valid if we use\u03c0\u2208\u25b3K instead of\u03c0\u2208{e1,..., eK}, and", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1145, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "13ae1bdd-64b5-4e4e-9a84-5a800dc049b9": {"__data__": {"id_": "13ae1bdd-64b5-4e4e-9a84-5a800dc049b9", "embedding": null, "metadata": {"page_label": "149", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc6ef1f9-c6d4-46f9-99d0-80611cc80901", "node_type": "4", "metadata": {"page_label": "149", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "68788568c595556b4bdee139d464984b0adc2fc02caca3a838dc51500d344799", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5.8. For loops 149\ncan therefore be seen as anextension of Eq. (5.5). If we now define\nfs(p,u1,..., uK) := cond(softargmax(p),g1(u1),...,g K(uK))\n=\nK\u2211\ni=1\n[softargmax(p)]i \u00b7gi(ui), (5.6)\nthe Jacobian becomes\n\u2202pfs(p,u1,..., uK) = \u2202softargmax(p)(g1(u1),...,g K(uK)),\nwhich isnon-null everywhere, allowing gradients to backpropagate\nthrough the computational graph.\nProbabilistic perspective\nFrom a probabilistic perspective, we can view Eq. (5.6) as the ex-\npectation of gi(ui), where i \u2208[K] is a categorical random variable\ndistributed according to acategorical distributionwith parameter\n\u03c0= softargmax(p):\nfs(p,u1,..., uK) = Ei\u223cCategorical(softargmax(p)) [gi(ui)] .\nTaking the expectation over theK possible branches makes the function\ndifferentiable with respect top, at the cost of evaluating all branches,\ninstead of a single one. Similarly as for the if-else case, we can compute\nthe variance if needed as\nVi\u223cCategorical(softargmax(p)) [gi(ui)]\n=Ei\u223cCategorical(softargmax(p))\n[\n(fs(p,u1,..., uK) \u2212gi(ui))2\n]\n.\nThis is illustrated in Fig. 5.7.\n5.8 For loops\nFor loops are a control flow for sequentially calling a fixed numberK\nof functions, reusing the output from the previous iteration. In full\ngenerality, a for loop can be written as follows.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1250, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e5ff1692-e5b6-4a62-89f3-ec1951e6a287": {"__data__": {"id_": "e5ff1692-e5b6-4a62-89f3-ec1951e6a287", "embedding": null, "metadata": {"page_label": "150", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25850888-dd2e-4bfa-80fb-ee32b3fed858", "node_type": "4", "metadata": {"page_label": "150", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "f79c590a9b472fa760ce7e1b9224dd2dbbacb45e109cd3936e8e9c5b5ae78327", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "150 Control flows\nAlgorithm 5.1r= forloop(s0)\nfor k:= 1,...,K do\nsk := fk(sk\u22121)\nr:= sK\nAs illustrated in Fig. 5.8, this defines a computation chain. Assuming\nthe functionsfk are all differentiable, this defines a valid computation\ngraph, we can therefore use automatic differentiation to differentiate\nforloop w.r.t. its inputs0. Feedforward networks, reviewed in Section 4.2,\ncan be seen asparameterized for loops, i.e.,\nfk(sk\u22121) := gk(sk\u22121,wk),\nfor some differentiable functiongk.\nExample 5.2(Unrolled gradient descent). Suppose we want to min-\nimize w.r.t.wthe function\nL(w,\u03bb) := 1\nN\nN\u2211\ni=1\n\u2113(h(xi,w),yi) + \u03bb\n2 \u2225w\u22252\n2.\nGiven an initializationw0, gradient descent (Section 16.1) performs\niterations of the form\nwk = f(wk\u22121,\u03b3k,\u03bb) := wk\u22121 \u2212\u03b3k\u22071L(wk\u22121,\u03bb).\nGradient descent can therefore be expressed as a for loop with\nfk(wk\u22121) := f(wk\u22121,\u03b3k,\u03bb).\nThis means that we can differentiate through the iterations of\ngradient descent, as long asf is differentiable, meaning thatL is\ntwice differentiable. This is useful for instance to perform gradient-\nbased optimization of the hyperparameters\u03b3k or \u03bb. This a special\ncase of bilevel optimization; see also Chapter 11.\nExample 5.3(Bubble sort). Bubble sort is a simple sorting algo-\nrithm that works by repeatedly swapping elements if necessary.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1286, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1acefc48-2267-43f2-a44d-78072640a737": {"__data__": {"id_": "1acefc48-2267-43f2-a44d-78072640a737", "embedding": null, "metadata": {"page_label": "151", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c4a4b07e-b0f0-4ec8-80b0-611da8a80d1c", "node_type": "4", "metadata": {"page_label": "151", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "012e145a4247e778fb6fea493e0b465ae60aead2213827bcf9d170561041c372", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5.9. Scan functions 151\n...\nFigure 5.8: A for loop forms a com-\nputation chain. A feed forward network\ncan be seen as a parameterized for loop,\nwhere each functionfk depends on some\nparameters wk.\n...\nFigure 5.9:Computation graph of the\nscan function. Sequence-to-sequence\nRNNs can be seen as a parameterized\nscan function.\nMathematically, swapping two elementsi and j can be written as\na function fromRN \u00d7[N] \u00d7[N] to RN defined by\nswap(v,i,j ) := v+ (vj \u2212vi)ei + (vi \u2212vj)ej.\nWe can then write bubble sort as\nfor i:= 1,...,N do\nfor j := 1,...,N \u2212i\u22121 do\nv\u2032:= swap(v,j,j + 1)\n\u03c0:= step(vj \u2212vj+1)\nv\u2190ifelse(\u03c0,v\u2032,v)\nReplacing the Heaviside step function with the logistic function\ngives a smoothed version of the algorithm.\n5.9 Scan functions\nScan is a higher-order function (meaning a function of a function)\noriginating from functional programming. It is useful to perform an\noperation f on individual elementsuk while carrying the resultsk of\nthat operation to the next iteration.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 977, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "17f7a772-7a2d-45b1-ab44-6d8be2322e4a": {"__data__": {"id_": "17f7a772-7a2d-45b1-ab44-6d8be2322e4a", "embedding": null, "metadata": {"page_label": "152", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "08fe8ec4-19b2-4f24-ab07-9f15fb558d83", "node_type": "4", "metadata": {"page_label": "152", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "01ae5618e50d3f56c7a397d471a94ac1b1b874d09eea3cfc5a4c271946d64e9f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "152 Control flows\nAlgorithm 5.2r= scan(s0,u1,..., uK)\nfor k:= 1,...,K do\nsk,vk := f(sk\u22121,uk)\nr:= (sK,v1,..., vK)\nAs illustrated in Fig. 5.9, this again defines a valid computational\ngraph and can be differentiated through using autodiff, assuming the\nfunction f is differentiable. Sequence-to-sequence RNNs, reviewed in\nSection 4.7, can be seen as aparameterized scan. An advantage\nof this abstraction is that parallel scan algorithms have been studied\nextensively in computer science (Blelloch, 1989; Senguptaet al., 2010).\nExample 5.4(Prefix sum). Scan can be seen as a generalization of\nthe prefix sum(a.k.a. cumulated sum) from the addition to any\nbinary operation. Indeed, a prefix sum amounts to perform\nv1 := u1\nv2 := u1 + u2\nv3 := u1 + u2 + u3\n...\nwhich can be expressed as a scan by defining\nvk := sk\u22121 + uk\nf(sk\u22121,uk) := (vk,vk)\nstarting froms0 = 0 (sK and vK are redundant in this case).\n5.10 While loops\n5.10.1 While loops as cyclic graphs\nA while loop is a control flow used to repeatedly perform an operation,\nreusing the output of the previous iteration, until a certain condition is\nmet. Supposef: S\u2192{ 0,1}is a function to determine whether to stop\n(\u03c0= 1) or continue (\u03c0= 0) andg: S\u2192S is a function for performing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1229, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b837b583-a44a-4016-9e0d-c826e5fa4a2d": {"__data__": {"id_": "b837b583-a44a-4016-9e0d-c826e5fa4a2d", "embedding": null, "metadata": {"page_label": "153", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "941a2f2a-f0e7-4985-ad28-aa84aeaf75b0", "node_type": "4", "metadata": {"page_label": "153", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "24e43e2c127cd1e467e306eb07f2d7750c9c2e69419ee523c8e717cfa1b8b9ee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5.10. While loops 153\nifelse\nFigure 5.10: A while loop can be represented as a cyclic graph. The while loop\nstops if\u03c0= 1 and performs another iterations\u2190g(s), \u03c0\u2190f(s) if \u03c0= 0.\nan operation. Then, without loss of generality, a while loop can be\nwritten as follows.\nAlgorithm 5.3r= whileloop(s)\n\u03c0\u2190f(s)\nwhile \u03c0= 0 do\ns\u2190g(s)\n\u03c0\u2190f(s)\nr:= s\nThis definition is somewhat cyclic, as we used thewhile keyword.\nHowever, we can equivalently rewrite the algorithm recursively.\nAlgorithm 5.4r= whileloop(s)\n\u03c0:= f(s)\nif \u03c0= 0 then\nr:= s\nelse\nr:= whileloop(g(s))\nUnlike for loops and scan, the number of iterations of while loops is\nnot known ahead of time, and may even be infinite. In this respect, a\nwhile loop can be seen as acyclic graph, as illustrated in Fig. 5.10.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 753, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fb67eeed-0054-4d4b-b6d5-f4f2bba2648f": {"__data__": {"id_": "fb67eeed-0054-4d4b-b6d5-f4f2bba2648f", "embedding": null, "metadata": {"page_label": "154", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c406a578-f1f4-4286-8e29-b411aca57da7", "node_type": "4", "metadata": {"page_label": "154", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "57a8a6e06fa5ee65b2ece0709a399de8934a075a35eedea1bfb2ac0222664d2a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "154 Control flows\nImportance of lazy evaluation\nWe can also implement Algorithm 5.4 in terms of theifelse function\ndefined in Section 5.6 as\nr:= ifelse(f(s),s,whileloop(g(s)))\n= f(s) \u00b7s+ (1 \u2212f(s)) \u00b7whileloop(g(s)).\nHowever, to avoid an infinite recursion, it is crucial thatifelse supports\nlazy evaluation. That is,whileloop(g(s)) in the definition above should\nbe evaluated if and only if\u03c0= f(s) = 0. In other words, the fact that\nf(s) \u2208{0,1}is crucial to ensure that the recursion is well-defined.\n5.10.2 Unrolled while loops\nTo avoid the issues with unbounded while loops, we can enforce that a\nwhile loop stops afterT iterations, i.e., we can truncate the while loop.\nUnrolling Algorithm 5.4 gives (here withT = 3)\n\u03c00 := f(s0)\nif \u03c00 = 1 then\nr:= s0\nelse\ns1 := g(s0),\u03c01 := f(s1)\nif \u03c01 = 1 then\nr:= s1\nelse\ns2 := g(s1),\u03c02 := f(s2)\nif \u03c02 = 1 then\nr:= s2\nelse\nr:= s3 := g(s2)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 875, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6d33ab2a-4634-492e-be28-e94e274f158f": {"__data__": {"id_": "6d33ab2a-4634-492e-be28-e94e274f158f", "embedding": null, "metadata": {"page_label": "155", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7afe79d1-666e-4f64-8417-3c9650697574", "node_type": "4", "metadata": {"page_label": "155", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "7a2d9ee2a4ec1cffa1c5430b49a3db3c78f0d30704bccc53f8b91c4db7706107", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5.10. While loops 155\nUsing theifelse function, we can rewrite it as\nr= ifelse(\u03c00,\ns0,\nifelse(\u03c01,\ns1,\nifelse(\u03c02,\ns2,\ns3)))\nwhich is itself equivalent to\nr= \u03c00s0 + (1 \u2212\u03c00) [\u03c01s1 + (1 \u2212\u03c01) [\u03c02s2 + (1 \u2212\u03c02)s3]]\n= \u03c00s0 + (1 \u2212\u03c00)\u03c01s1 + (1 \u2212\u03c00)(1 \u2212\u03c01)\u03c02s2 + (1 \u2212\u03c00)(1 \u2212\u03c01)(1 \u2212\u03c02)s3.\nMore generally, forT \u2208N, the formula is\nr=\nT\u2211\ni=0\n((1 \u2212\u03c00) ... (1 \u2212\u03c0i\u22121)) \u03c0isi\n=\nT\u2211\ni=0\n\uf8eb\n\uf8ed\ni\u22121\u220f\nj=0\n(1 \u2212\u03c0j)\n\uf8f6\n\uf8f8\u03c0isi,\nwhere we defined\nsi := g(si\u22121) := gi(s0) := g\u25e6\u00b7\u00b7\u00b7\u25e6 g\ued19 \ued18\ued17 \ued1a\ni times\n(s0) \u2208S\n\u03c0i := f(si) \u2208{0,1}.\nSee also (Petersen et al., 2021). If we further define the shorthand\nnotation\n\u02dc\u03c00 := \u03c00\n\u02dc\u03c0i :=\n\uf8eb\n\uf8ed\ni\u22121\u220f\nj=0\n1 \u2212\u03c0j\n\uf8f6\n\uf8f8\u03c0i i\u2208{1,...,T },\nso that \u02dc\u03c0:= (\u02dc\u03c00,\u02dc\u03c01,..., \u02dc\u03c0T) \u2208\u25b3T+1 is a discrete probability distribu-\ntion containing the probabilities to stop at each of theT iterations, we", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 769, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6ff9a6e7-b8dc-46ea-ba90-6013c3fffd43": {"__data__": {"id_": "6ff9a6e7-b8dc-46ea-ba90-6013c3fffd43", "embedding": null, "metadata": {"page_label": "156", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd8640cb-a525-4be0-9edf-43e24395e18a", "node_type": "4", "metadata": {"page_label": "156", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "3e14692df4da3d66128c727e2d147ef8e9fcfa90e8da633755c95cd3de15e72e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "156 Control flows\ncond\n...\n...\n...\nFigure 5.11:Computation graph of an unrolled truncated while loop. As in Fig. 5.5,\nwe depict continuous variables in dense lines and discrete variables in dashed lines.\nThe output of a while loop with at mostT iterations can be written as a conditional\nwith T + 1 branches, cond(\u02dc\u03c0,s0,..., sT) =\n\u2211T\nt=0 \u02dc\u03c0tst.\ncan rewrite the output of a truncated while using a conditional,\nr= cond(\u02dc\u03c0,s0,s1,..., sT) =\nT\u2211\nt=0\n\u02dc\u03c0tst.\nThis is illustrated in Fig. 5.11.\nExample 5.5(Computing the square root using Newton\u2019s method).\nComputing the square root\u221ax of a real numberx> 0 can be cast\nas a root finding problem, which we can solve using Newton\u2019s\nmethod. Starting from an initializations0, the iterations read\nsi+1 := g(si) := 1\n2\n(\nsi + x\nsi\n)\n.\nTo measure the error on iterationi, we can define\n\u03b5(si) := 1\n2(s2\ni \u2212x)2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 843, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a4d36f28-bd17-4ffd-badb-b4a8b6855ac1": {"__data__": {"id_": "a4d36f28-bd17-4ffd-badb-b4a8b6855ac1", "embedding": null, "metadata": {"page_label": "157", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "19de5903-10bd-40e4-b9a1-22147898db87", "node_type": "4", "metadata": {"page_label": "157", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "8a16056c7d8d0315cb3ffd170ebd9329cc2367788da208b20e21fb1a98feeee7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5.10. While loops 157\nAs a stopping criterion, we can then use\n\u03c0i :=\n\uf8f1\n\uf8f2\n\uf8f3\n1 if \u03b5(si) \u2264\u03c4\n0 otherwise\n= step(\u03c4 \u2212\u03b5(si)),\nwhere 0 < \u03c4\u226a1 is an error tolerance andstep is the Heaviside\nstep function.\n5.10.3 Markov chain perspective\nGiven the functiong: S \u2192Sand the initializations0 \u2208S, a while\nloop can only go through a discrete set of valuess0,s1,s2,... defined\nby si = g(si\u22121). This set is potentially countably infinite if the while\nloop is unbounded, and finite if the while loop is guaranteed to stop.\nWhether the loop moves from the statesi to the statesi+1, or stays\nat si, is determined by the stopping criterion\u03c0i \u2208{0,1}. To model\nthe state of the while loop, we can then consider aMarkov chain\nwith a discrete space{s0,s1,s2,... }, which we can always identify with\n{0,1,2,... }, with transition probabilities\nP(St+1 = si|St = sj) = pi,j :=\n\uf8f1\n\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3\n\u03c0i if i= j\n(1 \u2212\u03c0i) if i= j+ 1\n0 otherwise\n,\nand initial stateS0 = s0. Here,St is the value at iterationtof the loop.\nNote that since\u03c0i \u2208{0,1}, thepi,j values are \u201cdegenerate\u201d probabilities.\nHowever, this framework lets us generalize to a smooth version of the\nwhile loop naturally. To illustrate the framework, if the while loop stops\nat T = 3, the transition probabilities can be cast as a matrix\nP := (pi,j)T\ni,j=0 :=\ns0 s1 s2 s3\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\uf8f6\n\uf8f7\uf8f7\uf8f8\ns0 0 1 0 0\ns1 0 0 1 0\ns2 0 0 0 1\ns3 0 0 0 1\n.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1346, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fef1d7cc-b56a-4075-8069-a3675a4a4691": {"__data__": {"id_": "fef1d7cc-b56a-4075-8069-a3675a4a4691", "embedding": null, "metadata": {"page_label": "158", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c791a107-09fd-48a1-a566-a69a58d48299", "node_type": "4", "metadata": {"page_label": "158", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "a6446bbc926b4f0b79c52795105f0c2685da562ac94adf4e7ddecb17857827be", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "158 Control flows\nThe outputrof the while-loop is determined by the time at which the\nstate stays at the same value\nI = min{i\u2208{1,2,... }s.t. Si = Si\u22121}.\nNote thatI itself is a random variable, as it is defined by theSi variables.\nIt is called astopping time. The output of the chain is then\nr= E[SI]\n=\n+\u221e\u2211\ni=1\nP(I = i)E[Si|I = i]\n=\n+\u221e\u2211\ni=1\nP(I = i)si\u22121\n=\n+\u221e\u2211\ni=1\ni\u22122\u220f\nj=0\n(1 \u2212\u03c0j)\u03c0i\u22121si\u22121\n=\n+\u221e\u2211\ni=0\ni\u22121\u220f\nj=0\n(1 \u2212\u03c0j)\u03c0isi.\nBecause the stopping time is not known ahead of time, the sum overi\ngoes from0 to \u221e. However, if we enforce in the stopping criterion that\nthe while loop runs no longer thanT iterations, by setting\n\u03c0i := or(f(si),eq(i,T)) \u2208{0,1},\nwe then naturally recover the expression found by unrolling the while\nloop before,\nr= E[SI] =\nT\u2211\ni=0\ni\u22121\u220f\nj=0\n(1 \u2212\u03c0j)\u03c0isi.\nFor example, withT = 3, the transition probability matrix is\nP =\ns0 s1 s2 s3\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\uf8f6\n\uf8f7\uf8f7\uf8f8\ns0 \u03c00 1 \u2212\u03c00 0 0\ns1 0 \u03c01 1 \u2212\u03c01 0\ns2 0 0 \u03c02 1 \u2212\u03c02\ns3 0 0 0 1\n.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 922, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6baa806f-e2bb-4503-8d11-d600a208fea5": {"__data__": {"id_": "6baa806f-e2bb-4503-8d11-d600a208fea5", "embedding": null, "metadata": {"page_label": "159", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c6465774-0c35-4cec-9535-5cabdce7d516", "node_type": "4", "metadata": {"page_label": "159", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "4276d12fb43ac35313cb3892c523c417e8e02c4662fa83fb66ef5b2c239d2800", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5.11. Summary 159\nSmoothed while loops\nWith the help of this framework, we can backpropagate even through the\nwhile loop\u2019s stopping criterion, provided that we smooth out the predi-\ncate. For example, we saw that the stopping criterion in Example 5.5 is\nf(si) = step(\u03c4 \u2212\u03b5(si)) and therefore\n\u03c0i := or(f(si),eq(i,T)) \u2208{0,1}.\nDue to thestep function, the derivative of the while loop with respect\nto \u03c4 will always be0, just like it was the case for if-else statements. If\nwe change the stopping criterion tof(si) = sigmoid(\u03c4\u2212\u03b5(si)), we then\nhave (recall thator is well defined on[0,1] \u00d7[0,1])\n\u03c0i := or(f(si),eq(i,T)) \u2208[0,1].\nWith sigmoid, we obtain more informative derivatives. In particular,\nwith sigmoid = logistic, the derivatives w.r.t.\u03c4 are always non-zero.\nThe smoothed output is expressed as before as the expectation\nr= E[SI] =\nT\u2211\ni=0\ni\u22121\u220f\nj=0\n(1 \u2212\u03c0j)\u03c0isi\n=\nT\u2211\ni=0\ni\u22121\u220f\nj=0\n(1 \u2212sigmoid(\u03b5(si) \u2212\u03c4))sigmoid(\u03b5(si) \u2212\u03c4)si.\nInstead of enforcing a numberT of iterations, it is also possible to stop\nwhen the probability of stopping becomes high enough (Petersenet al.,\n2021), assuming that the probability of stopping converges to1.\n5.11 Summary\n\u2022 For conditionals, we saw that differentiating through the branch\nvariables is not problematic given a fixed predicate.\n\u2022 However, for the predicate variable, we saw that a differentiable\nrelaxation is required to avoid null derivatives.\n\u2022 We introduced soft comparison operators in a principled manner,\nusing a stochastic process perspective, as well as the continuous\nextension of logical operators.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1546, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dada8533-19a6-4585-886b-f7b5be30ca2a": {"__data__": {"id_": "dada8533-19a6-4585-886b-f7b5be30ca2a", "embedding": null, "metadata": {"page_label": "160", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4438ba88-20b1-475c-867c-4749033d1cf6", "node_type": "4", "metadata": {"page_label": "160", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "cf91ee56caef19148b00a8daa4d3ac131bd00224e66fdb8d5c73291a36bca602", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "160 Control flows\n\u2022 For loops and scan define valid computational graphs, as their\nnumber of iterations is fixed ahead of time. Feedforward networks\nand RNNs can be seen as parameterized for loops and scan,\nrespectively.\n\u2022 Unlike for loops and scan, the number of iterations of while loops\nis not known ahead of time and may even be infinite. However,\nunrolled while loops define valid directed acyclic graphs. We\ndefined a principled way to differentiate through the stopping\ncriterion of a while loop, thanks to a Markov chain perspective.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 541, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1ec86bf8-9986-4fe4-8889-3068ed1aa07c": {"__data__": {"id_": "1ec86bf8-9986-4fe4-8889-3068ed1aa07c", "embedding": null, "metadata": {"page_label": "161", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76b040cf-eb4f-4490-b4e0-f0fa73bb1f05", "node_type": "4", "metadata": {"page_label": "161", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "ef37df7187391d7808d3242a86d47cc37b24e9e43e92eb15189fed18a9944d50", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6\nData structures\nIn computer science, a data structure is a specialized format for organiz-\ning, storing and accessing data. Mathematically, a data structure forms\na so-called algebraic structure: it consists of a set and the functions\nto operate on that set. In this chapter, we review how to incorporate\ndata structures into differentiable programs, with a focus on lists and\ndictionaries.\n6.1 Lists\nA list is an ordered sequence of elements. We restrict ourselves to lists\nwhose elements all belong to the same value spaceV. Formally, we\ndenote a list of fixed lengthK with values inVby aK-tuple\nl:= (l1,..., lK) \u2208LK(V)\nwhere eachli \u2208V and where\nLK(V) := VK = V\u00d7\u00b7\u00b7\u00b7\u00d7V\ued19 \ued18\ued17 \ued1a\nK times\n.\n161", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 691, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c9e9b02f-d500-4da6-af12-82b3cb43bd3d": {"__data__": {"id_": "c9e9b02f-d500-4da6-af12-82b3cb43bd3d", "embedding": null, "metadata": {"page_label": "162", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2740a957-5269-4e3c-b5c7-8801afe45c1f", "node_type": "4", "metadata": {"page_label": "162", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "ad1cf99bed0a763133b62a10e7a81970d2afdf93a929a1226eed877a48169350", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "162 Data structures\n6.1.1 Basic operations\nGetting values\nWe first present how to retrieve values from a listl\u2208LK(V). We define\nthe functionlist.get: LK(V) \u00d7[K] \u2192V as\nlist.get(l,i) := li.\nThe function is continuous and differentiable inl\u2208LK(V) but not in\ni\u2208[K], as it is a discrete variable. In the particular caseV= R, LK(V)\nis equivalent toRK and we can therefore write\nlist.get(l,i) = \u27e8l,ei\u27e9,\nwhere {e1,..., eK}is the standard basis ofRK.\nSetting values\nWe now present how to replace values from a listl\u2208LK(V). We define\nthe functionlist.set: LK(V) \u00d7[K] \u00d7V\u2192L K(V) as\n[list.set(l,i, v)]j :=\n\uf8f1\n\uf8f2\n\uf8f3\nv if i= j\nlj if i\u0338= j\n,\nfor j \u2208[K]. In the functional programming spirit, the function returns\nthe whole new list, even though a single element has been modified.\nAgain, the function is continuous and differentiable inl\u2208LK(V) and\nv \u2208V but not ini \u2208[K]. In the particular caseV= R, given a list\nl= (l1,...,l K), we can write\nlist.set(l,i,v ) = (v\u2212li)ei.\nThat is, we subtract the old valueli and add the new valuev at the\nlocation i\u2208[K].\nImplementation\nA fixed-length list can be implemented as anarray, which enablesO(1)\nrandom access to individual elements. The hardware counterpart of\nan array is random access memory (RAM), in which memory can be\nretrieved by address (location).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1280, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4ba30823-0fd8-49fd-ac85-f2ea7285b84f": {"__data__": {"id_": "4ba30823-0fd8-49fd-ac85-f2ea7285b84f", "embedding": null, "metadata": {"page_label": "163", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2e3b474e-8a8d-4a13-8018-cd83473e3068", "node_type": "4", "metadata": {"page_label": "163", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "cb2b5aeb703160a49415b42fab94946f0f70b43a793d5a57977de3f9b1fdb5bd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6.1. Lists 163\n6.1.2 Operations on variable-length lists\nSo far, we focused on lists of fixed lengthK. We now turn our attention\nto variable-length lists, whose size can decrease or increase over time.\nIn addition to thelist.get and list.set functions, they support functions\nthat can change the size of a list.\nInitializing lists\nIn order to initialize a list, we definelist.init: V\u2192L 1(V) as\nlist.init(v) := (v),\nwhere used(v) to denote a1-tuple.\nPushing values\nIn order to add new values either to the left or to the right, we define\nlist.pushLeft: LK(V) \u00d7V\u2192L K+1(V) as\nlist.pushLeft(l,v) := (v,l1,..., lK).\nand list.pushRight: LK(V) \u00d7V\u2192L K+1(V) as\nlist.pushRight(l,v) := (l1,..., lK,v).\nPopping values\nIn order to remove values either from the left or from the right, we\ndefine list.popLeft: LK(V) \u2192LK\u22121(V) \u00d7V as\nlist.popLeft(l) := (l2,..., lK),l1\nand list.popRight: LK(V) \u2192LK\u22121(V) \u00d7V as\nlist.popRight(l) := (l1,..., lK\u22121),lK.\nThe setL0(V) is a singleton which contains the empty list.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 989, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "13f5eba0-1c19-40be-bfb0-ec855f2ca7d5": {"__data__": {"id_": "13f5eba0-1c19-40be-bfb0-ec855f2ca7d5", "embedding": null, "metadata": {"page_label": "164", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3912a02a-d236-4640-b5f0-d8e34dc8f2d5", "node_type": "4", "metadata": {"page_label": "164", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "0045ef63fdd0711d0adc7b525f4794b319f7a5484951a9bf709056c1d7e79776", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "164 Data structures\nInserting values\nThe pushLeft and pushRight functions can only insert values at the\nbeginning and at the end of a list, respectively. We now study the insert\nfunction, whose goal is to be able to add a new value at an arbitrary\nlocation, shifting all values to the right and increasing the list size by1.\nWe define the functionlist.insert : LK(V) \u00d7[K+ 1] \u00d7V\u2192L K+1(V) as\n[list.insert(l,i, v)]j :=\n\uf8f1\n\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3\nlj if j <i\nv if j = i\nlj\u22121 if j >i\n,\nforj \u2208[K+1].Asforthe list.set function,list.insert isreadilycontinuous\nand differentiable inland v, but not ini, as it is a discrete variable.\nAs special cases, we naturally recover\nlist.insert(l,1,v) = pushLeft(l,v),\nlist.insert(l,K + 1,v) = pushRight(l,v).\nDifferentiability\nThe list.init, list.push and list.pop functions are readily continuous and\ndifferentiable with respect to their arguments (a continuous relaxation\nis not needed). As for thelist.set function, the list.insert function is\ncontinuous and differentiable inland v, but not ini.\nImplementation\nUnder the hood, a variable-length list can be implemented as a linked\nlist or as a dynamic array. A linked list givesO(K) random access while\na dynamic array allowsO(1) random access, at the cost of memory\nreallocations.\nStacks and queues\nThelist.pushRight andlist.popRight functionscanbeusedtoimplement\na stack (last in first out a.k.a. LIFO behavior). Thelist.pushLeft and\nlist.popRight functions can be used to implement aqueue (first in first\nout a.k.a. FIFO behavior).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1501, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4eb85f59-4e13-41fc-8292-0972786ce823": {"__data__": {"id_": "4eb85f59-4e13-41fc-8292-0972786ce823", "embedding": null, "metadata": {"page_label": "165", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b6331ee-447e-4cfd-a8fc-4f5b1f5bb0db", "node_type": "4", "metadata": {"page_label": "165", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "d5b165e6b50469de3d8de5eb5471c05d5f6f665863ccc19e27465b6bac7ad1eb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6.1. Lists 165\n6.1.3 Continuous relaxations using soft indexing\nGetting values\nIn order to be able to differentiatelist.get w.r.t. indexing, a natural\nidea is to replace the integer indexi\u2208[K] by a distribution\u03c0i \u2208\u25b3K,\nwhich we can interpret as asoft index. An integer indexi \u2208[K] is\nthen equivalent to adelta distribution\u03c0i \u2208{e1,..., eK}. We define\nthe continuous relaxationlist.softGet: LK(V) \u00d7\u25b3K \u2192conv(V) as\nlist.softGet(l,\u03c0i) :=\nK\u2211\nj=1\n\u03c0i,jlj\n= cond(\u03c0i,l1,..., lK)\n= EI\u223cCategorical(\u03c0i)[lI],\nwhere cond is studied in Section 5.7. In the particular caseV= R, we\nobtain\nlist.softGet(l,i) = \u27e8l,\u03c0i\u27e9.\nThis is illustrated in Fig. 6.1.\nThe choice of the distribution \u03c0i = ( \u03c0i,1,...,\u03c0 i,K) encodes the\nimportance of the elements(l1,..., lK) w.r.t. li. If we consider that the\nsmaller |i\u2212j|is, the more relatedli and lj are, then it makes sense\nto define a distribution centered aroundi (i.e., such that the mode of\nthe distribution is achieved ati). For example, limiting ourselves to the\nneighbors li\u22121 and li+1 (i.e., a window of size1), we can define the\nsparse distribution\n\u03c0i := 1\n4 \u00b7ei\u22121 + 1\n2ei + 1\n4 \u00b7ei+1 \u2208\u25b3K.\nIn this particular case, the continuous relaxation of thelist.get function\ncan then be expressed as adiscrete convolution,\nlist.softGet(l,\u03c0i) = (list.get(l,\u00b7) \u2217\u03ba) (i) =\n\u221e\u2211\nj=\u2212\u221e\nlist.get(l,i \u2212j)\u03ba(j),\nwhere \u03ba(\u22121) := 1\n4 , \u03ba(1) := 1\n4 , \u03ba(0) := 1\n2 , and\u03ba(j) := 0 for j \u0338\u2208{\u22121,0,1}.\nAssuming V = RM, the computational complexity oflist.softGet is\nO(M \u00b7|supp(\u03c0i)|).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1474, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8e936397-905a-48a7-9342-bdce3216696e": {"__data__": {"id_": "8e936397-905a-48a7-9342-bdce3216696e", "embedding": null, "metadata": {"page_label": "166", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "98b4a3bd-50a2-434f-8a86-4cd5f91be920", "node_type": "4", "metadata": {"page_label": "166", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "c6519bef6ad99d3cef6a3e450ce0350ebf88ec5dbe1efbecc30cacfaa6e93242", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "166 Data structures\n3 421 5 3 421 5\nFigure 6.1:The list.get(l,i) function is continuous and differentiable inlbut not\nin i. Its relaxationlist.sofGet(l,\u03c0i) is differentiable in bothland \u03c0i. WhenV= R,\nlist.softGet(l,\u03c0i) can be seen as taking the inner product between the listland the\nprobability distribution\u03c0i, instead of the delta distribution (canonical vector)ei.\nSetting values\nTo differentiate w.r.t. indexing, we can define the continuous relaxation\nlist.softSet: LK(V) \u00d7\u25b3K \u00d7V\u2192L K(conv(V)) as\n[list.softSet(l,\u03c0i,v)]j := E[list.set(l,I, v)]j\n= P(I = j)v+ P(I \u0338= j)lj\n= \u03c0i,jv+ (1 \u2212\u03c0i,j)lj,\nwhere j \u2208[K] and I \u223cCategorical(\u03c0i). Equivalently, we can write\nlist.softSet(l,\u03c0i,v) = (\u03c0i,1v+ (1 \u2212\u03c0i,1)l1,...,\u03c0 1,Kv+ (1 \u2212\u03c01,K)lK)\n= (ifelse(\u03c0i,1,v,l1),..., ifelse(\u03c0i,K,v,lK)),\nwhere ifelse is studied in Section 5.6. Since\nifelse(\u03c0,u1,u0) = EI\u223cBernouilli(\u03c0)[uI],\nthis relaxation amounts to using an element-wise expectation. As a\nresult, the list output bylist.softSet takes values inconv(V) instead of\nV. Note however that whenV= RM, thenconv(V) = RM as well.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1056, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ab48d009-0c7d-4643-af73-b34eb7711219": {"__data__": {"id_": "ab48d009-0c7d-4643-af73-b34eb7711219", "embedding": null, "metadata": {"page_label": "167", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bed722d8-dcd6-43b2-8ca2-a83b7e40f440", "node_type": "4", "metadata": {"page_label": "167", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "fd9c10bfa7feaf831b1c90b189d34eded9746d9289364b8555a9dcb02ae0ce81", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6.1. Lists 167\nInserting values\nTo differentiate value insertion w.r.t. indexing, we can define the contin-\nuous relaxationlist.softInsert: LK(V) \u00d7\u25b3K+1 \u00d7V\u2192L K+1(conv(V))\n[list.softInsert(l,\u03c0i,v)]j := E[list.insert(l,I, v)]\n= P(I >j)lj + P(I = j)v+ P(I <j)lj\u22121,\nwhere I \u223cCategorical(\u03c0i). The three necessary probabilities can easily\nbe calculated forj \u2208[K+ 1] by\nP(I >j) =\n\uf8f1\n\uf8f2\n\uf8f3\n0 if j = K+ 1\n\u2211K+1\nk=j+1 \u03c0i,k otherwise\nP(I = j) = \u03c0i,j\nP(I <j) =\n\uf8f1\n\uf8f2\n\uf8f3\n0 if j = 1\n\u2211j\u22121\nk=1 \u03c0i,k otherwise\n.\nMulti-dimensional indexing\nIn multi-dimensional lists (arrays or tensors), each elementli \u2208V of\na listl\u2208LK1,...,KT(V) can now be indexed by a multivariate integer\ni= (i1,...,i T) \u2208[K1]\u00d7\u00b7\u00b7\u00b7\u00d7 [KT], whereT \u2208N is the number of axes of\nl. We can always flatten a multi-dimensional list into an uni-dimensional\nlist by replacing the multi-dimensional indexi\u2208[K1] \u00d7\u00b7\u00b7\u00b7\u00d7 [KT] by\na flat indexi\u2208[K1 ...K T]. The converse operation, converting a flat\nuni-dimensional array into a multi-dimensional array, is also possible.\nTherefore, there is abijection between [K] and [K1] \u00d7\u00b7\u00b7\u00b7\u00d7 [KT] for\nK := K1 ...K T.\nThis means that the previous discussion on soft indexing in the\nuni-dimensional setting readily applies to the multi-dimensional setting.\nAll it takes is the ability to define a probability distribution\u03c0i \u2208\n\u25b3K1\u00d7\u00b7\u00b7\u00b7\u00d7KT. For example, when working with images, we can define a\nprobabilitydistributionputtingprobabilitymassonlyontheneighboring\npixels of pixeli, a standard approach in image processing. Another\nsimple approach is to use a product of axis-wise probability distributions.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1564, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f7c0d771-ee6a-4ab8-bb11-21d9738a3078": {"__data__": {"id_": "f7c0d771-ee6a-4ab8-bb11-21d9738a3078", "embedding": null, "metadata": {"page_label": "168", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "16609c01-c14f-4621-8e29-4287e0689ec9", "node_type": "4", "metadata": {"page_label": "168", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "8de6446e4011e26d75214271980b5d19ba12837a635b89b4c64bede54eb37a18", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "168 Data structures\n6.2 Dictionaries\nA dictionary (a.k.a. associative array or map) is an unordered list of\nkey-value pairs, such that each possible key appears at most once in\nthe list. We denote the set of keys byKand the set of values byV(both\nbeing potentially infinite). We can then define the set of dictionaries of\nsize L from Kto Vby\nDL(K,V) := LL(K\u00d7V ) = (K\u00d7V )L\nand one such dictionary by\nd:= ((k1,v1),..., (kL,vL)) \u2208DL(K,V).\n6.2.1 Basic operations\nGetting values\nThe goal of thedict.get function is to retrieve the value associated with\na key, assuming that the dictionary contains this key. Formally, we\ndefine thedict.get: DL(K,V) \u00d7K\u2192V\u222a{\u221e} function as\ndict.get(d,k) :=\n\uf8f1\n\uf8f2\n\uf8f3\nvi if \u2203i\u2208[L] s.t. k= ki\n\u221e if k\u0338\u2208{k1,..., kL}\n.\nThe function is continuous and differentiable in the dictionaryd, but\nnot in the keyk. Equivalently, we can write the function as\ndict.get(d,k) :=\n\u2211L\ni=1 eq(k,ki)vi\n\u2211L\ni=1 eq(k,ki) .\nThe denominator encodes the fact that the function is undefined if no\nkey in the dictionarydmatches the keyk. Assumingk\u2208{k1,..., kL}\nand V= RM, we can also write\ndict.get(d,k) = vi where i= arg max\nj\u2208[L]\n\u2225k\u2212kj\u22252,\nwhich shows that we can seedict.get as anearest neighbor search.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1195, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "212c6696-5166-4a07-8824-c91c349a8c27": {"__data__": {"id_": "212c6696-5166-4a07-8824-c91c349a8c27", "embedding": null, "metadata": {"page_label": "169", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3214b439-bafd-422d-9907-feda1b4b6eef", "node_type": "4", "metadata": {"page_label": "169", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "a6805420ca303a0b39cf602c7fcba5cb6c28ece0afb687c12e9b34d513911dcd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6.2. Dictionaries 169\n0.0 0.5 1.0\nKeys\n0.0\n0.2\n0.4Values\n(k1, v1)\n(k2, v2) (k3, v3)\n(k4, v4)\nKey-value pairs\nKernel Estimator\nFigure 6.2:Given a set of key-value pairs(ki,vi) \u2208K\u00d7V defining a dictionaryd,\nwe can estimate a continuous mapping fromKto Vusing Nadaraya\u2013Watson kernel\nregression (here, illustrated withK= V= R). When keys are normalized to have\nunit norm, this recovers softargmax attention from Transformers.\nSetting values\nThe goal of thedict.set function is to replace the value associated with\nan existing key. Formally, we define thedict.set : DL(K,V) \u00d7K\u00d7V\u2192\nDL(K,V) function as\n(dict.set(d,k,v))i :=\n\uf8f1\n\uf8f2\n\uf8f3\n(ki,v) if ki = k\n(ki,vi) if ki \u0338= k\n.\nThe function leaves the dictionary unchanged if no key in the dictionary\nmatches the input keyk. The function is continuous and differentiable\nin dand v, but not ink.\nImplementation\nWhile we view dictionaries as lists of key-value pairs, in practice, a\ndictionary (a.k.a. associative array) is often implemented using a hash\ntable or search trees. The hardware counterpart of a dictionary is called\ncontent-addressable memory (CAM), a.k.a. associative memory.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1119, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3f641b18-ef26-4eb9-aafa-39a636923c41": {"__data__": {"id_": "3f641b18-ef26-4eb9-aafa-39a636923c41", "embedding": null, "metadata": {"page_label": "170", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "78b430b4-7272-415b-a2a2-73912578ee34", "node_type": "4", "metadata": {"page_label": "170", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "69f25f93b97dcc1d7911be771cad8f5c5ada22a9dfdd17e5f653ff13a19a102b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "170 Data structures\n6.2.2 Continuous relaxation using kernel regression\nA dictionary can been seen as a (potentially non-injective) function that\nassociates a valuevto each keyk. To obtain a continuous relaxation of\nthe operations associated to a dictionary, we can adopt a probabilistic\nperspective of the mapping from keys to values. We can view keys and\nvalues as two continuous random variablesK and V. We can express\nthe conditional PDFf(v|k) of V|K in terms of the joint PDFf(k,v)\nof (K,V ) and the marginal PDFf(k) of K as\nf(v|k) = f(k,v)\nf(k) .\nIntegrating, we obtain theconditional expectation\nE[V|K = k] =\n\u222b\nV\nf(v|k)vdv=\n\u222b\nV\nf(k,v)\nf(k) vdv.\nThis is theBayes predictor, in the sense thatE[V|K] is the minimizer\nof E[(h(K) \u2212V)2] over the space of measurable functionsh: K \u2192\nV. Using a sample ofL input-output pairs(ki,vi), corresponding to\nkey-value pairs in our case,Nadaraya\u2013Watson kernel regression\nestimates the joint PDF and the marginal PDF usingkernel density\nestimation (KDE). Using a product of isotropic kernels\u03ba\u03c3 and \u03c1\u03c3 for\nkey-value pairs, we can define\n\u02c6f\u03c3(k,v) := 1\nL\nL\u2211\ni=1\n\u03ba\u03c3(k\u2212ki)\u03c1\u03c3(v\u2212vi).\nThe corresponding marginal distribution on the keys is then given as\n\u02c6f\u03c3(k) :=\n\u222b\nV\n\u02c6f\u03c3(k,v)dv\n= 1\nL\nL\u2211\ni=1\n\u03ba\u03c3(k\u2212ki)\n\u222b\nV\n\u03c1\u03c3(v\u2212vi)dv\n= 1\nL\nL\u2211\ni=1\n\u03ba\u03c3(k\u2212ki).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1269, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bd7c2e12-38d9-41bd-89a6-ea87737c1191": {"__data__": {"id_": "bd7c2e12-38d9-41bd-89a6-ea87737c1191", "embedding": null, "metadata": {"page_label": "171", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b493f58-b0b2-495b-bce8-3e145def64b5", "node_type": "4", "metadata": {"page_label": "171", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "7338534a3700dbc334de5c0e2ef1ea634593693919fec051c9effc7e1f537240", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6.2. Dictionaries 171\nReplacing f with \u02c6f\u03c3, we obtain the following estimator of the conditional\nexpectation\n\u02c6E[V|K = k] :=\n\u222b\nV\n\u02c6f\u03c3(k,v)\n\u02c6f\u03c3(k)\nvdv\n=\n\u222b\nV\n1\nL\n\u2211L\ni=1 \u03ba\u03c3(k\u2212ki)\u03c1\u03c3(v\u2212vi)\n1\nL\n\u2211L\ni=1 \u03ba\u03c3(k\u2212ki) vdv\n=\n\u2211L\ni=1 \u03ba\u03c3(k\u2212ki)\n\u222b\nV\u03c1\u03c3(v\u2212vi)vdv\n\u2211L\ni=1 \u03ba\u03c3(k\u2212ki)\n=\n\u2211L\ni=1 \u03ba\u03c3(k\u2212ki)vi\n\u2211L\ni=1 \u03ba\u03c3(k\u2212ki) .\nIn the above, we assumed that\u03c1\u03c3(v\u2212vi) = pvi,\u03c3(v), wherepvi,\u03c3(v) is\nthe PDF of a distribution whose mean isvi, so that\n\u222b\nV\n\u03c1\u03c3(v\u2212vi)vdv= EV\u223cpvi,\u03c3[V] = vi.\nGiven a dictionaryd= ((k1,v1),..., (kL,vL)), we can therefore define\nthe dict.softGet: DL(K,V) \u00d7K\u2192 conv(V) function as\ndict.softGet(d,k) :=\n\u2211L\ni=1 \u03ba\u03c3(k\u2212ki)vi\n\u2211L\ni=1 \u03ba\u03c3(k\u2212ki) .\nThis kernel regression perspective on dictionaries was previously pointed\nout by Zhanget al.(2021). It is illustrated in Fig. 6.2 withK= V= R.\n6.2.3 Discrete probability distribution perspective\nWhile the set of possible keys K is potentially infinite, the set of\nkeys {k1,..., kL}\u2282K associated with a particular dictionaryd =\n((k1,v1),..., (kL,vL)) is finite. To a particular keyk, we can therefore\nassociate a discrete probability distribution\u03c0k = (\u03c0k,1,...,\u03c0 k,L) \u2208\u25b3L\nover the keys(k1,..., kL) of d, defined by\n\u03c0k,i := \u03ba\u03c3(k\u2212ki)\n\u2211L\nj=1 \u03ba\u03c3(k\u2212kj) \u2200i\u2208[L].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1176, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eacd58ec-ba79-4bb1-a19d-a0271eacb36b": {"__data__": {"id_": "eacd58ec-ba79-4bb1-a19d-a0271eacb36b", "embedding": null, "metadata": {"page_label": "172", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "58dbf862-3565-4a7d-9930-909136367af6", "node_type": "4", "metadata": {"page_label": "172", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "fc2ead0e111b3107e66cdb600026191cc31826f332c35dca4d652e915cdb2601", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "172 Data structures\n...\nWeight\nAvg\n...\nFigure 6.3:Computation graph of thedict.softGet function. We can use a kernel\n\u03ba\u03c3 to produce a discrete probability distribution\u03c0k = (\u03c0k,1,...,\u03c0 k,L) \u2208\u25b3L, that\ncaptures the affinity between the dictionary keys(k1,..., kL) and the input keyk.\nThe dict.softGet function can then merely be seen as a convex combination (weighted\naverage) of values(v1,..., vL) using the probability values(\u03c0k,1,...,\u03c0 k,L) as weights.\nThis distribution captures the affinity between the input keykand the\nkeys (k1,..., kL) of dictionaryd. As illustrated in Fig. 6.3, we obtain\ndict.softGet(d,k) = Ei\u223cCategorical(\u03c0k)[vi]\n=\nL\u2211\ni=1\n\u03c0k,ivi.\nIn the limit\u03c3\u21920, we recover\ndict.get(d,k) =\n\u2211L\ni=1 eq(k,ki)vi\n\u2211L\ni=1 eq(k,ki) .\nWhile the dict.get function is using a mapping from keys k \u2208\n{k1,..., kL}to integer indices[L], thedict.softGet function is using a\nmapping from keysk\u2208{k1,..., kL}to distributions\u03c0k \u2208\u25b3L. This\nperspective allows us to reuse the soft functions we developed for lists\nin Section 6.1. For example, we can softly replace the value associated\nwith keykby performing\nlist.softSet(d,\u03c0k,(k,v)).\nUnlike dict.set, the function is differentiable w.r.t. the distribution\u03c0k.\n6.2.4 Link with attention in Transformers\nIn the case when\u03ba\u03c3 is the Gaussian kernel, assuming that the keys\nare normalized to have unit norm (which is often the case in practical", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1373, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6f3726db-6965-44ad-bc4a-cd537fbafe11": {"__data__": {"id_": "6f3726db-6965-44ad-bc4a-cd537fbafe11", "embedding": null, "metadata": {"page_label": "173", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f710804d-cb57-4e2d-b4cf-8d27d2aeabf3", "node_type": "4", "metadata": {"page_label": "173", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "2cacc0e226ce9004228e8654b1ac75caffd1696a93995003465f40027d39120c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6.3. Summary 173\nimplementations (Schlaget al., 2021; Dehghaniet al., 2023)), we obtain\n\u03ba\u03c3(k\u2212ki) = exp(\u2212\u2225k\u2212ki\u22252\n2/(2\u03c32))\n= exp(\u2212(\u2225k\u22252\n2 + \u2225ki\u22252)/(2\u03c32)) exp(\u27e8k,ki\u27e9/\u03c32)\n= exp(\u2212\u03c32) exp(\u27e8k,ki\u27e9/\u03c32)\nso that\n\u03c0k,i = \u03ba\u03c3(k\u2212ki)\n\u2211L\nj=1 \u03ba\u03c3(k\u2212kj)\n= exp(\u27e8k,ki\u27e9/\u03c32)\n\u2211L\nj=1 exp(\u27e8k,kj\u27e9/\u03c32).\nWe recognize the softargmax operator. Given, a dictionary\nd= ((k1,v1),..., (kL,vL)), we thus recover attention from Transform-\ners (Vaswaniet al., 2017) as\ndict.softGet(d,k) = exp(\u27e8k,ki\u27e9/\u03c32)vi\n\u2211L\nj=1 exp(\u27e8k,kj\u27e9/\u03c32).\nTransformers can therefore be interpreted as relying on a differentiable\ndictionary mechanism. Besides Transformers, content-based memory\naddressing is also used in neural Turing machines (Graveset al., 2014).\n6.3 Summary\n\u2022 Operations on lists are continuous and differentiable w.r.t. the\nlist, but not w.r.t. the integer index. Similarly, operations on\ndictionaries are continuous and differentiable w.r.t. the dictionary,\nbut not w.r.t. the input key.\n\u2022 Similarly to the way we handled the predicate in conditionals,\nwe can replace the integer index (respectively the key) with a\nprobability distribution over the indices (respectively the keys).\n\u2022 This allows us to obtain a probabilistic relaxation of operations\non lists. In particular, the relaxation forlist.get amounts to per-\nforming a convolution. The relaxation fordict.get amounts to\ncomputing a conditional expectation using kernel regression.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1395, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "808dd9c4-31fc-43fe-b604-c005c7f32ee5": {"__data__": {"id_": "808dd9c4-31fc-43fe-b604-c005c7f32ee5", "embedding": null, "metadata": {"page_label": "174", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fc99e1ef-a4f1-47ce-84f1-e2a55b9e12c8", "node_type": "4", "metadata": {"page_label": "174", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "3aa2df33b7ef401fb48553cb82b277286dc7a27c8b71e75b4d1d9f008e287945", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "174 Data structures\n\u2022 When using a Gaussian kernel with keys normalized to unit norm,\nwe recover softargmax attention from Transformers.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 136, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "989bac15-172d-4fdd-91f6-057a74c31dbb": {"__data__": {"id_": "989bac15-172d-4fdd-91f6-057a74c31dbb", "embedding": null, "metadata": {"page_label": "175", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37e13078-b7ed-4e2d-8c99-2518a91b0d78", "node_type": "4", "metadata": {"page_label": "175", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "a3ab5b8830183b4d2c35f94f2e5316925638ca3d68cd2df7b5e1a932edb5339f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Part III\nDifferentiating through\nprograms", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 41, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "70f3672b-6cb0-453f-a1fe-ac41f1e8e730": {"__data__": {"id_": "70f3672b-6cb0-453f-a1fe-ac41f1e8e730", "embedding": null, "metadata": {"page_label": "176", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9580c8e-aa0a-4911-804c-d9eca0243c72", "node_type": "4", "metadata": {"page_label": "176", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "e6a8aef56efe99fc3b2dc01be6f785e88ed0546cf95a07cb299696d15df19516", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7\nFinite differences\nOne of the simplest way to numerically compute derivatives is to use\nfinitedifferences,whichapproximatetheinfinitesimaldefinitionofderiva-\ntives. Finite differences only requirefunction evaluations, and can\ntherefore work with blackbox functions (i.e., they ignore the composi-\ntional structure of functions). Without loss of generality, our exposition\nfocuses on computing directional derivatives\u2202f(w)[v], for a function\nf: E\u2192F , evaluated atw\u2208E, in the directionv\u2208E.\n7.1 Forward differences\nFrom Definition 2.4 and Definition 2.13, the directional derivative and\nmore generally the JVP are defined as a limit,\n\u2202f(w)[v] := lim\n\u03b4\u21920\nf(w+ \u03b4v) \u2212f(w)\n\u03b4 .\nThis suggests that we can approximate the directional derivative and\nthe JVP using\n\u2202f(w)[v] \u2248f(w+ \u03b4v) \u2212f(w)\n\u03b4 ,\n176", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 787, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "43d91298-f0ff-455e-a071-5faf90484ff1": {"__data__": {"id_": "43d91298-f0ff-455e-a071-5faf90484ff1", "embedding": null, "metadata": {"page_label": "177", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dd1225bf-0b70-46fd-b107-75f893995a76", "node_type": "4", "metadata": {"page_label": "177", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "74a039db1ff8536762bd142f13c94e54625fdc986c289945346cad7f173c5f63", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7.2. Backward differences 177\nfor some0 <\u03b4 \u226a1. This formula is called aforward difference. From\nthe Taylor expansion in Section 2.5.4, we indeed have\nf(w+\u03b4v)\u2212f(w) = \u03b4\u2202f(w)[v]+\u03b42\n2 \u22022f(w)[v,v]+\u03b43\n3! \u22023f(w)[v,v,v]+...\nso that\nf(w+ \u03b4v) \u2212f(w)\n\u03b4 = \u2202f(w)[v] + \u03b4\n2\u22022f(w)[v,v] + \u03b42\n3! \u22023f(w)[v,v,v] + ...\n= \u2202f(w)[v] + o(\u03b4).\nThe error incurred by choosing a finite rather than infinitesimal\u03b4 in the\nforward difference formula is called thetruncation error. The Taylor\napproximation above shows that this error is of the order ofo(\u03b4).\nHowever, we cannot choose a too small value of\u03b4, because the\nevaluation of the functionf on a computer rounds the value off to\nmachine precision. Mathematically, a scalar-valued functionf evaluated\non a computer becomes a function \u02dcf such that \u02dcf(w) \u2248 [f(w)/\u03b5]\u03b5,\nwhere [f(w)/\u03b5] denotes the closest integer off(w)/\u03b5 \u2208R and \u03b5 is\nthe machine precision, i.e., the smallest non-zero real number encoded\nby the machine. This means that the differencef(w+ \u03b4v) \u2212f(w)\nevaluated on a computer is prone toround-off errorof the order of\no(\u03b5). We illustrate the trade-off between truncation and round-off errors\nin Fig. 7.1.\n7.2 Backward differences\nAs an alternative, we can approximate the directional derivative and\nthe JVP by\n\u2202f(w)[v] \u2248f(w) \u2212f(w\u2212\u03b4v)\n\u03b4 ,\nfor some0 < \u03b4\u226a1. This formula is called abackward difference.\nFrom the Taylor expansion in Section 2.5.4, we easily verify that(f(w)\u2212\nf(w\u2212\u03b4v))/\u03b4 = \u2202f(w)[v] + o(\u03b4), so that the truncation error is the\nsame as for the forward difference.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1505, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "abf33d71-697a-45a9-9647-fd60a497cb42": {"__data__": {"id_": "abf33d71-697a-45a9-9647-fd60a497cb42", "embedding": null, "metadata": {"page_label": "178", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "92ad97ae-9fdc-479e-86b6-9cb8115d9f0d", "node_type": "4", "metadata": {"page_label": "178", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "541ad5c1830e5f06aaa02cffcddc396210af3a0ca156fee307ced5a667b8f6d5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "178 Finite differences\n10 13\n 10 11\n 10 9\n 10 7\n 10 5\n 10 3\n 10 1\n10 16\n10 14\n10 12\n10 10\n10 8\n10 6\n10 4\n10 2\nApproximation error\nRound-off error \n is dominant\nTruncation error \n is dominant\nForward difference\nCentral difference\nComplex Step\nFigure 7.1: Numerical differentiation off(x) := softplus(x) = log(1 + exp(x)),\nto approximate f\u2032(x) = logistic(x) at x = 1. The forward and central difference\nmethods induce both truncation error (for large\u03b4) and round-off error (for small\u03b4).\nThe complex step method enjoys smaller round-off error.\n7.3 Central differences\nRather than using an asymmetric formula to approximate the derivative,\nas in forward and backward differences, we can use a symmetric formula\n\u2202f(w)[v] \u2248f(w+ \u03b4v) \u2212f(w\u2212\u03b4v)\n2\u03b4 ,\nfor some0 <\u03b4 \u226a1. This formula is called acentral difference. From\nthe Taylor expansion in Section 2.5.4, we have\nf(w+ \u03b4v) \u2212f(w\u2212\u03b4v) =2\u03b4\u2202f(w)[v] + 2\u03b43\n3! \u22023f(w)[v,v,v]\n+ 2\u03b45\n5! \u22025f(w)[v,..., v] + ...\nso that\nf(w+ \u03b4v) \u2212f(w\u2212\u03b4v)\n2\u03b4 =\u2202f(w)[v] + \u03b42\n3! \u22023f(w)[v,v,v] + ...\n=\u2202f(w)[v] + o(\u03b42).\nWe see that the terms corresponding to derivatives ofeven order\ncanceled out, allowing the formula to achieveo(\u03b42) truncation error.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1156, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c6b672ce-c294-4b90-9a31-92791208d2e7": {"__data__": {"id_": "c6b672ce-c294-4b90-9a31-92791208d2e7", "embedding": null, "metadata": {"page_label": "179", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ec066805-4e13-438c-9bcb-1abf6c3bb143", "node_type": "4", "metadata": {"page_label": "179", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "586a7c618dbfc21410a392f2205da707ee859c8367963669a49353551f56dca7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7.4. Higher-accuracy finite differences 179\nFor any\u03b4 <1, the truncation error of the central difference is much\nsmaller than the one of the forward or backward differences as confirmed\nempirically in Fig. 7.1.\n7.4 Higher-accuracy finite differences\nThe truncation error can be further reduced by making use of additional\nfunction evaluations. One can generalize the forward difference scheme\nby a formula of the form\n\u2202f(w)[v] \u2248\np\u2211\ni=0\nai\n\u03b4f(w+ i\u03b4v)\nrequiring p+ 1 evaluations. To select theai and reach a truncation error\nof ordero(\u03b4p), we can use a Taylor expansion on each term of the sum\nto get\np\u2211\nk=0\nai\n\u03b4f(w+ i\u03b4v) =\np\u2211\nk=0\nai\np\u2211\nj=0\nij\u03b4j\u22121\nj! \u2202jf(w)[v,..., v] + o(\u03b4p).\nBy grouping the terms in the sum for each order of derivative, we obtain\na set ofp+ 1 equations to be satisfied by thep+ 1 coefficients a0,...,a p,\nthat is,\na0 + a1 + ... + ap = 0\na1 + 2a2 + ... + pap = 1\na1 + 2ja2 + ... + pjap = 0 \u2200j \u2208{2,...,p }.\nThis system of equations can be solved analytically to derive the co-\nefficients. Backward differences can be generalized similarly by using\n\u2202f(w)[v] \u2248\u2211p\ni=0\nai\n\u03b4 f(w\u2212i\u03b4v). Similarly, the central difference scheme\ncan be generalized by using\n\u2202f(w)[v] \u2248\np\u2211\ni=\u2212p\nai\n\u03b4f(w+ i\u03b4v),\nto reach a truncation error of ordero(\u03b42p). Solving for the coefficients\na\u2212p,...,a p as above reveals thata0 = 0. Therefore, only2pevaluations\nare necessary.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1355, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d669b237-6cc6-489e-996c-e83047a55198": {"__data__": {"id_": "d669b237-6cc6-489e-996c-e83047a55198", "embedding": null, "metadata": {"page_label": "180", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "97e96042-9d0b-413c-8cf1-1180199dc740", "node_type": "4", "metadata": {"page_label": "180", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "a6cab51d9886241961e4c1133cd3e54d4d3fe2d757ab2433e7e37efa2f61cd2d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "180 Finite differences\n7.5 Higher-order finite differences\nTo approximate higher order derivatives, we can follow a similar rea-\nsoning. Namely, we can generalize the forward difference scheme to\napproximate the derivative of orderk by\n\u2202kf(w)[v,..., v] \u2248\np\u2211\ni=0\nai\n\u03b4kf(w+ i\u03b4v).\nAs before, we can expand the terms in the sum. For the approximation\nto capture only thekth derivative, we now require the coefficientsai to\nsatisfy\n0ja0 + 1ja1 + 2ja2 + ... + pjap = 0 \u2200j \u2208{0,...,k \u22121}.\n0ka0 + 1ka1 + 2ka2 + ... + pkap = k!\n0ja0 + 1ja1 + 2ja2 + ... + pjap = 0 \u2200j \u2208{k+ 1,...,p }.\nWith the resulting coefficients, we obtain a truncation error of order\no(\u03b4p\u2212k+1), while makingp+ 1 evaluations. For example, forp= k= 2,\nwe can approximate the second-order derivative as\n\u22022f(w)[v,v] \u2248\u2212(3/2)f(x) + 2f(x+ \u03b4v) \u2212(1/2)f(x+ 2\u03b4v)\n\u03b42 ,\nwith a truncation error of ordero(\u03b4).\nThe central difference scheme can be generalized similarly by\n\u2202kf(w)[v,..., v] \u2248\np\u2211\ni=\u2212p\nai\n\u03b4kf(w+ i\u03b4v),\nto reach truncation errors of ordero(\u03b42p+2\u22122\u2308(k+1)/2\u2309). For example, for\nk= 2, p= 1, we obtain the second-order central difference\n\u22022f(w)[v,v] \u2248f(w+ \u03b4v) + f(w\u2212\u03b4v) \u22122f(w)\n\u03b42 .\nBy using a Taylor expansion we see that, this time, the terms corre-\nsponding to derivatives ofodd ordercancel out and the truncation\nerror iso(\u03b42) while requiring3 evaluations.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1312, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "90df7dc3-8109-40f3-a595-5ec6ddb3ff94": {"__data__": {"id_": "90df7dc3-8109-40f3-a595-5ec6ddb3ff94", "embedding": null, "metadata": {"page_label": "181", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "30c0b150-1c03-48f8-985e-a9ff33fa29a8", "node_type": "4", "metadata": {"page_label": "181", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "bdf160d9343fd32500d88080290e77fead409d5cc59e46209c9079f03384d4dc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7.6. Complex-step derivatives 181\n7.6 Complex-step derivatives\nSuppose f is well defined onCP, the space ofP-dimensional complex\nnumbers. Let us denote the imaginary unit byi = \u221a\u22121. Then, the\nTaylor expansion off reads\nf(w+ (i\u03b4)v) =f(w) + (i\u03b4)\u2202f(w)[v] + (i\u03b4)2\n2 \u22022f(w)[v,v]\n+ (i\u03b4)3\n3! \u22023f(w)[v,v,v] + ...\n=f(w) + (i\u03b4)\u2202f(w)[v] \u2212\u03b42\n2 \u22022f(w)[v,v]\n\u2212i(\u03b4)3\n3! \u22023f(w)[v,v,v] + ... .\nWe see that the real part corresponds to even-degree terms and the\nimaginary part corresponds to odd-degree terms. We therefore obtain\nRe(f(w+ (i\u03b4)v)) = f(w) + o(\u03b42)\nand\nIm\n(f(w+ (i\u03b4)v)\n\u03b4\n)\n= \u2202f(w)[v] + o(\u03b42).\nThis suggests that we can compute directional derivatives using the\napproximation\n\u2202f(w)[v] \u2248Im\n(f(w+ (i\u03b4)v)\n\u03b4\n)\n,\nfor 0 <\u03b4 \u226a1. This is called thecomplex-step derivativeapproxi-\nmation (Squire and Trapp, 1998; Martinset al., 2003).\nContrary to forward, backward and central differences, we see that\nonly asingle function callis necessary. A function call on complex\nnumbers may take roughly twice the cost of a function call on real\nnumbers. However, thanks to the fact that a difference of functions is\nno longer needed, the complex-step derivative approximation usually\nenjoys smaller round-off error as illustrated in Fig. 7.1. That said, one\ndrawback of the method is that all elementary operations within the\nprogram implementing the functionf must be well-defined on complex\nnumbers, e.g., using overloading.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1399, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8e961401-f8cf-4f63-8736-b65bf3e955b6": {"__data__": {"id_": "8e961401-f8cf-4f63-8736-b65bf3e955b6", "embedding": null, "metadata": {"page_label": "182", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ed626845-4764-4090-9e6a-017499d06b10", "node_type": "4", "metadata": {"page_label": "182", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "cf2c52d3502d0a4deeab25aa7d4383225e7b6551f6fc43a78acb887c4189b1cd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "182 Finite differences\nTable 7.1:Computational complexity in number of function evaluations for com-\nputing the directional derivative and the gradient of a functionf: RP \u2192R by finite\ndifferences and complex step derivatives.\nDirectional derivative Gradient\nForward difference 2 P + 1\nBackward difference 2 P + 1\nCentral difference 2 2 P\nComplex step 1 P\n7.7 Complexity\nWe now discuss the computational complexity in terms of function\nevaluations of finite differences and complex-step derivatives. For con-\ncreteness, as this is the most common use case in machine learning, we\ndiscuss the case of a singleM = 1 output, i.e., we want to differentiate\na functionf: RP \u2192R. Whether we use forward, backward or central\ndifferences, the computational complexity of computing the directional\nderivative\u2202f(w)[v] in any directionvamounts to two calls tof. For\ncomputing the gradient\u2207f(w), we can use (see Definition 2.7) that\n[\u2207f(w)]j = \u27e8\u2207f(w),ej\u27e9= \u2202f(w)[ej],\nfor j \u2208[P]. For forward and backward differences, we therefore need\nP+ 1 function calls to compute the gradient, while we need2P function\ncalls for central differences. For the complex step approximation, we need\nP complex function calls. We summarize the complexities in Table 7.1.\n7.8 Summary\n\u2022 Finite differences are a simple way to numerically compute deriva-\ntives using only function evaluations.\n\u2022 Central differences achieve smaller truncation error than forward\nand backward differences. It is possible to achieve smaller trunca-\ntion error, at the cost of more function evaluations.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1545, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "00deeea4-9094-4672-879a-3df8be6ad6c5": {"__data__": {"id_": "00deeea4-9094-4672-879a-3df8be6ad6c5", "embedding": null, "metadata": {"page_label": "183", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0943de38-d9c3-48cf-a5cf-ff0d66b26abe", "node_type": "4", "metadata": {"page_label": "183", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "5d85bf2ec110490647976db9cf787a8ae4ffb0ce8a4cf8d92b330b2ff1762d71", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7.8. Summary 183\n\u2022 Complex-step derivatives achieve smaller round-off error than\ncentral differences but require the function and the program\nimplementing it to be well-defined on complex numbers.\n\u2022 However, whatever the method used, finite differences require a\nnumber of function calls that is proportional to the number of\ndimensions. They are therefore seldom used in machine learning,\nwhere there can be millions or billions of dimensions. The main\nuse cases of finite differences are therefore i) for blackbox functions\nof low dimension and ii) for test purposes (e.g., checking that a\ngradient function is correctly implemented).\n\u2022 For modern machine learning, the main workhorse is automatic\ndifferentiation, as it leverages the compositional structure of func-\ntions. This is what we study in the next chapter.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 819, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5ea5f4e4-0ed5-415f-aa82-10e95e3df23a": {"__data__": {"id_": "5ea5f4e4-0ed5-415f-aa82-10e95e3df23a", "embedding": null, "metadata": {"page_label": "184", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "64bc5256-8465-45a3-8403-07d13c4061a4", "node_type": "4", "metadata": {"page_label": "184", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "defa79271d6e2d02081338ef5a422e35ac7456dec1561956906a726e83887224", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8\nAutomatic differentiation\nIn Chapter 2, we reviewed the fundamentals of differentiation and\nstressed the importance of two linear maps: the Jacobian-vector product\n(JVP) and its adjoint, the vector-Jacobian product (VJP). In this chap-\nter, we reviewforward-mode and reverse-mode autodiff using these\ntwo linear maps. We start withcomputation chainsand then gener-\nalize to feedforward networks and generalcomputation graphs. We\nalso review checkpointing, reversible layers and randomized estimators.\n8.1 Computation chains\nTo begin with, consider acomputation chain(Section 4.1.1) repre-\nsenting a functionf: S0 \u2192SK expressed as a sequence of compositions\nf := fK \u25e6\u00b7\u00b7\u00b7\u25e6 f1, wherefk: Sk\u22121 \u2192Sk. The computation off can be\n184", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 726, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "37f1706d-f151-49b8-b597-40417ecaa0da": {"__data__": {"id_": "37f1706d-f151-49b8-b597-40417ecaa0da", "embedding": null, "metadata": {"page_label": "185", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c64bded-b9c6-4710-9782-79c552f091f5", "node_type": "4", "metadata": {"page_label": "185", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "1491bbc7b30292c418334fcb7ab141465953e7a98438d756eb2842d84e91adb7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8.1. Computation chains 185\nunrolled into a sequence of operations\ns0 \u2208S0\ns1 := f1(s0) \u2208S1\n...\nsK := fK(sK\u22121) \u2208SK\nf(s0) := sK. (8.1)\nOur goal is to compute the variations off around a given inputs0. In\na feedforward network, this amounts to estimating the influence of a\ngiven inputs0 for fixed parameters (we will see how to estimate the\nvariations w.r.t. parameterswin the sequel).\nJacobian matrix. We first consider the computation of the full Jaco-\nbian \u2202f(s0), seen as amatrix, as the notation\u2202 indicates. Following\nProposition 2.2, we have\n\u2202f(s0) = \u2202fK(sK\u22121) ... \u2202f1(s0), (8.2)\nwhere \u2202fk(sk\u22121) are the Jacobians of the intermediate functions com-\nputed at s0,..., sK, as defined in Eq. (8.1). We may also want to\ncompute the tranpose of the Jacobian\n\u2202f(s0)\u22a4= \u2202f1(s0)\u22a4... \u2202fK(sK\u22121)\u22a4.\nIn both cases, the main drawback of this approach is computational:\ncomputing the full \u2202f(s0) requires to materialize the intermediate\nJacobians in memory and to perform matrix-matrix multiplications.\nHowever, in practice, computing the full Jacobian is rarely needed.\nIndeed, oftentimes, we only need to right-multiply or left-multiply with\n\u2202f(s0). This gives rise to forward-mode and reverse-mode autodiff,\nrespectively.\n8.1.1 Forward-mode\nWe now interpret the Jacobian\u2202f(s0) as alinear map, as the non-bold\n\u2202 indicates. Following Proposition 2.6,\u2202f(s0) is the composition of the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1370, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "093291fd-6bc1-4acd-8110-c4f54a5c1057": {"__data__": {"id_": "093291fd-6bc1-4acd-8110-c4f54a5c1057", "embedding": null, "metadata": {"page_label": "186", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ff3ff054-6a2f-4195-82f1-8382df810dc8", "node_type": "4", "metadata": {"page_label": "186", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "eb75229a0c7508b061ae19fde559483c5b311ce1137f491f2b80b6fad6d43fe0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "186 Automatic differentiation\n......\n... ...\nFigure 8.1:Forward-mode autodiff for a computation chain. For readability, we\ndenoted the intermediate JVP as a function of two variables\u2202fk : sk\u22121,tk\u22121 \u21a6\u2192\n\u2202fk(sk\u22121)[tk\u22121] with \u2202fk(sk\u22121)[tk\u22121] = tk.\nintermediate linear maps,\n\u2202f(s0) = \u2202fK(sK\u22121) \u25e6... \u25e6\u2202f1(s0).\nEvaluating \u2202f(s0) on aninput directionv\u2208S0 can be decomposed,\nlike the function Eq. (8.1) itself, into intermediate computations\nt0 := v\nt1 := \u2202f1(s0)[t0]\n...\ntK := \u2202fK(sK\u22121)[tK\u22121]\n\u2202f(s0)[v] := tK.\nEach intermediate\u2202fk(sk\u22121)[tk\u22121] amounts to a Jacobian-vector prod-\nuct (JVP) and can be performed in aforward manner, along the\ncomputation of the intermediate statessk. This can also be seen as\nmultiplying the matrix defined in Eq. (8.2) with a vector, fromright\nto left. This is illustrated in Fig. 8.1 and the procedure is summarized\nin Algorithm 8.1.\nComputational complexity. The JVP follows exactly the computations\nof f, with an additional variabletk being propagated. If we consider that\ncomputing \u2202fk is roughly as costly as computingfk, then computing a", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1066, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4c336bee-aa93-41ec-8d6f-0f5c43d943f7": {"__data__": {"id_": "4c336bee-aa93-41ec-8d6f-0f5c43d943f7", "embedding": null, "metadata": {"page_label": "187", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e745289b-b62d-45e7-9c59-d60bca128b61", "node_type": "4", "metadata": {"page_label": "187", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "9840e1dc79a924d145001d59fc50fcdf58fccf4d1b011732f9ea71485d4e8ea8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8.1. Computation chains 187\nAlgorithm 8.1Forward-mode autodiff for computation chains\nFunctions:f := fK \u25e6... \u25e6f1\nInputs: input s0 \u2208S0, input directionv\u2208S0\n1: Initialize t0 := v\n2: for k:= 1,...,K do\n3: Compute sk := fk(sk\u22121) \u2208Sk\n4: Compute tk := \u2202fk(sk\u22121)[tk\u22121] \u2208Sk\nOutputs: f(s0) := sK, \u2202f(s0)[v] = tK\nJVP has roughly twice the computational cost off. See Section 8.3.3\nfor a more general and more formal statement.\nMemory usage. The memory usage of a program at a given evaluation\nstep is the number of variables that need to be stored in memory to\nensure the execution of all remaining steps. The memory cost of a\nprogram is then the maximal memory usage over all evaluation steps.\nFor our purposes, we analyze the memory usage and memory cost\nby examining the given program. Formal definitions of operations on\nmemory such as read, write, delete and associated memory costs are\npresented by Griewank and Walther (2008, Chapter 4).\nFor example, to execute the chainf = fK \u25e6\u00b7\u00b7\u00b7\u25e6 f1, at each stepk,\nwe only need to have access tosk\u22121 to execute the rest of the program.\nAs we computesk, we can deletesk\u22121 from memory and replace it by\nsk. Therefore, the memory cost associated to the evaluation off is just\nthe maximal dimension of thesk variables.\nFor forward mode autodiff, as we follow the computations off, at\neach stepk, we only need to have access tosk\u22121 and tk\u22121 to execute the\nrest of the program. The memory used bysk\u22121 and tk\u22121 can directly be\nused forsk,tk once they are computed. The memory usage associated\nto the JVP is summarized in Fig. 8.2. Overall the memory cost of the\nJVP is then exactly twice the memory cost of the function itself.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1655, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "32c2d795-aac8-42b6-bd04-336a1827f21c": {"__data__": {"id_": "32c2d795-aac8-42b6-bd04-336a1827f21c", "embedding": null, "metadata": {"page_label": "188", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8aee8d6b-81e5-40ea-bb4d-e8c36a37af63", "node_type": "4", "metadata": {"page_label": "188", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "4b3938e1430c35a53b0934b2423e3e8a0b38e99b3677ad3981050ce869ec3ad7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "188 Automatic differentiation\nMemory\nusage\nAlgorithm steps\nFigure 8.2:Memory usage of forward-mode autodiff for a computation chain. Here\nt0 = v, sK = f(s0), tK = \u2202f(s0)[v].\n8.1.2 Reverse-mode\nIn machine learning, most functions whose gradient we need to compute\ntake the form\u2113\u25e6f, where\u2113 is a scalar-valued loss function andf is a\nnetwork. As seen in Proposition 2.3, the gradient takes the form\n\u2207(\u2113\u25e6f)(s0) = \u2202f(s0)\u2217[\u2207\u2113(f(s0))].\nThismotivatestheneedforapplyingthe adjoint\u2202f(s0)\u2217to\u2207\u2113(f(s0)) \u2208\nSK and more generally to anyoutput directionu\u2208SK. From Propo-\nsition 2.7, we have\n\u2202f(s0)\u2217= \u2202f1(s0)\u2217\u25e6... \u25e6\u2202fK(sK\u22121)\u2217.\nEvaluating \u2202f(s0)\u2217on an output directionu\u2208SK is decomposed as\nrK := u\nrK\u22121 := \u2202fK(sK\u22121)\u2217[rK]\n...\nr0 := \u2202f1(s0)\u2217[r1]\n\u2202f(s0)\u2217[u] := r0.\nEach intermediate adjoint\u2202fk(sk\u22121)\u2217 amounts to a vector-Jacobian\nproduct (VJP). The key difference with the forward mode is that the\nprocedure runsbackwardthrough the chain, hence the namereverse\nmode autodiff. This can also be seen as multiplying Eq. (8.2) fromleft\nto right. The procedure is illustrated in Fig. 8.3 and summarized in\nAlgorithm 8.2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1093, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "92f6e08a-f043-45e6-b417-edc6df733674": {"__data__": {"id_": "92f6e08a-f043-45e6-b417-edc6df733674", "embedding": null, "metadata": {"page_label": "189", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8339000a-9de5-4800-97a9-f14d097a8f98", "node_type": "4", "metadata": {"page_label": "189", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "eee77794ff8d07666dc95f958ad88fe3b602cbb82d8862a491d9434bb45c6b2f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8.1. Computation chains 189\nBackward pass\n......\n......\nForward pass\nFigure 8.3: Reverse mode of automatic differentiation for a computation chain.\nFor readability, we denoted the intermediate VJPs as functions of two variables\n\u2202f\u2217\nk : (sk\u22121,rk) \u21a6\u2192\u2202fk(sk\u22121)\u2217[rk], with\u2202fk(sk\u22121)\u2217[rk] = rk\u22121.\nAlgorithm 8.2Reverse-mode autodiff for computation chains\nFunctions:f := fK \u25e6... \u25e6f1,\nInputs: input s0 \u2208S0, output directionu\u2208SK\n1: for k:= 1,...,K do \u25b7 Forward pass\n2: Compute sk := fk(sk\u22121) \u2208Sk\n3: Initialize rK := u.\n4: for k:= K,..., 1 do \u25b7 Backward pass\n5: Compute rk\u22121 := \u2202fk(sk\u22121)\u2217[rk] \u2208Sk\u22121\nOutputs: f(s0) := sK, \u2202f(s0)\u2217[u] = r0", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 626, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b79ef5c6-0c66-4911-b425-0b25fc11f737": {"__data__": {"id_": "b79ef5c6-0c66-4911-b425-0b25fc11f737", "embedding": null, "metadata": {"page_label": "190", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5f57f609-5886-499b-9fe8-0ced7c3944d3", "node_type": "4", "metadata": {"page_label": "190", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "a50ef79af80af1ed40edf45a3ddd94d9a828cfb1ca7c6e0b849ce22dcb3db1f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "190 Automatic differentiation\nAlgorithm stepsForward pass Backward pass\nMemory\nusage\nFigure 8.4:Memory usage of reverse mode autodiff for a computation chain.\nComputational complexity. In terms of number of operations, the\nVJP simply passes two times through the chain, once forward, then\nbackward. If we consider the intermediate VJPs to be roughly as costly\nas the intermediate functions themselves, the VJP amounts just to twice\nthe cost of the original function, just as the JVP. See Section 8.3.3 for\na more generic and formal statement.\nMemory usage. Recall that the memory usage of a program at a given\nevaluation step is the number of variables that need to be stored in\nmemory to ensure the execution of the remaining steps. If we inspect\nAlgorithm 8.3, to execute all backward steps, that is the loop in line 4,\nwe need to have access to all the intermediate inputss0,..., sK\u22121.\nTherefore, the memory cost of reverse-mode autodiff is proportional to\nthe length of the chainK. Fig. 8.4 illustrates the memory usage during\nreverse mode autodiff. It grows linearly until the end of the forward\npass and then progressively decreases until it outputs the value of the\nfunction and the VJP. The memory cost can be mitigated by means of\ncheckpointing techniques presented in Section 8.5.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1290, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "88065c62-16f9-416a-bcf6-ba9e40768cbc": {"__data__": {"id_": "88065c62-16f9-416a-bcf6-ba9e40768cbc", "embedding": null, "metadata": {"page_label": "191", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c33e11ef-8da9-49b3-8f98-965587aab7df", "node_type": "4", "metadata": {"page_label": "191", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "3c93efed5e52fcb1b624b7dc1fa9faee0dd12fd9e64bb916ff638e88cc7b97eb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8.1. Computation chains 191\nDecoupled function and VJP evaluations.The additional memory\ncost of reverse mode autodiff comes with some advantages. If we need\nto compute \u2202f(s0)\u2217[ui] for n different output directionsui, we only\nneed to compute and store once the intermediate computationssk and\nthen maken calls to the backward pass. In other words, by storing in\nmemory the intermediate computationssk, we may instantiate aVJP\noperator, which we may apply to anyuthrough the backward pass.\nFormally, the forward and backward passes can be decoupled as\nforward(f,s0) := (f(s0),\u2202f (s0)\u2217)\nwhere\n\u2202f(s0)\u2217[u] := backward(u; s0,..., sK\u22121).\nIn functional programming terminology, the VJP\u2202f(s0)\u2217is aclosure,\nas it contains the intermediate computationss0,... sK. The same can\nbe done for the JVP\u2202f(s0) if we want to apply to multiple directions\nvi.\nExample 8.1(Multilayer perceptron with fixed parameters ). Asarun-\nning example, consider a multilayer perceptron (MLP) with one\nhidden layer and (for now) given fixed weights. As presented in\nChapter 4, an MLP can be decomposed as\ns0 = x\ns1 = f1(s0) = \u03c3(A1s0 + b1)\ns2 = f2(s1) = A2s1 + b2\nf(x) = s2,\nfor A1,A2,b1,b2 some fixed parameters and\u03c3 an activation func-\ntion such as the softplus activation function\u03c3(x) = log(1+ ex) with\nderivative\u03c3\u2032(x) = ex/(1 + ex).\nEvaluating the JVP off on an inputxalong a directionvcan", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1358, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0eb1db1a-a1e8-443c-ad3d-2ac783af26a8": {"__data__": {"id_": "0eb1db1a-a1e8-443c-ad3d-2ac783af26a8", "embedding": null, "metadata": {"page_label": "192", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "141329cb-b9d8-42e4-a0a8-ac844cbb395b", "node_type": "4", "metadata": {"page_label": "192", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "9edd85cd4f658628a0030ac1bbb2701e1c476c4ab6f17fb66837fb57514e0514", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "192 Automatic differentiation\nthen be decomposed as\nt0 = v\nt1 = \u03c3\u2032(A1s0 + b1) \u2299(A1t0)\nt2 = A2t1\n\u2202f(x)[v] = t2,\nwhere we used in the second line the JVP of element-wise function\nas in Example 8.4.\nEvaluating the VJP off at xrequires to evaluate the interme-\ndiate VJPs at the stored activations\nr2 = u\nr1 = \u2202f2(s1)\u2217[r2] = A\u22a4\n2 r2\nr0 = \u2202f1(s0)\u2217[r1] = A\u22a4\n1 (\u03c3\u2032(A1s0 + b1) \u2299r1)\n\u2202f(x)\u2217[u] = r0.\n8.1.3 Complexity of computing entire Jacobians\nIn this section, we analyze the time and space complexities of forward-\nmode and reverse-mode autodiff for computing theentire Jacobian\nmatrix\u2202f(s0) ofacomputationchain f = fK\u25e6\u00b7\u00b7\u00b7\u25e6f1,where fk: Sk\u22121 \u2192\nSk. We assumeSk \u2286RDk, DK = M and D0 = D. Therefore, we have\nf: RD \u2192RM and \u2202f(s0) \u2208RM\u00d7D.\nComplexity of forward-mode autodiff\nUsingDefinition2.9,wefindthatwecanextracteach column[\u2202f(s0)]:,j \u2208\nRM of the Jacobian matrix, forj \u2208[D], by multiplying with the stan-\ndard basis vectorej \u2208RD:\n[\u2202f(s0)]:,1 = \u2202f(s0)[e1]\n...\n[\u2202Df(s0)]:,D = \u2202f(s0)[eD].\nComputing the full Jacobian matrix therefore requiresD JVPs with\nvectors inRD. Assuming eachfk in the chain composition has the form", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1108, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b2a1ef57-3152-4336-889c-0dba5e4f6e85": {"__data__": {"id_": "b2a1ef57-3152-4336-889c-0dba5e4f6e85", "embedding": null, "metadata": {"page_label": "193", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d85b0bec-594a-4e20-8bf4-6224c2766a3e", "node_type": "4", "metadata": {"page_label": "193", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "c561d30f69e1f578590ae631003acede8a5f7e47a0d5c18991f26eccf8de0053", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8.1. Computation chains 193\nfk : RDk\u22121 \u2192RDk, seen as a matrix,\u2202fk(sk\u22121) has size Dk \u00d7Dk\u22121.\nTherefore, the computational cost ofD JVPs isO\n(\nD\u2211K\nk=1 DkDk\u22121\n)\n.\nThe memory cost isO(maxk\u2208[K] Dk), since we can release intermediate\ncomputations after each layer is processed. SettingD1 = \u00b7\u00b7\u00b7 = DK\u22121 =\nD for simplicity and usingDK = M, we obtain that the computational\ncost of computingD JVPs and therefore of computing the full Jacobian\nmatrix by forward-mode autodiff isO(MD2 + KD3). The memory cost\nis O(max{D,M}). If a function has a single-input (D = 1), then the\nforward mode computes the entire Jacobian at once, which reduces to\na single directional derivative.\nComplexity of reverse-mode autodiff\nUsing Definition 2.9, we find that we can extract eachrow of the\nJacobian matrix[\u2202f(s0)]i \u2208RD, fori\u2208[M], by multiplying with the\nstandard basis vectorei \u2208RM:\n[\u2202f(s0)]1 = \u2202f(s0)\u2217[e1]\n...\n[\u2202f(s0)]M = \u2202f(s0)\u2217[eM].\nComputing the full Jacobian matrix therefore requiresM VJPs with\nvectors inRM. Assuming as before that eachfk in the chain composition\nhas the formfk : RDk\u22121 \u2192RDk, the computational cost ofM VJPs\nis O\n(\nM\u2211K\nk=1 DkDk\u22121\n)\n. However, the memory cost isO(\u2211K\nk=1 Dk),\nas we need to store the intermediate computations for each of the\nK layers. Setting D0 = \u00b7\u00b7\u00b7 = DK\u22121 = D for simplicity and using\nDK = M, we obtain that the computational cost of computingM VJPs\nand therefore of computing the full Jacobian matrix by reverse-mode\nautodiff isO(M2D+ KMD2). The memory cost isO(KD+ M). If the\nfunction has a single output (M = 1), reverse-mode autodiff computes\nthe entire Jacobian at once, which reduces to the gradient.\nWhen to use forward-mode vs. reverse-mode autodiff?\nWe summarize the time and space complexities in Table 8.1. Generally,\nif M < D, reverse-mode is more advantageous at the price of some", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1811, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a2e88f3d-f5e1-4ffa-96a0-51f14c61044b": {"__data__": {"id_": "a2e88f3d-f5e1-4ffa-96a0-51f14c61044b", "embedding": null, "metadata": {"page_label": "194", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "61ec9db0-0eaf-4462-89f5-838a802748e5", "node_type": "4", "metadata": {"page_label": "194", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "9a0994e0a9bff1a36e69db2e873711b3370fc33e9eed35a2a71ce3e2e95956d1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "194 Automatic differentiation\nForward-mode Reverse-mode\nTime O(MD2 + KD3) O(M2D+ KMD2)\nSpace O(max{M,D}) O(KD+ M)\nTable 8.1:Time and space complexities of forward-mode and reverse-mode autodiff\nfor computing the full Jacobian of a chain of functionsf = fK \u25e6\u00b7\u00b7\u00b7\u25e6 f1, where\nfk: RD \u2192RD if k = 1,...,K \u22121 and fK: RD \u2192RM. We assume\u2202fk is a dense\nlinear operator. Forward mode requiresD JVPs. Reverse mode requiresM VJPs.\nmemory cost. IfM \u2265D, forward mode is more advantageous.\n8.2 Feedforward networks\nIn the previous section, we derived forward-mode autodiff and reverse-\nmode autodiff for computation chains with an inputs0 \u2208S0. In this\nsection, we now derive reverse-mode autodiff for feedforward networks,\nin which each layerfk is now allowed to depend explicitly on some\nadditional parameterswk \u2208Wk. The recursion is\ns0 := x\u2208S0\ns1 := f1(s0,w1) \u2208S1\n...\nsK := fK(sK\u22121,wK) \u2208SK\nf(x,w) := sK,\nwhere S0 = X and w = (w1,..., wK) \u2208W1 \u00d7\u00b7\u00b7\u00b7\u00d7W K. Eachfk is\nnow a function of two arguments. The first argument depends on the\nprevious layer, but the second argument does not. This is illustrated in\nFig. 8.5. We now explain how to differentiate a feedforward network.\n8.2.1 Computing the adjoint\nThe function has the formf: E \u2192F, where E:= X\u00d7 (W1 \u00d7\u00b7\u00b7\u00b7\u00d7\nWK) and F:= SK. From Section 2.3, we know that the VJP has the\nform \u2202f(x,w)\u2217: F \u2192E. Therefore, we want to be able to compute\n\u2202f(x,w)\u2217[u] \u2208E for anyu\u2208F.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1391, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ddb45ea1-60c9-4bd9-866a-739cd4d8f0a5": {"__data__": {"id_": "ddb45ea1-60c9-4bd9-866a-739cd4d8f0a5", "embedding": null, "metadata": {"page_label": "195", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d995420a-1d2c-4c76-aa40-e82fbc1ccdd8", "node_type": "4", "metadata": {"page_label": "195", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "d4cf375ea9d1af4ade77b91f3219d084f50dab912060c7b4c1f8c6beb6dbbd1e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8.2. Feedforward networks 195\nFortunately, the backward recursion is only a slight modification\nof the computation chain case. Indeed, since fk: Ek \u2192 Fk, where\nEk := Sk\u22121 \u00d7Wk and Fk := Sk, the intermediate VJPs have the form\n\u2202fk(sk\u22121,wk)\u2217: Fk \u2192Ek. We therefore arrive at the recursion\nrK = u\u2208SK\n(rK\u22121,gK) = \u2202fK(sK\u22121,wK)\u2217[rK] \u2208SK\u22121 \u00d7WK\n...\n(r0,g1) = \u2202f1(s0,w1)\u2217[r1] \u2208S0 \u00d7W1.\nThe final output is\n\u2202f(x,w)\u2217[u] = (r0,(g1,..., gK)).\n8.2.2 Computing the gradient\nWe often compose a network with a loss function\nL(w; x,y) := \u2113(f(x,w); y) \u2208R.\nFrom Proposition 2.7, the gradient is given by\n\u2207L(w; x,y) = (g1,..., gK) \u2208W1 \u00d7\u00b7\u00b7\u00b7\u00d7W K\nwhere\n\u2202f(x,w)\u2217[u] = (r0,(g1,..., gK)),\nwith u= \u2207\u2113(f(x,w); y) \u2208SK. The outputr0 \u2208S0, where S0 = X,\ncorresponds to the gradient w.r.t.x\u2208X and is typically not needed,\nexcept in generative modeling settings. The full procedure is summarized\nin Algorithm 8.3.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 875, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "37226689-635d-4627-aa04-26206f4dab38": {"__data__": {"id_": "37226689-635d-4627-aa04-26206f4dab38", "embedding": null, "metadata": {"page_label": "196", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f7d55c9-73c1-45db-a4bc-b7798681a84b", "node_type": "4", "metadata": {"page_label": "196", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "df9e80ec54c079d27f022982106ebba54a0f0353901198318c4095478e93ddb4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "196 Automatic differentiation\n......\nFigure 8.5:Computation graph of an MLP as a function of its parameters.\n......\n... ...\nForward pass\nBackward pass\n...\nFigure 8.6: Reverse mode of automatic differentiation, a.k.a., gradient back-\npropagation to compute the gradient of the loss of an MLP on an input label pair.\nFor readability, we denoted the intermediate VJPs as functions of three variables\n\u2202f\u2217\nk : (sk\u22121,wk\u22121,rk) \u21a6\u2192\u2202fk(sk\u22121,wk)\u2217[rk] with \u2202fk(sk\u22121,wk)\u2217[rk] = (rk\u22121,gk).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 475, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "57b077d8-d531-4e36-bdc1-c09344ef6b28": {"__data__": {"id_": "57b077d8-d531-4e36-bdc1-c09344ef6b28", "embedding": null, "metadata": {"page_label": "197", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c487abf-3a49-4492-9846-1d71e723168c", "node_type": "4", "metadata": {"page_label": "197", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "0383b95be8d3a4214502b23111b27f68bf2f630fc98f64a80b6f117708891586", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8.3. Computation graphs 197\nAlgorithm 8.3Gradient back-propagation for feedforward networks\nFunctions:f1,...,f K in sequential order\nInputs: data point(x,y) \u2208X\u00d7Y\nparameters w= (w1,... wK) \u2208W1 \u00d7\u00b7\u00b7\u00b7\u00d7W K\n1: Initialize s0 := x \u25b7 Forward pass\n2: for k:= 1,...,K do\n3: Compute and storesk := fk(sk\u22121,wk) \u2208Sk\n4: Compute \u2113(sK; y) and u:= \u2207\u2113(sK; y) \u2208SK\n5: Initialize rK := u\u2208SK \u25b7 Backward pass\n6: for k:= K,..., 1 do\n7: Compute (rk\u22121,gk) := \u2202fk(sk\u22121,wk)\u2217[rk] \u2208Sk\u22121 \u00d7Wk\n8: Outputs: L(w; x,y) := \u2113(sK; y), \u2207L(w; x,y) = (g1,...,g K)\n8.3 Computation graphs\nIn the previous sections, we reviewed autodiff for computation chains\nand its extension to feedforward networks. In this section, we review its\ngeneralization to computation graphs, introduced in Section 4.1.3. Our\nformalism assumes, without loss of generality, that functions can take\nmultiple inputs but only produce a single output, as we can always group\nmultiple outputs as a tuple. This enables a one-to-one correspondence\nbetween output variablessk and functionsfk.\n8.3.1 Forward mode\nThe forward mode corresponds to computing the JVP of the program\nin an input directionv \u2208S0. In the case of acomputation chain\nf := fK \u25e6\u00b7\u00b7\u00b7\u25e6 f1, eachfk takes asingle input sk\u22121 \u2208Sk\u22121 and each\n\u2202fk(sk\u22121) takes a single input directiontk\u22121 \u2208Sk\u22121. The forward pass\non iterationk\u2208[K] is then, starting fromt0 := v,\nsk := fk(sk\u22121) \u2208Sk\ntk := \u2202fk(sk\u22121)[tk\u22121] \u2208Sk.\nIn the case of acomputation graph, specified by functionsf1,...,f K\nintopologicalorder,each fk maynowtake multipleinputs(si1 ,..., sipk) \u2208", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1530, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "79b47871-2683-46da-8f8b-54e02c18df01": {"__data__": {"id_": "79b47871-2683-46da-8f8b-54e02c18df01", "embedding": null, "metadata": {"page_label": "198", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c30fab1-1d82-4d2f-a9c7-e79da4b3781c", "node_type": "4", "metadata": {"page_label": "198", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "07a796a93589fa55d4d6893202aa04b1b4ad3fc2756432f249b8541a3d954f33", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "198 Automatic differentiation\nSi1 \u00d7\u00b7\u00b7\u00b7\u00d7S ipk, and each\u2202fk(si1 ,..., sipk) takes as many input direc-\ntions (ti1 ,..., tipk) \u2208Si1 \u00d7\u00b7\u00b7\u00b7\u00d7S ipk, where(i1,...,i pk) := pa(k) are\nthe parents offk. The forward pass on stepk \u2208[K] then computes\nboth intermediate inputs and directions as\nsk := fk(si1 ,..., sipk) \u2208Sk\ntk := \u2202fk(si1 ,..., sipk)[ti1 ,..., tipk] \u2208Sk.\nLet us recall that\u2202ifk means that we differentiatefk w.r.t. to itsith\nargument. Using the fan-in rule in Proposition 2.8, we obtain that the\nderivatives are propagated as\ntk =\npk\u2211\nj=1\n\u2202jfk(si1 ,..., sipk)[tij] \u2208Sk.\nThe final output is\u2202f(s0)[v] = tK. The resulting generic forward-mode\nprocedure is summarized in Fig. 8.7 and in Algorithm 8.4. Intuitively,\nthe algorithm consists in computing and summing intermediate JVPs\nalong the forward pass. Although not explicitly mentioned, we can\nrelease sk and tk from memory when no child node depends on nodek.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 909, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1fd9f536-8556-4cf7-a119-d689740c95c1": {"__data__": {"id_": "1fd9f536-8556-4cf7-a119-d689740c95c1", "embedding": null, "metadata": {"page_label": "199", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "74266f51-215b-455f-bb8c-5e0cc9aab1bd", "node_type": "4", "metadata": {"page_label": "199", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "a918b6b4ea015067e2d55d62a80dd146605b7cc57c5992b376260409ce978696", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8.3. Computation graphs 199\nForward modeForward pass\nFigure 8.7: Forward mode automatic differentiation in a computation graph.\nAlgorithm 8.4Forward-mode autodiff for computation graphs\nFunctions:f1,...,f K in topological order\nInputs: input s0 \u2208S0, input directionv\u2208S0\n1: Initialize t0 := v\n2: for k:= 1,...,K do \u25b7 Forward pass\n3: Retrieve parent nodes(i1,...,i pk) := pa(k)\n4: Compute sk := fk(si1 ,..., sipk) \u2208Sk\n5: Compute\ntk := \u2202fk(si1 ,..., sipk)[ti1 ,..., tipk]\n=\npk\u2211\nj=1\n\u2202jfk(si1 ,..., sipk)[tij] \u2208Sk.\n6: Outputs: f(s0) := sK \u2208SK, \u2202f(s0)[v] = tK \u2208SK", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 557, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2cfe2f4a-748c-4940-bc05-679328690c4b": {"__data__": {"id_": "2cfe2f4a-748c-4940-bc05-679328690c4b", "embedding": null, "metadata": {"page_label": "200", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ddf6f29c-7c52-4056-9f15-e5517257bf7f", "node_type": "4", "metadata": {"page_label": "200", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "e74364b33951cec5eeed9263be49f024a722787dcc5f10c68de6421b165c194f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "200 Automatic differentiation\n8.3.2 Reverse mode\nThe reverse mode corresponds to computing the VJP of the program\nin an output directionu\u2208SK. In the case of acomputation chain\nf := fK \u25e6\u00b7\u00b7\u00b7\u25e6 f1, each intermediate variablesk is used only onceto\ncompute the next variablesk+1 := fk+1(sk). To compute the VJP of\nthe chain, it then suffices to reverse the order of the operations and to\nuse the corresponding VJPs. Since each function takes a single input,\neach VJP\u2202fk(sk)\u2217[rk] produces asingle output directionrk\u22121 \u2208Sk.\nA backward pass therefore computes fromk := K to k := 1, starting\nfrom rK := u,\nrk\u22121 := \u2202fk(sk\u22121)\u2217[rk] \u2208Sk\u22121.\nThe final output isr0 := \u2202f(s0)\u2217[u].\nIn computation graphs, intermediate variables may be used more\nthan once, since a nodek may have several children nodes. This com-\nplicates reversing the computation graph. To circumvent this issue,\nfollowing the formalism of Roy Frostig (Murphy, 2023, Section 6.2.2.2),\nwe may formally distinguish between the outputsk of fk and the inputs\nsk\u2192j of fj for j \u2208ch(k), the children offk. We do so by introducing\nan operation that simply duplicatessk,\ndup(sk) := (sk,..., sk).\nThe tuple output bydup is of length|ch(k)|. The forward pass can then\nbe formally rewritten as, fork\u2208[K],\nsk := fk(si1\u2192k,..., sipk\u2192k)\nwhere (i1,...,i pk) := pa(k) followed by\n(sk\u2192j1 ,..., sk\u2192jck\n) := dup(sk),\nwhere (j1,...,j ck) := ch(k). The benefit of this approach is that each\nduplicated output is input only once to the subsequent child functions.\nThanks to this, we can now associate to each variable,si\u2192j or sk, a\nsingle corresponding intermediate variable,ri\u2192j or rk, in the backward\npass. The reverse mode can then simply be written by going through\nthe VJPs of the functionsfk and the VJP ofdup in reverse order.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1758, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "80c92746-c83b-4514-ad3e-8d938ac332cf": {"__data__": {"id_": "80c92746-c83b-4514-ad3e-8d938ac332cf", "embedding": null, "metadata": {"page_label": "201", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a323310-010e-44d4-8069-511a61b6b033", "node_type": "4", "metadata": {"page_label": "201", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "e3c5f0bd59df598735055813f19d2f78e8c10901d881e34c786420f8a63626db", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8.3. Computation graphs 201\nFor the VJP of thedup operation, let us denote the intermedi-\nate variables in the backward pass associated tosk\u2192j1 ,..., sk\u2192jck\nby\nrk\u2192j1 ,..., rk\u2192jck\n. Following the fan-out rule in Proposition 2.9, the\nVJP of thedup operation on iterationk is\nrk = dup(sk)\u2217[rk\u2192j1 ,..., rk\u2192jck\n]\n=\nck\u2211\ni=1\ndupi(sk)\u2217[rk\u2192ji]\n=\nck\u2211\ni=1\nrk\u2192ji.\nIn the last line, we used thatdupi(s) = sby definition of the duplication\noperation so\u2202dupi(s) = I and \u2202dupi(s)\u2217= I. The VJP ofdup justifies\nwhy, if an intermediate valuesk is used by later functionsfj1 ,...,f jck,\nfor (j1,...,j ck) := ch(k), the derivatives with respect tosk need to sum\nall the variations through thefj functions into the variablerk.\nFor the VJP of thefk functions, following the fan-in rule in Propo-\nsition 2.8, the VJP returnsmultiple output variations,\nri1\u2192k,..., ripk\u2192k = \u2202fk(si1\u2192k,..., sipk\u2192k)\u2217[rk],\nwhere (i1,...,i pk) := pa(k), and where, forj \u2208{1,...,p k},\nrij\u2192k := \u2202jfk(si1\u2192k,..., sipk\u2192k)\u2217[rk] \u2208Sij.\nThe overall formalism is summarized in Fig. 8.8.\nImplementation\nThe duplication operationdup is just a formalism to mathematically\nderive the reverse mode. In practice, the variablessk are not duplicated\nbut accessed several times during the backward pass. The variablesrk\nare computed by accumulatingrk\u2192j into rk each time a variablerk\u2192j\nis computed. Therefore, for eachk\u2208[K], we can compute the VJP and\nperform the in-place updates,\nri1\u2192k,..., ripk\u2192k = \u2202fk(si1 ,..., sipk)\u2217[rk]\nrij \u2190rij + rij\u2192k \u2200j \u2208{1,...,p k}.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1494, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3d6706de-e0ee-413c-911e-1eaf3ee50683": {"__data__": {"id_": "3d6706de-e0ee-413c-911e-1eaf3ee50683", "embedding": null, "metadata": {"page_label": "202", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5c5e39cd-5ad3-4427-9b1c-2ca86afa6c94", "node_type": "4", "metadata": {"page_label": "202", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "04f07ffd2f1c0a05eb0c9a9049ffebd8900973dbd6124c35b3fd580ce5228db2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "202 Automatic differentiation\nForward pass Reverse mode\nFigure 8.8: Reverse mode automatic differentiation in a computation graph.\nAlgorithm 8.5Reverse-mode autodiff for computation graphs\nFunctions:f1,...,f K in topological order\nInputs: input s0 \u2208S0, output directionu\u2208SK\n1: for k:= 1,...,K do \u25b7 Forward pass\n2: Retrieve parent nodes(i1,...,i pk) := pa(k)\n3: Compute sk := fk(si1 ,..., sipk) \u2208Sk\n4: Instantiate VJPlk := \u2202fk(si1 ,..., sipk)\u2217\n5: Initialize rK := u, rk \u21900 \u2200k\u2208{0,...,K \u22121} \u25b7 Backward pass\n6: for k:= K,..., 1 do\n7: Retrieve parent nodes(i1,...,i pk) = pa(k)\n8: Compute ri1\u2192k,..., ripk\u2192k := lk[rk]\n9: Compute rij \u2190rij + rij\u2192k \u2208Sij \u2200j \u2208{1,...,p k}\n10: Outputs: f(s0) := sK \u2208SK, \u2202f(s0)\u2217[u] = r0 \u2208S0\nThe topological ordering ensures thatrk has been fully computed when\nwe reachfk. The resulting generic reverse-mode procedure is presented\nin Algorithm 8.5.\nExample 8.2(Example of forward and reverse modes). WeuseFig.8.9\nto illustrate the forward and reverse modes in a computation graph.\nLet us assume that the intermediate variabless1,..., s7 have readily", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1068, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "38bd51fb-c307-4eb6-92e9-7a649f93afd2": {"__data__": {"id_": "38bd51fb-c307-4eb6-92e9-7a649f93afd2", "embedding": null, "metadata": {"page_label": "203", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f95450ef-580c-4067-97f4-a2850064e928", "node_type": "4", "metadata": {"page_label": "203", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "0636d7c6246f615a00e4dfde2257118a07723a646507c2e76776499676c39e1f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8.3. Computation graphs 203\nFigure 8.9:Same computation graph as Fig. 4.3 but without the actual definition\nof eachfk, for simplicity.\nbeen computed. Theforward modecorresponds to\nt0 := v\nt1 := \u2202f1(s0)[t0]\nt2 := \u2202f2(s0)[t0]\nt3 := \u2202f3(s1)[t1]\nt4 := \u2202f4(s2,s3)[t2,t3] = \u22021f4(s2,s3)[t2] + \u22022f4(s2,s3)[t3]\nt5 := \u2202f5(s1,s4)[t1,t4] = \u22021f5(s1,s4)[t1] + \u22022f5(s1,s4)[t4]\nt6 := \u2202f6(s5)[t5]\nt7 := \u2202f7(s4,s6)[t4,t6] = \u22021f7(s4,s6)[t4] + \u22022f7(s4,s6)[t6].\nThe reverse modecorresponds to\n(r4\u21927,r6\u21927) := \u2202f7(s4,s6)\u2217[r7] r7 := u\nr5\u21926 := \u2202f6(s5)\u2217[r6] r6 := r6\u21927\n(r1\u21925,r4\u21925) := \u2202f5(s1,s4)\u2217[r5] r5 := r5\u21926\n(r2\u21924,r3\u21924) := \u2202f4(s2,s3)\u2217[r4] r4 := r4\u21925 + r4\u21927\nr1\u21923 := \u2202f3(s1)\u2217[r3] r3 := r3\u21924\nr0\u21922 := \u2202f2(s0)\u2217[r2] r2 := r2\u21924\nr0\u21921 := \u2202f1(s0)\u2217[r1] r1 := r1\u21923 + r1\u21925\nr0 := r0\u21921 + r0\u21922.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 755, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "129059a7-c96c-4e67-8b33-768de33a8395": {"__data__": {"id_": "129059a7-c96c-4e67-8b33-768de33a8395", "embedding": null, "metadata": {"page_label": "204", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c77cf7a-6d97-41c7-8c75-22ce66eee3df", "node_type": "4", "metadata": {"page_label": "204", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "b2eac7fbb53f9f75be42cca4e026dd672f528fb34085572661eafab34b7aa1fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "204 Automatic differentiation\n8.3.3 Complexity, the Baur-Strassen theorem\nFor computing the gradient of a functionf: E \u2192R represented by\na computation graph, we saw that the reverse mode is more efficient\nthan the forward mode. As we previously stated, assuming that the\nelementary functionsfk in the DAG and their VJP have roughly the\nsame computational complexity, thenf and \u2207f have roughly the same\ncomputational complexity. This fact is crucial and is the pillar on which\nmodernmachinelearningrelies:itallowsustooptimizehigh-dimensional\nfunctions by gradient descent.\nFor arithmetic circuits, reviewed in Section 4.1.4, this crucial fact\nis made more precise in the celebrated Baur-Strassen theorem (Baur\nand Strassen, 1983). Iff is a polynomial, then so is its gradient\u2207f.\nThe theorem gives an upper bound on the size of the best circuit for\ncomputing \u2207f from the size of the best circuit for computingf.\nProposition 8.1(Baur-Strassen\u2019s theorem). For any polynomial\nf: E\u2192 R, we have\nS(\u2207f) \u22645 \u00b7S(f),\nwhere the sizeS(f) of a polynomialf is defined in Definition 4.1.\nA simpler proof by backward induction was given by Morgenstern\n(1985). See also the proof of Theorem 9.10 in Chenet al.(2011). For\ngeneral computation graphs, that have more primitive functions than\njust + and \u00d7, a similar result can be obtained; see, e.g., (Bolteet al.,\n2022, Theorem 2).\n8.4 Implementation\n8.4.1 Primitive functions\nAn autodiff system implements a setAof primitive or elementary\nfunctions, which serve as building blocks for creating other functions, by\nfunction composition. For instance, we saw that in arithmetic circuits\n(Section 4.1.4), A= {+,\u00d7}. More generally,Amay contain all the\nnecessary functions for expressing programs. We emphasize, however,\nthat Ais not necessarily restricted to low-level functions such as log", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1815, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "892bc3c4-6965-4feb-bd37-988c1fcc6d21": {"__data__": {"id_": "892bc3c4-6965-4feb-bd37-988c1fcc6d21", "embedding": null, "metadata": {"page_label": "205", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1bb84dbb-5682-470a-a005-33658e4f2130", "node_type": "4", "metadata": {"page_label": "205", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "cf3b549560c556c9754192e5fbc83ad3adb7de24a8f0604674297db379e1d218", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8.4. Implementation 205\nand exp, but may also contain higher-level functions. For instance,\neven though the log-sum-exp can be expressed as the composition\nof elementary operations (log, sum, exp), it is usually included as a\nprimitive on its own, both because it is a very commonly-used building\nblock, but also for numerical stability reasons.\n8.4.2 Closure under function composition\nEach functionfk in a computation graph belongs to a setF, the class of\nfunctions supported by the system. A desirable property of an autodiff\nimplementation is that the setFis closed under function composition,\nmeaning that iff \u2208F and g \u2208F, thenf \u25e6g \u2208F. This means that\ncomposed functions can themselves be used for composing new functions.\nThis property is also crucial for supporting higher-order differentiation\n(Chapter 9) and automatic linear transposition (Section 8.4.4). When\nfk is a composition of elementary functions inA, then fk itself is a\nnested DAG. However, we can alwaysinline each composite function,\nsuch that all functions in the DAG belong toA.\n8.4.3 Examples of JVPs and VJPs\nAn autodiff system must implement for eachf \u2208A its JVP for support-\ning the forward mode, and its VJP for supporting the reverse mode.\nWe give a couple of examples. We start with the JVP and VJP of linear\nfunctions.\nExample 8.3(JVP and VJP of linear functions). Considerthematrix-\nvector product f(W) = Wx \u2208RM, where x\u2208RD is fixed and\nW \u2208RM\u00d7D. As already mentioned in Section 2.3.1, the JVP off\nat W \u2208RM\u00d7D along an input directionV \u2208RM\u00d7D is simply\n\u2202f(W)[V] = f(V) = Vx \u2208RM.\nTo find the associated VJP, we note that for anyu \u2208RM and\nV \u2208 RM\u00d7D, we must have \u27e8\u2202f(W)[V],u\u27e9 = \u27e8V,\u2202f (W)\u2217[u]\u27e9.\nUsing the properties of the trace, we have\n\u27e8\u2202f(W)[V],u\u27e9= \u27e8Vx,u\u27e9= tr(x\u22a4V\u22a4u) = \u27e8V,ux\u22a4\u27e9.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1756, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1e640f64-77a3-43e8-980f-73d58fccb425": {"__data__": {"id_": "1e640f64-77a3-43e8-980f-73d58fccb425", "embedding": null, "metadata": {"page_label": "206", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4d509d84-a327-4d09-ae43-ee5f38f3f4ed", "node_type": "4", "metadata": {"page_label": "206", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "ebed2d90b81081b23598e11c1c5fef6601243bcc8b3831afc32a021f9a8d6652", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "206 Automatic differentiation\nTherefore, we find that the VJP is given by\n\u2202f(W)\u2217[u] = ux\u22a4\u2208RM\u00d7D.\nSimilarly, consider now a matrix-matrix productf(W) = WX,\nwhere W \u2208RM\u00d7D and where X \u2208RD\u00d7N is fixed. The JVP at\nW \u2208RM\u00d7D along an input directionV \u2208RM\u00d7D is simply\n\u2202f(W)[V] = f(V) = VX \u2208RM\u00d7N.\nThe VJP along the output directionU \u2208RM\u00d7N is\n\u2202f(W)\u2217[U] = UX\u22a4\u2208RM\u00d7D.\nAnother simple example are element-wise separable functions.\nExample 8.4(JVP and VJP of separable function). Considerthefunc-\ntion f(w) := (g1(w1),...,g P(wP)), where eachgi: R \u2192R has a\nderivativeg\u2032\ni. The Jacobian matrix is then a diagonal matrix\n\u2202f(w) = diag(g\u2032\n1(w1),...,g \u2032\nP(wP)) \u2208RP\u00d7P.\nIn this case, the JVP and VJP are actually the same\n\u2202f(w)[v] = \u2202f(w)\u2217[v] = (g\u2032\n1(w1),...,g \u2032\nP(wP)) \u2299v,\nwhere \u2299indicates element-wise multiplication.\n8.4.4 Automatic linear transposition\nOn first sight, if we want to support both forward ans reverse modes, it\nappears like we need to implement both the JVP and the VJP for each\nprimitive operationf \u2208A. Fortunately, there exists a way to recover\nVJPs from JVPs, and vice-versa.\nWe saw in Section 2.3 that ifl(w) is a linear map, then its JVP is\n\u2202l(w)[v] = l(v) (independent ofw). Conversely, the VJP is\u2202l(w)\u2217[u] =\nl\u2217(u), wherel\u2217is the adjoint operator ofl (again, independent ofw).\nLet us definel(u; w) := \u2202f(w)\u2217[u], i.e., the VJP off in the output\ndirection u. Sincel(u; w) is linear inu, we can apply the reasoning", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1410, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1b7c1878-8830-40d6-931f-1a9eb42328ae": {"__data__": {"id_": "1b7c1878-8830-40d6-931f-1a9eb42328ae", "embedding": null, "metadata": {"page_label": "207", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8be1d785-5b3f-470b-a382-1f254cf7fa8a", "node_type": "4", "metadata": {"page_label": "207", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "d6a2de5c65141ee7ca7cca55cc15335833d75af73e83996f573668160496abff", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8.5. Checkpointing 207\nabove to compute its VJP\n\u2202l(u; w)\u2217[v] = l\u2217(v; w) = \u2202f(w)\u2217\u2217[v] = \u2202f(w)[v],\nwhich is independent ofu. In words, the VJP of a VJP is the cor-\nresponding JVP! This means that we can implement forward-mode\nautodiff even if we only have access to VJPs. As an illustration and\nsanity check, we give the following example.\nExample 8.5(Automatic transpose of \u201cdot\u201d). If we define\nf(x,W) := Wx, from Example 8.3, we know that\n\u2202f(x,W)\u2217[u] = (W\u22a4u,ux\u22a4)\n= (f(u,W\u22a4),f(x\u22a4,u))\n=: l(u; x,W).\nUsing Proposition 2.9, we obtain\n\u2202l(u; x,W)\u2217[v,V] = f(v,W) + f(x,V)\n= Wv + Vx\n= \u2202f(x,W)[v,V].\nThe other direction, automatically creating a VJP from a JVP, is\nalso possible but is more technical and relies on the notion ofpartial\nevaluation(Frostiget al., 2021; Radulet al., 2022).\n8.5 Checkpointing\nWe saw that forward-mode autodiff can release intermediate computa-\ntions from memory along the way, while reverse-mode autodiff needs to\ncache all of them. This means that the memory complexity of reverse-\nmode autodiff, in its standard form, grows linearly with the number of\nnodes in the computation graph. A commonly-used technique to cir-\ncumvent this issue is checkpointing, which trades-off computation time\nfor better memory usage. Checkpointing works by selectively storing\nonly a subset of the intermediate values, calledcheckpoints, and by\nrecomputing others on the fly. The specific choice of the checkpoint lo-\ncations in the computation graph determines the memory-computation", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1487, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1f01178e-8188-4d03-a5bc-293808050f76": {"__data__": {"id_": "1f01178e-8188-4d03-a5bc-293808050f76", "embedding": null, "metadata": {"page_label": "208", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4cb64ad2-e81e-4211-bb56-f08c48a57143", "node_type": "4", "metadata": {"page_label": "208", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "14a3704f0e85c7040a6dd87bdbef4880e3ae62afd98dd681c9d910e0db744dfb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "208 Automatic differentiation\nsplit\ncheckpoints\nFigure 8.10:Checkpointing trades-off computation time for better memory usage\nby selectively storing only a subset of the intermediate values, called checkpoints,\nand by recomputing others on the fly. Recursive halving and dynamic programming\nare two divide-and-conquer strategies to select the checkpoint locations.\ntrade-off. While it is possible to heuristically set checkpoints at user-\nspecified locations, it is also possible to perform a checkpointing strategy\nalgorithmically, as studied in depth by Griewank (1992) and Griewank\nand Walther (2008). In this section, we review two divide-an-conquer al-\ngorithms: recursive halving and dynamic programming. Our exposition\nfocuses on computation chainsf = fK \u25e6... \u25e6f1, withfk : RD \u2192RD\nfor simplicity.\nComputational and memory complexities at two extremes.Let C(K)\nbe the number of calls to the individual functionsfk (we ignore the\ncost of computing the intermediate VJPs) andM(K) be the number\nof function inputs cached, when performing reverse-mode autodiff on\na chainf = fK \u25e6... \u25e6f1. On one extreme, if we store all intermediate\ncomputations, as done in Algorithm 8.1, to compute only the VJP\n\u2202f(s0)\u2217[u], we have\nC(K) = K\u22121 and M(K) = K.\nThis is optimal w.r.t. computational complexity, but suboptimal w.r.t.\nmemory. On the other extreme, if we only store the initial input, as", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1383, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "100d86cb-9423-4c7c-9bf5-7198ec31bb2b": {"__data__": {"id_": "100d86cb-9423-4c7c-9bf5-7198ec31bb2b", "embedding": null, "metadata": {"page_label": "209", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "20195e71-1e7a-46e4-90c9-f240d4041656", "node_type": "4", "metadata": {"page_label": "209", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "79cead9173a78d6623add5e10706eb2f8f6417dfcb83e25f389a083892b66146", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8.5. Checkpointing 209\nAlgorithm 8.6Reverse-mode autodiff with constant memory\nvjp_full_recompute(fK \u25e6... \u25e6f1,s0,u) := \u2202(fK \u25e6... \u25e6f1)(s0)\u2217[u]\nInputs: Chain fK\u25e6... \u25e6f1, inputs0 \u2208S0, output directionu\u2208SK\n1: if K = 1 then\n2: return \u2202f1(s0)\u2217[u]\n3: else\n4: Set rK = u\n5: for k:= K,..., 1 do\n6: Compute sk\u22121 = (fk\u22121 \u25e6... \u25e6f1)(s0)\n7: Compute rk\u22121 = \u2202fk(sk\u22121)\u2217[rk]\n8: return: r0\ndone in Algorithm 8.6, then we have\nC(K) = K(K\u22121)/2 and M(K) = 1.\nThis is optimal w.r.t. memory but leads to a computational complexity\nthat is quadratic inK.\n8.5.1 Recursive halving\nAs a first step towards obtaining a better computation-memory trade-off,\nwe may split the chainsK = fK \u25e6\u00b7\u00b7\u00b7\u25e6 f1(s0) as\nsK/2 = fK/2 \u25e6... \u25e6f1(s0)\nsK = fK \u25e6... \u25e6fK/2+1(sK/2),\nfor K even. Then, rather than recomputing all intermediate computa-\ntions sk from the inputs0 as in Algorithm 8.6, we can storesK/2 and\nrecompute sk for k>K/ 2 starting fromsK/2. Formally, this strategy\namounts to the following steps.\n1. Compute sK/2 = fK/2 \u25e6... \u25e6f1(s0)\n2. Compute rK/2 = vjp_full_recompute(fK\u25e6... \u25e6fK/2+1,sK/2,u)\n3. Compute r0 = vjp_full_recompute(fK/2 \u25e6... \u25e6f1,s0,rK/2)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1114, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9dd49142-1de6-4f62-9aae-4eefcb803c29": {"__data__": {"id_": "9dd49142-1de6-4f62-9aae-4eefcb803c29", "embedding": null, "metadata": {"page_label": "210", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1177bfe4-3430-4637-97cb-2b957b9cb091", "node_type": "4", "metadata": {"page_label": "210", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "d186d6c8ef5531dc5a2fe30d461db19bb2abb4b2c34c0339154080bd1ce18d69", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "210 Automatic differentiation\nAlgorithm 8.7Reverse-mode autodiff with recursive halving\nvjp_halving(fK \u25e6... \u25e6f1,s0,u) := \u2202(fK \u25e6... \u25e6f1)(s0)\u2217[u]\nFunctions:Chain fK \u25e6... \u25e6f1\nInputs: input s0 \u2208S0, output directionu\u2208SK\n1: if K = 1 then\n2: return \u2202f1(s0)\u2217[u]\n3: else\n4: Compute sK/2 = fK/2 \u25e6... \u25e6f1(s0)\n5: Compute rK/2 = vjp_halving(fK \u25e6... \u25e6fK/2+1,sK/2,u)\n6: Compute r0 = vjp_halving(fK/2 \u25e6... \u25e6f1,s0,rK/2)\n7: return: r0\nAt the expense of having to store the additional checkpointsK/2, this\nalready roughly halves the computational complexity compared to Al-\ngorithm 8.6.\nWe can then apply this reasoning recursively, as formalized in Al-\ngorithm 8.7. The algorithm is known asrecursive binary schedule\n(Griewank, 2003) and illustrated in Fig. 8.11. In terms of number of\nfunction evaluationsC(K), for K even, we makeK/2 function calls,\nand we call the procedure recursively twice, that is,\nC(K) = 2C(K/2) + K/2.\nIf the chain is of length1, we directly use the VJP, soC(1) = 0. Hence,\nthe numbers of function calls, ifK is a power of2, is\nC(K) = K\n2 log2 K.\nIn terms of memory usage, Algorithm 8.7 usess0 not only at line 4 but\nalso at line 6. So when the algorithm is called recursively on the second\nhalf of the chain at line 5, one memory slot is taken bys0. This line is\ncalled recursively until the chain is reduced to a single function. At that\npoint, the total number of memory slots used is equal to the number\nof times we split the function in half, that is,log2 K for K a power\nof 2. On the other hand, the inputs0 is no longer used after line 6\nof Algorithm 8.7. At that line, the memory slot taken bys0 can be\nconsumed by the recursive call on the first-half. In other words, calling", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1691, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "98807f67-27fd-4937-a4fb-c27fea75f204": {"__data__": {"id_": "98807f67-27fd-4937-a4fb-c27fea75f204", "embedding": null, "metadata": {"page_label": "211", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b18c590e-95e1-4b38-8f5c-032599242ec6", "node_type": "4", "metadata": {"page_label": "211", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "0fa5208e3f7cd2cabe8dd1e06a8bcea5e29467e4b40b9a71e8c95f23cae93269", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8.5. Checkpointing 211\n0\n1\n2\n4\n6\n8\nFunction step \nTime step\nStorage in memoryForward computation Backward computation\nFigure 8.11: Illustration of checkpointing with recursive halving, for a chain of\n8 functions. The chain is first fully evaluated while storing some computations\nas checkpoints in memory. Then, during the backward pass, we recompute some\nintermediate values from the latest checkpoint available. In contrast, vanilla reverse-\nmode autodiff (with full caching of the intermediate computations) would lead to a\nsimple triangle shape.\nthe algorithm recursively on the first half does not incur extra memory\ncost. So ifK is a power of2, the memory cost of Algorithm 8.7 is\nM(K) = log2 K.\n8.5.2 Dynamic programming\nRecursive halving requireslog2 K memory slots for a chain of lengthK.\nHowever, as illustrated in Fig. 8.11, at a given time step, all memory\nslots may not be exploited.\nTo optimize the approach, we observe that recursive halving is\njust one instance of a program that splits the chain and calls itself\nrecursively on each part. In other words, it is a form ofdivide-and-\nconquer algorithm. Rather than splitting the chain in half, we may\nconsider splitting the chain at some indexl. One split is used to reverse\nthe computations froml+ 1 to K by a recursive call that consumes one\nmemory slot. The other split is used on a recursive call that reverses\nthe computations from0 to l. That second call does not require an\nadditional memory slot, as it can use directly the memory slot used", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1513, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eecbb970-1f2f-4d2c-b934-744b9041e0e5": {"__data__": {"id_": "eecbb970-1f2f-4d2c-b934-744b9041e0e5", "embedding": null, "metadata": {"page_label": "212", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7222784a-317a-49e9-9f16-19cd60b2bcac", "node_type": "4", "metadata": {"page_label": "212", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "b2ccf5fca0e67b97dc9bd3e01cd27b32e043238c6de840571dca512e16f88f8d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "212 Automatic differentiation\nby the original inputs0. To split the chain in such two parts, we need\nl intermediate computations to go froms0 to sl. The computational\ncomplexity C(k,s), counted as the number of function evaluations, for\na chain of lengthk with s memory slots then satisfies the recurrence\nC(k,s) = C(k\u2212l,s \u22121) + C(l,s) + l,\nfor alll\u2208{1,...,k \u22121}. By simply takingl= k/2, we recover exactly\nthe computational complexity of recursive halving. To refine the latter,\nwe may split the chain by selectingl to minimize the complexity. An\noptimal scheme must satisfy the recursive equation,\nC\u2217(k,s) := min\n1\u2264l\u2264K\u22121\n{C\u2217(k\u2212l,s \u22121) + C\u2217(l,s) + l}. (8.3)\nNote thatC\u2217(K,S) can be computed fromC\u2217(k,s) for k= 1,...,K \u22121,\ns= 1,...,S \u22121. This suggests adynamic programmingapproach to\nfind an optimal scheme algorithmically. For a chain of lengthk= 1, the\ncost is null as we directly reverse the computation, soC\u2217(1,s) := 0. On\nthe other hand for a memorys= 1, there is only one possible scheme\nthat saves only the initial input as in Algorithm 8.6, soC\u2217(k,1) :=\n(k(k\u22121))/2. The valuesC\u2217(k,s) can then be computed incrementally\nfrom k= 1 to K and s= 1 to S using Eq. (8.3). The optimal splits can\nbe recorded along the way as\nl\u2217(k,s) := arg min\n1\u2264l\u2264k\u22121\n{C\u2217(k\u2212l,s \u22121) + C\u2217(l,s) + l}.\nThe optimal split forK,S can then be found bybacktracking the\noptimal splits along both branches corresponding toC\u2217(k\u2212l,s \u22121)\nand C\u2217(l,s). As the final output consists in traversing a binary tree,\nit was called treeverse (Griewank, 1992). Note that the dynamic\nprogramming procedure is generic and could a priori incorporate varying\ncomputational costs for the intermediate functionsfk.\nAnalytical formula\nIt turns out that we can also find an optimal schemeanalytically. This\nscheme was found by Griewank (1992), following the analysis of optimal\ninversions of sequential programs by divide-and-conquer algorithms", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1896, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4f8381af-79bf-41eb-9d8d-76356459c5a6": {"__data__": {"id_": "4f8381af-79bf-41eb-9d8d-76356459c5a6", "embedding": null, "metadata": {"page_label": "213", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f2b073d5-2327-424e-86b3-f1e568fc00a4", "node_type": "4", "metadata": {"page_label": "213", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "e05e3f955b4f3a6fe56b3a64d06b7fd8a471ee99f7ef8fa66f807f154e95cfe8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8.6. Reversible layers 213\ndone by Grimmet al.(1996); see also Griewank (2003, Section 6) for\na simple proof. The main idea consists in considering the number of\ntimes an evaluation stepfk is repeated. As we split the chain atl, all\nsteps from1 to l will be repeated at least once. In other words, treating\nthe second half of the chain incurs one memory cost, while treating the\nfirst half of the chain incurs one repetition cost. Griewank (1992) shows\nthat for fixedK, S, we can find the minimal number of repetitions\nanalytically and build the corresponding scheme with simple formulas\nfor the optimal splits.\nCompared to the dynamic programming approach, it means that we\ndo not need to compute the pointersl\u2217(k,s), and we can use a simple\nformula to setl\u2217(k,s). We still need to traverse the corresponding binary\ntree given K,S and l\u2217(k,s) to obtain the schedules. Note that such\noptimal scheme does not take into account varying computational costs\nfor the functionsfk.\n8.5.3 Online checkpointing\nThe optimal scheme presented above requires knowing the total number\nof nodes in the computation graph ahead of time. However, when\ndifferentiating through for example a while loop (Section 5.10), this is\nnot the case. To circumvent this issue, online checkpointing schemes\nhave been developed and proven to be nearly optimal (Stumm and\nWalther, 2010; Wanget al., 2009). These schemes start by defining a set\nof S checkpoints with the firstS computations, then these checkpoints\nare rewritten dynamically as the computations keep going. Once the\ncomputations terminate, the optimal approach presented above for a\nfixed length is applied on the set of checkpoints recorded.\n8.6 Reversible layers\n8.6.1 General case\nThe memory requirements of reverse-mode autodiff can be completely\nalleviated when the functionsfk are invertible (meaning thatf\u22121\nk exists)\nand whenf\u22121\nk is easily accessible. In that case, rather than storing the\nintermediate computationssk\u22121, necessary to compute the VJPrk \u21a6\u2192", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1995, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "51ff03e0-de60-4ab5-a8da-595486e20c9d": {"__data__": {"id_": "51ff03e0-de60-4ab5-a8da-595486e20c9d", "embedding": null, "metadata": {"page_label": "214", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "299cb7f9-c960-4da6-ad81-914ecc56d4d5", "node_type": "4", "metadata": {"page_label": "214", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "f37bb794a05a03a590ea874411e49aa5b7068009636a159880953b5bfd5489f4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "214 Automatic differentiation\nAlgorithm 8.8Reverse-mode autodiff for reversible chains.\nFunctions:f := fK \u25e6... \u25e6f1, with eachfk invertible\nInputs: input s0 \u2208S0, output directionu\u2208SK\n1: Compute sK = fK \u25e6... \u25e6f1(s0)\n2: for k:= K,..., 1 do\n3: Compute sk\u22121 = f\u22121\nk (sk)\n4: Compute rk\u22121 = \u2202fk(sk\u22121)\u2217[rk]\nOutputs: f(s0) := sK, \u2202f(s0)\u2217[u] = r0\n\u2202fk(sk\u22121)\u2217[rk], one can compute them on the fly during the backward\npass fromsk using sk\u22121 = f\u22121\nk (sk). We summarize the procedure for\nthe case of computation chains in Algorithm 8.8. Compared to vanilla\nreverse-mode autodiff in Algorithm 8.2, the algorithm has optimal\nmemory complexity, as we can releasesk and rk as we go.\nIn practice,f\u22121\nk often does not exist or may not be easily accessible.\nHowever, network architectures can be constructed to be easily invertible\nby design. Examples include reversible residual networks (Gomezet\nal., 2017), orthonormal RNNs (Helfrich et al., 2018), neural ODEs\n(Section 12.6), and momentum residual neural networks (Sanderet al.,\n2021a); see also references therein.\n8.6.2 Case of orthonormal JVPs\nWhen the JVP of eachfk is an orthonormal linear mapping, i.e.,\n\u2202fk(sk\u22121)\u22121 = \u2202fk(sk\u22121)\u2217,\nit is easy to check that the VJP off = fK \u25e6... \u25e6f1 is equal to the JVP\nof f\u22121 = f\u22121\n1 \u25e6... \u25e6f\u22121\nK , that is\n\u2202f(s0)\u2217[u] = \u2202f\u22121(sK)[u].\nIn other words, in the case of orthormal JVPs, reverse-mode autodiff of\nf coincides with forward-mode autodiff off\u22121.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1419, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fd7dd8b2-714a-40f6-9707-41d940436ad6": {"__data__": {"id_": "fd7dd8b2-714a-40f6-9707-41d940436ad6", "embedding": null, "metadata": {"page_label": "215", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea51f9dc-eb85-48e2-840c-bdd4d02b5b13", "node_type": "4", "metadata": {"page_label": "215", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "0c35738ac838904fdcd265c1ce9a84fa17afc05b18f7aed3650a0618514b4cec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8.7. Randomized forward-mode gradient estimator 215\nReverse mode\nRandomized forward mode\nFigure 8.12:The randomized forward-mode gradient estimator only requires forward\npasses, but it suffers from high variance, even more so in high dimension.\n8.7 Randomized forward-mode gradient estimator\nForward-mode autodiff does not require to store intermediate activations.\nHowever, for a functionf: RP \u2192R, computing the gradient\u2207f using\nforward-mode autodiff requiresP JVPs, which is intractable ifP is large.\nCan we approximate\u2207f with fewer JVPs? The following proposition\ngives anunbiased estimatorof \u2207f that only involves JVPs.\nProposition 8.2(Unbiased forward-mode estimator of the gradient).\nLet f: RP \u2192R be a differentiable function. Then,\n\u2207f(\u00b5) = EZ\u223cp[\u2202f(\u00b5)[Z]Z]\n= EZ\u223cp[\u27e8\u2207f(\u00b5),Z\u27e9Z] .\nwhere p:= Normal(0,1)P is the isotropic Gaussian distribution.\nThis estimator is for instance used by Baydin et al. (2022). It\ncan be seen as the zero-temperature limit of the gradient of a\nperturbed function, estimated by the score-function estimator (SFE);\nsee Section 14.4.6.\nIn practice, the expectation above can be approximated by drawing\nM noise vectorsz1,..., zM, and averaging\u27e8\u2207f(\u00b5),zi\u27e9over i\u2208[M].\nA word of caution: while this estimator can be useful for example\nwhen we do not want to store the intermediate activations for memory\nreasons, this of course comes at the cost of increasing the variance,\nwhich influences the convergence rate of SGD, as seen in Section 16.2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1466, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0e3033d2-c5c4-4a88-b829-4f3e1630e7a6": {"__data__": {"id_": "0e3033d2-c5c4-4a88-b829-4f3e1630e7a6", "embedding": null, "metadata": {"page_label": "216", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "983d58d4-f85d-4b23-ad2d-72917ec8c1f9", "node_type": "4", "metadata": {"page_label": "216", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "634736bbb8f3442705a29105665d8aa4e3d317659689907e0daf742d755b96ba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "216 Automatic differentiation\n8.8 Summary\n\u2022 Computer programs can be seen as directed acyclic graphs, where\nnodes correspond to the output of intermediate operations in\nthe program, and edges represent the dependencies of current\noperations on past operations.\n\u2022 Automatic differentiation (autodiff) for a functionf: RP \u2192RM\nhas two main modes: forward mode and reverse mode.\n\u2022 The forward mode: i) uses JVPs, ii) builds the Jacobian one\ncolumn at a time, iii) is efficient for tall Jacobians (M \u2265P), iv)\nneed not store intermediate computations.\n\u2022 The reverse mode: i) uses VJPs, builds the Jacobian one row at\na time, iii) is efficient for wide Jacobians (P \u2265M), iv) needs to\nstore intermediate computations, in order to be computationally\noptimal.\n\u2022 To trade computational efficiency for better memory efficiency,\nwe can use checkpointing techniques.\n\u2022 The complexity of computing the gradient of a functionf: RP \u2192\nR using the reverse mode is at most a constant time bigger than\nthat of evaluating the function itself. This is the Baur-Strassen\ntheorem, in arithmetic circuits. This astonishing result is one of\nthe pillars of modern machine learning.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1153, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "26a29e68-b270-42e2-811f-444ef84cc7eb": {"__data__": {"id_": "26a29e68-b270-42e2-811f-444ef84cc7eb", "embedding": null, "metadata": {"page_label": "217", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "15cf80fc-ac05-4e5c-8cdc-8c92fefa1b63", "node_type": "4", "metadata": {"page_label": "217", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "c2ddbc36282657aa9e9dbaa63350fcb83f72f5650c29d425a50fba4ada5d5bae", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9\nSecond-order automatic differentiation\nWe review in this chapter how to perform automatic differentiation for\nsecond-order derivatives.\n9.1 Hessian-vector products\nWe consider in this section a functionf: E \u2192R. Similarly to the\nJacobian, for most purposes, we do not need access to the full Hessian\nbut rather to the Hessian-vector product (HVP)\u22072f(w)[v] at w\u2208E,\nin a directionv\u2208E, as defined in Definition 2.19. The latter can be\ncomputed in four different ways, depending on how we combine the two\nmain modes of autodiff.\n9.1.1 Four possible methods\nAn HVP can be computed in four different ways.\n1. Reverse on reverse:The Hessian can be seen as the transposed\nJacobian of the gradient, hence the HVP can be computed as the\nVJP of the gradient,\n\u22072f(w)[v] = \u2202(\u2207f)(w)\u2217[v].\n217", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 778, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "708d7042-45e8-4a19-b38f-fe4bf01289ee": {"__data__": {"id_": "708d7042-45e8-4a19-b38f-fe4bf01289ee", "embedding": null, "metadata": {"page_label": "218", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f94225fc-5d8a-457f-a27a-94a3b0b99c85", "node_type": "4", "metadata": {"page_label": "218", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "a4eefaa49b5af97d33486a6d7de4f02fed8e54087760b2167a2585da3bd5ddea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "218 Second-order automatic differentiation\n2. Forward on reverse: Owing to its symmetry (see Proposi-\ntion 2.10), the Hessian can also be seen as the Jacobian of the\ngradient, hence the HVP can be computed as theJVP of the\ngradient,\n\u22072f(w)[v] = \u2202(\u2207f)(w)[v].\n3. Reverse on forward:Recall that for any functiong: E\u2192E , the\nVJP can equivalently be defined as the gradient along an output\ndirection v\u2208E, that is,\n\u2202g(w)\u2217[v] = \u2207\u27e8g,v\u27e9(w),\nwhere we recall the shorthand\u27e8g,v\u27e9(w) := \u27e8v,g(w)\u27e9, so that\n\u27e8g,v\u27e9is a function ofw. In our case, we can therefore rewrite the\nreverse-on-reverse approach as\n\u2202(\u2207f)(w)\u2217[v] = \u2207\u27e8\u2207f,v\u27e9(w).\nWe know that\u27e8\u2207f,v\u27e9(w) = \u27e8\u2207f(w),v\u27e9= \u2202f(w)[v] is the JVP\nof f at walong v. Therefore, we can also compute the HVP as\nthe gradient of the JVPof f at walong v,\n\u22072f(w)[v] = \u2207(\u2202f(\u00b7)[v])(w),\nwhere we use the notation(\u2202f(\u00b7)[v])(w) := \u2202f(w)[v] to insist on\nthe fact that it is a function ofw.\n4. Forward on forward:Finally, we can use the definition of the\nHVP in Definition 2.19 as a vector of second partial derivatives\nalong vand each canonical direction. That is, assumingE= RP,\nwe can compute theJVP of the JVPP times,\n\u22072f(w)[v] = (\u22022f(w)[v,ei])P\ni=1.\nThe four different ways of computing the HVP are summarized in\nTable 9.1.\n9.1.2 Complexity\nTo get a sense of the computational and memory complexity of the four\napproaches, we consider a chain of functionsf := fK \u25e6\u00b7\u00b7\u00b7\u25e6 f1 as done", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1392, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0d9165bd-af64-42b2-a785-042065993498": {"__data__": {"id_": "0d9165bd-af64-42b2-a785-042065993498", "embedding": null, "metadata": {"page_label": "219", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b5d2ce3d-ef4c-4b57-a17b-5154010af782", "node_type": "4", "metadata": {"page_label": "219", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "eb9cae54a0514122af6723b523d757ff9dcfaaff9c072085f9ec0f37629b99b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9.1. Hessian-vector products 219\nMethod Computation\nReverse on reverse (VJP of gradient) \u2202(\u2207f)(w)\u2217[v]\nForward on reverse (JVP of gradient) \u2202(\u2207f)(w)[v]\nReverse on forward (gradient of JVP) \u2207(\u2202f(\u00b7)[v])(w)\nForward on forward (JVPs of JVPs) (\u22022f(w)[v,ei])P\ni=1\nTable 9.1:Four different ways of computing the HVP\u22072f(w)[v].\nFigure 9.1:Computation graph corresponding to reverse mode autodiff for eval-\nuating the gradient off = fK \u25e6...f 1. While f is a simple chain,\u2207f is a DAG.\nin Section 8.1. To simplify our analysis, we assumefk: RP \u2192RP for\nk\u2208{1,...,K \u22121}and fK: RP \u2192R.\nThe computation graph of the reverse mode is illustrated in Fig. 9.1.\nWhile f = fK \u25e6\u00b7\u00b7\u00b7\u25e6 f1 would be represented by a simple chain, the\ncomputational graph of\u2207f is no longer a chain: it is a DAG. This\nis due to the computations of\u2202fk(sk\u22121)[rk], where bothsk\u22121 and rk\ndepend ons0.\nWe illustrate the computation graphs of reverse-on-reverse and\nforward-on-reverse in Fig. 9.2 and Fig. 9.3 respectively. By applying\nreverse mode on reverse mode, at each fan-in operationsk\u22121,rk \u21a6\u2192\n\u2202fk(sk\u22121)[rk], the reverse mode on\u2207f branches out in two paths that\nare later merged by a sum. By applying forward mode on top of reverse\nmode, the flow of computations simply follows the one of\u2207f.\nWith this in mind, following a similar calculation as for Table 8.1,\nwe obtain the following results. We assume that each\u2202fk(sk\u22121) is a\ndense linear operator, so that its application has the same cost as a\nmatrix-vector multiplication. For the memory complexity, we consider\nthat the inputs of each operation is saved to compute the required", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1585, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2b0c1bee-5b53-427a-bd43-a3a1b56e058a": {"__data__": {"id_": "2b0c1bee-5b53-427a-bd43-a3a1b56e058a", "embedding": null, "metadata": {"page_label": "220", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fb71f892-6744-476c-b66b-05366fa0b4e0", "node_type": "4", "metadata": {"page_label": "220", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "f33435a3b2cd7c35e366bc503920cf7a4a38931d445b1923c4f6612fc52eae5e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "220 Second-order automatic differentiation\nGradient computation\nby reverse mode auto-diff\nHVP computations\nby reverse mode on top of reverse mode\nFigure 9.2:Computation graph for computing the HVP\u22072f(x)[v] by using reverse\nmode on top of reverse mode. As the computation graph of\u2207f induces fan-in\noperations sk\u22121,rk \u21a6\u2192\u2202fk(sk\u22121)[rk], the reverse mode applied on \u2207f induces\nbranching of the computations at each such node.\nGradient computation\nby reverse mode auto-diff\nHVP computations\nby forward mode on top of reverse mode\nFigure 9.3:Computation graph for computing the HVP\u22072f(x)[v] by using forward\nmode on top of reverse mode. The forward mode naturally follows the computations\ndone for the gradient, except that it passes through the derivatives of the intermediate\noperations.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 782, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7074808d-8a8b-469e-bfb8-ab52ef7d80e8": {"__data__": {"id_": "7074808d-8a8b-469e-bfb8-ab52ef7d80e8", "embedding": null, "metadata": {"page_label": "221", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "567a01c0-2e14-4ca9-8648-6995d6c0d780", "node_type": "4", "metadata": {"page_label": "221", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "bda1c3497070e8a5d8d3781f100192eaeb30bf447af0c0239b9868649d50f3a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9.1. Hessian-vector products 221\nderivatives in the backward passes.\n1. Reverse on reverse:O(KP2) time andO(KP) space.\n2. Forward on reverse:O(KP2) time andO(KP) space.\n3. Reverse on forward:O(KP2) time andO(KP) space.\n4. Forward on forward:O(KP3) time andO(3P) space for the\nP JVPs withe1,..., eP.\nWe see that, for chains of functions, \u201creverse on reverse\u201d, \u201cforward\non reverse\u201d and \u201creverse on forward\u201d all have similar time complexities\nup to some constant factors. Using reverse mode on top of reverse\nmode requires storing the information backpropagated, i.e., therk\n(resp. the information forwarded, i.e., thetk in Fig. 8.1), to perform\nthe final reverse pass. By using forward mode on top of reverse mode,\nthis additional cost is not incurred, making it slightly less memory\nexpensive. In addition, reverse mode on top of reverse mode induces a\nfew additional summations due to the branching and merge operations\ndepicted in Fig. 9.2. The same holds when using reverse on top of\nforward as we cannot avoid fan-in operations (this time of the form\nsk\u22121,tk\u22121 \u21a6\u2192\u2202fk(sk\u22121)[tk\u22121]). Unfortunately, \u201cforward on forward\u201d is\nprohibitively expensive.\nTo summarize, among the four approaches presented to compute\nHVPs, the forward-over-reverse mode is a priori the most preferable\nin terms of computational and memory complexities. Note, however,\nthat computations of higher derivatives can benefit from dedicated\nautodiff implementations such as Taylor mode autodiff, that do not\nmerely compose forward and reverse modes. For general functionsf, it\nis reasonable to benchmark the first three methods to determine which\nmethod is the best for the function at hand.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1660, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2f71a0a1-ae8d-4b8b-b164-341bf70908d4": {"__data__": {"id_": "2f71a0a1-ae8d-4b8b-b164-341bf70908d4", "embedding": null, "metadata": {"page_label": "222", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "97fbf5fc-62cc-4c7a-8a1a-f4810ce0206d", "node_type": "4", "metadata": {"page_label": "222", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "a74d6154ba2d00889440fc671a3077b7718973f827f49a9e7d6c6a78e64dfc04", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "222 Second-order automatic differentiation\n9.2 Gauss-Newton matrix\n9.2.1 An approximation of the Hessian\nThe Hessian matrix\u22072L(w) of a functionL: W\u2192 R is often used to\nconstruct a quadratic approximation ofL(w),\nL(w+ v) \u2248\u27e8\u2207L(w),v\u27e9+ 1\n2\u27e8v,\u22072L(w)v\u27e9.\nUnfortunately, whenLis nonconvex,\u22072L(w) is typically anindefinite\nmatrix, which means that the above approximation is anonconvex\nquadratic w.r.t. v. For instance, ifL= \u2113\u25e6f with \u2113 convex, thenL is\nconvex iff is linear, but it is typically nonconvex iff is nonlinear. The\n(generalized) Gauss-Newton matrix is a principled alternative to the\nHessian, which is defined forL:= \u2113\u25e6f.\nDefinition 9.1(Gauss-Newton matrix). Given a differentiable func-\ntion f : W\u2192M and a twice differentiable function\u2113: M\u2192 R, the\n(generalized) Gauss-Newton matrix of the compositionL= \u2113\u25e6f\nevaluated at a pointw\u2208W is defined as\n\u22072\nGN(\u2113\u25e6f)(w) := \u2202f(w)\u2217\u22072\u2113(f(w))\u2202f(w).\nAs studied in Section 17.2, the Gauss-Newton matrix is a key ingre-\ndient of the Gauss-Newton method. An advantage of the Gauss-Newton\nmatrix is its positive semi-definiteness provided that\u2113 is convex.\nProposition 9.1(Positive semi-definiteness of the GN matrix). If\u2113is\nconvex, then\u22072\nGN(\u2113\u25e6f)(w) is positive semi-definite for allf.\nThis means that the approximation\nL(w+ v) \u2248\u27e8\u2207L(w),v\u27e9+ 1\n2\u27e8v,\u22072\nGNL(w)v\u27e9\nis aconvex quadraticw.r.t. v.\nUsing the chain rule, we find that the Hessian ofL= \u2113\u25e6f decomposes\ninto the sum of two terms (see also Proposition 9.7).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1442, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "770f5be5-07b4-4c47-ac95-f40129feb453": {"__data__": {"id_": "770f5be5-07b4-4c47-ac95-f40129feb453", "embedding": null, "metadata": {"page_label": "223", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8b970236-bcbb-407f-9cbb-c789d878070b", "node_type": "4", "metadata": {"page_label": "223", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "5bee0eb9efd9df23772f4a8ad3ee645a0e61c39e392f09cc15dc315d0a46bd9a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9.2. Gauss-Newton matrix 223\nProposition 9.2(Approximation of the Hessian). Forf differentiable\nand \u2113 twice differentiable, we have\n\u22072(\u2113\u25e6f)(w) = \u2202f(w)\u2217\u22072\u2113(f(w))\u2202f(w) + \u22022f(w)\u2217[\u2207\u2113(f(w))]\n= \u22072\nGN(\u2113\u25e6f)(w) +\nZ\u2211\nj=1\n\u2207j\u2113(f(w))\u22072fj(w).\nIf f is linear, then the Hessian and Gauss-Newton matrices coincide,\n\u22072(\u2113\u25e6f)(w) = \u22072\nGN(\u2113\u25e6f)(w).\nThe Gauss-Newton operator\u22072\nGN(\u2113\u25e6f) can therefore be seen as an\napproximation of the Hessian\u22072(\u2113\u25e6f), with equality iff is linear.\n9.2.2 Gauss-Newton chain rule\nA chain rule for computing the Hessian of a composition of two functions\nis presented in Proposition 9.7, but the formula is relatively complicated,\ndue to the cross-terms. In contrast, a Gauss-Newton chain rule is\nstraightforward.\nProposition 9.3(Gauss-Newton chain rule).\n\u22072\nGN(\u2113\u25e6f \u25e6g)(w) = \u2202g(w)\u2217\u22072\nGN(\u2113\u25e6f)(g(w))\u2202g(w).\n9.2.3 Gauss-Newton vector product\nAs for the Hessian, we rarely need to materialize the full Gauss-Newton\nmatrix in memory. Indeed, we can define the Gauss-Newton vector\nproduct (GNVP), a linear map for a directionv\u2208W, as\n\u22072\nGN(\u2113\u25e6f)(w)[v] := \u2202f(w)\u2217\u22072\u2113(f(w))\u2202f(w)v, (9.1)\nwhere \u22072\u2113(\u03b8)u is the HVP of\u2113, a linear map fromMto M. The\nGNVP can be computed using the JVP off, the HVP of\u2113 and the\nVJP of f. Instantiating the VJP requires1 forward pass throughf,\nfrom which we get both the valuef(w) and the adjoint linear map\nu\u21a6\u2192(\u2202f(w)\u2217u). Evaluating the VJP requires1 backward pass through", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1390, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "05d27cd7-c9c1-4a2f-b587-02a40923f143": {"__data__": {"id_": "05d27cd7-c9c1-4a2f-b587-02a40923f143", "embedding": null, "metadata": {"page_label": "224", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1589d060-28a6-4258-9600-7954de7b610c", "node_type": "4", "metadata": {"page_label": "224", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "187360548c6716dbce844d6f7088d6b00d2813d95b074e80860e41144835f26d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "224 Second-order automatic differentiation\nf. Evaluating the JVP requires1 forward pass throughf. In total,\nevaluating v\u21a6\u2192\u22072\nGN(\u2113\u25e6f)(w)vtherefore requires2 forward passes and\n1 backward pass throughf.\n9.2.4 Gauss-Newton matrix factorization\nIn this section, we assumeW\u2286 RP and M\u2286 RM. When\u2113 is convex,\nwe know that the Gauss-Newton matrix is positive semi-definite and\ntherefore it can be factorized into\u22072\nGN(\u2113\u25e6f)(w) = VV \u22a4 for some\nV \u2208RP\u00d7R, whereR \u2264min{P,M}is the rank of the matrix. Such a\ndecomposition can actually be computed easily from a factorization of\nthe Hessian of\u2113. For instance, suppose we know the eigendecomposition\nof the Hessian of\u2113, \u22072\u2113(f(w)) = \u2211M\nj=1 \u03bbiuiu\u22a4\ni , where theui are the\neigenvectors and the\u03bbi \u22650 are the eigenvalues (which we know are\nnon-negative due to positive semidefiniteness). Then, the Gauss-Newton\nmatrix can be decomposed as\n\u22072\nGN(\u2113\u25e6f) =\nM\u2211\nj=1\n\u03bbi\u2202f(w)\u2217uiu\u22a4\ni \u2202f(w)\u2217\n=\nM\u2211\nj=1\n(\u221a\n\u03bbi\u2202f(w)\u2217ui\n)(\u221a\n\u03bbi\u2202f(w)\u2217ui\n)\u22a4\n=\nM\u2211\nj=1\nviv\u22a4\ni where vi :=\n\u221a\n\u03bbi\u2202f(w)\u2217ui.\nStacking the vectorsvi into a matrixV = (v1,..., vM), we recover\nthe factorization\u22072\nGN(\u2113\u25e6f)(w) = VV \u22a4. To form this decomposition,\nwe need to perform the eigendecomposition of\u22072\u2113(f(w)) \u2208RM\u00d7M,\nwhich takesO(M3) time. We also needM calls to the VJP off at w.\nCompared to the direct implementation in Eq. (9.1), the factorization,\nonce computed, allows us to compute the Gauss-Newton vector product\n(GNVP) as\u22072\nGN(\u2113\u25e6f)(w)[v] = VV \u22a4v. The factorization only requires\nP \u00d7M memory, while the direct implementation in Eq. (9.1) requires\nus to maintain the intermediate computations off. The computation-\nmemory trade-offs therefore depend on the function considered.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1650, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fc006b9c-9740-4c95-9919-d8afbfecf107": {"__data__": {"id_": "fc006b9c-9740-4c95-9919-d8afbfecf107", "embedding": null, "metadata": {"page_label": "225", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b87e47d0-6eac-43dd-931e-bc7c9d093c9e", "node_type": "4", "metadata": {"page_label": "225", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "a969da4bd7a962faecbfb1b55b79e39ed271baaab3cc0873e97eb64ec6e1739a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9.3. Fisher information matrix 225\n9.2.5 Stochastic setting\nSuppose the objective function is of the form\nL(w; x,y) := \u2113(f(w; x); y).\nWith some slight abuse of notation, we then have that the Gauss-Newton\nmatrix associated with a pair(x,y) is\n\u22072\nGNL(w; x,y) := \u2202f(w; x)\u2217\u22072\u2113(\u03b8; y)\u2202f(w; x).\nGiven a distribution \u03c1 over (x,y) pairs, the Gauss-Newton matrix\nassociated with the averaged loss\nL(w) := EX,Y\u223c\u03c1[L(w; X,Y )]\nis then\n\u22072\nGNL(w) = EX,Y\u223c\u03c1\n[\n\u22072\nGNL(w; X,Y )\n]\n.\n9.3 Fisher information matrix\n9.3.1 Definition using the score function\nThe Fisher information is a way to measure the amount of information\nin a random variableS.\nDefinition 9.2(Fisher information matrix). The Fisher informa-\ntion matrix, or Fisher for short, associated with the negative\nlog-likelihood L(w; S) = \u2212log qw(S) of a probability distribution\nqw with parameterswis the covariance of the gradients ofL at w\nfor S distributed according toqw,\n\u22072\nFL(w) := ES\u223cqw[\u2207L(w; S) \u2297\u2207L(w; S)]\n= ES\u223cqw[\u2207wlog qw(S) \u2297\u2207wlog qw(S)].\nThe gradient\u2207wlog qw(S) is known as thescore function.\nAs studied in Section 17.3, the Fisher information matrix is a key\ningredient of the natural gradient descent method.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1162, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9265280e-6404-437d-a8f4-1dad38ed0224": {"__data__": {"id_": "9265280e-6404-437d-a8f4-1dad38ed0224", "embedding": null, "metadata": {"page_label": "226", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2e1da7df-8474-4df4-aa8d-4450ab07add5", "node_type": "4", "metadata": {"page_label": "226", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "9435be3caee4c688f983bd88f12b9ef8dd4b2d64b43f65fb6f898b1703c86370", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "226 Second-order automatic differentiation\n9.3.2 Link with the Hessian\nProvided that the probability distribution is twice differentiable w.r.t.\nwwith integrable second derivatives, the Fisher information matrix can\nalso be expressed as the Hessian of the negative log-likelihood (Amari,\n1998; Martens, 2020).\nProposition 9.4(Connection with the Hessian). The Fisher infor-\nmation matrix of the negative log-likelihoodL(w; S) = \u2212log qw(S)\nsatisfies\n\u22072\nFL(w) = ES\u223cqw[\u22072L(w; S)] = ES\u223cqw[\u2212\u22072\nwlog qw(S)].\nRemark 9.1(Empirical Fisher). We emphasize that in the above\ndefinitions, S is sampled from the model distributionqw, not from\nthe data distribution\u03c1. That is, we have\n\u22072\nFL(w) = ES\u223cqw[\u2207wlog qw(S)\u2207wlog qw(S)\u22a4]\n\u0338= ES\u223c\u03c1[\u2207wlog qw(S)\u2207wlog qw(S)\u22a4]\nThe latter is sometimes called ambiguously the \u201cempirical\u201d Fisher,\nthough this name has generated confusion (Kunstneret al., 2019).\n9.3.3 Equivalence with the Gauss-Newton matrix\nSo far, we discussed the Fisher information for a generic random variable\nS \u223cqw. We now discuss the supervised probabilistic learning setting\nwhere S = (X,Y ) and where, using the product rule of probability,\nwe define the PDFqw(X,Y ) := \u03c1X(X)p\u03b8(Y), with the shorthand\u03b8:=\nf(w; X).\nProposition 9.5(Fisher matrix in supervised setting). Suppose\n(X,Y ) \u223cqw where the PDF ofqw is qw(X,Y ) := \u03c1X(X)p\u03b8(Y).\nIn that case, the Fisher information matrix of the negative log-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1388, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f070ccd3-58aa-4066-a1bc-d5679e129d3a": {"__data__": {"id_": "f070ccd3-58aa-4066-a1bc-d5679e129d3a", "embedding": null, "metadata": {"page_label": "227", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bd732d7b-1080-43b9-bb48-5f7b313d80b1", "node_type": "4", "metadata": {"page_label": "227", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "6f8b39cea52069cd5bad327eaa68abe602169a29936be7010cc9451deef3a8c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9.3. Fisher information matrix 227\nlikelihood L(w; x,y) = \u2212log qw(x,y) decomposes as,\n\u22072\nFL(w) = E(X,Y)\u223cqw[\u2207wlog qw(X,Y ) \u2297\u2207wlog qw(X,Y )]\n= EX\u223c\u03c1X [EY\u223cp\u03b8 [\u2207wlog p\u03b8(Y) \u2297\u2207wlog p\u03b8(Y)]]\n= EX\u223c\u03c1X\n[\n\u2202f(w; X)\u2217\u22072\nF\u2113(\u03b8)\u2202f(w; X)\n]\n,\nwhere we defined the shorthand\u03b8:= f(w; X) and where we defined\nthe negative log-likelihood loss\u2113(\u03b8; Y) := \u2212log p\u03b8(Y).\nWhen p\u03b8 is an exponential family distribution, we can show that the\nFisher information matrix and the Gauss-Newton matrix are equivalent.\nProposition 9.6(Equivalence between Fisher and Gauss-Newton). If\np\u03b8 is an exponential family distribution, then\n\u22072\nFL(w) = EX\u223c\u03c1XEY\u223cp\u03b8[\u2207L(w; X,Y ) \u2297\u2207L(w; X,Y )]\n= EX\u223c\u03c1XEY\u223cp\u03b8[\u2202f(w; X)\u2217\u2207\u2113(\u03b8; Y) \u2297\u2207\u2113(\u03b8; Y)\u2202f(w; X)]\n= EX\u223c\u03c1XEY\u223cp\u03b8[\u2202f(w; X)\u2217\u22072\u2113(\u03b8; Y)\u2202f(w; X)]\n= EX,Y\u223c\u03c1\n[\n\u22072\nGNL(w; X,Y )\n]\n,\nwhere \u03c1X(x) :=\n\u222b\n\u03c1(x,y)dy.\nProof. From Proposition 3.3, ifp\u03b8 is an exponential family distribution,\n\u22072\u2113(\u03b8,y) is actually independent ofy. Using Bartlett\u2019s second identity\nEq. (12.3), we then obtain\n\u22072\u2113(\u03b8; \u00b7) = EY\u223cp\u03b8[\u22072\u2113(\u03b8; Y)]\n= EY\u223cp\u03b8[\u22072\u2113(\u03b8; Y)]\n= EY\u223cp\u03b8[\u2212\u22072\n\u03b8log p\u03b8(Y)]\n= EY\u223cp\u03b8[\u2207\u03b8log p\u03b8(Y) \u2297\u2207\u03b8log p\u03b8(Y)]\n= EY\u223cp\u03b8[\u2207\u2113(\u03b8; Y) \u2297\u2207\u2113(\u03b8; Y)],\nwhere we used\u00b7to indicate that the results holds for ally. Plugging the\nresult back in the Fisher information matrix concludes the proof.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1230, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c22699ae-0f17-4c3d-abd9-92a665bd27d5": {"__data__": {"id_": "c22699ae-0f17-4c3d-abd9-92a665bd27d5", "embedding": null, "metadata": {"page_label": "228", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a6d0fd9e-6acf-4eef-8e14-4fc8ea23eea7", "node_type": "4", "metadata": {"page_label": "228", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "9c7995b265ae78cc30149a4eafd617e6e9da8b8e08c4ed10c7f654eda843ec2b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "228 Second-order automatic differentiation\n9.4 Inverse-Hessian vector product\n9.4.1 Definition as a linear map\nWe saw in Section 17.1 that Newton\u2019s method uses iterations as\nwt+1 = wt \u2212\u22072L(wt)\u22121\u2207L(wt).\nThe inverse is well-defined if for exampleLis strictly convex. Otherwise,\nwe saw that some additional regularization can be added. Newton\u2019s\nmethod therefore requires to accessinverse-Hessian vector products\n(IHVPs), as defined below.\nDefinition 9.3(Inverse-Hessian vector product). For a twice differ-\nentiable function L : RP \u2192R, we define theinverse-Hessian\nVector Product(IHVP) ofL at w\u2208RP as the linear map\nu\u21a6\u2192\u22072L(w)\u22121u,\nprovided that it exists. In other words, it is the linear map which\nto uassociates vsuch that\u22072L(w)v= u.\n9.4.2 Implementation with matrix-free linear solvers\nNumerous direct methods exist to compute the inverse of a matrix,\nsuch as the Cholesky decomposition, QR decomposition and Gaussian\nelimination. However, these algorithms require accessing elementary\nentries of the matrix, while an autodiff framework gives access to the\nHessian through HVPs. Fortunately, there exists so-calledmatrix-free\nalgorithms, that can solve a linear system of equations\nH[v] = u\nby only accessing the linear mapv \u21a6\u2192H[v] for any v. Among such\nalgorithms, we have theconjugate gradient(CG) method, that applies\nfor H positive-definite, i.e., such that\u27e8v,H[v]\u27e9>0 for allv\u0338= 0, or the\ngeneralized minimal residual(GMRES) method, that applies for\nany invertibleH. A longer list of solvers can be found in public software\nsuch as SciPy (Virtanenet al., 2020). The IHVP of a strictly convex", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1594, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "095e96e7-91c5-4737-a391-2fd5080fd894": {"__data__": {"id_": "095e96e7-91c5-4737-a391-2fd5080fd894", "embedding": null, "metadata": {"page_label": "229", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "528130f7-6c0b-4c99-930a-1ffeccb02d6a", "node_type": "4", "metadata": {"page_label": "229", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "92bf53d3e40ce0c2dacc4c118d0e11ff25997fedb04d2abec9795e2eacc06048", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9.4. Inverse-Hessian vector product 229\nfunction (ensuring that the Hessian is positive definite) can therefore\nbe computed by instantiating CG on the HVP,\n\u22072L(w)\u22121u\u2248CG(\u22072L(w)[\u00b7],u).\nPositive-definiteness of the Hessian is indeed guaranteed for strictly\nconvex functions for example, while for generic non-convex functions,\nsuch property may be verified around a minimizer but not in general.\nThe conjugate gradient method is recalled in Algorithm 9.1 in its\nsimplest form. In theory, the exact solution of the linear system is found\nafter at mostT = P iterations of CG, though in practice numerical\nerrors may prevent from getting an exact solution.\nAlgorithm 9.1Conjugate gradient method\nInputs: linear mapH[\u00b7] : RP \u2192RP, targetu\u2208RP, initialization\nv0 (default 0), number of iterationsT (default P), target accuracy\n\u03b5 (default machine precision)\n1: r0 = u\u2212H[v0]\n2: p0 = r0\n3: for t= 0,...T do\n4: \u03b1t = \u27e8rt,rt\u27e9\n\u27e8pt,H[pt]\u27e9\n5: vt+1 = vt + \u03b1tpt\n6: rt+1 = rt \u2212\u03b1tH[pt]\n7: if \u27e8rt+1,rt+1\u27e9\u2264 \u03b5 then break\n8: \u03b2t = \u27e8rt+1,rt+1\u27e9\n\u27e8rt,rt\u27e9\n9: pt+1 = rt+1 + \u03b2tpt\nOutput: vT, such thatH[vT] \u2248u\n9.4.3 Complexity\nFor a given matrixH \u2208RP\u00d7P, solving Hv = u can be done with\ndecomposition methods (LU, QR, Cholesky) inO(P3) time. For matrix-\nfree methods such as CG or GMRES, the cost per iteration isO(P2).\nSince they theoretically solve the linear system inO(P) iterations, the\ncost to obtain an exact solution is theoretically the same,O(P3).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1421, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e74843d6-e310-4e39-b2a2-12c35c0947b0": {"__data__": {"id_": "e74843d6-e310-4e39-b2a2-12c35c0947b0", "embedding": null, "metadata": {"page_label": "230", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "58468748-a088-4fe9-95c1-b5efb4d1f77e", "node_type": "4", "metadata": {"page_label": "230", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "ec4290f0baf10cd148ecea739060aa89970729ab8f928f13ad3f1f3ac57c2ad0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "230 Second-order automatic differentiation\nHowever, CG or GMRES differ from decomposition methods in that\nthey are iterative methods, meaning that, at each iteration, they get\ncloser to a solution. Unlike decomposition methods, this means that we\ncan stop them before an exact solution is found. In practice, the number\nof iterations required to find a good approximate solution depends on\nthe matrix. Well conditioned matrices require only few iterations. Badly\nconditioned matrices lead to some numerical instabilities for CG, so\nthat more thanP iterations may be needed to get a good solution. In\ncontrast, decomposition methods proceed in two steps: first they build\na decomposition ofH at a cost ofO(P3), and second they solve a linear\nsystem at a cost ofO(P2), by leveraging the structure. LU and QR\ndecompositions are known to be generally more stable and are therefore\noften preferred in practice, when we can access entries of H at no cost.\nIf we do not have access to the HessianH, but only to its HVP,\naccessing entries ofH comes at a prohibitive cost. Indeed, entries ofH\ncan still be recovered from HVPs, sincee\u22a4\ni Hej = Hi,j, but accessing\neach row or column ofH costs one HVP (matrix-vector product). To\naccess the information necessary to use a decomposition method, we\ntherefore needP calls to HVPs before being able to actually compute\nthe solution. For the same number of calls, CG or GMRES will already\nhave found an approximate solution. In addition, a CG method does\nnot require to store any memory.\n9.5 Second-order backpropagation\n9.5.1 Second-order Jacobian chain rule\nThe essential ingredient to develop forward-mode and reverse-mode\nautodiff hinged upon the chain rule for composed functions,h= g\u25e6f.\nFor second derivatives, a similar rule can be obtained. To do so, we\nslightly abuse notations and denote\n\u22022h(w)\u2217[u] := \u22072\u27e8h,u\u27e9(w) \u2208RP\u00d7P,\nwhere h : RP \u2192 RQ, w \u2208 RP, u \u2208 RQ, and where we recall the\nshorthand notation \u27e8u,h\u27e9(w) := \u27e8u,h(w)\u27e9. Moreover, we view the\nabove quantity as a linear map. Strictly speaking, the superscript\u2217is", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2055, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "23f99809-d2e4-411d-9e65-60bed9dd9c39": {"__data__": {"id_": "23f99809-d2e4-411d-9e65-60bed9dd9c39", "embedding": null, "metadata": {"page_label": "231", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37984659-24ed-4eb8-93a1-0109b8ba3938", "node_type": "4", "metadata": {"page_label": "231", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "f224e3e6e217cbb10b09260fb365dd65564bf44e64d7ad8eb6af109955d37491", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9.5. Second-order backpropagation 231\nnot a linear adjoint anymore, sincev1,v2 \u21a6\u2192\u22022h(w)[v1,v2] is no longer\nlinear but bilinear. However, this superscript plays the same role as the\nVJP, since it takes an output vector and returns the input derivatives\nthat correspond to infinitesimal variations along that output vector.\nProposition 9.7(Hessian chain-rule). For two twice differentiable\nfunctions f : RP \u2192RM and g: RM \u2192RQ, the second directional\nderivative of the compositiong\u25e6f is a bilinear map fromRP \u00d7RP\nto RQ along input directionsv1,v2 \u2208RP of the form\n\u22022(g\u25e6f)(w)[v1,v2] = \u2202g(f(w))[\u22022f(w)[v1,v2]]\n+ \u22022g(f(w))[\u2202f(w)[v1],\u2202f (w)[v2]].\nThe Hessian of the compositiong\u25e6f along an output direction\nu\u2208RQ is, seen as a linear map,\n\u22022(g\u25e6f)(w)\u2217[u] = \u22022f(w)\u2217[\u2202g(f(w))\u2217u] (9.2)\n+ \u2202f(w)\u2217\u22022g(f(w))\u2217[u]\u2202f(w).\nFor the composition off: RP \u2192RM with a scalar-valued function\nfunction \u2113: RM \u2192R, we have in matrix form\n\u22072(\u2113\u25e6f)(w) =\nM\u2211\nj=1\n(\u2207\u2113(f(w)))j\u22072fj(w)\n+ \u2202f(w)\u22a4\u22072\u2113(f(w))\u2202f(w).\nNote that, while the Hessian is usually defined for scalar-valued\nfunctions h: RP \u2192R, the above definition is for a generalized notion\nof Hessian that works for any functionh: RP \u2192RQ.\nThe Hessian back-propagation rule in Eq. (9.2) reveals two terms.\nThe first one\u22022f(w)\u2217[\u2202g(f(w))\u2217u] simply computes the Hessian of\nthe intermediate function along the output direction normally back-\npropagated by a VJP. The second term\u2202f(w)\u2217\u22022g(f(w))\u2217[u]\u2202f(w)\nshows how intermediate first-order variations influence second order\nderivatives of the output.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1506, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c6493ac8-de7a-4885-a45a-9b52f593fb05": {"__data__": {"id_": "c6493ac8-de7a-4885-a45a-9b52f593fb05", "embedding": null, "metadata": {"page_label": "232", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "365d3f67-4e38-4bfe-b76f-b39a52fd18cc", "node_type": "4", "metadata": {"page_label": "232", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "e54a7619b82b0230062065ecd53342ab4436a7c38fa90373472d3c567550357e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "232 Second-order automatic differentiation\nExample 9.1(Composition with an elementwise nonlinear function).\nConsider the element-wise application of a twice differentiable\nscalar-valued functionf(x) = (f(xi))M\ni=1 followed by some twice\ndifferentiable function\u2113. Note that\u22072fi(x) = f\u2032\u2032(xi)eie\u22a4\ni . Hence,\nthe Hessian of the composition reads\n\u22072(\u2113\u25e6f)(x) =\nM\u2211\ni=1\n(\u2207\u2113(f(x)))if\u2032\u2032(xi)eie\u22a4\ni\n+ diag(f\u2032(x))\u22072\u2113(f(x)) diag(f\u2032(x))\n= diag(\u2207\u2113(f(w)) \u2299f\u2032\u2032(x))\n+ \u22072\u2113(f(x)) \u2299(f\u2032(x)f\u2032(x)\u22a4),\nwhere f\u2032(x) := (f\u2032(xi))M\ni=1 and f\u2032\u2032(x) := (f\u2032\u2032(xi))M\ni=1.\nExample 9.2(Hessian of the composition with a linear function). Consider\na linear functionf(W) = Wx, forW \u2208RM\u00d7D, composed with\nsome twice differentiable function\u2113 : RM \u2192R. From Proposi-\ntion 9.7, we get, in terms of linear maps,\n\u22072(\u2113\u25e6f)(W) = \u2202f(W)\u2217\u22072\u2113(f(W))\u2202f(W).\nAs already noted in Section 2.3, we have that\u2202f(W)[V] = Vx\nand \u2202f(W)\u2217[u] = ux\u22a4. Hence, the Hessian seen as a linear map\nreads\n\u22072(\u2113\u25e6f)(W)[V] = \u2202f(W)\u2217[\u22072\u2113(f(W))[\u2202f(W)[V]]] = HVxx\u22a4,\nwhere H:= \u22072\u2113(f(W)).\n9.5.2 Computation chains\nFor a simple computation chainf = fK \u25e6... \u25e6f1 as in Section 8.1, the\nformula derived in Proposition 9.7 suffices to develop an algorithm that\nbackpropagates the Hessian, as shown in Algorithm 9.2. Compared to\nAlgorithm 8.2, we simply backpropagate both the vectorsrk and the\nmatrices Rk using intermediate first and second derivatives.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1358, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c2e5a885-85b8-4add-aba5-2be88ab105c7": {"__data__": {"id_": "c2e5a885-85b8-4add-aba5-2be88ab105c7", "embedding": null, "metadata": {"page_label": "233", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ce55cd0a-ec3d-46d8-b946-8a6d0523b392", "node_type": "4", "metadata": {"page_label": "233", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "1e8a4ba5798a4c113296e4f49e8004aa107a54f1bb4a084df98e581330eddfc6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9.5. Second-order backpropagation 233\nAlgorithm 9.2Hessian backprop for computation chains\nFunctions:f := fK \u25e6... \u25e6f1,\nInputs: input x, output directionu\n1: Initialize and stores0 := x \u25b7 Forward pass\n2: for k:= 1,...,K do\n3: Compute and storesk := fk(sk\u22121)\n4: Initialize rK := \u2207\u2113(sK), RK := \u22072\u2113(sK) \u25b7 Backward pass\n5: for k:= K,..., 1 do\n6: Compute rk\u22121 := \u2202fk(sk\u22121)\u2217[rk]\n7: Compute Rk\u22121 := \u22022fk(sk\u22121)\u2217[rk] + \u2202fk(sk\u22121)\u2217Rk\u2202fk(sk\u22121)\n8: Release sk\u22121 from memory\nOutputs: \u2113(f(x)) = \u2113(sK), \u2207(\u2113\u25e6f)(x) = r0, \u22072(\u2113\u25e6f)(x) = R0\n9.5.3 Fan-in and fan-out\nFor generic computation graphs (see Section 8.3), we saw that multi-\ninput functions (fan-in) were crucial. For Hessian backpropagation in\ncomputation graphs, we therefore need to develop a similar formula.\nProposition 9.8(Hessian chain-rule for fan-in). Considern+1 twice\ndifferentiable functionsf1,...,f n and g with fi : RP \u2192RMi and\ng : RM1 \u00d7... \u00d7RMn \u2192RQ. The Hessian of g \u25e6f for f(w) =\n(f1(w),...,f n(w)) along an output directionu\u2208RQ is given by\n\u22022(g\u25e6f)(w)\u2217[u] =\nn\u2211\ni=1\n\u22022fi(w)\u2217[\u2202ig(f(w))\u2217[u]]\n+\nn\u2211\ni,j=1\n\u2202fi(w)\u2217\u22022\ni,jg(f(w))\u2217[u]\u2202fj(w).\nThe gradient backpropagation expression for fan-in is simple because\nthe functionsfi are not linked by any path. In contrast, the Hessian\nbackpropagation involves cross-product terms\n\u2202fi(w)\u2217\u22022\ni,jg(f(w))\u2217[u]\u2202fj(w) for i\u0338= j. The nodes associated to the\nfi computations cannot be treated independently anymore.\nOn the other hand, developing a backpropagation rule for fan-out\ndoes not pose any issue, since each output function can be treated", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1525, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5f63ca01-dd7a-460c-8fb2-245aaf9147f2": {"__data__": {"id_": "5f63ca01-dd7a-460c-8fb2-245aaf9147f2", "embedding": null, "metadata": {"page_label": "234", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b5537a91-0cfa-4a1a-8620-0ed97ab0b8e6", "node_type": "4", "metadata": {"page_label": "234", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "96ff6b4732ec285e168cb1c9c6eee20df81f932d3f68acdc4275c22d74d33863", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "234 Second-order automatic differentiation\nindependently.\nProposition 9.9(Hessian chain-rule for fan-out). Considern+1 twice\ndifferentiable functionsg1,...,g n and f with gi : RM \u2192RQi and\nf : RP \u2192RM. The Hessian ofg\u25e6f for g(w) = (g1(w),...,g n(w))\nalong a directionu= (u1,..., un) \u2208RQ1 \u00d7... \u00d7RQn is given by\n\u22022(g\u25e6f)(w)\u2217[u] =\nn\u2211\ni=1\n\u22022f(w)\u2217[\u2202gi(f(w))\u2217[ui]]\n+\nn\u2211\ni=1\n\u2202f(w)\u2217\u22022gi(f(w))\u2217[u]\u2202f(w).\n9.6 Block diagonal approximations\nRather than computing the whole Hessian or Gauss-Newton matrices,\nwe can consider computing block-diagonal or diagonal approximations,\nwhich are easier to invert. The approximation rules we present in this\nsection build upon the Hessian chain rule studied in Section 9.5.\n9.6.1 Feedforward networks\nRecall the definition of a feedforward network:\ns0 := x\nsk := fk(sk\u22121,wk) \u2200k\u2208{1,...,K }\nf(x,w) := sK,\nwhere w:= (w1,..., wK). Rather than computing the entire Hessian of\n\u2113\u25e6f w.r.t. w, we can compute the Hessians w.r.t. each set of parameters\nwk. For the case of computation chains, the Hessian backpropagation\nrecursion we used in Algorithm 9.2 was\nRk\u22121 := \u22022fk(sk\u22121)\u2217[rk] + \u2202fk(sk\u22121)\u2217Rk\u2202fk(sk\u22121).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1122, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bd151ad5-0ffb-4732-a139-71cbd9e211d2": {"__data__": {"id_": "bd151ad5-0ffb-4732-a139-71cbd9e211d2", "embedding": null, "metadata": {"page_label": "235", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a313570b-8c59-4b5a-aec6-c43d9339d521", "node_type": "4", "metadata": {"page_label": "235", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "d2891acc3e07f9988642340eff6f02909a4ba0605b597ccdaabfcd7783ade09b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9.6. Block diagonal approximations 235\nExtending this recursion to the feedforward network case, we obtain,\nstarting fromrK := \u2207\u2113(sK) and RK := \u22072\u2113(sK),\nrk\u22121 := \u2202fk(sk\u22121,wk)\u2217[rk]\n(\nRk\u22121 \u223c\n\u223c Hk\n)\n:= \u22022fk(sk\u22121,wk)\u2217[rk]\n+ \u2202fk(sk\u22121,wk)\u2217Rk\u2202fk(sk\u22121,wk),\nwhere we used\u223cto indicate that these blocks are not used. The Hessians\nw.r.t each set of parameters are then\nR0 = \u22072\nxx(\u2113\u25e6f)(x,w)\nH1 = \u22072\nw1w1 (\u2113\u25e6f)(x,w)\n...\nHK = \u22072\nwKwK(\u2113\u25e6f)(x,w)).\nThe validity of this result stems from the fact that we can view the\nHessian w.r.t.wk as computing the Hessian w.r.t.wk of\n\u02dcfK \u25e6... \u25e6\u02dcfk+1 \u25e6fk(sk\u22121,wk)\nwhere \u02dcfi := fi(\u00b7,wk), fori \u2208{k+ 1,...,K }. As the computations of\nthe block-wise Hessians share most of the computations, they can be\nevaluated in a single backward pass just as the gradients.\nExample 9.3(Block-wise computation of the Gauss-Newton matrix).\nOur blockwise backpropagation scheme can readily be adapted for\nthe Gauss-Newton matrix as\n(\nRk\u22121 \u223c\n\u223c Gk\n)\n:= \u2202fk(sk\u22121,wk)\u2217Rk\u2202fk(sk\u22121,wk),\nstarting fromRK := \u22072\u2113(sK). The outputsR0,G0,..., GK give a\nblock-wise approximation of the Gauss-Newton matrix.\nNow, consider a simple multilayer perceptron such that\nfk(sk\u22121,wk) := a(Wksk\u22121) with wk := vec(Wk)\nUsing Example 9.2 and Example 9.1 adapted to the Gauss-Newton", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1253, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3ffd2c78-51a6-49ab-8990-81758a7e79f3": {"__data__": {"id_": "3ffd2c78-51a6-49ab-8990-81758a7e79f3", "embedding": null, "metadata": {"page_label": "236", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c683e188-5d11-4e7b-ae98-d9030bea324a", "node_type": "4", "metadata": {"page_label": "236", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "c831bd9a7de32d79b6ffdc2dc246033aed339dea8b8c2c08f8a1658978dd6d7b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "236 Second-order automatic differentiation\nmatrix, we can compute the block-wise decomposition of the Gauss-\nNewton matrix as, fork= K,..., 1,\nRk\u22121 := W\u22a4\nk JkWk\nJk := Rk \u2299(a\u2032(Wksk\u22121)a\u2032(Wksk\u22121)\u22a4)\nGk := Jk \u2297sk\u22121s\u22a4\nk\u22121\nstarting fromRK := \u22072\u2113(sK). The outputsG1,..., GK correspond\nto the block-wise elements of the Gauss-Newton matrix off for the\nvectorized weightsw1,..., wK. Similar computations were done in\nKFRA (Botevet al., 2017) and BackPack (Dangelet al., 2019).\n9.6.2 Computation graphs\nFor generic computation graphs, consider a functionf(x,w) defined\nby, denotingi1,...,i pk := pa(k),\nsk := fk(si1 ,..., sipk) \u2200k\u2208{1,...,K }\nsuch thatf(x,w) = sK, andk is following a topological ordering of the\ngraph (see Section 8.3). We can consider the following backpropagation\nscheme, fork= K,..., 1 and j \u2208pa(k)\nrij \u2190rij + \u2202jfk(si1 ,...,s ipk)\u2217[rk] (9.3)\nRij \u2190Rij + \u22022\njjfk(si1 ,...,s ipk)\u2217[rk]\n+ \u2202jfk(si1 ,...,s ipk)\u2217Rk\u2202jfk(si1 ,...,s ipk), (9.4)\nstarting from RK := \u22072\u2113(sK) and rK := \u2207\u2113(sK). Recall that for\nmultiple inputs, the chain-rule presented in Proposition 9.8 involves\nthe cross-derivatives. For this reason the back-propagation scheme\nin Eq. (9.3) only computes an approximation. For example, one can\nverify that using Eq. (9.3) to compute the Hessian of\u2113(f1(w),f2(w))\ndoes not provide an exact expression for the Hessian off. This scheme\nis easy to implement and may provide a relevant proxy for the Hessian.\n9.7 Diagonal approximations\nSimilarly to the idea of designing a backpropagation scheme that ap-\nproximates blocks of the Hessian, we can design a backpropagation", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1580, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "846728f7-be78-4fdb-861f-bd48ae7288dc": {"__data__": {"id_": "846728f7-be78-4fdb-861f-bd48ae7288dc", "embedding": null, "metadata": {"page_label": "237", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4bd1b38d-e6fc-43fc-828b-aa957c90ed8f", "node_type": "4", "metadata": {"page_label": "237", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "f67d0eff87df069200b41c8fe94962657ee8038ee6a5eb2a9e3b0d815e21d133", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9.7. Diagonal approximations 237\nscheme that approximates the diagonal of the Hessian. The approach\nwas originally proposed by Becker and Le Cun (1988) for feedforward\nnetworks, but our exposition, new to our knowledge, has the benefit\nthat it naturally extends to computational graphs, as we shall see.\n9.7.1 Computation chains\nThe idea stems from modifying the Hessian backpropagation rule\nin Proposition 9.7 to only keep the diagonal of the Hessian. Formally,\ngiven a matrixM \u2208RD\u00d7D, we denote bydiag(M) = (Mii)D\ni=1 \u2208RD\nthe vector of diagonal entries ofM, and for a vectorm \u2208RD, we\ndenote Diag(m) = \u2211D\ni=1 mieie\u22a4\ni the diagonal matrix with entriesmi.\nFor the backpropagation of the Hessian of\u2113\u25e6fK \u25e6... \u25e6f1, we see from\nAlgorithm 9.2 thatdiag(Hk\u22121) can be expressed in terms ofHk as\ndiag(Hk\u22121) = diag(\u22022fk(sk\u22121)\u2217rk)\n+ diag(\u2202fk(sk\u22121)\u2217Hk\u2202fk(sk\u22121)).\nUnfortunately, that recursion needs access to the whole HessianHk,\nand would therefore be too expensive. A natural idea is to modify the\nrecursion to approximatediag(Hk) by backpropagatingvectors:\ndk\u22121 := diag(\u22022fk(sk\u22121)\u2217rk)\n+ diag(\u2202fk(sk\u22121)\u2217Diag(dk)\u2202fk(sk\u22121)).\nThe diagonal matrix Diag(dk) serves as a surrogate for Hk. Each\niteration of this recursion can be computed in linear time in the output\ndimension Dk since\ndk\u22121,i =\nDk\u2211\nj=1\nrk,j \u00b7\u22022\ni,ifk,j(sk\u22121) +\nDk\u2211\nj=1\ndk,j(\u2202ifk,j(sk\u22121))2.\nTo initialize the recursion, we can setdK := diag(\u22072\u2113(sK)). As an\nalternative, as proposed by Elsayed and Mahmood (2022), ifHK has\na simple form, we can use\u22072\u2113(sK) instead ofDiag(dK) at the first\niteration. This is the case for instance iffK is a cross-entropy loss. The\nrecursion is repeated until we obtain the approximate diagonal Hessian\nd0 \u2248diag(\u22072(\u2113\u25e6f)(x)). The gradientsrk, needed to computedk, are\ncomputed along the way and the algorithm can therefore also return\nr0 = \u2207(\u2113\u25e6f)(x).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1826, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9b224c32-0261-4188-8e83-73ffe87dc31d": {"__data__": {"id_": "9b224c32-0261-4188-8e83-73ffe87dc31d", "embedding": null, "metadata": {"page_label": "238", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e8858b7a-bdc4-47e0-9763-e923ab713dc5", "node_type": "4", "metadata": {"page_label": "238", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "c894d7f803107a16f9cfccd62678c33ada78b795ee279dbe2d9bfbf94b2e3298", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "238 Second-order automatic differentiation\n9.7.2 Computation graphs\nAlthough this diagonal approximation was originally derived for feed-\nforward networks Becker and Le Cun (1988), it is straightforward to\ngeneralize it to computation graphs. Namely, for a functionf(x,w) de-\ncomposed along a computation graph, we can backpropagate a diagonal\napproximation in reverse topological order as\nrij \u2190rij + \u2202jfk(si1 ,...,s ipk)\u2217[rk]\ndij \u2190dij + diag(\u22022\njjfk(si1 ,...,s ipk)\u2217[rk])\n+ diag(\u2202jfk(si1 ,...,s ipk)\u2217Diag(dk)\u2202jfk(si1 ,...,s ipk)), (9.5)\nfor j \u2208pa(k), starting fromrK = \u2207\u2113(sK) and dK = diag(\u22072\u2113(sK))\nor Diag(dK) = \u22072\u2113(sK). To implement such an algorithm, each ele-\nmentary function in the computational graph needs to be augmented\nwith an oracle that computes the Hessian diagonal approximation of\nthe current function, given the previous ones. An example with MLPs\nis presented in Example 9.4.\nExample 9.4(Hessian diagonal approximation for MLPs ). Consider\na multilayer perceptron\nsk := ak(Wksk\u22121) \u2200k\u2208{1,...,K \u22121}\nf(w,x) := sK\nstarting froms0 = x. Hereak is the element-wise activation func-\ntion (potentially the identity) andwencapsulates the weight ma-\ntrices W1,..., WK. We consider the derivatives w.r.t. the flattened\nmatrices, so that gradients and diagonal approximations w.r.t. these\nflattened quantities are vectors. The backpropagation scheme(9.5)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1360, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "13fe5629-634b-42d1-92eb-e036eabf8b07": {"__data__": {"id_": "13fe5629-634b-42d1-92eb-e036eabf8b07", "embedding": null, "metadata": {"page_label": "239", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5fe7f062-582b-4afc-ad82-a1be59b2c4cf", "node_type": "4", "metadata": {"page_label": "239", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "5e58e27f20cdd77441ccd49dd6351d900d76b33f0645bb6e351fb78b693f03a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9.8. Randomized estimators 239\nthen reduces to, denotingtk = Wksk\u22121,\nrk\u22121 := W\u22a4\nk (a\u2032(tk) \u2299rk)\ngk := vec((a\u2032(tk) \u2299rk)s\u22a4\nk\u22121)\n\u03b4k := rk \u2299a\u2032\u2032(tk) + dk \u2299a\u2032(tk)2\ndk\u22121 :=\n\uf8eb\n\uf8ed\nDk\u2211\nj=1\nW2\nk,ij\u03b4k,j\n\uf8f6\n\uf8f8\nDk\ni=1\nhk := vec(\u03b4k(s2\nk\u22121)\u22a4)\nstarting fromrK = \u2207\u2113(sK) and, e.g.,dK = diag(\u22072\u2113(sK)). The\nalgorithmreturns g1,..., gK asthegradientsof f w.r.t.w1,..., wK,\nwith wi = vec(Wi), andh1,..., hK as the diagonal approximations\nof the Hessian w.r.t.w1,..., wK.\n9.8 Randomized estimators\nIn this section, we describe randomized estimators of the diagonal of\nthe Hessian or Gauss-Newton matrices.\n9.8.1 Girard-Hutchinson estimator\nWe begin with a generic estimator, originally proposed for trace es-\ntimation by Girard (1989) and extended by Hutchinson (1989). Let\nA\u2208RP\u00d7P be an arbitrary square matrix, whose matrix-vector product\n(matvec) is available. Suppose\u03c9\u2208RP is an isotropic random vector,\ni.e., such thatE\u03c9\u223cp[\u03c9\u03c9\u22a4] = I. For example, two common choices are\nthe Rademacher distributionp= Uniform({\u22121,1}) and the standard\nnormal distributionp= Normal(0,I). Then, we have\nE\u03c9\u223cp[\u27e8\u03c9,A\u03c9\u27e9] = tr(A).\nApplicationsincludegeneralizedcross-validation,computingtheKullback-\nLeibler divergence between two Gaussians, and computing the deriva-\ntives of the log-determinant.\nThe approach can be extended (Bekaset al., 2007; Baston and\nNakatsukasa, 2022; Hallmanet al., 2023) to obtain an estimator of the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1374, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3b18f951-c536-4fe5-b000-c7cea8aebd3c": {"__data__": {"id_": "3b18f951-c536-4fe5-b000-c7cea8aebd3c", "embedding": null, "metadata": {"page_label": "240", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c689466-5b87-4a04-85f6-903fb33cbd7f", "node_type": "4", "metadata": {"page_label": "240", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "6d5bb226c753b628317515e46473d47ead93572e607ba50f36e6f25ef474bb69", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "240 Second-order automatic differentiation\ndiagonal ofA,\nE\u03c9\u223cp[\u03c9\u2299A\u03c9] = Diag(A),\nwhere \u2299denotes the Hadamard product (element-wise multiplication).\nThis suggests that we can use the Monte-Carlo method to estimate the\ndiagonal ofA,\nDiag(A) \u22481\nS\nS\u2211\ni=1\n\u03c9i \u2299A\u03c9i,\nwith equality asS \u2192\u221e, since the estimator is unbiased. Since, as\nreviewed in Section 9.1 and Section 9.2, we know how to multiply\nefficiently with the Hessian and the Gauss-Newton matrices, we can\napply the technique with these matrices. The variance is determined\nby the numberS of samples drawn and therefore by the number of\nmatvecs performed. More elaborated approaches have been proposed to\nfurther reduce the variance (Meyeret al., 2021; Epperlyet al., 2023).\n9.8.2 Bartlett estimator for the factorization\nSuppose the objective function is of the formL(w; x,y) := \u2113(f(w; x); y)\nwhere \u2113 is the negative log-likelihood\u2113(\u03b8; y) := \u2212log p\u03b8(y) of an ex-\nponential family distribution, and\u03b8:= f(w; x), for some networkf.\nWe saw from the equivalence between the Fisher and Gauss-Newton\nmatrices in Proposition 9.6 (which follows from the Bartlett identity)\nthat\n\u22072\nGNL(w; x,\u00b7) = EY\u223cp\u03b8[\u2202f(w; x)\u2217\u2207\u2113(\u03b8; Y) \u2297\u2207\u2113(\u03b8; Y)\u2202f(w; x)]\n= EY\u223cp\u03b8[\u2207L(w; x,Y ) \u2297\u2207L(w; x,Y )],\nwhere \u00b7indicates that the result holds for any value of the second\nargument. This suggests a Monte-Carlo scheme\n\u22072\nGNL(w; x,\u00b7) \u22481\nS\nS\u2211\nj=1\n[\u2207L(w; x,yij) \u2297\u2207L(w; x,yij)]\nwhere yi1 ,..., yiS \u223cp\u03b8 and \u03b8 = f(w,x). In words, we can approxi-\nmate the Gauss-Newton matrix withS gradient computations. This\nfactorization can also be used to approximate the GNVP in Eq. (9.1).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1579, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d66b0cc2-30da-4000-bcdd-d729e6fe5b50": {"__data__": {"id_": "d66b0cc2-30da-4000-bcdd-d729e6fe5b50", "embedding": null, "metadata": {"page_label": "241", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "afbe1199-2c08-4d64-bac7-a6d73abbb760", "node_type": "4", "metadata": {"page_label": "241", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "1e7bbe0816ff7ec3444b6ec189f3226bb8174b71ae5fe08d534613796c041c0d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9.8. Randomized estimators 241\n9.8.3 Bartlett estimator for the diagonal\nFollowing a similar approach, we obtain\ndiag(\u22072\nGNL(w; x,\u00b7)) = EY\u223cp\u03b8[\u2207L(w; x,Y ) \u2299\u2207L(w; x,Y )],\nwhere\u2299indicates the element-wise (Hadamard) product. Using a Monte-\nCarlo scheme, samplingyi1 ,..., yiS from p\u03b8, we therefore obtain\ndiag(\u22072\nGNL(w; x,\u00b7)) \u22481\nS\nS\u2211\nj=1\n\u2207L(w; x,yij) \u2299\u2207L(w; x,yij),\nwith equality when all labels in the support ofp\u03b8 have been sampled.\nThat estimator, used for instance in (Weiet al., 2020, Appendix C.1.),\nrequires access to individual gradients evaluated at the sampled\nlabels. Another possible estimator of the diagonal is given by\n1\nS diag(\u22072\nGNL(w; x,\u00b7))\n=EY1,...,YS\u223cp\u03b8\n[\n\u22071\nS\nS\u2211\ni=1\nL(w; x,Yi) \u2299\u22071\nS\nS\u2211\ni=1\nL(w; x,Yi)\n]\n.\nLetting \u03b3i := \u2207L(w; x,Yi), this follows from\nE\n\uf8ee\n\uf8f0\u2211\ni\n\u03b3i \u2299\n\u2211\nj\n\u03b3j\n\uf8f9\n\uf8fb= E\n\uf8ee\n\uf8f0\u2211\ni\n\u03b3i \u2299\u03b3i +\n\u2211\ni\u0338=j\n\u03b3i \u2299\u03b3j\n\uf8f9\n\uf8fb\n= E\n[\u2211\ni\n\u03b3i \u2299\u03b3i\n]\nwhere we used thatE[\u03b3i \u2299\u03b3j] = E[\u03b3i] \u2299E[\u03b3j] = 0 since \u03b3i and \u03b3j are\nindependent variables fori \u0338= j and have zero mean, from Bartlett\u2019s\nfirst identity Eq. (12.2). We can then use the Monte-Carlo method to\nobtain\n1\nS diag(\u22072\nGNL(w; x,\u00b7)) \u2248\n\uf8eb\n\uf8ed\u22071\nS\nS\u2211\nj=1\nL(w; x,yij)\n\uf8f6\n\uf8f8\u2299\n\uf8eb\n\uf8ed\u22071\nS\nS\u2211\nj=1\nL(w; x,yij)\n\uf8f6\n\uf8f8,\nwith equality when all labels in the support ofp\u03b8 have been sampled.\nThis estimator can be more convenient to implement, since it only needs\naccess to the gradient of theaveragedlosses. However, it may suffer\nfrom higher variance. A special case of this estimator is used by Liu\net al.(2023), where they draw only oneyfor eachx.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1479, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0277977d-8f9d-4402-bc5a-3ebc3ca924db": {"__data__": {"id_": "0277977d-8f9d-4402-bc5a-3ebc3ca924db", "embedding": null, "metadata": {"page_label": "242", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "be631d02-1b61-48ae-8415-b151ff592cfd", "node_type": "4", "metadata": {"page_label": "242", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "88de09c41589ace376587a8f10b29896079c4415de883f253e53e6e983788550", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "242 Second-order automatic differentiation\n9.9 Summary\n\u2022 By using a Hessian chain rule, we can develop a \u201cHessian backprop-\nagation\u201d. While it is reasonably simple for computation chains,\nit becomes computationally prohibitive for computation graphs,\ndue to the cross-product terms occurring with fan-in.\n\u2022 A better approach is to use Hessian-vector products (HVPs). We\nsaw that there are four possible methods to compute HVPs, but\nthe forward-over-reverse method is a priori the most efficient.\nSimilarly as for computing gradients, computing HVPs is only a\nconstant times more expensive than evaluating the function itself.\n\u2022 The Gauss-Newton matrix associated with the composition\u2113\u25e6f\ncan be seen as an approximation of the Hessian. It is a positive\nsemidefinite matrix if\u2113 is convex, and can be used to build a\nprincipled quadratic approximation of a function. It is equivalent\nto the Fisher information matrix in the case of exponential families.\nGauss-Newton-vector products can be computed efficiently, like\nHVPs.\n\u2022 We also described other approximations, such as (block) diagonal\napproximations, and randomized estimators.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1129, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ee2a87f6-72b3-4a97-8946-01a9ab88235b": {"__data__": {"id_": "ee2a87f6-72b3-4a97-8946-01a9ab88235b", "embedding": null, "metadata": {"page_label": "243", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c812b12d-3e92-4c63-9341-f85e32ffb5c6", "node_type": "4", "metadata": {"page_label": "243", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "6305117841d859d673c73ff9c94e4940b56f91540287145f96ff709e7b7a19ba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10\nInference in graphical models as differentiation\nA graphical model specifies how random variables depend on each\nother and therefore determines how their joint probability distribution\nfactorizes. In this chapter, we review key concepts in graphical models\nand how they relate to differentiation, drawing in the process analogies\nwith computation chains and computation graphs.\n10.1 Chain rule of probability\nThe chain rule of probability is a fundamental law in probability theory\nfor computing thejoint probabilityof events. In the case of only two\nevents A1 and A2, it reduces to theproduct rule\nP(A1 \u2229A2) = P(A2|A1)P(A1).\nFor two discrete random variablesS1 and S2, using the eventsA1 :=\n{S1 = s1}and A2 := {S2 = s2}, the product rule becomes\nP(S1 = s1,S2 = s2) = P(S2 = s2|S1 = s1)P(S1 = s1).\nMore generally, using the product rule, we have forK events\nP(A1 \u2229... \u2229AK) = P(AK |A1 \u2229... \u2229AK\u22121) P(A1 \u2229... \u2229AK\u22121) .\n243", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 921, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ba487b7a-6877-4e84-9d54-8b239aaf133d": {"__data__": {"id_": "ba487b7a-6877-4e84-9d54-8b239aaf133d", "embedding": null, "metadata": {"page_label": "244", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "91782f20-756b-4344-953b-1b7d636a3fc4", "node_type": "4", "metadata": {"page_label": "244", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "6b7e5bd8cbbb164563c4230b60667e7966dfe6242859251dab576392f6bb0cb1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "244 Inference in graphical models as differentiation\nApplying the product rule one more time, we have\nP(A1 \u2229... \u2229AK\u22121) = P(AK\u22121 |A1 \u2229... \u2229AK\u22122) P(A1 \u2229... \u2229AK\u22122) .\nRepeating the process recursively, we arrive at thechain rule of\nprobability\nP(A1 \u2229... \u2229AK) =\nK\u220f\nj=1\nP(Aj |A1 \u2229\u00b7\u00b7\u00b7\u2229 Aj\u22121)\n=\nK\u220f\nj=1\nP\n\uf8eb\n\uf8edAj\n\u23d0\u23d0\u23d0\u23d0\u23d0\nj\u22121\u22c2\ni=1\nAi\n\uf8f6\n\uf8f8.\nFor K discrete random variablesSj, using the eventsAj := {Sj = sj},\nthe chain rule of probability becomes\nP(S1 = s1,...,S K = sK) =\nK\u220f\nj=1\nP(Sj = sj |S1 = s1,...,S j\u22121 = sj\u22121).\nImportantly, this factorization holdswithout any independence as-\nsumption on the variablesS1,...,S K. In other words, the space of\nprobability distributions specified by the joint probability on the left-\nhand side, and the space of probability distributions specified by the\nproduct of conditional probabilities on the right-hand side, are the\nsame. We can further simplify the factorization if we make additional\nconditional independence assumptions.\n10.2 Conditional independence\nWe know that if two eventsA and B are independent, then\nP(A|B) = P(A).\nSimilarly, if two random variablesS1 and S2 are independent, then\nP(S2 = s2|S1 = s1) = P(S2 = s2).\nMore generally, if we work withK variables S1,...,S K, some variables\nmay depend on each other, while others may not. To simplify the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1289, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "250eac4b-19f8-4afa-88ad-f1dfcdead992": {"__data__": {"id_": "250eac4b-19f8-4afa-88ad-f1dfcdead992", "embedding": null, "metadata": {"page_label": "245", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b8f628c-4ca5-4e88-92e0-a9dc929dd94d", "node_type": "4", "metadata": {"page_label": "245", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "b552ffab1a48c4922487a841b2d85079e572dec3cfb52dbc224230b17050583f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10.3. Inference problems 245\nnotation, given a setC, we define the shorthands\nSC:= (Si: i\u2208C)\nsC:= (si: i\u2208C).\nWe say that a variableSj is independent ofSD conditioned on SC,\nwith C\u2229D = \u2205, if for anysj,sC,sD\nP(Sj = sj |SC= sC,SD= sD) = P(Sj = sj |SC= sC).\n10.3 Inference problems\n10.3.1 Joint probability distributions\nWe consider a collection ofK variables s:= (s1,..., sK), potentially\nordered or unordered. Eachsbelongs to theCartesian product\nS:= S1 \u00d7\u00b7\u00b7\u00b7\u00d7S K. Throughout this chapter, we assume that the sets\nSk are discrete for concreteness, withSk := {v1,..., vMk}. Note that\nbecause Sk is discrete, we can always identify it with{1,...,M k}. A\ngraphical model specifies ajoint probability distribution\nP(S = s) = P(S1 = s1,...,S K = sK)\n= p(s)\n= p(s1,..., sK),\nwhere pis the probability mass function of the joint probability distribu-\ntion. Summing over the Cartesian product of all possible configurations,\nwe obtain \u2211\ns\u2208S\np(s) =\n\u2211\ns1,...,sK\u2208S\np(s1,..., sK) = 1.\nAs we shall see, the graph of a graphical model encodes thedependen-\ncies between the variables(S1,...,S K) and therefore how their joint\ndistribution factorizes. Given access to a joint probability distribution,\nthere are severalinference problems one typically needs to solve.\n10.3.2 Likelihood\nA simple task is to compute the likelihood of some observations\ns= (s1,..., sK),\nP(S1 = s1,...,S k = sk) = p(s1,..., sk).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1388, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ffaaffc8-87e8-44d1-a9c7-346e2d234ea9": {"__data__": {"id_": "ffaaffc8-87e8-44d1-a9c7-346e2d234ea9", "embedding": null, "metadata": {"page_label": "246", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "39b8ae14-7e62-4fb5-bd30-05fa87b4b8b2", "node_type": "4", "metadata": {"page_label": "246", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "638c3264a2c42e842b7f3bf5efb767d2acc97d576c3cd4aae1d6971c15c20999", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "246 Inference in graphical models as differentiation\nIt is also common to compute thelog-likelihood,\nlog P(S1 = s1,...,S k = sk) = log p(s1,..., sk).\n10.3.3 Maximum a-posteriori inference\nAnother common task is to compute the most likely configuration,\narg max\ns1\u2208S1,...,sK\u2208SK\np(s1,..., sK).\nThis is themode of the joint probability distribution. This is also known\nas maximum a-posteriori (MAP) inference in the literature (Wainwright\nand Jordan, 2008).\n10.3.4 Marginal inference\nThe operation ofmarginalization consists in summing (or integrat-\ning) over all possible values of a given variable in a joint probability\ndistribution. This allows us to compute themarginal probabilityof\nthe remaining variables. For instance, we may want to marginalize all\nvariables butSk = sk. To do so, we define the Cartesian product\nCk(sk) := S1 \u00d7\u00b7\u00b7\u00b7\u00d7S k\u22121\ued19 \ued18\ued17 \ued1a\nAk\u22121\n\u00d7{sk}\u00d7Sk+1 \u00d7\u00b7\u00b7\u00b7\u00d7S K\ued19 \ued18\ued17 \ued1a\nBk+1\n. (10.1)\nSumming over all variables butSk, we obtain the marginal probability\nof Sk = sk as\nP(Sk = sk) =\n\u2211\ns1,...,sK\u2208Ck(sk)\np(s1,..., sK)\n=\n\u2211\ns1,...,sk\u22121\u2208Ak\u22121\n\u2211\nsk+1,...,sK\u2208Bk+1\np(s1,..., sK)\nDefining similarly\nCk,l(sk,sl) := S1 \u00d7\u00b7\u00b7\u00b7\u00d7{ sk}\u00d7\u00b7\u00b7\u00b7\u00d7{ sl}\u00d7\u00b7\u00b7\u00b7\u00d7S K,\nwe obtain\nP(Sk = sk,Sl = sl) =\n\u2211\ns1,...,sK\u2208Ck,l(sk,sl)\np(s1,..., sK).\nIn particular, we may want to compute the marginal probability of two\nconsecutive variables,P(Sk\u22121 = sk\u22121,Sk = sk).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1330, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "450e4647-d842-44fb-80d0-1c8cf35a50ae": {"__data__": {"id_": "450e4647-d842-44fb-80d0-1c8cf35a50ae", "embedding": null, "metadata": {"page_label": "247", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4035b2de-dfb7-42ef-b246-39d1e917c221", "node_type": "4", "metadata": {"page_label": "247", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "fd34f73ed2dbb29cfa0a9f8cc14151c20e02307fc8688b76ce9fe69e5041ed4e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10.3. Inference problems 247\n10.3.5 Expectation, convex hull, marginal polytope\nAnother common operation is to compute the expectation of\u03d5(S) under\na distributionp. It is defined by\n\u00b5:= ES\u223cp[\u03d5(S)] =\n\u2211\ns\u2208S\np(s)\u03d5(s) \u2208M\nFor the expectation underp\u03b8, we write\n\u00b5(\u03b8) := ES\u223cp\u03b8[\u03d5(S)] =\n\u2211\ns\u2208S\np\u03b8(s)\u03d5(s) \u2208M.\nIn exponential family distributions (Section 3.4), the function\u03d5is called\na statistic. It decomposes as\n\u03d5(s) := (\u03d5C(sC))C\u2208C,\nwhere C\u2286 [K]. Intuitively,\u03d5(s) can be thought as anencoding or\nembedding of s(a potentially discrete object such as a sequence of\nintegers) in a vector space. Under this decomposition, we can also\ncompute\n\u00b5C:= ES[\u03d5C(SC)] =\n\u2211\ns\u2208S\np(s)\u03d5C(sC).\nConvex hull\nThe mean\u00b5belongs to theconvex hullof \u03d5(S) := {\u03d5(s): s\u2208S},\nM:= conv(\u03d5(S)) :=\n{ \u2211\ns\u2208S\np(s)\u03d5(s): p\u2208P(S)\n}\n,\nwhere P(S) is the set of all possible probability distributions overS. In\nother words,Mis the set of all possible convex combinations of\u03d5(s)\nfor s\u2208S. The vertices ofMare all thes\u2208S.\nCase of binary encodings: the marginal polytope\nIn the special case of a discrete setSk = {v1,..., vM}and of abinary\nencoding (indicator function)\u03d5(s), the setMis called themarginal\npolytope (Wainwright and Jordan, 2008), because each point\u00b5 \u2208", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1205, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5801212d-6f81-44a0-8c36-169c28c4b61a": {"__data__": {"id_": "5801212d-6f81-44a0-8c36-169c28c4b61a", "embedding": null, "metadata": {"page_label": "248", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37ab87d4-79d5-4f8e-9977-8843b13f3033", "node_type": "4", "metadata": {"page_label": "248", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "b7f9dd6460f182391e0365ccdd3bd905ed481221024d919ce584abd840aa1aac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "248 Inference in graphical models as differentiation\nMcontains marginal probabilities. To see why, consider theunary\npotential\n[\u03d5(s)]k,i = [\u03d5k(sk)]i = I(sk = vi) (10.2)\nwhere I(p) := 1 if p is true,0 otherwise. We then obtain the marginal\nprobability ofSk = vi,\n[\u00b5]k,i = ES[\u03d5(S)k,i]\n= ESk[\u03d5k(Sk)i]\n= ESk[I(Sk = vi)]\n=\n\u2211\nsk\u2208Sk\nP(Sk = sk)I(sk = vi)\n= P(Sk = vi).\nLikewise, consider thepairwise potential\n[\u03d5(s)]k,l,i,j = [\u03d5k,l(sk,sl)]i,j = I(sk = vi,sl = vj). (10.3)\nWe then obtain the marginal probability ofSk = vi and Sl = vj,\n[\u00b5]k,l,i,j = ES[\u03d5(S)k,l,i,j]\n= ESk,Sl[\u03d5k,l(Sk,Sl)i,j]\n= ESk,Sl[I(Sk = vi,Sl = vj)]\n=\n\u2211\nsk\u2208Sk\n\u2211\nsl\u2208Sl\nP(Sk = sk,Sl = sl)I(sk = vi,sl = vj)\n= P(Sk = vi,Sl = vj).\nWe can do the same with higher-order potential functions.\n10.3.6 Complexity of brute force\nApart from computing the likelihood, which is trivial, computing the\nmarginal, mode and expectation by brute force takesO(\u220fK\nk=1 |Sk|) time.\nIn particular, if|Sk|= M \u2200k\u2208[K], brute force takesO(MK) time.\n10.4 Markov chains\nIn this section, we briefly review Markov chains. Our notation is chosen\nto emphasize the analogies with computation chains.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1124, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f2f2bc27-1c39-4282-8a7c-9b5b956f5451": {"__data__": {"id_": "f2f2bc27-1c39-4282-8a7c-9b5b956f5451", "embedding": null, "metadata": {"page_label": "249", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c5800b7c-34dd-4ea4-bbb1-7288e1e8288c", "node_type": "4", "metadata": {"page_label": "249", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "1f116457dfc13b7c51744ceb924ae4bbffb95475b50016a3f23f22a88a1bcff5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10.4. Markov chains 249\n10.4.1 The Markov property\nWhen random variables are organized sequentially as S1,...,S K,\na simple example of conditional independence is when each variable\nSk \u2208Sk only depends on the previous variableSk\u22121 \u2208Sk\u22121, that is,\nP(Sk = sk |Sk\u22121 = sk\u22121,...,S 1 = s1) = P(Sk = sk |Sk\u22121 = sk\u22121)\n:= pk(sk |sk\u22121),\nA probability distribution satisfying the above is said to satisfy the\nMarkov property, and is called aMarkov chain. A computation\nchain is specified by the functionsfk, that take sk\u22121 as input and\noutput sk. In analogy, a Markov chain is specified by theconditional\nprobability distributionspk of Sk given Sk\u22121. We can then define the\ngenerative process\nS0 := s0\nS1 \u223cp1(\u00b7| S0)\nS2 \u223cp2(\u00b7| S1)\n...\nSK \u223cpK(\u00b7| SK\u22121).\nStrictly speaking, we should writeSk |Sk\u22121 \u223cpk(\u00b7| Sk\u22121). We choose\nour notation both for conciseness and for analogy with computation\nchains. Furthermore, to simplify the notation, we assume without loss\nof generality thatS0 is deterministic (if this is not the case, we can\nalways move S0 to S1 and add a dummy variable asS0). That is,\nP(S0 = s0) = p0(s0) := 1 and S0 := {s0}. This amounts to setting the\ninitial distributionof S1 as\nP(S1 = s1) := P(S0 = s0)P(S1 = s1|S0 = s0) = P(S1 = s1|S0 = s0).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1239, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "52ba9edf-10eb-49d7-9f1b-ecc2c22dc26c": {"__data__": {"id_": "52ba9edf-10eb-49d7-9f1b-ecc2c22dc26c", "embedding": null, "metadata": {"page_label": "250", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b6b4be5-1d4a-44cb-94c2-16aeaeca4a39", "node_type": "4", "metadata": {"page_label": "250", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "2060148331a9605df6c59c2e857fd78116c9eed6344ccfaeae5df1ded4358a33", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "250 Inference in graphical models as differentiation\n...\nend\n...\n...\n...\nstart\n...\nend\n...\n...\n...\nstart\nFigure 10.1: Left:Markov chain. Right: Computation graph of the forward-\nbackward and the Viterbi algorithms: a lattice.\nWe can then compute the joint probability of the Markov chain by\nP(S1 = s1,...,S K = sK) = p(s1,..., sK)\n=\nK\u220f\nk=1\nP(Sk = sk |Sk\u22121 = sk\u22121)\n=\nK\u220f\nk=1\npk(sk |sk\u22121),\nwhere we left the dependence ons0 implicit, sincep0(s0) = 1. A Markov\nchain with Sk = {1,2,3}is illustrated in Fig. 10.1. A chain defines\na totally ordered set{1,...,K }, since two nodes in the graph are\nnecessarily linked to each other by a path.\nExample 10.1(Chain of categorical distributions). Supposeourgoal\nis predict, fromx \u2208X , a sequence of lengthK, where eachSk\nbelongs toSk = {1,...,M }. In natural language processing, this\ntask is called sequence tagging. We can define\nSk \u223cCategorical(\u03c0k\u22121,k,Sk\u22121 )\nwhere\n\u03c0k\u22121,k,i := softargmax(\u03b8k\u22121,k,i) \u2208\u25b3M\n= (\u03c0k\u22121,k,i,j)M\nj=1\n\u03b8k\u22121,k,i := (\u03b8k\u22121,k,i,j)M\nj=1 \u2208RM\n\u03b8k\u22121,k,i,j := fk\u22121,k(x,i,j, wk) \u2208R.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1032, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "95f59716-7855-44ac-936a-27b925e9c270": {"__data__": {"id_": "95f59716-7855-44ac-936a-27b925e9c270", "embedding": null, "metadata": {"page_label": "251", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b028a9ee-877f-47df-b366-303c167478c7", "node_type": "4", "metadata": {"page_label": "251", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "946cfe88d2eb3c94ae581aba9bd88f2a0d56116d6c2fa74b4ecd0f8c3b134f33", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10.4. Markov chains 251\nWe therefore have\nP(Sk = j |Sk\u22121 = i) = pk(j |i)\n= \u03c0k\u22121,k,i,j\n= [softargmax(\u03b8k\u22121,k,i)]j\n= exp(\u03b8k\u22121,k,i,j)\u2211\nj\u2032exp(\u03b8k\u22121,k,i,j\u2032)\nand\nlog P(Sk = j |Sk\u22121 = i) = log pk(j |i)\n= \u03b8k\u22121,k,i,j \u2212logsumexp(\u03b8k\u22121,k,i)\n= \u03b8k\u22121,k,i,j \u2212log\n\u2211\nj\u2032\nexp(\u03b8k\u22121,k,i,j\u2032).\nWe emphasize that becausek\u22121 and k are always consecutive,\nthe representation\u03b8k\u22121,k,i,j is inefficient; we could use\u03b8k,i,j instead.\nOur notation is designed for consistency with Markov random fields.\n10.4.2 Time-homogeneous Markov chains\nA time-homogeneous discrete-time Markov chain corresponds to the\ncase when the distribution ofSk given Sk\u22121 is the same regardless ofk:\np1 = \u00b7\u00b7\u00b7 = pK = p.\nThe finite-space case corresponds to when eachSk \u2208S can take a\nfinite set of valuesS= {v1,..., vM}and\nP(Sk = vj |Sk\u22121 = vi) = p(vj|vi) = \u03c0i,j,\nwhere \u03c0i,j \u2208[0,1] is thetransition probabilityfrom vi to vj. Because\nthe set S= {v1,..., vM}is discrete, we can always identify it with\n{1,...,M }. That is, we can instead write\nP(Sk = j |Sk\u22121 = i) = p(j|i) = \u03c0i,j.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1018, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e71487d6-0795-4797-ba61-3e9813e83173": {"__data__": {"id_": "e71487d6-0795-4797-ba61-3e9813e83173", "embedding": null, "metadata": {"page_label": "252", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af0387a8-178c-4e13-bb4e-7ca1a107c4a6", "node_type": "4", "metadata": {"page_label": "252", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "eff930a902cf138bad8c55acca324d96f148f190241604a04672960049bdea14", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "252 Inference in graphical models as differentiation\n10.4.3 Higher-order Markov chains\nMore generally, anth-order Markov chain may depend, not only on the\nlast variable, but on the lastn variables,\nP(Sk = sk |Sk\u22121 = sk\u22121,...,S 1 = s1)\n=P(Sk = sk |Sk\u22121 = sk\u22121,...,S k\u2212n = sk\u2212n)\n=pk(sk|sk\u22121,..., sk\u2212n).\nAutoregressive models such as Transformers (Section 4.8) can be seen as\nspecifying a higher-order Markov chain, with a context window of sizen.\nThe larger context makes exact inference using dynamic programming\ncomputationally intractable. This is why practitioners usebeam search\nor ancestral sampling(Section 10.5.3) instead.\n10.5 Bayesian networks\nIn this section, we briefly review Bayesian networks. Our notation is\nchosen to emphasize the analogies with computation graphs.\n10.5.1 Expressing variable dependencies using DAGs\nMarkov chains and more generally higher-order Markov chains are a spe-\ncial case of Bayesian network. Similarly to computation graphs reviewed\nin Section 8.3, variable dependencies can be expressed using a directed\nacyclic graph (DAG)G= (V,E), where the verticesV= {1,...,K }\nrepresent variables and edgesErepresent variable dependencies. The set\n{i1,...,i nk}= pa(k) \u2286V, wherenk := |pa(k)|, indicates the variables\nSi1 ,...,S ink that Sk depends on. This defines apartially ordered\nset (poset). For notational simplicity, we again assume without loss of\ngenerality thatS0 is deterministic. A computation graph is specified\nby functionsf1,...,f K in topological order. In analogy, aBayesian\nnetworkis specified byconditional probability distributionspk of Sk", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1590, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b7efa100-207d-4fa4-96a5-f1e6c6c52986": {"__data__": {"id_": "b7efa100-207d-4fa4-96a5-f1e6c6c52986", "embedding": null, "metadata": {"page_label": "253", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4275b810-7d87-4a4f-8355-1332465e1133", "node_type": "4", "metadata": {"page_label": "253", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "cef1ae124be0cfc6ef130928f4618be028e5d50e330a71b515fafa4953034e3c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10.5. Bayesian networks 253\ngiven Spa(k). We can then define thegenerative process\nS0 := s0\nS1 \u223cp1(\u00b7| S0)\nS2 \u223cp2(\u00b7| Spa(2))\n...\nSK \u223cpK(\u00b7| Spa(K)).\nUsing the chain rule of probability and variable independencies expressed\nby the DAG, thejoint probability distributionis then (assuming a\ntopological order forS0,S1,...,S K)\nP(S = s) := P(S1 = s1,...,S K = sK)\n=\nK\u220f\nk=1\nP(Sk = sk|Spa(k) = spa(k))\n:=\nK\u220f\nk=1\npk(sk|spa(k))\nThisrepresentationiswellsuitedtoexpress causalrelationshipsbetween\nrandom variables.\n10.5.2 Parameterizing Bayesian networks\nIn a Bayesian framework, observed data, latent variables, parameters\nand noise variables are all treated as random variables. If the conditional\ndistribution pk associated to nodek depends on some parameters, they\ncan be provided topk as conditioning, using parent nodes.\nA Bayesian network is specified by the conditional distributionspk.\nTherefore, unlike computation graphs, there is no notion of functionfk in\naBayesiannetwork.However,therootnodesoftheBayesiannetworkcan\nbe the output of a neural network. For instance, autoregressive models,\nsuch as RNNs or Transformers, specify the conditional probability\ndistributionofatokengivenpasttokens,andthechainruleofprobability\nis used to obtain a probability distribution over entire sequences.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1288, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a29468ce-c643-4e96-a6ec-7e1b8fa618ba": {"__data__": {"id_": "a29468ce-c643-4e96-a6ec-7e1b8fa618ba", "embedding": null, "metadata": {"page_label": "254", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c363337f-f492-41ed-acdf-df77440b3772", "node_type": "4", "metadata": {"page_label": "254", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "7e28952ff74d9ecc7e1d485b97f276ee6df0b30358d85b64c8a132a62932d41b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "254 Inference in graphical models as differentiation\n10.5.3 Ancestral sampling\nA major advantage of Bayesian networks is that, provided that each\nconditional distributionpk is normalized, the joint distribution ofS =\n(S1,...,S K) is automatically normalized. This means that we can very\neasily draw i.i.d. samples from the joint distribution, by following the\ngenerative process: we follow the topological orderk = 1,...,K and\non iteration k we draw a valuesk \u223cpk(\u00b7|spa(k)) conditioned on the\nprevious valuesspa(k). This is known asancestral sampling.\n10.6 Markov random fields\n10.6.1 Expressing factors using undirected graphs\nA Markov random field (MRF), a.k.a. undirected graphical model,\nspecifies a distribution that factorizes as\nP(S = s) = p(s) := 1\nZ\n\u220f\nC\u2208C\n\u03c8C(sC),\nwhere C is the set of maximalcliques of G, that is, subsets ofVthat\nare fully connected,Z is a normalization constant defined by\nZ :=\n\u2211\ns\u2208S\n\u220f\nC\u2208C\n\u03c8C(sC),\nand \u03c8C: SC\u2192R+ is apotential function(a.k.a. compatibility func-\ntion), withSC:= (Sj)j\u2208C. According to the Hammersley-Clifford theo-\nrem, an MRF can be equivalently defined in terms of Markov properties;\nwe refer the interested reader to Wainwright and Jordan (2008). For the\nsake of this chapter, the definition above is sufficient for our purposes.\nExample 10.2(Markov chains as Markov random fields). Forachain,\nletting S = (S1,...,S K) and s= (s1,..., sK), recall that\nP(S = s) =\nK\u220f\nk=1\npk(sk |sk\u22121).\nThis is equivalent to an MRF withZ = 1 (since a chain is auto-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1493, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "948b987b-3d71-4b30-804c-e689af9744a2": {"__data__": {"id_": "948b987b-3d71-4b30-804c-e689af9744a2", "embedding": null, "metadata": {"page_label": "255", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b4600a08-325e-424d-8707-101e6dff251c", "node_type": "4", "metadata": {"page_label": "255", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "c7ec3228e310b5ebdffd075884ef1ec639a781436a7ab9e6e085a98e68ddcdc4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10.6. Markov random fields 255\nmatically normalized),\nC := {{0,1},{1,2},..., {K\u22121,K}}\nand with potential function\n\u03c8{k\u22121,k}(sk\u22121,sk) := pk(sk|sk\u22121).\nMore generally, a Bayesian network can be similarly written as an\nMRF by creating appropriate potential functions corresponding to\nthe parents of each node.\n10.6.2 MRFs as exponential family distributions\nLet us define the potential functions\n\u03c8C(sC; \u03b8C) := exp(\u27e8\u03b8C,\u03d5C(sC)\u27e9)\nfor some sufficient statistic function \u03d5C: SC \u2192 \u0398C and parameters\n\u03b8C\u2208\u0398C. Then,\np\u03b8(s) := 1\nZ(\u03b8)\n\u220f\nC\u2208C\n\u03c8C(sC; \u03b8C)\n= 1\nZ(\u03b8)\n\u220f\nC\u2208C\nexp(\u27e8\u03b8C,\u03d5C(sC)\u27e9)\n= 1\nZ(\u03b8) exp\n(\u2211\nC\u2208C\n\u27e8\u03b8C,\u03d5C(sC)\u27e9\n)\n= 1\nZ(\u03b8) exp (\u27e8\u03b8,\u03d5(s)\u27e9)\n= exp (\u27e8\u03b8,\u03d5(s)\u27e9\u2212A(\u03b8))", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 645, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d4d538e2-edaa-4276-8e57-488d142b6fa2": {"__data__": {"id_": "d4d538e2-edaa-4276-8e57-488d142b6fa2", "embedding": null, "metadata": {"page_label": "256", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b291cc6-1c9d-406d-92ea-48c488ba1a1d", "node_type": "4", "metadata": {"page_label": "256", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "2898fd1222ab7d23813e41d928252407682160fc4f41fb2aba3d915759669ddd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "256 Inference in graphical models as differentiation\nwhere\n\u03d5(s) := (\u03d5C(sC))C\u2208C\n\u03b8:= (\u03b8C)C\u2208C\nZ(\u03b8) :=\n\u2211\ns\u2208S\n\u220f\nC\u2208C\n\u03c8C(sC; \u03b8C)\n=\n\u2211\ns\u2208S\nexp (\u27e8\u03b8,\u03d5(s)\u27e9)\nA(\u03b8) := log Z(\u03b8)\nTherefore, for this choice of potential functions, we can view an MRF\nas an exponential family distribution (Section 3.4) withnatural pa-\nrameters \u03b8, sufficient statistic\u03d5 and log-partition function A(\u03b8).\nExample 10.3(Ising model). The Ising model is a classical example\nof MRF. LetY = (Y1,...,Y M) \u2208{0,1}M be an unordered collec-\ntion of binary variablesYi \u2208{0,1}. This forms a graphG= (V,E),\nwhere V= [M] and E\u2286 V2, such that(i,j) \u2208E means thatYi in-\nteracts withYj. In statistical physics,Yi may indicate the presence\nor absence of particles, or the orientation of magnets. In image\nprocessing, Yi may represent a black and white pixel. In multi-label\nclassification, Yi may indicate the presence or absence of a label.\nThe probability ofy= (y1,...,y M) \u2208{0,1}M is then\nP(Y = y) = p\u03b8(y)\n= exp\n\uf8eb\n\uf8ed\u2211\ni\u2208V\n\u03b8iyi +\n\u2211\n(i,j)\u2208E\n\u03b8i,jyiyj \u2212A(\u03b8)\n\uf8f6\n\uf8f8\n= exp\n(\u2211\nC\u2208C\n\u27e8\u03b8C,\u03d5C(y)\u27e9\u2212A(\u03b8)\n)\n,\nwhere C := V\u222aE and \u03b8\u2208R|V|+|E| is the concatenation of(\u03b8i)i\u2208V\nand (\u03b8i,j)(i,j)\u2208E. These models are also known asBoltzmann ma-\nchines in a neural network context. MAP inference in general\nIsing models is known to be NP-hard, but when the interaction\nweights \u03b8i,j are non-negative, MAP inference can be reduced to", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1345, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d6d03f6b-ac42-414a-9f93-73850f1807bb": {"__data__": {"id_": "d6d03f6b-ac42-414a-9f93-73850f1807bb", "embedding": null, "metadata": {"page_label": "257", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a14ac0b-ccd1-42c2-a39a-0af461bbff27", "node_type": "4", "metadata": {"page_label": "257", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "7818b0531fd459320111181a9af92f7347c85bc5c906c8159df8895b3eac6129", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10.7. Inference on chains 257\ngraph cut algorithms (Greiget al., 1989). There are two ways the\nabove equation can be extended. First, we can use higher-order\ninteractions, such asyiyjyk for (i,j,k ) \u2208V3. Second, we may want\nto use categorical variables, which leads to thePotts model.\n10.6.3 Conditional random fields\nConditional random fields (Laffertyet al., 2001; Sutton, McCallum,\net al., 2012) are a special case of Markov random field, in which a\nconditioning variable isexplicitly incorporated. For example, when\nthe goal is to predict a variableyconditioned on a variablex, CRFs\nare defined as\nP(Y = y|X = x) = p(y|x) = 1\nZ(x)\n\u220f\nC\u2208C\n\u03c8C(yC,x).\nNote that the potential functions\u03a8C are allowed to depend on the\nwhole x, asxis just a conditioning variable.\n10.6.4 Sampling\nContrary to Bayesian networks, MRFs require an explicit normalization\nconstant Z. As a result, sampling from a distribution represented by a\ngeneral MRF is usually more involved than for Bayesian networks. A\ncommonly-used technique isGibbs sampling.\n10.7 Inference on chains\nIn this section, we review how to perform marginal inference and\nmaximum a-posteriori inference on joint distributions of the form\np(s1,..., sK) = 1\nZ\nK\u220f\nk=1\n\u03c8k(sk\u22121,sk),\nwhere\nZ :=\n\u2211\ns\u2208S\nK\u220f\nk=1\n\u03c8k(sk\u22121,sk\u22121)\nand where we used\u03c8k as a shorthand for\u03c8k\u22121,k, sincek\u22121 and k are\nconsecutive. As explained in Example 10.2, this also includes Markov", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1394, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "684649f1-f1f3-4ce4-b484-92ede919b206": {"__data__": {"id_": "684649f1-f1f3-4ce4-b484-92ede919b206", "embedding": null, "metadata": {"page_label": "258", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6d52553b-bbcb-468c-bb2d-105943b58346", "node_type": "4", "metadata": {"page_label": "258", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "85c2f8769d707c78590bb552c4a4ea4641d93bc0d003abfd75e0b88115ba0a98", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "258 Inference in graphical models as differentiation\nchains by setting\n\u03c8k(sk\u22121,sk) := pk(sk |sk\u22121),\nin which caseZ = 1.\n10.7.1 The forward-backward algorithm\nThe key idea of the forward-backward algorithm is to use thedistribu-\ntivity of multiplication over addition to write\nZ =\n\u2211\ns1\u2208S1\n\u03c81(s0,s1)\n\u2211\ns2\u2208S2\n\u03c82(s1,s2) \u00b7\u00b7\u00b7\n\u2211\nsK\u2208SK\n\u03c8K(sK\u22121,sK).\nWe can compute these sums recursively, eitherforward or backward.\nRecalling the definitions ofAk\u22121 and Bk+1 in Eq. (10.1), we define the\nsummations up toand down tok,\n\u03b1k(sk) :=\n\u2211\ns1,...,sk\u22121\u2208Ak\u22121\nk\u220f\nj=1\n\u03c8j(sj\u22121,sj)\n=\n\u2211\nsk\u22121\u2208Sk\u22121\n\u03c8k(sk\u22121,sk) \u00b7\u00b7\u00b7\n\u2211\ns1\u2208S1\n\u03c82(s1,s2)\u03c81(s0,s1)\n\u03b2k(sk) :=\n\u2211\nsk+1,...,sK\u2208Bk+1\nK\u220f\nj=k+1\n\u03c8j(sj\u22121,sj)\n=\n\u2211\nsk+1\u2208Sk+1\n\u03c8k+1(sk,sk+1) \u00b7\u00b7\u00b7\n\u2211\nsK\u2208SK\n\u03c8K(sK\u22121,sK).\nWe can compute the two quantities by recursing forward and backward\n\u03b1k(sk) =\n\u2211\nsk\u22121\u2208Sk\u22121\n\u03c8k(sk\u22121,sk)\u03b1k\u22121(sk\u22121)\n\u03b2k(sk) =\n\u2211\nsk+1\u2208Sk+1\n\u03c8k+1(sk,sk+1)\u03b2k+1(sk+1)\nwhere we defined the initializations\n\u03b11(s1) := \u03c81(s0,s1) \u2200s1 \u2208S1\n\u03b2K(sK) := 1 \u2200sK \u2208SK.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 957, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "29587923-d2b4-4b8c-ba66-d7a2d1349fd8": {"__data__": {"id_": "29587923-d2b4-4b8c-ba66-d7a2d1349fd8", "embedding": null, "metadata": {"page_label": "259", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e0ecfe1c-768f-45ed-a5a7-1b656ae8c938", "node_type": "4", "metadata": {"page_label": "259", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "111d8622ec516f5d06f119a9a640a3b3fd8e632a11fa8ed0a47630dcd31ae903", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10.7. Inference on chains 259\nThe normalization term can then be computed by\nZ =\n\u2211\nsK\u2208SK\n\u03b1K(sK)\u03b2K(sK) =\n\u2211\ns1\u2208S1\n\u03b11(s1)\u03b21(s1)\nand themarginal probabilitiesby\nP(Sk = sk) = 1\nZ\u03b1k(sk)\u03b2k(sk)\nP(Sk\u22121 = sk\u22121,Sk = sk) = 1\nZ\u03b1k\u22121(sk\u22121)\u03c8k(sk\u22121,sk)\u03b2k(sk).\nWe can also compute the conditional probabilities by\nP(Sk = sk |Sk\u22121 = sk\u22121) = P(Sk\u22121 = sk\u22121,Sk = sk)\nP(Sk\u22121 = sk\u22121)\n= \u03b1k\u22121(sk\u22121)\u03c8k(sk\u22121,sk)\u03b2k(sk)\n\u03b1k\u22121(sk\u22121)\u03b2k\u22121(sk\u22121)\n= \u03c8k(sk\u22121,sk)\u03b2k(sk)\n\u03b2k\u22121(sk\u22121) .\nIn practice, the two recursions are often implemented in thelog-domain\nfor numerical stability,\nlog \u03b1k(sk) = log\n\u2211\nsk\u22121\u2208Sk\u22121\nexp(log \u03c8k(sk\u22121,sk) + log\u03b1k\u22121(sk\u22121))\nlog \u03b2k(sk) = log\n\u2211\nsk+1\u2208Sk+1\nexp(log \u03c8k+1(sk,sk+1) + log\u03b2k+1(sk+1)).\nWe recognize thelog-sum-exp operator, which can be implemented\nin a numerically stable way (Section 4.4.2). The overalldynamic\nprogrammingprocedure, a.k.a.forward-backwardalgorithm (Baum\nand Petrie, 1966; Rabiner, 1989), is summarized in Algorithm 10.1. We\nnotice that the forward and backward passes are actually independent\nof each other, and can therefore be performed in parallel.\n10.7.2 The Viterbi algorithm\nSimilarly, using the distributivity of multiplication over maximization,\nmax\ns1\u2208S1,...,sK\u2208SK\nK\u220f\nk=1\n\u03c8k(sk\u22121,sk)\n= max\nsK\u2208SK\nmax\nsK\u22121\u2208SK\u22121\n\u03c8K(sK\u22121,sK) ... max\ns1\u2208S1\n\u03c82(s1,s2)\u03c81(s0,s1).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1272, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d0e610e3-3f2e-4329-a135-6d504e106993": {"__data__": {"id_": "d0e610e3-3f2e-4329-a135-6d504e106993", "embedding": null, "metadata": {"page_label": "260", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0d1640d9-f170-430b-911d-a029b32ef988", "node_type": "4", "metadata": {"page_label": "260", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "4e09177fd92fdb3e38f6d5b72130bab56337e623846199173e8bff49ba8db938", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "260 Inference in graphical models as differentiation\nAlgorithm 10.1Marginal inference on a chain\nPotential functions:\u03c81,...,\u03c8 K\nInput: s0\n1: Initialize \u03b11(s1) := \u03c81(s0,s1) \u2200s1 \u2208S1\n2: for k:= 2,...,K do \u25b7 Forward pass\n3: for sk \u2208Sk do\n4: \u03b1k(sk) :=\n\u2211\nsk\u22121\u2208Sk\u22121\n\u03c8k(sk\u22121,sk)\u03b1k\u22121(sk\u22121)\n5: Initialize \u03b2K(sK) := 1 \u2200sK \u2208SK\n6: for k:= K\u22121,..., 1 do \u25b7 Backward pass\n7: for sk \u2208Sk do\n8: \u03b2k(sk) :=\n\u2211\nsk+1\u2208Sk+1\n\u03c8k+1(sk,sk+1)\u03b2k+1(sk+1)\n9: Compute Z =\n\u2211\nsK\u2208SK\n\u03b1K(sK)\u03b2K(sK) =\n\u2211\nsK\u2208SK\n\u03b1K(sK)\nOutputs: \u2200k\u2208[K]:\nP(Sk = sk) = 1\nZ\u03b1k(sk)\u03b2k(sk)\nP(Sk\u22121 = sk\u22121,Sk = sk) = 1\nZ\u03b1k\u22121(sk\u22121)\u03c8k(sk\u22121,sk)\u03b2k(sk)\nLet us define fork\u2208[K]\n\u03b4k(sk) := max\nsk\u22121\u2208Sk\u22121\n\u03c8k(sk\u22121,sk) ... max\ns1\u2208S1\n\u03c82(s1,s2)\u03c81(s0,s1).\nWe can compute these quantities recursively, since fork\u2208[K]\n\u03b4k(sk) = max\nsk\u22121\u2208Sk\u22121\n\u03c8k(sk\u22121,sk)\u03b4k\u22121(sk\u22121),\nwith \u03b41(sk) := \u03c8(s0,sk). We finally have\nmax\ns1\u2208S1,...,sK\u2208SK\np(s1,..., sK) = 1\nZ max\nsK\u2208SK\n\u03b4K(sK).\nIn practice, for numerical stability, we often implement the forward\nrecursion in the log-domain. Using the fact that the logarithm is\nmonotonic, we indeed have for allk\u2208[K]\nlog \u03b4k(sk) = max\nsk\u22121\u2208Sk\u22121\nlog \u03c8k(sk\u22121,sk) + log\u03b4k\u22121(sk\u22121).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1108, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "59799741-7f51-4f77-9a07-eb26b7aaad93": {"__data__": {"id_": "59799741-7f51-4f77-9a07-eb26b7aaad93", "embedding": null, "metadata": {"page_label": "261", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2205f9be-d3e5-45fa-85be-d6dddd0be8a3", "node_type": "4", "metadata": {"page_label": "261", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "6573aacd06434fa52511a43ebce01b1514382c568143e918796095cf45b9aa5a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10.8. Inference on trees 261\nTo enable efficientbacktracking, during the forward pass, we compute\nqk(sk) := arg max\nsk\u22121\u2208Sk\u22121\n\u03c8k(sk\u22121,sk)\u03b4k\u22121(sk\u22121)\nwhich can be thought asbackpointers from s\u22c6\nk to s\u22c6\nk\u22121.\nThe resulting dynamic programming procedure, a.k.a.Viterbi algo-\nrithm (Viterbi, 1967; Forney, 1973), is summarized in Algorithm 10.2.\nAlgorithm 10.2MAP inference on a chain\nPotential functions:\u03c81,...,\u03c8 K\nInput: s0\n1: Initialize \u03b41(s1) := \u03c81(s0,s1) \u2200s1 \u2208S1\n2: for k:= 2,...,K do \u25b7 Forward pass\n3: for sk \u2208Sk do\n4: \u03b4k(sk) := max\nsk\u22121\u2208Sk\u22121\n\u03c8k(sk\u22121,sk)\u03b4k\u22121(sk\u22121)\n5: qk(sk) := arg max\nsk\u22121\u2208Sk\u22121\n\u03c8k(sk\u22121,sk)\u03b4k\u22121(sk\u22121)\n6: \u03b4\u22c6 := max\nsK\u2208SK\n\u03b4K(sK)\n7: s\u22c6\nK := arg max\nsK\u2208SK\n\u03b4K(sK)\n8: for k:= K\u22121,..., 1 do \u25b7 Backtracking\n9: s\u22c6\nk := qk+1(s\u22c6\nk+1)\nOutputs: max\ns1\u2208S1,...,sK\u2208SK\np(s1,..., sK) \u221d\u03b4\u22c6\narg max\ns1\u2208S1,...,sK\u2208SK\np(s1,..., sK) = (s\u22c6\n1,..., s\u22c6\nK)\n10.8 Inference on trees\nMore generally, efficient inference based on dynamic programming\ncan be performed when dependencies between variables are expressed\nusing atree or polytree. The resulting marginal inference and MAP\ninference algorithms are often referred to as thesum-product and\nmax-sum algorithms. The sum-product algorithm is also known as\nbelief propagationor message passing, since it can be interpreted\nas propagating \u201clocal messages\u201d through the graph. See for instance\n(Wainwright and Jordan, 2008, Section 2.5.1) for more details.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1390, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0f38306d-b966-43c9-80bb-5a013b2326c9": {"__data__": {"id_": "0f38306d-b966-43c9-80bb-5a013b2326c9", "embedding": null, "metadata": {"page_label": "262", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fb8f9468-7cdf-4fe4-8c31-88a856eac71e", "node_type": "4", "metadata": {"page_label": "262", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "3e697afd4074c760b1238d5802a0f11a8d38e5773caeb2331c01f266319df5a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "262 Inference in graphical models as differentiation\n10.9 Inference as differentiation\nIn this section, we review the profound connections between differenti-\nating the log-partition function of an exponential family distribution\non one hand, and performing marginal inference (as well as maximum\na-posteriori inference in the zero-temperature limit) on the other hand.\n10.9.1 Inference as gradient of the log-partition\nWe first discuss a well-known fact in the graphical model literature: when\nusing a binary encoding as the sufficient statistic\u03d5 in an exponential\nfamily distribution, the gradient\u2207A(\u03b8) of the log-partitionA(\u03b8) gathers\nall the marginals (Wainwright and Jordan, 2008).\nTo see why, recall from Section 3.4 the definition of an exponential\nfamily distribution\np\u03b8(s) = h(s) exp [\u27e8\u03b8,\u03d5(s)\u27e9\u2212A(\u03b8)]\nand of its log-partition\nA(\u03b8) := log\n\u2211\ns\u2208S\nh(s) exp [\u27e8\u03b8,\u03d5(s)\u27e9] .\nFrom Proposition 3.2,\n\u00b5(\u03b8) := \u2207A(\u03b8) = EY\u223cp\u03b8[\u03d5(Y)] \u2208M.\nTherefore, with thebinary encodingsin Eq. (10.2) and Eq. (10.3),\nP(Sk = vi) = [\u2207A(\u03b8)]k,i\nP(Sk = vi,Sl = vj) = [\u2207A(\u03b8)]k,l,i,j.\nPut differently, if we have an efficient algorithm for computingA(\u03b8), we\ncan performreverse-mode autodiffon A(\u03b8) to obtain\u2207A(\u03b8), and\ntherefore obtain the marginal probabilities. Following Section 8.3.3, the\ncomplexity of computing all marginal probabilities is therefore roughly\nthe same as that of computingA(\u03b8).\nIn the special case of chains, we obtain\nP(Sk = vi) = [\u2207A(\u03b8)]k,i = 1\nZ\u03b1k(vi)\u03b2k(vi)\nP(Sk\u22121 = vi,Sk = vj) = [\u2207A(\u03b8)]k\u22121,k,i,j = 1\nZ\u03b1k\u22121(vi)\u03c8k(vi,vj)\u03b2k(vj),", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1520, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a4457f8e-1b7e-42b5-882a-b9137da39417": {"__data__": {"id_": "a4457f8e-1b7e-42b5-882a-b9137da39417", "embedding": null, "metadata": {"page_label": "263", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9bb7b024-485f-4f3c-b531-95ea75c81dc8", "node_type": "4", "metadata": {"page_label": "263", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "881e798dc71dc5f1612ea30520981e904cafeaa0239bbe69473d3be580f24321", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10.9. Inference as differentiation 263\nwhere we left the dependence ofZ, \u03b1 and \u03b2 on \u03b8implicit.\nIf we defineA\u03b5(\u03b8) := \u03b5A(\u03b8/\u03b5), in the zero-temperature limit\u03b5\u21920,\nwe obtain that \u00b5(\u03b8) is a binary encoding of the mode, i.e., of the\nmaximum a-posteriori inference solution.\nWe now show i) how to unify the forward pass of the forward-\nbackward and Viterbi algorithms using semirings and softmax operators\nii) how to compute the gradient of the log-partition using backpropaga-\ntion.\n10.9.2 Semirings and softmax operators\nThe forward passes in the forward-backward and Viterbi algorithms are\nclearly similar. In fact, they can be formally linked to each other using\nsemirings.\nDefinition 10.1(Semiring). A semiring is a setK equipped with\ntwo binary operations(\u2295,\u2297) such that\n\u2022 \u2297is commutative and associative,\n\u2022 \u2295is associative and distributive over\u2295,\n\u2022 \u2297and \u2295have identity element\u00af0 and \u00af1, respectively.\nWe use the notations\u2295, \u2297, \u00af0 and \u00af1 to clearly distinguish them from\nthe classical addition, multiplication,0 and 1.\nWe recall the following laws for binary operations:\n\u2022 Commutativityof \u2295: a\u2295b= b\u2295a,\n\u2022 Associativity of \u2295: a\u2295(b\u2295c) = (a\u2295b) \u2295c,\n\u2022 Distributivity of \u2297over \u2295: a\u2297(b\u2295c) = (a\u2297b) \u2295(a\u2297c).\nA set equipped with a binary operation supporting associativity and an\nidentity element is called amonoid. A monoid such that every element\nhas an inverse element is called agroup. The difference between a ring\nand a semiring is that the latter only requires(K,\u2295) and (K,\u2297) to be\nmonoids, not groups.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1495, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "16bb5c6b-e74d-4fd6-922e-cf158944d990": {"__data__": {"id_": "16bb5c6b-e74d-4fd6-922e-cf158944d990", "embedding": null, "metadata": {"page_label": "264", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0c95cbc2-14f1-44f9-8775-f2446121473d", "node_type": "4", "metadata": {"page_label": "264", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "e735eed5aaefcca8947f3e3cd2a7de7006a8bf4e50edeb506900508a98f776c8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "264 Inference in graphical models as differentiation\nEquipped with these definitions, we can interpret the forward passes\nin the Viterbi and forward-backward algorithms as follows:\n\u2022 the forward-backward algorithm in the exponential domain uses\nthe semiringR+ equipped with(+,\u00d7) and identity elements(0,1);\n\u2022 the Viterbi algorithm in the log domain uses the semiringR\nequipped with(max,+) and identity elements(\u2212\u221e,0);\n\u2022 the forward-backward algorithm in the log domain uses the semir-\ning R equipped with (max\u03b5,+) and identity elements(\u2212\u221e,0),\nwhere we defined the soft max operator (log-add-exp)\nmax\u03b5(a,b) := \u03b5log((exp(a) + exp(b))/\u03b5),\nwith \u03b5:= 1 by default.\nIt can be checked that indeedmax\u03b5 is commutative, associative, and\naddition is distributive over max\u03b5. Its identity element is \u2212\u221e. By\nassociativity,\nmax\u03b5(a1,max\u03b5(a2,a3)) = logsumexp\u03b5(a1,a2,a3)\n= \u03b5log\n\u2211\ni\nexp(ai/\u03b5).\nIn contrast, note that the sparsemax in Section 13.5 is not associative.\nThanks to associativity, we can introduce the shorthand notations\nmax\u03b5\nv\u2208V\nf(v) := \u03b5log\n\u2211\nv\u2208V\nexp(f(v)/\u03b5) \u2208R.\nand\nargmax\u03b5\nv\u2208V\nf(v) :=\n(\nexp(f(v\u2032)/\u03b5)/\n\u2211\nv\u2208V\nexp(f(v)/\u03b5)\n)\nv\u2032\u2208V\n\u2208P(V).\nMany algorithms can be generalized thanks to the use of semirings;\nsee among others Aji and McEliece (2000) and Mohriet al.(2008). The\ndistributive and associative properties play a key role in breaking down\nlarge problems into smaller ones (Verdu and Poor, 1987).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1393, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "daf264a2-e62f-45ab-b3ab-e519a994c823": {"__data__": {"id_": "daf264a2-e62f-45ab-b3ab-e519a994c823", "embedding": null, "metadata": {"page_label": "265", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4e5b6f02-cc0a-43d9-b8cd-9e98c06e4ec1", "node_type": "4", "metadata": {"page_label": "265", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "d3681a32ffe2c49dec8cd65099b2bc2b14c3dd43cc1a19f3c0fda12044330568", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10.9. Inference as differentiation 265\n10.9.3 Inference as backpropagation\nIn this section, we show that, algorithmically, backtracking is recovered\nas a special case of backpropagation. See also (Eisner, 2016; Mensch\nand Blondel, 2018).\nFor notation simplicity, we assumeS0 = {1}and Sk = {1,...,M }\nfor allk\u2208[K]. We focus on the case\nlog \u03c8k(i,j) = \u27e8\u03b8k,\u03d5k(i,j)\u27e9= \u03b8k,i,j.\nWe also introduce the shorthands\na1,j := log \u03b11(j) = \u03b81,1,j\nak,j := log \u03b1k(j) = max\u03b5\ni\u2208[M]\n\u03b8k,i,j + ak\u22121,i\nand\nqk,j := argmax\u03b5\ni\u2208[M]\n\u03b8k,i,j + ak\u22121,i.\nOur goal is to compute the gradient w.r.t.\u03b8\u2208RK\u00d7M\u00d7M of\nlog Z = A= max\u03b5\nj\u2208[M]\naK,j.\nThe soft argmax counterpart of this quantity is\nQ:= argmax\u03b5\nj\u2208[M]\naK,j \u2208\u25b3M,\nwhere we usedP([M]) = \u25b3M.\nComputing the gradient ofA is similar to computing the gradient\nof a feedforward network, in the sense that\u03b8k influences not onlyak\nbut alsoak+1,...,a K. Let us introduce the adjoint variable\nrk,i := \u2202A\n\u2202ak,i\n,\nwhich we initialize as\nrK,i = \u2202A\n\u2202aK,i\n= Qi.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 960, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7b7f4e99-ef34-49ef-8b25-cd8c63421550": {"__data__": {"id_": "7b7f4e99-ef34-49ef-8b25-cd8c63421550", "embedding": null, "metadata": {"page_label": "266", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "61b17e81-1c2f-45ab-ad38-403a4d89862b", "node_type": "4", "metadata": {"page_label": "266", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "bd6673adfde94b5981ac4f74505682a3c66efc353b9be26334a32f022a5bbf0a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "266 Inference in graphical models as differentiation\nSince \u03b8k,i,j directly influencesak,j, we have fork \u2208[K], i \u2208[M] and\nj \u2208[M]\n\u00b5k,i,j := \u2202A\n\u2202\u03b8k,i,j\n= \u2202A\n\u2202ak,j\n\u00b7 \u2202ak,j\n\u2202\u03b8k,i,j\n= rk,j \u00b7qk,j,i.\nSince ak,i directly influences ak+1,j for j \u2208 [M], we have for k \u2208\n{1,...,K \u22121}and i\u2208[M]\nrk,i = \u2202A\n\u2202ak,i\n=\n\u2211\nj\u2208[M]\n\u2202A\n\u2202ak+1,j\n\u2202ak+1,j\n\u2202ak,i\n=\n\u2211\nj\u2208[M]\nrk+1,jqk+1,j,i\n=\n\u2211\nj\u2208[M]\n\u00b5k+1,i,j.\nWe summarize the procedure in Algorithm 10.3. The forward pass\nuses the softmax operatormax\u03b5 and the softargmax operatorargmax\u03b5.\nIn the hard max case, in Algorithm 10.2, we usedqto store backpointers\nfromintegertointeger.Inthesoftmaxcase,inAlgorithm10.3,weused q\nto storesoft backpointers, that is, discrete probability distributions. In\nthe zero-temperature limit, backpropagation outputs a binary encoding\nof the solution of backtracking.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 817, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "54c264db-7628-40a6-bd16-77144685579b": {"__data__": {"id_": "54c264db-7628-40a6-bd16-77144685579b", "embedding": null, "metadata": {"page_label": "267", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ac648878-e6f1-48ec-9807-115c802405f7", "node_type": "4", "metadata": {"page_label": "267", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "121892d3371c7579eaf2a1fa59f5fee52e95f3169fc0605f8be7086900541add", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10.10. Summary 267\nAlgorithm 10.3Inference on a chain as backprop with max operators\nInput: \u03b8\u2208RK\u00d7M\u00d7M\nMax operator:max\u03b5\n1: Initialize a1,j := \u03b81,1,j \u2200j \u2208[M]\n2: for k:= 2,...,K do \u25b7 Forward pass\n3: for j \u2208[M] do\n4: ak,j := max\u03b5\ni\u2208[M]\n\u03b8k,i,j + ak\u22121,j \u2208R\n5: qk,j := argmax\u03b5\ni\u2208[M]\n\u03b8k,i,j + ak\u22121,j \u2208\u25b3M\n6: A:= max\u03b5\ni\u2208[M]\naK,i \u2208R\n7: Q:= argmax\u03b5\ni\u2208[M]\naK,i \u2208\u25b3M\n8: Initialize rK,j = Qj \u2200j \u2208[K]\n9: for k:= K\u22121,..., 1 do \u25b7 Backward pass\n10: for i\u2208[M] do\n11: for j \u2208[M] do\n12: \u00b5k+1,i,j = rk+1,j \u00b7qk+1,j,i\n13: rk,i \u2190\u00b5k+1,i,j\nOutputs: max\u03b5\ni1,...,iK\u2208[M]K\n\u03b81,1,i1 + \u2211K\nk=2 \u03b8k,ik\u22121,ik = A, \u2207A(\u03b8) = \u00b5\n10.10 Summary\n\u2022 Graphical models represent the conditional dependencies between\nvariables and therefore specify how their joint distribution factor-\nizes.\n\u2022 There are clear analogies between the worlds of functions and of\ndistributions: the counterparts of computation chains and compu-\ntation graphs are Markov chains and Bayesian networks.\n\u2022 Inference on chains and more generally on trees, for exponential\nfamily distributions, is equivalent, both statistically and algorith-\nmically, to differentiating the log-partition function.\n\u2022 The forward-backward algorithm can be seen as using a sum-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1178, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "95dac4eb-26c2-45cf-9425-9c5f3463a8c3": {"__data__": {"id_": "95dac4eb-26c2-45cf-9425-9c5f3463a8c3", "embedding": null, "metadata": {"page_label": "268", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "82aa7df4-4666-46b5-9ba4-d0d6beb3dee6", "node_type": "4", "metadata": {"page_label": "268", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "493f6a50efe2f7a58f6b797e8939eb65ddcf1b4d3b541cd822c7d496a8d45c92", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "268 Inference in graphical models as differentiation\nproduct algebra, while the Viterbi algorithm can be seen as using\na max-plus algebra. Equivalently, in the log domain, we can see\nthe former as using a soft max, and the latter as using a hard\nmax.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 250, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "441160b0-59ed-4324-aa32-ec8907752b1b": {"__data__": {"id_": "441160b0-59ed-4324-aa32-ec8907752b1b", "embedding": null, "metadata": {"page_label": "269", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "97d5dc5d-e376-4753-8958-3334e88cd9b2", "node_type": "4", "metadata": {"page_label": "269", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "8708161db97e40e78a43aa4e6479a21aac78e13a8fa6ef410a5cf65234181b40", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11\nDifferentiating through optimization\nIn this chapter, we study how to differentiate through optimization\nproblems, and more generally through nonlinear systems of equations.\n11.1 Implicit functions\nImplicit functions are functions that do not enjoy an explicit decompo-\nsition into elementary functions, for which automatic differentiation, as\nstudied in Chapter 8, can therefore not be directly applied. We describe\nin this chapter techniques to differentiate through such functions and\nhow to integrate them into an autodiff framework.\nFormally, we will denote an implicit function byw\u22c6(\u03bb), where\nw\u22c6: \u039b \u2192 W. One question is then how to compute the Jacobian\n\u2202w\u22c6(\u03bb). As a first application one can considersensitivity analysis\nof a system. For example,w\u22c6(\u03bb) could correspond to the equilibrium\nstate of a physical system and in this case,\u2202w\u22c6(\u03bb) would tell us about\nthe sensitivity of the system to some parameters\u03bb\u2208\u039b.\n269", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 924, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a143c1ac-a7a4-4579-ae74-eb38dd2782d2": {"__data__": {"id_": "a143c1ac-a7a4-4579-ae74-eb38dd2782d2", "embedding": null, "metadata": {"page_label": "270", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7bd14eba-fc09-4bc0-89c5-2c5c2dd01e1a", "node_type": "4", "metadata": {"page_label": "270", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "8dc3493ba5b276aefd60f5b4a36b7cf5a48c4441f95b3aa222cc3cda08bea197", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "270 Differentiating through optimization\n11.1.1 Optimization problems\nAnotherexampleisafunctionimplicitlydefinedasthesolution(assumed\nunique) of an optimization problem\nw\u22c6(\u03bb) := arg max\nw\u2208W\nf(w,\u03bb),\nwhere f: W\u00d7 \u039b \u2192R and Wdenotes a constraint set. Note that we\nuse anarg max for convenience, but the same applies when using an\narg min.\n11.1.2 Nonlinear equations\nMore generally, w\u22c6(\u03bb) can be defined as the root of some function\nF: W\u00d7 \u039b \u2192 W, i.e., w\u22c6(\u03bb) is implicitly defined as the function\nsatisfying the (potentially nonlinear) system of equations\nF(w,\u03bb) = 0\nfor all\u03bb\u2208\u039b.\n11.1.3 Application to bilevel optimization\nBesides sensitivity analysis, another example of application isbilevel\noptimization. Many times, we want to minimize a function defined as\nthe composition of a fixed function and the solution of an optimization\nproblem. Formally, letf,g : W\u00d7 \u039b \u2192R. We consider the composition\nh(\u03bb) defined as\nh(\u03bb) := g(w\u22c6(\u03bb),\u03bb), where w\u22c6(\u03bb) := arg max\nw\u2208W\nf(w,\u03bb). (11.1)\nThis includes for instance hyperparameter optimization, wheref is an\ninner log-likelihood objective, g is an outer validation loss,w \u2208W\nare model parameters and\u03bb\u2208\u039b are model hyperparameters, such as\nregularization strength, as illustrated in Fig. 11.1. To minimizeh(\u03bb) one\ngenerally resorts to a gradient descent scheme w.r.t.\u03bb, which requires\ncomputing \u2207h(\u03bb). Assuming thatw\u22c6(\u03bb) is differentiable at\u03bb, by the\nchain rule, we obtain the Jacobian\n\u2202h(\u03bb) = \u22021g(w\u22c6(\u03bb),\u03bb)\u2202w\u22c6(\u03bb) + \u22022g(w\u22c6(\u03bb),\u03bb).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1456, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c0f38dca-aada-463e-a115-10e380657dcd": {"__data__": {"id_": "c0f38dca-aada-463e-a115-10e380657dcd", "embedding": null, "metadata": {"page_label": "271", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0c0aaf4a-1170-46f5-8932-d9c2af794a90", "node_type": "4", "metadata": {"page_label": "271", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "15330c29587e3bb176c9bc1e8d9f839ff78c3c9af73a195841f414487f5f5cf5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11.2. Envelope theorems 271\n0 1 2\n10\n5\n0\n5\nw ( 1)\nw ( 2)\nw ( )\nFigure 11.1:Hyperparameter optimizationin nonlinearregression can becast asa bi-\nlevel optimization problem. Each line corresponds to the estimator obtained by fitting\nsome training data (in blue circles) using a different hyperparameter\u03bb. Formally,\ndenoting f the training objective, the estimators arew\u22c6(\u03bb) := arg minwf(w; \u03bb).\nThe goal is to find the best hyperparameter that fits some validation data (here in\ncyan diamonds), that is, minimizingh(\u03bb) := g(w\u22c6(\u03bb),\u03bb), whereg is the validation\nobjective. A too small\u03bb1 leads to overfitting the training objective and performs\nbadly on validation objective. Conversely, a larger\u03bb2 underfits both training and\nvalidation objectives. The optimal parameter\u03bb\u22c6 minimizes the validation objective\nand may be obtained by iterating gradient descent w.r.t.\u03bb. This requires gradients\nof h(\u03bb) = g(w\u22c6(\u03bb),\u03bb) w.r.t. \u03bb.\nUsing \u2202h(\u03bb)\u22a4= \u2207h(\u03bb) (see Remark 2.4), we obtain the gradient\n\u2207h(\u03bb) = \u2202w\u22c6(\u03bb)\u22a4\u22071g(w\u22c6(\u03bb),\u03bb) + \u22072g(w\u22c6(\u03bb),\u03bb).\nThe only problematic term is\u2202w\u22c6(\u03bb), as it requiresargmax differenti-\nation. Indeed, most of the time, there is no explicit formula forw\u22c6(\u03bb)\nand it does not decompose into elementary functions.\n11.2 Envelope theorems\nIn the special caseg = f, the compositionh defined in Eq. (11.1) is\nsimply given by\nh(\u03bb) = f(w\u22c6(\u03bb),\u03bb) = max\nw\u2208W\nf(w,\u03bb).\nThat is, we no longer needargmax differentiation, but only max\ndifferentiation, which, as we shall now see is much easier. The function\nh is often called avalue function(Fleming and Rishel, 2012). The", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1557, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b3ee40de-6ff6-4c97-9d20-1bb2fd78d12f": {"__data__": {"id_": "b3ee40de-6ff6-4c97-9d20-1bb2fd78d12f", "embedding": null, "metadata": {"page_label": "272", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c9a19900-6f4c-444a-8796-5d5bfacd6b5b", "node_type": "4", "metadata": {"page_label": "272", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "e7aa6be4f94cd482686c0a386aca68a05466c1707441791fde416022d64eb9d3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "272 Differentiating through optimization\nFunctions  for varying \nFunction  \nFigure 11.2:The graph ofh(\u03bb) = maxw\u2208Wf(w,\u03bb) is the upper-envelope of the\ngraphs of the functions\u03bb\u21a6\u2192f(w,\u03bb) for allw\u2208W.\nreason for the name \u201cenvelope\u201d is illustrated in Fig. 11.2. We emphasize\nthat there is not one, but several envelope theorems, depending on the\nassumptions onf.\n11.2.1 Danskin\u2019s theorem\nWhen f is concave-convex, we can use Danskin\u2019s theorem.\nTheorem 11.1(Danskin\u2019s theorem). Let f: W\u00d7\u039b \u2192R and Wbe\na compact convex set. Let\nh(\u03bb) := max\nw\u2208W\nf(w,\u03bb)\nand\nw\u22c6(\u03bb) := arg max\nw\u2208W\nf(w,\u03bb).\nIf f is concavein w, convex in \u03bb, and the maximumw\u22c6(\u03bb) is\nunique, then the functionh is differentiable with gradient\n\u2207h(\u03bb) = \u22072f(w\u22c6(\u03bb),\u03bb).\nIf the maximum is not unique, we get a subgradient.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 763, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1e05c857-8084-44fa-a2bf-6f6013512da1": {"__data__": {"id_": "1e05c857-8084-44fa-a2bf-6f6013512da1", "embedding": null, "metadata": {"page_label": "273", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c5df4b95-e582-4b35-9771-6bdbea5478d7", "node_type": "4", "metadata": {"page_label": "273", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "78fca217b95c3edf5f71668d09af4bc37eb44feef00aa342216925c7154c15e7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11.2. Envelope theorems 273\nInformally, Danskin\u2019s theorem means that we can treatw\u22c6(\u03bb) as if\nit were a constant of\u03bb, i.e., we do not need to differentiate through it,\neven though it depends on\u03bb. Danskin\u2019s theorem can also be used to\ndifferentiate through a minimum,h(\u03bb) = minw\u2208Wf(w,\u03bb), iff(w,\u03bb)\nis convex inwand concave in\u03bb, as we now illustrate.\nExample 11.1(Ilustration of Danskin\u2019s theorem). Let us define\nh(\u03bb) := minw\u2208R f(w,\u03bb), wheref(w,\u03bb) := \u03bb\n2 w2 + bw+ c and \u03bb> 0.\nLet w\u22c6(\u03bb) be the minimum. The derivative off w.r.t. \u03bb is 1\n2 w2.\nFrom Danskin\u2019s theorem, we haveh\u2032(\u03bb) = 1\n2 w\u22c6(\u03bb). Let us check\nthat this result is correct. The derivative off w.r.t. w is \u03bbw+ b.\nSetting it to zero, we getw\u22c6(\u03bb) = \u2212b\n\u03bb. We thus obtainh\u2032(\u03bb) = 1\n2\nb2\n\u03bb2 .\nPlugging w\u22c6(\u03bb) back intof(w,\u03bb), we geth(\u03bb) = \u22121\n2\nb2\n\u03bb + c. Using\n( 1\n\u03bb)\u2032= \u22121\n\u03bb2 , we indeed obtain the same result forh\u2032(\u03bb).\nDanskin\u2019s theorem has a simple interpretation for functions that are\nlinear in\u03bbas shown below.\nExample 11.2(Convex conjugate). Let f(w,\u03bb) := \u27e8w,\u03bb\u27e9\u2212\u2126(w)\nwith \u2126 convex. We then haveh(\u03bb) = maxw\u2208W \u27e8w,\u03bb\u27e9\u2212\u2126(w) =:\n\u2126\u2217(\u03bb), where\u2126\u2217denotes the convex conjugate of\u2126. Sincef satisfies\nthe conditions of Danskin\u2019s theorem and since we have\u22072f(w,\u03bb) =\nw, we obtain\u2207h(\u03bb) = \u2207\u2126\u2217(\u03bb) = w\u22c6(\u03bb). In other words, in this\nspecial case, the gradient of the max is equal to the argmax. This\nis due to the fact thatf(w,\u03bb) is linear in\u03bb.\nAnother application is saddle point optimization.\nExample 11.3(Saddle point problem). Consider the saddle point\nproblem min\u03bb\u2208\u039b maxw\u2208Wf(w,\u03bb). If it is difficult to minimize w.r.t.\n\u03bbbut easy to maximize w.r.t.w, we can rewrite the problem as\nmin\u03bb\u2208\u039b h(\u03bb), whereh(\u03bb) := maxw\u2208Wf(w,\u03bb), and use\u2207h(\u03bb) to\nperform (projected) gradient descent w.r.t.\u03bb.\n11.2.2 Rockafellar\u2019s theorem\nA related theorem can be proved under different assumptions onf, in\nparticular without concavity w.r.t.w.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1851, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "25b33a9a-922e-4f48-a8d4-acfbdcc44cfb": {"__data__": {"id_": "25b33a9a-922e-4f48-a8d4-acfbdcc44cfb", "embedding": null, "metadata": {"page_label": "274", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ab5d00d9-4931-41f8-968d-6ac14ffec0ed", "node_type": "4", "metadata": {"page_label": "274", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "c4930ae2aa8d9dc364a9bf81bff39deb1bc806f5a021a766625fc83e5685c67e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "274 Differentiating through optimization\nTheorem 11.2(Rockafellar\u2019s envelope theorem). Let f: W\u00d7 \u039b \u2192\nR and Wbe a compact convex set. Let\nh(\u03bb) := max\nw\u2208W\nf(w,\u03bb)\nand\nw\u22c6(\u03bb) := arg max\nw\u2208W\nf(w,\u03bb).\nIf f is continuously differentiable in \u03bb for all w \u2208 W, \u22071f is\ncontinuous and the maximumw\u22c6(\u03bb) is unique, then the function\nh is differentiable with gradient\n\u2207h(\u03bb) = \u22072f(w\u22c6(\u03bb),\u03bb).\nSee Rockafellar and Wets (2009, Theorem 10.31). Compared to\nDanskin\u2019s theorem, Rockafellar\u2019s theorem does not requiref to be\nconcave-convex, but requires stronger assumptions on the differentiabil-\nity off.\n11.3 Implicit function theorem\n11.3.1 Univariate functions\nThe implicit function theorem (IFT) provides conditions under which\nan implicit relationship of the formF(w,\u03bb) = 0 can be rewritten as a\nfunction w= w\u22c6(\u03bb) locally, and provides a way to compute its derivative\nw.r.t. \u03bb.\nTheorem 11.3(Implicit function theorem, univariate case). Let\nF: R \u00d7R \u2192R. Assume F(w,\u03bb) is a continuously differentiable\nfunction in a neighborhoodUof (w0,\u03bb0) such thatF(w0,\u03bb0) = 0\nand \u22021F(w0,\u03bb0) \u0338= 0. Then there exists a neighborhoodV\u2286U of\n(w0,\u03bb0) in which there is a functionw\u22c6(\u03bb) such that\n\u2022 w\u22c6(\u03bb0) = w0,\n\u2022 F(w\u22c6(\u03bb),\u03bb) = 0 for all\u03bb in the neighborhoodV,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1213, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "279fb32f-3c93-4695-a44b-9e7fb5e9d9c4": {"__data__": {"id_": "279fb32f-3c93-4695-a44b-9e7fb5e9d9c4", "embedding": null, "metadata": {"page_label": "275", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f87b5543-43c6-4971-9eb3-31906a5f6a3f", "node_type": "4", "metadata": {"page_label": "275", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "3f3e7fad197d7bd8ab008ec1666849cda9e92241b91a1e2527b2215818b4dc13", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11.3. Implicit function theorem 275\nx\ny\n(1, 0)\n(0, 1)\n(x0, y0)\nx = 1 y2\nFigure 11.3:The circle equationF(x,y) := x2 + y2 \u22121 = 0 is not a function from\ny\u2208[\u22121,1] to x\u2208[\u22121,1], as there are always two possiblex values, x=\n\u221a\n1 \u2212y2 or\nx= \u2212\n\u221a\n1 \u2212y2. However, locally around some point(x0,y0), e.g., such thatx0 >0\nand y0 >0 (upper-right quadrant), the functionx= x\u22c6(y) =\n\u221a\n1 \u2212y2 is well-defined.\nThe implicit function theorem gives conditions for such a function to exist locally\nand provides a way to compute its derivative.\n\u2022 \u2202w\u22c6(\u03bb) = \u2212\u22022F(w\u22c6(\u03bb),\u03bb)\n\u22021F(w\u22c6(\u03bb),\u03bb) .\nWe postpone the proof to the multivariate case and begin with a\nclassical example of application of the theorem.\nExample 11.4(Equation of the unit circle). We usew\u2261xand \u03bb\u2261\ny for clarity. LetF(x,y) := x2 + y2 \u22121. In general, we cannot\nrewrite the unit circle equationF(x,y) = 0 as a function fromy\nto x, because for everyy \u2208[\u22121,1], there are always two possible\nx values, namely,x=\n\u221a\n1 \u2212y2 or x= \u2212\n\u221a\n1 \u2212y2. However, locally\naround some point (x0,y0), e.g., such thatx0 > 0 and y0 > 0\n(upper-right quadrant), the functionx= x\u22c6(y) =\n\u221a\n1 \u2212y2 is well-\ndefined. Using\u22021F(x,y) = 2xand \u22022F(x,y) = 2y, we get\u2202x\u22c6(y) =\n\u2212\u22022F(x\u22c6(y),y)\n\u22021F(x\u22c6(y),y) = \u2212 2y\n2x\u22c6(y) = \u2212 y\u221a\n1\u2212y2 inthatneighborhood(theupper\nright quadrant in this case). This is indeed the same derivative\nexpression as if we used the chain rule on\n\u221a\n1 \u2212y2 and is well-\ndefined ony\u2208[0,1).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1395, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3129273e-8317-42ee-822e-c6006bee143b": {"__data__": {"id_": "3129273e-8317-42ee-822e-c6006bee143b", "embedding": null, "metadata": {"page_label": "276", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fed9a6c7-2183-4a4a-a380-44b17676cf27", "node_type": "4", "metadata": {"page_label": "276", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "11e1547015da95c0bed4d7543225b105ab0b4eb3f97ea71547df1132e9c90ece", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "276 Differentiating through optimization\nIn the above simple example, we can easily derive an explicit function\nrelating y to x in a given neighborhood, but this is not always the case.\nThe IFT gives us conditions guaranteeing that such functionexists and\na way todifferentiate it, but not a way toconstruct such a function.\nIn fact, findingw\u22c6(\u03bb) such thatF(w\u22c6(\u03bb),\u03bb) = 0 typically involves a\nroot finding algorithm, an optimization algorithm, a nonlinear system\nsolver, etc.\nExample 11.5(Polynomial). Let F(w,\u03bb) = w5 + w3 + w\u2212\u03bb. Ac-\ncording to the Abel-Ruffini theorem (Tignol, 2015), quintics (poly-\nnomials of degree5) do no enjoy roots in terms of radicals and\none must resort to numerical root finding. In addition, odd-degree\npolynomials have real roots. Moreover,\u22021F(w,\u03bb) = 5w4 + 3w2 + 1\nis strictly positive. Therefore, by the intermediate value theorem,\nthere must be only one rootw\u22c6(\u03bb) such thatF(w\u22c6(\u03bb),\u03bb) = 0. This\nunique root can for example be found by bisection. Using the IFT,\nits derivative is found to be\u2202w\u22c6(\u03bb) = (5w\u22c6(\u03bb)4 + 3w\u22c6(\u03bb)2 + 1)\u22121.\nWhile an implicit function is differentiable at a point if the assump-\ntions of the IFT hold in a neighborhood of that point, the reciprocal is\nnot true: failure of the IFT assumptions does not necessarily mean that\nthe implicit function is not differentiable, as we now illustrate.\nExample 11.6(IFT conditions are not necessary for differentiability).\nConsider F(w,\u03bb) = (w\u2212\u03bb)2. We clearly have thatF(w\u22c6(\u03bb),\u03bb) = 0\nif we definew\u22c6(\u03bb) = \u03bb, the identity function. It is clearly differen-\ntiable for all\u03bb, yet the assumptions of the IFT fail, since we have\n\u22021F(w,\u03bb) = 2(w\u2212\u03bb) and therefore\u22021F(0,0) = 0.\n11.3.2 Multivariate functions\nWe now present the IFT in the general multivariate setting. Informally,\nif F(w\u22c6(\u03bb),\u03bb) = 0, then by the chain rule, we have\n\u22021F(w\u22c6(\u03bb),\u03bb)\u2202w\u22c6(\u03bb) + \u22022F(w\u22c6(\u03bb),\u03bb) = 0,\nmeaning that the Jacobian\u2202w\u22c6(\u03bb), assuming that it exists, satisfies\n\u2212\u22021F(w\u22c6(\u03bb),\u03bb)\u2202w\u22c6(\u03bb) = \u22022F(w\u22c6(\u03bb),\u03bb).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1948, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8319fefb-ad3e-4dc6-8e19-e48fbac048f2": {"__data__": {"id_": "8319fefb-ad3e-4dc6-8e19-e48fbac048f2", "embedding": null, "metadata": {"page_label": "277", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ac00b750-e85f-4271-abef-c92778783b27", "node_type": "4", "metadata": {"page_label": "277", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "2c61fc6d5d3326243bd3ba30496acca721ccb9120bd231aa588e86cfb14616e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11.3. Implicit function theorem 277\nThe IFT gives us conditions for the existence of\u2202w\u22c6(\u03bb).\nTheorem 11.4(Implicit function theorem, multivariate case). Letus\ndefine F: W\u00d7\u039b \u2192W. AssumeF(w,\u03bb) is a continuously differen-\ntiable function in a neighborhood of(w0,\u03bb0) such thatF(w0,\u03bb0) =\n0 and \u22021F(w0,\u03bb0) is invertible, i.e., its determinant is nonzero.\nThen there exists a neighborhood of\u03bb0 in which there is a function\nw\u22c6(\u03bb) such that\n\u2022 w\u22c6(\u03bb0) = w0,\n\u2022 F(w\u22c6(\u03bb),\u03bb) = 0 for all\u03bbin the neighborhood,\n\u2022 \u2212\u22021F(w\u22c6(\u03bb),\u03bb)\u2202w\u22c6(\u03bb) = \u22022F(w\u22c6(\u03bb),\u03bb)\n\u21d0\u21d2\u2202w\u22c6(\u03bb) = \u2212\u22021F(w\u22c6(\u03bb),\u03bb)\u22121\u22022F(w\u22c6(\u03bb),\u03bb).\nWe begin with a simple unconstrained optimization algorithm.\nExample 11.7(Unconstrained optimization). Assume we want to\ndifferentiate through w\u22c6(\u03bb) = arg minw\u2208RP f(w,\u03bb), where f is\nstrictly convex inw, which ensures that the solution is unique.\nFrom the stationary conditions, if we defineF(w,\u03bb) := \u22071f(w,\u03bb),\nthen w\u22c6(\u03bb) is uniquely characterized as the root of F in the\nfirst argument, i.e., F(w\u22c6(\u03bb),\u03bb) = 0. We have \u22021F(w,\u03bb) =\n\u22072\n1f(w,\u03bb), the Hessian off in w, and\u22022F(w,\u03bb) = \u22022\u22071f(w,\u03bb),\nthe cross derivatives off in w and \u03bb. Therefore, assuming that\nthe Hessian is well-defined and invertible at(w\u22c6(\u03bb),\u03bb), we can\nuse the IFT to differentiate throughw\u22c6(\u03bb) and obtain\u2202w\u22c6(\u03bb) =\n\u2212(\u22072\n1f(w\u22c6(\u03bb),\u03bb))\u22121\u22022\u22071f(w\u22c6(\u03bb),\u03bb).\nNext, we generalize the previous example, by allowing constraints\nin the optimization problem.\nExample 11.8(Constrained optimization). Now, assume we want\nto differentiate throughw\u22c6(\u03bb) = arg minw\u2208Cf(w,\u03bb), wheref is\nstrictly convex inw and C \u2286Wis a convex set. A solution is\ncharacterized by the fixed point equationw\u22c6(\u03bb) = PC(w\u22c6(\u03bb) \u2212", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1611, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "176a00cc-9caa-4a1c-8bd8-bc4f2411a485": {"__data__": {"id_": "176a00cc-9caa-4a1c-8bd8-bc4f2411a485", "embedding": null, "metadata": {"page_label": "278", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40dbb883-ecc4-4f7f-ac3a-694401f56fcb", "node_type": "4", "metadata": {"page_label": "278", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "998def6a129fe99c62e25f18906a105f901f0ab80ec73cafb3cfb3965c0aa9e2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "278 Differentiating through optimization\n\u03b7\u22071f(w\u22c6(\u03bb),\u03bb)),forany \u03b7 >0,where P(y) := arg minx\u2208C\u2225x\u2212y\u22252\n2\nis the Euclidean projection ofy onto C. Therefore,w\u22c6(\u03bb) is the\nroot ofF(w,\u03bb) = w\u2212PC(w\u2212\u03b7\u22071f(w,\u03bb)) (see Chapter 16). We\ncan differentiate throughw\u22c6(\u03bb) using the IFT, assuming that the\nconditions of the theorem apply. Note that\u22021F(w,\u03bb) requires\nthe expression of the Jacobian\u2202PC(y). Fortunately,PC(y) and its\nJacobian are easy to compute for many setsC(Blondel et al., 2021).\n11.3.3 JVP and VJP of implicit functions\nTo integrate an implicit functionw\u22c6(\u03bb) in an autodiff framework, we\nneed to be able to compute its JVP or VJP. This is the purpose of the\nnext proposition.\nProposition 11.1(JVP and VJP of implicit functions). Letw\u22c6: \u039b \u2192\nWbe a function implicitly defined as the solution ofF(w\u22c6(\u03bb),\u03bb) =\n0, for some functionF: W\u00d7 \u039b \u2192W. Define\nA:= \u2212\u22021F(w\u22c6(\u03bb),\u03bb)\nB := \u22022F(w\u22c6(\u03bb),\u03bb).\nAssume the assumptions of the IFT hold. The JVPt:= \u2202w\u22c6(\u03bb)v\nin the input directionv\u2208\u039b is obtained by solving the linear system\nAt= Bv.\nThe VJP\u2202w\u22c6(\u03bb)\u2217uin the output directionu\u2208W is obtained by\nsolving the linear system\nA\u2217r= u.\nUsing the solutionr, we get\n\u2202w\u22c6(\u03bb)\u2217u= \u2202w\u22c6(\u03bb)\u2217A\u2217r= B\u2217r.\nNote that in the above linear systems, we can access toA and B as\nlinear maps, the JVPs ofF. Their adjoints,A\u2217and B\u2217, correspond to\nthe VJPs ofF. To solve these systems, we can therefore usematrix-free\nsolvers as detailed in Section 9.4. For example, whenAis symmetric pos-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1429, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "171cd2ce-b2a5-409c-bff9-bc7c51011c05": {"__data__": {"id_": "171cd2ce-b2a5-409c-bff9-bc7c51011c05", "embedding": null, "metadata": {"page_label": "279", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fb5c9bb7-3512-4c6e-b1c9-cb6dc5efca8d", "node_type": "4", "metadata": {"page_label": "279", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "9830f6f06881ff5f9d613bb2ee6d6c68e44a2985dcdba9825a1c4a23a66d1a9f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11.3. Implicit function theorem 279\nitive semi-definite, we can use the conjugate gradient method (Hestenes,\nStiefel, et al., 1952). WhenAis not symmetric positive definite, we can\nuse GMRES (Saad and Schultz, 1986) or BiCGSTAB (Vorst and Vorst,\n1992).\n11.3.4 Proof of the implicit function theorem\nWe prove the theorem using the inverse function theorem presented\nin Theorem 11.5. Define\nf(\u03bb,w) = (\u03bb,F(w,\u03bb))\nwhich goes fromRQ \u00d7RP onto RQ \u00d7RP. The Jacobian off is\n\u2202f(\u03bb,w) =\n(\nI 0\n\u22022F(w,\u03bb) \u22021F(w,\u03bb)\n)\n.\nSo at w0,\u03bb0, we havedet(\u2202f(\u03bb0,w0)) = det(I) det(\u22021F(w0,\u03bb0)) >\n0 since we assumed\u22021F(w0,\u03bb0) invertible. By the inverse function\ntheorem, the function f is then invertible in a neighborhoodN of\nf(\u03bb0,w0) = (\u03bb0,0). In particular, it is invertible inN \u2229{(\u03bb,0),\u03bb\u2208\nRQ}. The solution of the implicit equation in a neighborhood of\u03bb0\nis then(\u03bb,w\u2217(\u03bb)) = f\u22121(\u03bb,0). By the inverse function theorem,f\u22121\nis continuously differentiable inverse and so isw\u2217(\u03bb). The derivative\n\u2202w\u2217(\u03bb) from the differential of the inverse as\n(\n\u223c \u223c\n\u2202w\u2217(\u03bb) \u223c\n)\n= \u2202f\u22121(\u03bb,0),\nandbytheinversefunctiontheorem,wehave \u2202f\u22121(\u03bb,0) = (\u2202f(\u03bb,w\u2217(\u03bb)))\u22121.\nSo using block matrix inversions formula\n(\nA B\nC D\n)\u22121\n=\n(\n\u223c \u223c\n\u2212(D\u2212CA\u22121B)\u22121CA\u22121 \u223c\n)\n,\nwe get the claimed expression. Though we expressed the proof in terms of\nJacobians and matrices, the result naturally holds for the corresponding\nlinear operators, JVPs, VJPs, and their inverses.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1386, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "675c36cd-9a80-45ca-99b6-7d2d45c8ca2d": {"__data__": {"id_": "675c36cd-9a80-45ca-99b6-7d2d45c8ca2d", "embedding": null, "metadata": {"page_label": "280", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f3a22cd8-3fbf-4dc4-9044-72336948ecce", "node_type": "4", "metadata": {"page_label": "280", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "5093f8d1af581fa1edd978da86a8db164abd35656ed297ce0d4bc186ab162dc6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "280 Differentiating through optimization\n11.4 Adjoint state method\n11.4.1 Differentiating nonlinear equations\nWe describe in this section the adjoint state method (a.k.a. adjoint\nmethod, method of adjoints, adjoint sensitivity method). The method\ncan be used to compute the gradient of the composition of an explicit\nfunction and an implicit function, defined through an equality\nconstraint (e.g., a nonlinear equation). The method dates back\nto C\u00e9a (1986).\nSuppose a variables\u2208S (which corresponds to astate in optimal\ncontrol) is implicitly defined given some parametersw\u2208W through\nthe (potentially nonlinear) equationc(s,w) = 0, wherec: S\u00d7W\u2192S .\nAssuming s is uniquely determined for allw \u2208W, this defines an\nimplicit function s\u22c6(w) from W to Ssuch that c(s\u22c6(w),w) = 0.\nGiven an objective functionL: S\u00d7W\u2192 R, the goal of the adjoint\nstate method is then to compute the gradient of\nL(w) := L(s\u22c6(w),w).\nHowever, this is not trivial ass\u22c6(w) is an implicit function. For instance,\nthis can be used to convert theequality-constrained problem\nmin\nw\u2208W\nL(s,w) s.t. c(s,w) = 0.\ninto theunconstrained problem\nmin\nw\u2208W\nL(s\u22c6(w),w).\nAccess to\u2207L(w) allows us to solve this problem by gradient descent.\nProposition 11.2(Adjoint state method). Let c: S\u00d7W\u2192S be a\nmapping defining constraints of the formc(s,w). Assume that for\neachw\u2208W, there exists a uniques\u22c6(w) satisfying c(s\u22c6(w),w) = 0\nand thats\u22c6(w) is differentiable. The gradient of\nL(w) := L(s\u22c6(w),w),\nfor some differentiable functionL: S\u00d7W\u2192 R, is given by\n\u2207L(w) = \u22072L(s\u22c6(w),w) + \u22022c(s\u22c6(w),w)\u2217r\u22c6(w),", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1538, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1367af17-1289-418f-8448-54066615ccd9": {"__data__": {"id_": "1367af17-1289-418f-8448-54066615ccd9", "embedding": null, "metadata": {"page_label": "281", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f1627189-3761-48b5-9304-b33ea5a90b52", "node_type": "4", "metadata": {"page_label": "281", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "c4d0ba2aadb32c3c889ce85b9f1d72964f6e4b264ea411294adc1cd78d5ffc03", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11.4. Adjoint state method 281\nwhere r\u22c6(w) is the solution of the linear system\n\u22021c(s\u22c6(w),w)\u2217r= \u2212\u22071L(s\u22c6(w),w).\nAs shown in the proof below,r\u22c6(w) corresponds to aLagrange\nmultiplier. The linear system can be solved using matrix-free solvers.\n11.4.2 Relation with envelope theorems\nBecause sis uniquely determined for anyw\u2208W byc(s,w) = 0, we can\nalternatively rewriteL(w) as the trivial minimization or maximization,\nL(w) = min\ns\u2208S\nL(s,w) s.t. c(s,w) = 0\n= max\ns\u2208S\nL(s,w) s.t. c(s,w) = 0.\nTherefore, the adjoint state method can be seen as an envelope theorem\nfor computing \u2207L(w), for the case whenw is involved inboth the\nobjective function and in theequality constraint.\n11.4.3 Proof using the method of Lagrange multipliers\nClassically, the adjoint state method is derived using the method of\nLagrange multipliers. Let us introduce theLagrangian associated with\nL and c,\nL(s,w,r) := L(s,w) + \u27e8r,c(s,w)\u27e9,\nwhere r\u2208S is theLagrange multiplierassociated with the equality\nconstraint c(s,w) = 0. In the optimal control literature,r is often\ncalled theadjoint variableor adjoint state. The gradients of the\nLagrangian are\n\u2207sL(s,w,r) = \u22071L(s,w) + \u22021c(s,w)\u2217r\n\u2207wL(s,w,r) = \u22072L(s,w) + \u22022c(s,w)\u2217r\n\u2207rL(s,w,r) = c(s,w),\nwhere \u2202ic(s,w)\u2217are theadjoint operators. Setting\u2207rL(s,w,r) to\nzero gives the constraintc(s,w) = 0. Setting\u2207sL(s,w,r) to zero gives\nthe so-calledadjoint state equation\n\u22021c(s,w)\u2217r= \u2212\u22071L(s,w).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1397, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "874a1eca-2d6a-4061-9c47-1948275bfdf6": {"__data__": {"id_": "874a1eca-2d6a-4061-9c47-1948275bfdf6", "embedding": null, "metadata": {"page_label": "282", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "82165b48-2776-4a83-be7b-e8ceec57907e", "node_type": "4", "metadata": {"page_label": "282", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "e8ec25f0d204d6ca577b1a2f7ebc01018b58191e459b4079609bf5abd657897d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "282 Differentiating through optimization\nSolving this linear system w.r.t.rats= s\u22c6(w) gives the adjoint variable\nr\u22c6(w). We then get\n\u2207L(w) = \u22072L(s\u22c6(w),w,r\u22c6(w))\n= \u22072L(s\u22c6(w),w) + \u22022c(s\u22c6(w),w)\u2217r\u22c6(w),\nwhich concludes the proof.\n11.4.4 Proof using the implicit function theorem\nA more direct proof is possible thanks to the implicit function theorem\n(Section 11.3). Using the chain rule, we get\n\u2207L(w) = \u22072L(s\u22c6(w),w) + \u2202s\u22c6(w)\u2217\u22071L(s\u22c6(w),w),\nwhere \u2202s\u22c6(w)\u2217is the VJP ofs\u22c6, a linear map fromSto W.\nComputationally, the main difficulty is to apply\u2202s\u22c6(w)\u2217 to the\nvector u= \u22071L(s\u22c6(w),w) \u2208S. Using the implicit function theorem\n(Section 11.3) on the implicit functionc(s\u22c6(w),w) = 0, and Proposi-\ntion 11.1, we get the linear systemA\u2217r= u, whereA\u2217:= \u22021c(s\u22c6(w),w)\u2217\nis a linear map fromSto S. After solving forr, we get\u2202s\u22c6(w)\u2217u= B\u2217r,\nwhere B\u2217:= \u22022c(s\u22c6(w),w)\u2217is a linear map fromSto W. Putting ev-\nerything together, we get\n\u2207L(w) = \u22072L(s\u22c6(w),w) + \u22022c(s\u22c6(w),w)\u2217r.\n11.4.5 Reverse mode as adjoint method with backsubstitution\nIn this section, we revisit reverse-mode autodiff from the perspective\nof the adjoint state method. For clarity, we focus our exposition on\nfeedforward networks with inputx \u2208X and network weightsw =\n(w1,..., wk) \u2208W1 \u00d7... \u00d7WK,\ns0 := x\u2208X\ns1 := f1(s0,w1) \u2208S1\n...\nsK := fK(sK\u22121,wK) \u2208SK\nf(w) := sK. (11.2)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1304, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f828bec8-92a4-455f-9e6b-df0c0023156e": {"__data__": {"id_": "f828bec8-92a4-455f-9e6b-df0c0023156e", "embedding": null, "metadata": {"page_label": "283", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "42cfe601-5767-410a-adbc-209738a671c5", "node_type": "4", "metadata": {"page_label": "283", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "e899dd1128f8abe9ba672084d19db9bc0041c26158d22f11274fc5d959fdee03", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11.4. Adjoint state method 283\nHere we focus on gradients with respect to the parametersw, hence\nthe notation f(w). We can use the adjoint state method to recover\nreverse-mode autodiff, and prove itscorrectness in the process. While\nwe focus for simplicity on feedforward networks, our exposition can be\ngeneralized to computation graphs.\nFeedforward networks as the solution of a nonlinear equation\nWhilewedefinedthesetofintermediatecomputations s= (s1,..., sK) \u2208\nS1 \u00d7... \u00d7SK as a sequence of operations, they can also be defined as\nthe unique solution of thenonlinear equationc(s,w) = 0, where\nc(s,w) :=\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\ns1 \u2212f1(x,w1)\ns2 \u2212f2(s1,w2)\n...\nsK \u2212fK(sK\u22121,wK)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n.\nThis defines animplicit functions\u22c6(w) = (s\u22c6\n1(w),..., s\u22c6\nK(w)), the so-\nlution of this nonlinear system, which is given by the variabless1,..., sK\ndefined in Eq. (11.2). The output of the feedforward network is then\nf(w) = s\u22c6\nK(w).\nIn machine learning, the final layers\u22c6\nK(w) is typically fed into a\nloss \u2113, to define\nL(w) := \u2113(s\u22c6\nK(w); y).\nNote that an alternative is to writeL(w) as\nL(w) = min\ns\u2208S\n\u2113(s; y) s.t. c(s,w) = 0.\nMore generally, if we just want to compute the VJP ofs\u22c6\nK(w) in\nsome directionuK \u2208SK, we can define the scalar-valued function\nL(w) := \u2113(s\u22c6\nK(w); uK) := \u27e8s\u22c6\nK(w),uK\u27e9\nso that\n\u2202f(w)\u2217uK = \u2207L(w).\nLet us defineu\u2208S1 \u00d7\u00b7\u00b7\u00b7\u00d7S K\u22121 \u00d7SK as u:= (0,..., 0,\u22071\u2113(f(w); y))\n(gradient of the loss\u2113 case) or u := (0,..., 0,uK) (VJP of f in the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1419, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4392a67f-6a2e-4ece-9654-4a4777de8699": {"__data__": {"id_": "4392a67f-6a2e-4ece-9654-4a4777de8699", "embedding": null, "metadata": {"page_label": "284", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b54f5097-cbf7-4e37-92f9-12fe10928cfa", "node_type": "4", "metadata": {"page_label": "284", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "321725668b75dc5cfd9aebde299dea6e737cd92a4f5399d3cbd006117742f759", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "284 Differentiating through optimization\ndirection uK case). Using the adjoint state method, we know that the\ngradient of this objective is obtained as\n\u2207L(w) = \u22022c(s(w),w)\u2217r\u22c6(w),\nfor r\u22c6(w) the solution of the linear system\n\u22021c(s(w),w)\u2217r= \u2212u.\nSolving the linear system using backsubtitution\nThe JVP of the constraint functioncat s\u22c6(w), materialized as a matrix,\ntakes the form of ablock lower-triangular matrix\n\u22021c(s\u22c6(w),w) =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nI 0 ... ... 0\n\u2212A1 I ... ...\n0 \u2212A2 I ... ...\n... ... ... ... 0\n0 ... 0 \u2212AK I\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n,\nwhere Ak := \u22021fk(sk\u22121,wk). Crucially the triangular structure of the\nJVP stems from the fact that each intermediate activation only de-\npends from the past intermediate activations. Therefore, the constraints,\ncorresponding to the lines of the Jacobian, cannot introduce non-zero\nvalues beyond its diagonal. The VJP takes the form of ablock upper-\ntriangular matrix\n\u22021c(s\u22c6(w),w)\u2217=\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nI \u2212A\u2217\n1 0 ... 0\n0 I \u2212A\u2217\n2\n... ...\n... ... I ... 0\n... ... ... \u2212A\u2217\nK\n0 ... ... 0 I\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n.\nSolving an upper triangular system like\u22021c(s(w),w)\u2217r= ucan then\nbe done efficiently bybacksubstitution. Starting from the last adjoint\nstate rK = u, we can compute each adjoint staterk from that computed\nat k+ 1. Namely, fork\u2208(K\u22121,..., 1), we have\nrk \u2212A\u2217\nk+1rk+1 = 0 \u21d0\u21d2rk = \u2202fk+1(sk,wk+1)\u2217rk+1.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1310, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5ed5da24-f655-4372-aa18-0327d6d9a8ee": {"__data__": {"id_": "5ed5da24-f655-4372-aa18-0327d6d9a8ee", "embedding": null, "metadata": {"page_label": "285", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ed604304-3672-4c9f-b030-4ef69890d838", "node_type": "4", "metadata": {"page_label": "285", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "88801a82fbaf0ab5e036e166b49069d83da4a45d664f39f83b387f4a6c0e968e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11.5. Inverse function theorem 285\nThe VJPs with respect to the parameters are then obtained by\n\u22022c(s(w),w)\u2217r=\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u22022f1(x,w1)\u2217r1\n\u22022f2(s1(w),w2)\u2217r2\n...\n\u22022fK(s1(w),wK)\u2217rK\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n,\nrecovering reverse-mode autodiff.\nThe Lagrangian perspective of backpropagation for networks with\nseparate parametersw= (w1,..., wK) is well-known; see for instance\nLeCun (1988) or Recht (2016). The Lagrangian perspective of back-\npropagation through time (Werbos, 1990) for networks with shared\nparameter wis discussed for instance by Franceschiet al.(2017). Our\nexposition uses the adjoint state method, which can itself be proved\neither using the method of Lagrange multipliers (Section 11.4.3) or by\nthe implicit function theorem (Section 11.4.4), combined with backsub-\ntitution for solving the upper-triangular linear system. Past works often\nminimize overwbut we do not require this, as gradients are not neces-\nsarily used for optimization. Our exposition also supports computing\nthe VJP of any vector-valued functionf, while existing works derive\nthe gradient of a scalar-valued loss function.\n11.5 Inverse function theorem\n11.5.1 Differentiating inverse functions\nIn some cases (see for instance Section 12.4.4), it is useful to compute\nthe Jacobian of an inverse functionf\u22121. The inverse function theorem\nbelow allows us to relate the Jacobian off\u22121 with the Jacobian off.\nTheorem 11.5(Inverse function theorem). Assume f: W\u2192W is\ncontinuously differentiable with invertible Jacobian\u2202f(w0) at w0.\nThen f is bijective from a neighborhood ofw0 to a neighborhood\nof f(w0). Moreover, the inversef\u22121 is continuously differentiable\nnear \u03c90 = f(w0) and the Jacobian of the inverse\u2202f\u22121(\u03c9) is\n\u2202f(w)\u2202f\u22121(\u03c9) = I \u21d4\u2202f\u22121(\u03c9) = (\u2202f(w))\u22121,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1715, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6ff136c8-a4a1-404a-9943-7732f50a9b5d": {"__data__": {"id_": "6ff136c8-a4a1-404a-9943-7732f50a9b5d", "embedding": null, "metadata": {"page_label": "286", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "01117d63-472d-4d1d-befb-7b50753e6d14", "node_type": "4", "metadata": {"page_label": "286", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "01134f2ae6f006a19a79fbe5cc3228a83af4376b5b32c05a1a9304143ad81612", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "286 Differentiating through optimization\nwith w= f\u22121(\u03c9).\n11.5.2 Link with the implicit function theorem\nThe inverse function theorem can be used to prove the implicit function\ntheorem; see proof of Theorem 11.4. Conversely, recall that, in order to\nuse the implicit function theorem, we need to choose a root objective\nF: W\u00d7 \u039b \u2192W . If we setW = \u039b = RQ and F(w,\u03c9) = f(w) \u2212\n\u03c9, with f: RQ \u2192RQ, then we have that the rootw\u22c6(\u03c9) satisfying\nF(w\u22c6(\u03c9),\u03c9) = 0 is exactlyw\u22c6(\u03c9) = f\u22121(\u03c9). Moreover,\u22021F(w,\u03c9) =\n\u2202f(w) and \u22022F(w,\u03c9) = \u2212I. By applying the implicit function theorem\nwith thisF, we indeed recover the inverse function theorem.\n11.5.3 Proof of inverse function theorem\nWe first give a proof of the formula assuming thatf\u22121 is well-defined\nand continuously differentiable in a neighborhood off(w0). In that\ncase, we have for any\u03c9in a neighborhood off(w0),\nf \u25e6f\u22121(\u03c9) = \u03c9.\nDifferentiating both sides w.r.t.\u03c9, we get\n\u2202f(f\u22121(\u03c9))\u2202f\u22121(\u03c9) = I,\nwhere I is the identity function inRQ. In particular, forw= f\u22121(\u03c9)\nwe recover the formula presented in the statement.\nNow, it remains to show that invertibility of the JVP ensures that\nthe function is invertible in a neighborhood off(w0) and that the\ninverse is continuously differentiable. For that, denotel= \u2202f(w0) such\nthat l\u22121 is well-defined by definition.f is invertible with continuously\ndifferentiable inverse, if and only ifl\u22121(f(w)) \u2212f(w0) is invertible with\ncontinuously differentiable inverse. So without loss of generality, we\nconsider \u2202f(w0) = I, f(w0) = 0, w0 = 0.\nAs f is continuously differentiable, there exists a neighborhood\nN = {w: \u2225w\u2212w0\u22252 \u2264\u03b4}on which we have\u2225\u2202f(w) \u2212I \u22252 \u22641/2. In\nthis neighborhood, the functiong(w) = f(w) \u2212wis contractive by the\nmean value theorem with contraction factor1/2. For any\u03c9such that", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1763, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "79db8bda-27f4-4a9a-aba4-95ba99bd7f2d": {"__data__": {"id_": "79db8bda-27f4-4a9a-aba4-95ba99bd7f2d", "embedding": null, "metadata": {"page_label": "287", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "013af61f-0a2f-4a18-98ef-6982616901c6", "node_type": "4", "metadata": {"page_label": "287", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "438798fe71d3d5fd74503e8983beaf90f1d4770f55f6c1a331e76cb83f7d02c7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11.6. Summary 287\n\u2225\u03c9\u2212f(w0)\u22252 \u2264\u03b4/2, the sequencewk+1 = wk\u2212f(wk) \u2212\u03c9\u2032remains in\nNand converges (since it is a Cauchy sequence by the contraction ofg)\nto a unique fixed pointw\u221e satisfying w\u221e= w\u221e\u2212f(w\u221e) \u2212\u03c9 \u21d0\u21d2\nf(w\u221e) = \u03c9. This shows the existence of the inverse in the neighborhood\nM = {\u03c9: \u2225\u03c9\u2212\u03c90\u22252 \u2264\u03b4/2}of \u03c90 = f(w0) onto N.\nWe tackle now the differentiability (hence the continuity) off\u22121.\nFor any\u03c9 in the neighborhood of\u03c90 with inversew:= f\u22121(\u03c9) \u2208N,\nthe JVP off at wsatisfies by assumption\u2225\u2202f(w) \u2212I \u22252 \u22641/2. Hence,\na= \u2202f(w) \u2212I defines a convergent seriesb= \u2211+\u221e\nk=0 ak and one verifies\neasily thatb= \u2202f(w)\u22121, that is\u2202f(w) is invertible and\u2225\u2202f(w)\u22121\u2225\u2264 2.\nTo compute the JVP of the inverse, we consider then\u2202f(w)\u22121 as the\ncandidate JVP and examine\n\u2225f\u22121(\u03c9+ \u03b7) \u2212f(\u03c9) \u2212(\u2202f(w))\u22121\u03b7\u22252\n\u2225\u03b7\u22252\n.\nDenote thenvsuch thatf\u22121(\u03c9+ \u03b7) = w+ v. Asg(w) = f(w) \u2212wis\n1/2-contractive inN, we have\u2225v\u2212\u03b7\u22252 = \u2225g(w+v)\u2212g(w)\u22252 \u22641/2\u2225v\u22252.\nSo \u2225v\u22252 \u2265\u2225\u03b7\u2225/2. We then get\n\u2225f\u22121(\u03c9+ \u03b7) \u2212f(\u03c9) \u2212(\u2202f(w))\u22121\u03b7\u22252\n\u2225\u03b7\u22252\n= \u2225v\u2212(\u2202f(w))\u22121(f(w+ v) \u2212f(w))\u22252\n\u2225\u03b7\u22252\n\u22644\u2225f(w+ v) \u2212f(w) \u2212\u2202f(w)v\u22252\n\u2225u\u22252\nAs\u2225\u03b7\u22252 \u21920,wehave \u2225v\u22252 \u21920 andso \u2225f(w+v)\u2212f(w)\u2212\u2202f(w)v\u22252/\u2225v\u22252 \u2192\n0. Hence, f\u22121 is differentiable with JVP \u2202f\u22121(\u03c9) = ( \u2202f(w))\u22121 =\n(\u2202f(f\u22121(\u03c9)))\u22121. This shows thatf\u22121 is continuous and so\u2202f\u22121(\u03c9) is\ncontinuous as a composition of continuous functions.\n11.6 Summary\n\u2022 Implicit functions are functions that cannot be decomposed into\nelementary operations and for which autodiff can therefore not\nbe directly applied. Examples are optimization problems and\nnonlinear equations.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1494, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "31d569e2-2a8c-4449-9c2c-401c094901f6": {"__data__": {"id_": "31d569e2-2a8c-4449-9c2c-401c094901f6", "embedding": null, "metadata": {"page_label": "288", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5310c3ff-082b-4199-a04a-13f542506f08", "node_type": "4", "metadata": {"page_label": "288", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "67819661d8845e42a0605e4ddd5fd7b9146e56f44c0b64e3501392f56e5b25b4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "288 Differentiating through optimization\n\u2022 Envelope theorems can be used for differentiating through the\nmin or max value (not solution) of a function.\n\u2022 More generally, the implicit function theorem allows us to dif-\nferentiate through implicit functions. It gives conditions for the\nexistence of derivatives and how to obtain them.\n\u2022 The adjoint state method can be used to obtain the gradient of\nthe composition of an explicit function and of an implicit function,\nspecified by equality constraints. It can be used to prove the\ncorrectness of reverse-mode autodiff.\n\u2022 The inverse function theorem can be used to differentiate function\ninverses.\n\u2022 In a sense, the implicit function theorem can be thought as the\nmother theorem, as it can be used to prove envelope theorems,\nthe adjoint state method and the inverse function theorem.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 834, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "558427bd-522b-4862-9e34-5af7433feceb": {"__data__": {"id_": "558427bd-522b-4862-9e34-5af7433feceb", "embedding": null, "metadata": {"page_label": "289", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ed12660-8fb3-4ebb-bbc1-16464737d75b", "node_type": "4", "metadata": {"page_label": "289", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "bd3ffc1f3c771e21605a6e6325d2b1d71d68c5c6061f873c3bf9363b3a9227fc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12\nDifferentiating through integration\nIn this chapter, we study how to differentiate through integrals, with a\nfocus on expectations and solutions of ordinary differential equations.\n12.1 Differentiation under the integral sign\nGiven two Euclidean spaces\u0398 and Y, and a functionf : \u0398 \u00d7Y\u2192 R,\nwe often want to differentiate an integral of the form\nF(\u03b8) :=\n\u222b\nY\nf(\u03b8,y)dy.\nProvided that we can swap integration and differentiation, we have\n\u2207F(\u03b8) =\n\u222b\nY\n\u2207\u03b8f(\u03b8,y)dy.\nThe conditions enabling us to do so are best examined in the context\nof measure theory. We refer the reader to e.g. (Cohn, 2013) for a course\non measure theory and Flanders (1973) for an in-depth study of the\ndifferentiation under the integral sign. Briefly, if\u0398 = Y = R, the\nfollowing conditions are sufficient.\n1. f is measurable in both its arguments, andf(\u03b8,\u00b7) is integrable\nfor almost all\u03b8\u2208\u0398 fixed,\n289", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 866, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1748f0ae-adc8-44e5-b8d2-1d66e082e4a1": {"__data__": {"id_": "1748f0ae-adc8-44e5-b8d2-1d66e082e4a1", "embedding": null, "metadata": {"page_label": "290", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af8f683f-ff06-4f40-b4c4-766dd47576de", "node_type": "4", "metadata": {"page_label": "290", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "a5f189668a3359bce62de2dd6b5a4a294cddb789e2796afcafca14c4c56170aa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "290 Differentiating through integration\n2. f(\u00b7,y) is absolutely continuous for almost all y \u2208 Y, that is,\nthere exists an integrable function g(\u00b7,y) such that f(\u03b8,y) =\nf(\u03b80,y) +\n\u222b\u03b8\n\u03b80 g(\u03c4,y)d\u03c4,\n3. \u22021f(\u03b8,y) (which exists almost everywhere iff(\u00b7,y) is absolutely\ncontinuous), is locally integrable, that is, for any closed interval\n[\u03b80,\u03b81], the integral\n\u222b\u03b81\n\u03b80\n\u222b\n|\u22021f(\u03b8,y)|dyd\u03b8 is finite.\nAny differentiable functionf : \u0398 \u00d7Y\u2192 R is absolutely continuous.\nHowever, the conditions also hold iff is just absolutely continuous, that\nis, iff(\u00b7,y) is differentiable for almost ally. This weaker assumption\ncan be used to smooth out differentiable almost-everywhere functions,\nsuch as the ReLu, as we study in Section 14.4.\n12.2 Differentiating through expectations\nA special case of differentiating through integrals is differentiating\nthrough expectations. We can distinguish between two cases, depending\non whether the parameters\u03b8we wish to differentiate are involved in\nthe distribution or in the function, whose expectation we compute.\n12.2.1 Parameter-independent distributions\nWe first consider expectations of the form\nF(\u03b8) := EY\u223cp[g(Y,\u03b8)] =\n\u222b\nY\ng(y,\u03b8)p(y)dy,\nfor a random variableY \u2208Y\u2286 RM, distributed according to a distri-\nbution p, and a functiong: Y\u00d7 \u0398 \u2192R. Importantly, the distribution\nis independent of the parameters\u03b8. Under mild conditions recalled\nin Section 12.1, we can swap differentiation and integration to obtain\n\u2207F(\u03b8) = \u2207\u03b8\n\u222b\nY\ng(y,\u03b8)p(y)dy\n=\n\u222b\nY\n\u2207\u03b8g(y,\u03b8)p(y)dy\n= EY\u223cp[\u2207\u03b8g(Y,\u03b8)].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1493, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d8fa2dd2-7e45-4adf-a33f-6e2116642ad9": {"__data__": {"id_": "d8fa2dd2-7e45-4adf-a33f-6e2116642ad9", "embedding": null, "metadata": {"page_label": "291", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3e5b8c62-32ec-45ab-a485-1855e9d663cc", "node_type": "4", "metadata": {"page_label": "291", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "5d9736cf4217fb04d901113911614984a7b23fb8710421a6bd46b7f6a106a829", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12.2. Differentiating through expectations 291\nGenerally, the expectation cannot be computed in closed form. However,\nprovided that we can sample fromp, we can define a Monte-Carlo\nestimator of the value\n\u02c6FN(\u03b8) := 1\nN\nN\u2211\ni=1\ng(Yi,\u03b8)\nand of the gradient\n\u2207\u02c6FN(\u03b8) = 1\nN\nN\u2211\ni=1\n\u2207\u03b8g(Yi,\u03b8),\nfor N i.i.d. samplesY1,...,Y N from p. These estimators are unbiased,\nmeaning thatE[ \u02c6FN(\u03b8)] = F(\u03b8) and E[\u2207\u02c6FN(\u03b8)] = \u2207F(\u03b8), and converge\nto the true quantity asN \u2192+\u221e. This suggests a simple implementation\nin an autodiff framework of the approximation of\u2207F(\u03b8):\n1. Sample y1,...,y n from p.\n2. Compute \u02c6FN(\u03b8) = 1\nn\n\u2211n\ni=1 g(yi,\u03b8).\n3. Compute the gradient\u2207\u02c6FN(\u03b8) by automatic differentiation.\nComputing higher order derivatives follow the same principle: to get\nan approximation of \u22072F(\u03b8), we can simply compute\u22072 \u02c6FN(\u03b8) by\nautodiff. As such, the implementation delineated above is akin to the\n\u201cdiscretize-then-optimize\u201d approach used to differentiate through the\nsolution of an ODE (Section 12.6): we implement an approximation of\nthe objective and simply call autodiff on it.\n12.2.2 Parameter-dependent distributions\nA more challenging case arises when the distribution depends on the\nparameters \u03b8:\nE(\u03b8) := EY\u223cp\u03b8[g(Y)] =\n\u222b\nY\ng(y)p\u03b8(y)dy,\nwhere Y \u2208Y \u2286RM is a random variable, distributed according to\na distribution p\u03b8 parameterized by \u03b8 \u2208\u0398 and where g: Y \u2192R is,\ndepending on the setting, potentially a blackbox function (i.e., we do not", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1420, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "22febf1d-08fc-4aad-8506-4f46e17328b1": {"__data__": {"id_": "22febf1d-08fc-4aad-8506-4f46e17328b1", "embedding": null, "metadata": {"page_label": "292", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b932d09b-ace6-446b-9a2c-310f54c38d73", "node_type": "4", "metadata": {"page_label": "292", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "a6d121aa3056d3c947c57cb6a604def995d1f78f1f4ea67c416fe207708a1b5f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "292 Differentiating through integration\nhave access to its gradients). Typically,\u03b8\u2208\u0398 could be parameters we\nwish to estimate, or it could indirectly be generated by\u03b8= f(x,w) \u2208\u0398,\nwhere f is a neural network with parametersw\u2208W we wish to estimate.\nThe main difficulty in computing\u2207E(\u03b8) stems from the fact that\u03b8\nare the parameters of the distributionp\u03b8. Estimating an expectation\nE(\u03b8) = EY\u223cp\u03b8[g(Y)] usingMonte-Carloestimationrequiresustosample\nfrom p\u03b8. However, it is not clear how to differentiateE w.r.t. \u03b8if \u03b8is\ninvolved in the sampling process.\nContinuous case\nWhen Y is a continuous set (that is,p\u03b8(y) is a probability density\nfunction), we can rewriteE(\u03b8) as\nE(\u03b8) =\n\u222b\nY\np\u03b8(y)g(y)dy.\nProvided that we can swap integration and differentiation (see Sec-\ntion 12.1), we then have\n\u2207E(\u03b8) = \u2207\u03b8\n\u222b\nY\np\u03b8(y)g(y)dy\n=\n\u222b\nY\n\u2207\u03b8p\u03b8(y)g(y)dy.\nUnfortunately, this integral is not an expectation and it could be in-\ntractable in general.\nDiscrete case\nWhen Yis a discrete set (that is,p\u03b8(y) is a probability mass function),\nwe can rewriteE(\u03b8) as\nE(\u03b8) =\n\u2211\ny\u2208Y\np\u03b8(y)g(y).\nWe then obtain\n\u2207E(\u03b8) =\n\u2211\ny\u2208Y\ng(y)\u2207\u03b8p\u03b8(y).\nAgain \u2207E(\u03b8) is not an expectation. We therefore cannot use Monte-\nCarlo estimation to estimate the gradient. Instead, we can compute it", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1230, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "644c77ec-6250-48c7-8872-dff2e4dd8692": {"__data__": {"id_": "644c77ec-6250-48c7-8872-dff2e4dd8692", "embedding": null, "metadata": {"page_label": "293", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f622ab2b-aef0-488a-b325-534dfbf2d40e", "node_type": "4", "metadata": {"page_label": "293", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "078354d58c392800e4faed2ac6e551839fd67469be5db6ffde5414defe8ce15a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12.2. Differentiating through expectations 293\nby brute force, i.e., by summing over all possibley\u2208Y. However, this is\nclearly only computationally tractable if|Y|is small or ifp\u03b8 is designed\nto have sparse support, i.e., so that the set{y \u2208Y : p\u03b8(y) \u0338= 0 }is\nsmall. Moreover, even if these conditions hold, summing overycould be\nproblematic ifg(y) is expensive to compute. Therefore, exact gradients\nare seldom used in practice.\nIn Sections 12.3 and 12.4, we review the score function and pathwise\ngradient estimators, to (approximately) compute\u2207E(\u03b8), allowing us\nto optimize\u03b8(or wusing the chain rule) by gradient-based algorithms.\n12.2.3 Application to expected loss functions\nDifferentiating through expectations is particularly useful when working\nwith expected loss functions of the form\nL(\u03b8; y) := E\u02c6Y\u223cp\u03b8\n[\u2113( \u02c6Y, y)],\nwhere yis some ground truth. Equivalently, we can set\u2113= \u2212r, where\nr is areward function. As we shall see, the score function estimator\nwill support a discrete loss function\u2113: Y\u00d7Y\u2192 R, while the pathwise\ngradient estimator will require a differentiable loss function\u2113: RM\u00d7Y\u2192\nR. Intuitively, L(\u03b8; y) will be low if p\u03b8 assigns high probability to\npredictions \u02c6ywith low loss value\u2113(\u02c6y,y).\nIn the classification setting, whereY= [M], p\u03b8 is often chosen to be\nthe Gibbs distribution, which is a categorical distribution induced\nby a softargmax\np\u03b8(y) := exp(\u03b8y)\u2211\ni\u2208[M] exp(\u03b8i) = [softmax(\u03b8)]y \u2208(0,1),\nwhere \u03b8y := f(x,y, w) \u2208R are logits produced by a neural networkf.\nMore generally, in the structured prediction setting, whereY\u2286 RM but\n|Y|\u226b M, we often use the distribution\np\u03b8(y) := exp(\u27e8\u03d5(y),\u03b8\u27e9)\u2211\ny\u2032\u2208Yexp(\u27e8\u03d5(y\u2032),\u03b8\u27e9),\nwhere \u03b8= f(x,w) \u2208RM.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1657, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "da9bb393-983c-406c-97b0-6de958ebc17b": {"__data__": {"id_": "da9bb393-983c-406c-97b0-6de958ebc17b", "embedding": null, "metadata": {"page_label": "294", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c19a2081-d274-41e5-865a-2788c35a50c1", "node_type": "4", "metadata": {"page_label": "294", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "57153c8e9ceae047dab90445b1f90c75a7492332d1763fd2bfbc93cf41310259", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "294 Differentiating through integration\nGiven a distribution\u03c1 over X\u00d7Y , we then want to minimize the\nexpected loss function, also known asrisk,\nR(w) := E(X,Y)\u223c\u03c1[L(f(X,w); Y)].\nTypically, minimizing R(w) is done through some form of gradient\ndescent, which requires us to be able to compute\n\u2207R(w) = E(X,Y)\u223c\u03c1[\u2207wL(f(X,w); Y)]\n= E(X,Y)\u223c\u03c1[\u22022f(x,w)\u2217\u2207L(f(X,w); Y)].\nComputing \u2207R(w) therefore boils down to computing the gradient of\nL(\u03b8; y), which is the gradient of an expectation.\n12.2.4 Application to experimental design\nIn experimental design, we wish to minimize a functiong(\u03bb), which we\nassume costly to evaluate. As an example, evaluatingg(\u03bb) could require\nus to run a scientific experiment with parameters\u03bb\u2208RQ. As another\nexample, in hyperparameter optimization, evaluatingg(\u03bb) would require\nus to run a learning algorithm with hyperparameters\u03bb\u2208RQ. Instead\nof solving the problemarg min\u03bb\u2208RQ g(\u03bb), we can lift the problem to\nprobability distributions and solvearg min\u03b8\u2208RM E(\u03b8), whereE(\u03b8) =\nE\u03bb\u223cp\u03b8[g(\u03bb)]. This requires the probability distributionp\u03b8 to assign\nhigh probability to\u03bbvalues that achieve smallg(\u03bb) value. Solving this\nproblem by stochastic gradient descent requires us to be able to compute\nestimates of\u2207E(\u03b8). This can be done for instance with SFE explained\nin Section 12.3, which does not require gradients ofg, unlike implicit\ndifferentiation explained in Chapter 11. This approach also requires us\nto choose a distributionp\u03b8 over \u03bb. For continuous hyperparameters,\na natural choice would be the normal distribution\u03bb\u223cNormal(\u00b5,\u03a3),\nsetting \u03b8= (\u00b5,\u03a3). Once we obtained\u03b8by minimizingE(\u03b8), we need a\nway to recover\u03bb. This can be done for example by choosing the mode of\nthe distribution, i.e.,arg max\u03bb\u2208RQ p\u03b8(\u03bb), or the mean of the distribution\nE\u03bb\u223cp\u03b8(\u03bb)[\u03bb]. Of course, in the case of the normal distribution, they\ncoincide.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1830, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f0047d2f-1670-4805-9119-d2265c0d9828": {"__data__": {"id_": "f0047d2f-1670-4805-9119-d2265c0d9828", "embedding": null, "metadata": {"page_label": "295", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4cdb16f1-42f7-4dd3-9f29-c7150a5fe163", "node_type": "4", "metadata": {"page_label": "295", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "1c1b222917b05a0932e8665e72b3d48e3eeb0f19480dc34d431f8442e52dbe3b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12.3. Score function estimators, REINFORCE 295\n12.3 Score function estimators, REINFORCE\n12.3.1 Scalar-valued functions\nThe key idea of thescore function estimator(SFE), also known as\nREINFORCE, is to rewrite\u2207E(\u03b8) as an expectation. The estimator is\nbased on thelogarithmic derivative identity\n\u2207\u03b8log p\u03b8(y) = \u2207\u03b8p\u03b8(y)\np\u03b8(y) \u21d0\u21d2\u2207\u03b8p\u03b8(y) = p\u03b8(y)\u2207\u03b8log p\u03b8(y).\nUsing this identity, we obtain the following gradient estimator.\nProposition 12.1(SFE for scalar-valued functions). Given a family\nof distributionsp\u03b8 on Y, for\u03b8\u2208\u0398, define\nE(\u03b8) := EY\u223cp\u03b8[g(Y)] =\n\u222b\nY\np\u03b8(y)g(y)dy,\nwhere Y \u2208Y\u2286 RM and g: Y\u2192 R. Then,\n\u2207E(\u03b8) = EY\u223cp\u03b8[g(Y)\u2207\u03b8log p\u03b8(Y)].\nProof.\n\u2207E(\u03b8) =\n\u222b\nY\n\u2207\u03b8p\u03b8(y)g(y)dy\n=\n\u222b\nY\np\u03b8(y)g(y)\u2207\u03b8log p\u03b8(y)dy\n= EY\u223cp\u03b8[g(Y)\u2207\u03b8log p\u03b8(Y)].\nThe gradient of the log-PDF w.r.t.\u03b8, \u2207\u03b8log p\u03b8(y), is known as the\nscore function, hence the estimator name. SFE is suitable when two\nrequirements are met: it is easy to sample fromp\u03b8 and the score function\nis available in closed form. Since the SFE gradient is an expectation,\nwe can use Monte-Carlo estimation to compute an unbiased estimator\nof \u2207E(\u03b8):\n\u2207E(\u03b8) \u2248\u02c6\u03b3N(\u03b8) := 1\nN\nN\u2211\ni=1\ng(Yi)\u2207\u03b8log p\u03b8(Yi), (12.1)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1123, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1f5590c4-ec79-4a3d-9120-7f7a41f6d3e5": {"__data__": {"id_": "1f5590c4-ec79-4a3d-9120-7f7a41f6d3e5", "embedding": null, "metadata": {"page_label": "296", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "06b0417f-ac73-4f91-80f6-2980f6551940", "node_type": "4", "metadata": {"page_label": "296", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "15150e877b9e6a59fd8dbfaaf6a79a779792cd203d916b004af2e2973747fdaa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "296 Differentiating through integration\nwhere Y1,...,Y N are sampled fromp\u03b8.\nInterestingly, the gradient of g is not needed in this estimator.\nTherefore, there is no differentiability assumption aboutg. This is why\nSFE is useful wheng is a discrete loss function or more generally a\nblackbox function.\nExample 12.1(SFE with a language model). In a language model,\nthe probability of a sentencey= (y1,...,y L) is typically factored\nusing the chain rule of probability (see Section 10.1)\np\u03b8(y) := p\u03b8(y1)p\u03b8(y2|y1) ...p \u03b8(yL|y1,...,y L\u22121),\nwhere p\u03b8 is modeled using a transformer or RNN. Note that the\nprobabilities are normalized by construction, so there is no need for\nan explicit normalization constant. Thanks to this factorization, it is\neasy to sample fromp\u03b8 using ancestral sampling (see Section 10.5.3)\nand the log-probability enjoys the simple expression\n\u2207\u03b8log p\u03b8(y) = \u2207\u03b8log p\u03b8(y1) + \u2207\u03b8log p\u03b8(y2|y1) + ...\n+ \u2207\u03b8log p\u03b8(yL|y1,...,y L\u22121).\nThis gradient is easy to compute, since the token-wise distributions\np\u03b8(yj|y1,...,y j\u22121) are typically defined using a softargmax. We can\ntherefore easily compute\u2207E(\u03b8) under p\u03b8 using SFE. This is for\ninstance useful to optimize an expected reward, in order to finetune\nor align a language model (Ziegleret al., 2019).\nAnother example when\u2207\u03b8p\u03b8(y) is available in closed form is in the\ncontext of reinforcement learning, wherep\u03b8(y) is a Markov Decision\nProcess (MDP) and is called the policy. Applying the SFE leads to the\n(vanilla) policy gradient method (Suttonet al., 1999) and can then be\nused to compute the gradient of an expected cumulative reward. How-\never, SFE is more problematic when used with the Gibbs distribution,\ndue to the explicit normalization constant.\nExample 12.2(SFE with a Gibbs distribution). The Gibbs distribu-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1777, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5cd3f0f6-fd29-474d-bb9c-550167fac11f": {"__data__": {"id_": "5cd3f0f6-fd29-474d-bb9c-550167fac11f", "embedding": null, "metadata": {"page_label": "297", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0fa73be9-e013-4cfb-8b21-71ab9a522688", "node_type": "4", "metadata": {"page_label": "297", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "ae5642aa1c880907230f7ac56abd25c0e0d87492a4b6c7bde2e515809d2259f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12.3. Score function estimators, REINFORCE 297\ntion is parameterized, for\u03b8\u2208RY,\np\u03b8(y) := exp(\u03b8y/\u03b3\u2212A(\u03b8)) = exp(\u03b8y/\u03b3)/exp(A(\u03b8))\nwhere we defined the log-partition function\nA(\u03b8) := log\n\u2211\ny\u2208Y\nexp(\u03b8y/\u03b3).\nA typical parametrization is\u03b8y = f(x,y, w) with f the output of\nnetwork on a samplexwith parametersw. We then have\nlog p\u03b8(y) = \u03b8y/\u03b3\u2212A(\u03b8),\nso that\n\u2207\u03b8log p\u03b8(y) = ey/\u03b3\u2212\u2207A(\u03b8).\nWe therefore see that\u2207\u03b8log p\u03b8(y) crucially depends on\u2207A(\u03b8), the\ngradient of the log-partition. This gradient is available for some\nstructured setsY, see e.g. (Mensch and Blondel, 2018), but not in\ngeneral.\nAs another example, we apply SFE in Section 14.4 to derive the\ngradient of perturbed functions.\nDifferentiating through both the distribution and the function\nSuppose both the distribution and the function now depend on\u03b8. When\ng is scalar-valued and differentiable w.r.t.\u03b8, we want to differentiate\nE(\u03b8) := EY\u223cp\u03b8[g(Y,\u03b8)].\nUsing the product rule, we obtain\n\u2207E(\u03b8) = EY\u223cp\u03b8[g(Y,\u03b8)\u2207\u03b8log p\u03b8(Y)] + EY\u223cp\u03b8[\u2207\u03b8g(Y,\u03b8)].\nDifferentiating through joint distributions\nSuppose we now want to differentiate through\nE(\u03b8) := EY1\u223cp\u03b8,Y2\u223cq\u03b8[g(Y1,Y2)].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1104, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cf498972-e055-4742-9c03-31f25d56e4a7": {"__data__": {"id_": "cf498972-e055-4742-9c03-31f25d56e4a7", "embedding": null, "metadata": {"page_label": "298", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "514eb90c-1cbe-4892-81d1-68464aa8022a", "node_type": "4", "metadata": {"page_label": "298", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "05d6e0fac2f8b5e22b017c019ff22205c0edc126e05b3d5a3177c756e600015e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "298 Differentiating through integration\nThe gradient is then given by\n\u2207E(\u03b8) = EY1\u223cp\u03b8,Y2\u223cq\u03b8[(\u2207\u03b8log p\u03b8(Y1) + \u2207log q\u03b8(Y2))g(Y1,Y2)],\nwhich is easily seen by applying Proposition 12.1 on the joint distribution\n\u03c1\u03b8 := p\u03b8\u00b7q\u03b8. The extension to more than two variables is straightforward.\n12.3.2 Variance reduction\nBias and variance\nRecall the definition of\u02c6\u03b3N in Eq. (12.1). SFE is anunbiasedestimator,\nmeaning that\n\u2207E(\u03b8) = E[\u02c6\u03b3N(\u03b8)],\nwhere the expectation is taken with respect to theN samples drawn.\nSince the gradient is vector-valued, we need to define a scalar-valued\nnotion of variance. We do so by using the squared Euclidean distance\nin the usual variance definition to define\nV[\u02c6\u03b3N(\u03b8)] := E[\u2225\u02c6\u03b3N(\u03b8) \u2212\u2207E(\u03b8)\u22252\n2]\n= E[\u2225\u02c6\u03b3N(\u03b8)\u22252\n2] \u2212\u2225\u2207E(\u03b8)\u22252\n2.\nThe variance naturally goes to zero asN \u2192\u221e.\nBaseline\nSFE is known to suffer fromhigh variance(Mohamed et al., 2020).\nThis means that this estimator may require us to draw many samples\nfrom the distributionp\u03b8 to work well in practice. One of the simplest\nvariance reduction technique consists in shifting the functiong with a\nconstant \u03b2, called abaseline, to obtain\n\u2207E(\u03b8) = EY\u223cp\u03b8[(g(Y) \u2212\u03b2)\u2207\u03b8log p\u03b8(Y)].\nThe reason this is still a valid estimator of\u2207E(\u03b8) stems from\nEY\u223cp\u03b8[\u2207\u03b8log p\u03b8(Y)] = EY\u223cp\u03b8\n[\u2207\u03b8p\u03b8(Y)\np\u03b8(Y)\n]\n= \u2207\u03b8EY\u223cp\u03b8[1]\n= \u2207\u03b81\n= 0,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1273, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d7a059f1-e149-41ad-a834-40e8c4b7ca19": {"__data__": {"id_": "d7a059f1-e149-41ad-a834-40e8c4b7ca19", "embedding": null, "metadata": {"page_label": "299", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a75ee081-c84c-46d1-8f9c-6fb9275c313b", "node_type": "4", "metadata": {"page_label": "299", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "6be37f540a70f1e5e69adf7a621d265561c31165de611b635389770d0f3c617f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12.3. Score function estimators, REINFORCE 299\nfor any valid distributionp\u03b8. The baseline\u03b2 is often set to the running\naverage of past values of the functiong, though it is neither optimal\nnor does it guarantee to lower the variance (Mohamedet al., 2020).\nControl variates\nAnother general technique arecontrol variates. Let us denote the\nexpectation of a functionh: RM \u2192R under the distributionp\u03b8 as\nH(\u03b8) := EY\u223cp\u03b8[h(Y)].\nSuppose thatH(\u03b8) and its gradient\u2207H(\u03b8) are known in closed form.\nThen, for any\u03b3 \u22650, we clearly have\nE(\u03b8) = EY\u223cp\u03b8[g(Y)]\n= EY\u223cp\u03b8[g(Y) \u2212\u03b3(h(Y) \u2212H(\u03b8))]\n= EY\u223cp\u03b8[g(Y) \u2212\u03b3h(Y)] + \u03b3H(\u03b8)\nand therefore\n\u2207E(\u03b8) = \u2207\u03b8EY\u223cp\u03b8[g(Y) \u2212\u03b3h(Y)] + \u03b3\u2207H(\u03b8).\nApplying SFE, we then obtain\n\u2207E(\u03b8) = EY\u223cp\u03b8[(g(Y) \u2212\u03b3h(Y))\u2207\u03b8log p\u03b8(Y)] + \u03b3\u2207H(\u03b8).\nExamples ofhinclude a bound onf or a second-order Taylor expansion\nof f, assuming that these approximations are easier to integrate thanf\n(Mohamed et al., 2020).\n12.3.3 Vector-valued functions\nIt is straightforward to extend the SFE to vector-valued functions.\nProposition 12.2(SFE for vector-valued functions). Given a family\nof distributionsp\u03b8 on Y, for\u03b8\u2208\u0398, define\nE(\u03b8) := EY\u223cp\u03b8[g(Y)] =\n\u222b\nY\np\u03b8(y)g(y)dy,\nwhere Y \u2208Y, g: Y\u2192G . The JVP ofE at \u03b8\u2208\u0398 along v\u2208\u0398 is\n\u2202E(\u03b8)v= EY\u223cp\u03b8[\u27e8\u2207\u03b8log p\u03b8(Y),v\u27e9g(Y)] \u2208G", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1226, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8853cbbc-65c8-4c0e-a552-2aaa57fcf7fe": {"__data__": {"id_": "8853cbbc-65c8-4c0e-a552-2aaa57fcf7fe", "embedding": null, "metadata": {"page_label": "300", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "07d3b489-6490-4da8-b93a-8591cf3ef247", "node_type": "4", "metadata": {"page_label": "300", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "9586af4086015661f77833c2554f368b8b8d0fb74e70e68e10f11041149e5d65", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "300 Differentiating through integration\nand the VJP ofE at \u03b8\u2208\u0398 along u\u2208G is\n\u2202E(\u03b8)\u2217u= EY\u223cp\u03b8[\u2207\u03b8log p\u03b8(Y)\u27e8u,g(Y)\u27e9] \u2208\u0398\nThe Jacobian ofE at \u03b8\u2208\u0398 can then be written as\n\u2202E(\u03b8) = EY\u223cp\u03b8[g(Y) \u2297\u2207\u03b8log p\u03b8(Y)],\nwhere \u2297denote the outer product.\nProof. The VJP ofE at \u03b8\u2208\u0398 along u\u2208\u0398 amounts to compute the\ngradient of the scalar function\n\u27e8E(\u03b8),u\u27e9= EY\u223cp\u03b8[\u27e8g(Y),u\u27e9]\nThe expression of the VJP follows by using the SFE on the scalar valued\nintegrand \u27e8g(Y),u\u27e9. The JVP is obtained as the adjoint operator of the\nVJP and the Jacobian follows.\nDifferentiating through both the distribution and the function\nIf \u03b8now influences both the distribution and the function,\nE(\u03b8) := EY\u223cp\u03b8[g(Y,\u03b8)],\nthen, we obtain\n\u2202E(\u03b8) = EY\u223cp\u03b8[g(Y,\u03b8) \u2297\u2207\u03b8log p\u03b8(Y)] + EY\u223cp\u03b8[\u2202\u03b8g(Y,\u03b8)].\n12.3.4 Second derivatives\nUsing the previous subsection withg(y,\u03b8) = g(y)\u2207\u03b8log p\u03b8(\u03b8), we easily\nobtain an estimator of the Hessian.\nProposition 12.3(SFE for the Hessian). Let us define the scalar-\nvalued functionE(\u03b8) := EY\u223cp\u03b8[g(Y)]. Then,\n\u22072E(\u03b8) =EY\u223cp\u03b8[g(Y)\u2207\u03b8log p\u03b8(Y) \u2297\u2207\u03b8log p\u03b8(Y)]+\nEY\u223cp\u03b8[g(Y)\u22072\n\u03b8log p\u03b8(Y)].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1043, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e2f5e2c0-454f-431a-91c6-2ecacfb20c17": {"__data__": {"id_": "e2f5e2c0-454f-431a-91c6-2ecacfb20c17", "embedding": null, "metadata": {"page_label": "301", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "918dd188-3baa-4b8c-9a6b-79fc1e6b307a", "node_type": "4", "metadata": {"page_label": "301", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "9c4ec1d70a5feeb301562ff044bf0719c0dfee3c41d247d4fab9895727f18afd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12.4. Path gradient estimators, reparametrization trick 301\nThis can also be derived using the second-order log-derivative\n\u22072\n\u03b8log p\u03b8(y) = 1\np\u03b8(y)\u22072\n\u03b8p\u03b8(y) \u2212 1\np\u03b8(y)2 \u2207\u03b8p\u03b8(y) \u2297\u2207\u03b8p\u03b8(y)\nso that\n\u22072\n\u03b8p\u03b8(y) = p\u03b8(y)\n[\n\u22072\n\u03b8log p\u03b8(y) + \u2207\u03b8log p\u03b8(y) \u2297\u2207\u03b8log p\u03b8(y)\n]\n.\nLink with the Bartlett identities\nThe Bartlett identities are expressions relating the moments of the score\nfunction (gradient of the log-likelihood function). Using Proposition 12.1\nwith g(y) = 1 and\n\u222b\nYp\u03b8(y)dy= 1, we obtain\nEY\u223cp\u03b8[\u2207\u03b8log p\u03b8(Y)] = 0, (12.2)\nwhich is known as Bartlett\u2019s first identity. Similarly, using Proposi-\ntion 12.3, we obtain\nEY\u223cp\u03b8[\u22072\n\u03b8log p\u03b8(Y)] + EY\u223cp\u03b8[\u2207\u03b8log p\u03b8(Y) \u2297\u2207\u03b8log p\u03b8(Y)]\n=EY\u223cp\u03b8[\u22072\n\u03b8log p\u03b8(Y)] + cov[logp\u03b8(Y)]\n=0,\n(12.3)\nwhich is known as Bartlett\u2019s second identity.\n12.4 Path gradient estimators, reparametrization trick\nAs we saw previously, the main difficulty in computing gradients of\nexpectations arises when the parameters\u03b8play a role in the distribution\np\u03b8 being sampled. The key idea of path gradient estimators (PGE),\nalso known as reparametrization trick, is to rewrite the expectation in\nsuch a way that the parameters are moved from the distribution to the\nfunction, using achange of variable.\n12.4.1 Location-scale transforms\nThe canonical example of path gradient estimator is differentiating\nthrough the expectation\nE(\u00b5,\u03c3) := EU\u223cNormal(\u00b5,\u03c32)[g(U)],", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1353, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "377c6b41-04ec-41fb-ad24-ed0801ca3916": {"__data__": {"id_": "377c6b41-04ec-41fb-ad24-ed0801ca3916", "embedding": null, "metadata": {"page_label": "302", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "91bf0ea9-93ea-4bcf-9afe-d2542d1fc4d7", "node_type": "4", "metadata": {"page_label": "302", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "d3549affad92caa02c4548bf4fc02ad030cb133fddebaddd6e162dfb9b37d3b4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "302 Differentiating through integration\nwhere g: R \u2192R is a differentiable function. If we letZ \u223cNormal(0,1),\nit is easy to check thatU = \u00b5+ \u03c3Z. We can therefore write\nE(\u00b5,\u03c3) = EZ\u223cNormal(0,1)[g(\u00b5+ \u03c3Z)].\nThe key advantage is that we can now easily compute the derivatives\nby mere application of the chain rule, since the parameters\u00b5and \u03c3 are\nmoved from the distribution to the function:\n\u2202\n\u2202\u00b5E(\u00b5,\u03c3) = EZ\u223cNormal(0,1)[g\u2032(\u00b5+ \u03c3Z)]\n\u2202\n\u2202\u03c3E(\u00b5,\u03c3) = \u03c3\u00b7EZ\u223cNormal(0,1)[g\u2032(\u00b5+ \u03c3Z)].\nThe change of variable\nU := \u00b5+ \u03c3Z (12.4)\nis called alocation-scale transform. Such a transformation exists,\nnot only for the normal distribution, but forlocation-scale family\ndistributions, i.e., distributions parametrized by a location parameter\u00b5\nand a scale parameter\u03c3 >0, such thatU is distributed according to a\ndistribution in the same family asZ is distributed. Besides the normal\ndistribution, examples of location-scale family distributions include the\nCauchy distribution, the uniform distribution, the logistic distribution,\nthe Laplace distribution, and Student\u2019st-distribution.\nWe can easily relate the cumulative distribution function (CDF)\nand the probability density function (PDF) ofZ to that ofU, and\nvice-versa.\nProposition 12.4(CDF and PDF of location-scale family distributions).\nLet FZ(z) := P(Z \u2264z) and fZ(z) := F\u2032\nZ(z). IfU := \u00b5+ \u03c3Z, then\nFZ(z) = FU(\u00b5+ \u03c3z) \u21d0\u21d2FU(u) = FZ\n(u\u2212\u00b5\n\u03c3\n)\nfZ(z) = \u03c3fU(\u00b5+ \u03c3z) \u21d0\u21d2fU(u) = 1\n\u03c3fZ\n(u\u2212\u00b5\n\u03c3\n)\n.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1413, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dd355122-7365-4853-b418-a469c5a34456": {"__data__": {"id_": "dd355122-7365-4853-b418-a469c5a34456", "embedding": null, "metadata": {"page_label": "303", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1598ded2-4a38-49a2-a027-6b7c724dd361", "node_type": "4", "metadata": {"page_label": "303", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "f35cbf7305cf917bd6fbc9ab3b5d984e9547179ac745748aece4741732d593d9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12.4. Path gradient estimators, reparametrization trick 303\nProof. We have\nFZ(z) = P(Z \u2264z)\n= P\n(U \u2212\u00b5\n\u03c3 \u2264z\n)\n= P(U \u2264\u00b5+ \u03c3z)\n= FU(\u00b5+ \u03c3z)\nand we obtainfZ(z) by differentiatingFZ(z).\n12.4.2 Differentiable transforms\nWe can generalize the idea of path gradient estimator (PGE) to any\nchange of variable\nU := T(Z,\u03b8),\nwhere T: RM \u00d7RQ \u2192RM is a differentiable transformation. For exam-\nple, if we gather\u00b5 and \u03c3 as \u03b8:= (\u00b5,\u03c3), we can write the location-scale\ntransform as\nU = T(Z,\u03b8) = \u00b5+ \u03c3Z.\nWe can derive the path gradient estimator for any such differentiable\ntransformation T.\nProposition 12.5(Path gradient estimator). Let us define\nE(\u03b8) := EU\u223cp\u03b8[g(U)],\nwhere U \u2208U \u2286RM and g: RM \u2192R is differentiable. Suppose\nthere is a differentiable transformationT: RM \u00d7RQ \u2192RM such\nthat ifZ \u223cp (where p does not depend on\u03b8) andU := T(Z,\u03b8),\nthen U \u223cp\u03b8. Then, we have\nE(\u03b8) = EZ\u223cp[h(Z,\u03b8)] = EZ\u223cp[g(T(Z,\u03b8))],", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 882, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "23b7d867-013f-4db8-9799-801b4004a0aa": {"__data__": {"id_": "23b7d867-013f-4db8-9799-801b4004a0aa", "embedding": null, "metadata": {"page_label": "304", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e05d6d91-6042-4560-b39f-60c85b3f6f8f", "node_type": "4", "metadata": {"page_label": "304", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "b5f6827bad9e3c258647ae2f4375e121a0eba1f9ec4e1b3f8b3c179ede1c6a7a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "304 Differentiating through integration\nwhere h(z,\u03b8) := g(T(z,\u03b8)). This implies\n\u2207E(\u03b8) = EZ\u223cp[\u22072h(Z,\u03b8)]\n= EZ\u223cp[\u22022T(Z,\u03b8)\u2217\u2207g(T(Z,\u03b8))].\nThe path gradient estimator (a.k.a. reparametrization trick) gives an\nunbiased estimatorof \u2207E(\u03b8). It has however two key disadvantages.\nFirst, it assumes thatg is differentiable (almost everywhere), which\nmay not always be the case. Second, it assumes thatg is well-defined\non RM, not onU, which could be problematic for some discrete loss\nfunctions, such as the zero-one loss function or ranking loss functions.\nAs an example of differentiable transform, in machine learning, we\ncan sample Gaussian noiseZ and make it go through a neural network\nwith parametersw to generate an imageX := T(Z,w). In statistics,\nmany distributions are related to each other through differentiable\ntransforms, as we recall below.\nExample 12.3(Some differentiable transforms in statistics). Wegive\nbelow a non-exhaustive list of differentiable transform examples.\n\u2022 If X \u223cNormal(\u00b5,\u03c32), thenexp(X) \u223cLognormal(\u00b5,\u03c32).\n\u2022 If U \u223cUniform(0,1), then\u2212log(U)/\u03bb\u223cExponential(\u03bb).\n\u2022 If X1,...,X N \u223c Exponential(\u03bb) (i.i.d.), then \u2211N\ni=1 Xi \u223c\nGamma(N,\u03bb).\n\u2022 IfXi \u223cGamma(\u03b1i,\u03b8) fori\u2208[K],then\n(\nX1\u2211K\ni=1 Xi\n,..., XK\u2211K\ni=1 Xi\n)\n\u223c\nDirichlet(\u03b11,...,\u03b1 K).\n12.4.3 Inverse transforms\nThe inverse transform method can be used for sampling from a proba-\nbility distribution, given access to its associatedquantile function.\nRecall that the cumulative distribution function (CDF) associated with\na random variableY is the functionFY : R \u2192[0,1] defined by\nFY(y) := P(Y \u2264y).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1556, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0f8ac128-806e-48e2-84af-b9c27981ada7": {"__data__": {"id_": "0f8ac128-806e-48e2-84af-b9c27981ada7", "embedding": null, "metadata": {"page_label": "305", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d43c9a0c-6dea-4d6c-ab58-41537e895770", "node_type": "4", "metadata": {"page_label": "305", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "21f0bae4bc488b102814d150cf5d4f4364c5228b9200c13d8e592401d5245547", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12.4. Path gradient estimators, reparametrization trick 305\nThe quantile function is then a functionQY : [0,1] \u2192R such that\nQY(\u03c0) = y for \u03c0 = FY(y). Assuming FY is continuous and strictly\nincreasing, we have thatQY is theinverse CDF,\nQY(\u03c0) = F\u22121\nY (\u03c0).\nIn the general case of CDF functions that are not strictly increasing,\nthe quantile function is usually defined as\nQY(\u03c0) := inf{y\u2208R: \u03c0\u2264FY(y)}.\nGiven access to the quantile functionQY(\u03c0) associated with a distribu-\ntion p, inverse transform sampling allows us to sample fromp by first\ndrawing a sample from theuniform distributionand then making\nthis sample go through the quantile function.\nProposition 12.6(Inverse transform sampling). Suppose Y \u223cp,\nwhere p is a distribution with quantile function QY. If U \u223c\nUniform(0,1), thenQY(U) \u223cp.\nProof. If \u03c0\u2264FY(t), then by definition ofQY, QY(\u03c0) \u2264t. If\u03c0\u2265FY(t),\nthen by definition ofQY, FY(QY(\u03c0)) \u2265\u03c0, soFY(QY(\u03c0)) \u2265FY(t) and\nsince a CDF is always non-decreasing,QY(\u03c0) \u2265t. Hence, we have,\nQY(\u03c0) \u2264t \u21d0\u21d2\u03c0\u2264FY(t), so\nP(QY(U) \u2264t) = P(U \u2264FY(t))\n= FY(t).\nThe CDFs ofQY(U) and Y coincide, hence they have the same distri-\nbution.\nIf the quantile function is differentiable, we can therefore use it\nas atransformation within thereparametrization trick. Indeed,\nif Y \u223cp\u03b8, wherep\u03b8 is a distribution with parameter\u03b8 and quantile\nfunction QY(\u03c0,\u03b8), then we have\nE(\u03b8) = EY\u223cp\u03b8[g(Y)] = E\u03c0\u223cUniform(0,1)[g(QY(\u03c0,\u03b8))]\nand therefore, by the reparametrization trick (Proposition 12.5),\n\u2207E(\u03b8) = E\u03c0\u223cUniform(0,1)[\u22022QY(\u03c0,\u03b8)\u2217\u2207g(QY(\u03c0,\u03b8))].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1502, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6d5f1b0b-159f-44ec-9c58-2e12edc4895b": {"__data__": {"id_": "6d5f1b0b-159f-44ec-9c58-2e12edc4895b", "embedding": null, "metadata": {"page_label": "306", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e070b5ba-5842-4e1f-888d-adebce7e46ea", "node_type": "4", "metadata": {"page_label": "306", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "2d39dbdc97b02b1ccb2057f547df734a7446da7145425f60855e954a1d6fcb2c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "306 Differentiating through integration\nExample 12.4(Examples of quantile functions). If\nY \u223cExponential(\u03bb),theCDFof Y is\u03c0= FY(y) = 1\u2212exp(\u2212\u03bby) for\ny\u22650 and therefore the quantile function isQY(\u03c0,\u03bb) = \u2212log(1\u2212\u03c0)\n\u03bb .\nIf Y \u223cNormal(\u00b5,\u03c32), the CDF isFY(y) = 1\n2\n[\n1 + erf\n(\ny\u2212\u00b5\n\u03c3\n\u221a\n2\n)]\nand\nthe quantile function isQY(\u03c0,\u03b8) = \u00b5+ \u03c3\n\u221a\n2 \u00b7erf\u22121(2\u03c0\u22121), where\n\u03b8= (\u00b5,\u03c3). This therefore defines an alternative transformation to\nthe location-scale transformation in Eq. (12.4).\nNote that, in the above example, the error functionerf and its\ninverse do not enjoy analytical expressions but autodiff packages usually\nprovide numerical routines to compute them and differentiate through\nthem. Nonetheless, one caveat of the inverse transform is that it indeed\nrequires access to (approximations of) the quantile function and its\nderivatives, which may be difficult for complicated distributions.\n12.4.4 Pushforward operators\nPushforward distributions\nWe saw so far that the reparametrization trick is based on using a\nchange of variables in order to differentiate an expectation w.r.t. the\nparameters of the distribution. In this section, we further formalize that\napproach using pushforward distributions.\nDefinition 12.1(Pushforward distribution). Suppose Z \u223cp, where\np is a distribution overZ. Given a continuous mapT: Z \u2192U,\nthe pushforward distribution ofp through T is the distributionq\naccording to whichU := T(Z) \u2208U is distributed, i.e.,U \u223cq.\nAlthough not explicit in the above, the transformationT can depend\non some learnable parameters, for example ifT is a neural network.\nIntuitively, the pushforward distribution is obtained by moving the\nposition of all the points in the support ofp. We give a few examples\nbelow.\n\u2022 Inverse transform sampling studied in Section 12.4.3 can be seen\nas performing the pushforward of the uniform distribution through\nT = Q, whereQ is the quantile function.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1881, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c98d1768-63c2-40e5-81bf-bc03a8063109": {"__data__": {"id_": "c98d1768-63c2-40e5-81bf-bc03a8063109", "embedding": null, "metadata": {"page_label": "307", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6bab5e62-71f0-4f1d-a0c4-6ea43aa7a949", "node_type": "4", "metadata": {"page_label": "307", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "f9b74f587d8ff86fce2461c725cc6737d6c0e29235ac96d4a3eac4d7cb287890", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12.4. Path gradient estimators, reparametrization trick 307\n\u2022 The Gumbel trick studied in Section 14.5 can be seen as a the push-\nforward of Gumbel noise throughT = argmax (a discontinuous\nfunction).\n\u2022 Gumbel noise can itself be obtained by pushing forward the\nuniform distribution throughT = \u2212log(\u2212log(\u00b7)) (Remark 14.3).\n\u2022 In a generative modeling setting, as we mentioned previously, we\nuse the pushforward of Gaussian noise through a parametrized\ntransformation X = T(Z,w) called a generator, typically a neural\nnetwork.\n\u2022 It is possible to define distributions over (sparse) probability\nvectors by sampling then projecting (Farinhaset al., 2021).\nA crucial aspect of the pushforward distributionq is that it can be\nimplicitly defined, meaning that we do not necessarily need to know\nthe explicit form of the associated PDF. In fact, it is easy to tosample\nfrom q, provided that it is easy to sample fromp:\nU \u223cq \u21d0\u21d2Z \u223cp,U := T(Z).\nHence the usefulness of the pushforward distribution ingenerative\nmodeling. Furthermore, ifp has associated PDFpZ, we can compute\nthe expectation of a functionf according toq as\nEU\u223cq[f(U)] = EZ\u223cp[f(T(Z))] =\n\u222b\nZ\nf(T(z))pZ(z)dz,\neven though we do not know the explicit form of the PDF ofq.\nPushforward measures\nMore generally, we can define the notion of pushforward, in the language\nof measures. DenoteM(Z) the set of measures on a setZ. Ameasure\n\u03b1 \u2208M(Z), that has a densityd\u03b1(z) := pZ(z)dz, can be integrated\nagainst a funtcionf as\n\u222b\nZ\nf(z)d\u03b1(z) =\n\u222b\nZ\nf(z)pZ(z)dz.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1497, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "472b41d6-f007-4ac5-9405-68134254eb19": {"__data__": {"id_": "472b41d6-f007-4ac5-9405-68134254eb19", "embedding": null, "metadata": {"page_label": "308", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b867c3e-4c02-4595-b142-bc52a060fa2f", "node_type": "4", "metadata": {"page_label": "308", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "14fb36581132481e636141053f7cae81dfd958e3e75dec0201988ab5aaf299b0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "308 Differentiating through integration\nA measure\u03b1is called a probability measure if it is positive and satisfies\n\u03b1(Z) =\n\u222b\nZd\u03b1(z) =\n\u222b\nZpZ(z)dz = 1 . See Peyr\u00e9 and Cuturi (2019,\nChapter 2) for a concise introduction.\nDefinition 12.2(Pushforward operator and measure). Given a con-\ntinuous mapT: Z\u2192U and some measure\u03b1\u2208M(Z), the push-\nforward measure\u03b2 = T\u266f\u03b1\u2208M(U) is such that for all continuous\nfunctions f \u2208C(U)\n\u222b\nU\nf(u)d\u03b2(u) =\n\u222b\nZ\nf(T(z))d\u03b1(z).\nEquivalently, for any measurable setA\u2282U , we have\n\u03b2(A) = \u03b1({z\u2208Z: T(z) \u2208A}) = \u03b1(T\u22121(A)),\nwhere T\u22121(A) = {z\u2208Z : T(z) \u2208A}.\nImportantly, the pushforward operator preserves positivity and mass,\ntherefore if\u03b1is a probability measure, then so isT\u266f\u03b1. The pushforward\nof a probability measure therefore defines a pushforward distribution\n(since a distribution can be parametrized by a probability measure).\n12.4.5 Change-of-variables theorem\nWe saw that a pushforward distribution associated with a variableU\nis implicitly defined through a transformU := T(Z) and can be easily\nsampled from as long as it is easy to sampleZ. However, in some\napplications (e.g., density estimation), we may want to know the PDF\nassociated with U. Assuming the transformT is invertible, we have\nZ = T\u22121(U) and therefore forA\u2286U , we have\nP(U \u2208A) = P(Z \u2208T\u22121(A)) =\n\u222b\nT\u22121(A)\npZ(z)dz.\nUsing thechange-of-variables theoremfrom multivariate calculus,\nassuming T\u22121 is available, we can give an explicit formula for the PDF\nof the pushforward distribution, see e.g. (Schwartz, 1954; Taylor, 2002).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1505, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1f72c991-109e-4a33-9699-13c28c83e122": {"__data__": {"id_": "1f72c991-109e-4a33-9699-13c28c83e122", "embedding": null, "metadata": {"page_label": "309", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "02a5fecb-4917-482e-aac8-2f164068b592", "node_type": "4", "metadata": {"page_label": "309", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "5b5b051a0ee493d676f4907b94f0976d591d5b797c439830c6c3519ee4aa5b96", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12.5. Stochastic programs 309\nProposition 12.7(PDF of the pushforward distribution). Suppose\nZ \u223cp, where p is a distribution overZ, with PDF pZ. Given\na diffeomorphism T: Z \u2192U (i.e., an invertible and differen-\ntiable map), the pushforward distribution ofp through T is the\ndistribution q such thatU := T(Z) \u223cq and its PDF is\nqU(u) = |det(\u2202T\u22121(u))|pZ(T\u22121(u)),\nwhere \u2202T\u22121(u) is the Jacobian ofT\u22121 : U\u2192Z .\nUsing this formula, we obtain\nP(U \u2208A) =\n\u222b\nA\npU(u)du\n=\n\u222b\nA\n|det(\u2202T\u22121(u))|pZ(T\u22121(u))du.\nUsing the inverse function theorem (Theorem 11.5), we then have\n\u2202T\u22121(u) = (\u2202T(T\u22121(u)))\u22121,\nunder the assumption that T(z) is continuously differentiable and\nhas invertible Jacobian\u2202T(z). Normalizing flowsare parametrized\ntransformations T designed such thatT\u22121 and its Jacobian\u2202T\u22121 are\neasy to compute; see e.g. Kobyzevet al.(2019) and Papamakarioset al.\n(2021) for a review.\n12.5 Stochastic programs\nA stochastic program is a program that involves some form of random-\nness. In a stochastic program, the final output, as well as intermediate\nvariables, may therefore be random variables. In other words, a stochas-\ntic program induces a probability distribution over program outputs,\nas well as over execution trajectories.\n12.5.1 Stochastic computation graphs\nA stochastic program can be represented by a stochastic computation\ngraph as originally introduced by Schulmanet al.(2015). Departing from\nthat work, our exposition explicitly supports two types of intermediate", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1461, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "469607df-af30-40a6-85fd-3332491e200b": {"__data__": {"id_": "469607df-af30-40a6-85fd-3332491e200b", "embedding": null, "metadata": {"page_label": "310", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd1c4dc5-0ad7-47e8-a4b2-f2d46794f0d2", "node_type": "4", "metadata": {"page_label": "310", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "bf3dc224e4c17d82f42208ed4189f5820c12085c4ca73381bd9b189434f43e38", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "310 Differentiating through integration\noperations: sampling from aconditional distributionor evaluating a\nfunction. These operations can produce eitherdeterministicvariables\nor random variables.\nFunction and distribution nodes\nFormally, we define a stochastic computation graph as a directed acyclic\ngraph G= (V,E), whereV= Vf \u222aVp, Vf is the set of function nodes\nand Vp is the set of distribution nodes. Similarly to computation graphs\nreviewed in Section 4.1.3, we number the nodes asV= {0,1,...,K }.\nNode 0 corresponds to the inputs0 \u2208S0, which we assume to be deter-\nministic. It is the variable with respect to which we wish to differentiate.\nNode K corresponds to the program outputSK \u2208SK, which we assume\nto be a random variable. A nodek\u2208{1,...,K }can either be afunc-\ntion nodek \u2208Vf with an associated functionfk or adistribution\nnode k\u2208Vp, with associated conditional distributionpk. A stochastic\nprogram has at least one distribution node, the source of randomness.\nOtherwise, it is a deterministic program. As for computation graphs,\nthe set of edgesEis used to represent dependencies between nodes. We\ndenotes the parents of nodek by pa(k).\nDeterministic and random variables\nWe distinguish between two types of intermediate variables:deter-\nministic variables sk and random variables Sk. Therefore, a dis-\ntribution pk or a function fk may receive both types of variables\nas conditioning or input. It is then convenient to splitpa(k) as\npa(k) = determ(k) \u222arandom(k), where we defined thedeterminis-\ntic parents determ(k) := {i1,...,i pk}and the random parents\nrandom(k) := {j1,...,j qk}. Therefore,si1 ,..., sipk are the determinis-\ntic parent variables andSj1 ,...,S jqk are the random parent variables,\nof nodek.\nExecuting a stochastic program\nWe assume that nodes0,1,...,K are in topological order (if this is\nnot the case, we need to perform a topological sort). Given parent", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1893, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3ed2de99-970d-4b72-adbc-1e0a79599a6b": {"__data__": {"id_": "3ed2de99-970d-4b72-adbc-1e0a79599a6b", "embedding": null, "metadata": {"page_label": "311", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4e616ed4-0694-4627-a179-7d09b338ec3f", "node_type": "4", "metadata": {"page_label": "311", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "734e10029d3cdfc9798320ef58a1c33367e21b18f86bec9f2a5b79946975176c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12.5. Stochastic programs 311\nvariablessi1 ,..., sipk and Sj1 ,...,S jqk, a nodek\u2208{1,...,K }produces\nan output as follows.\n\u2022 If k\u2208Vp (distribution node), the output is\nSk \u223cpk(\u00b7| sdeterm(k),Srandom(k))\n\u21d0\u21d2Sk \u223cpk(\u00b7| si1 ,..., sipk,Sj1 ,...,S jqk)\nNote that technicallypk is the distribution ofSk conditioned on its\nparents, not the distribution ofSk. Therefore, we should in princi-\nple writeSk |sdeterm(k),Srandom(k) \u223cpk(\u00b7| sdeterm(k),Srandom(k)).\nWe avoid this notation for conciseness and for symmetry with\nfunction nodes.\nContrary to a function node, a distribution node can have no\nparents. That is, ifk\u2208Vp, it is possible thatpa(k) = \u2205. A good\nexample would be a parameter-free noise distribution.\n\u2022 If k\u2208Vf (function node), the output is in general\nSk := fk(sdeterm(k),Srandom(k))\n:= fk(si1 ,..., sipk,Sj1 ,...,S jqk)\nand in the special caseqk = |random(k)|= 0, the output is\nsk := fk(sdeterm(k))\n:= fk(si1 ,..., sipk).\nUnless the associated conditional distributionpk is a delta distribution,\nthat puts all the probability mass on a single point, the output of a\ndistribution node k \u2208Vp is necessarily a random variableSk \u2208Sk.\nFor function nodesk\u2208Vf, the output of the functionfk is a random\nvariable Sk \u2208Sk if at least one of the parents ofk produces a random\nvariable. Otherwise, if all parents ofk produce deterministic variables,\nthe output offk is a deterministic variablesk \u2208Sk.\nTheentireprocedureissummarizedinAlgorithm12.1.Weemphasize\nthat SK = f(s0) \u2208SK is a random variable. Therefore, a stochastic\nprogram (implicitly) induces a distribution overSK, and also over\nintermediate random variablesSk. Executing the stochastic program\nallows us to draw samples from that distribution.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1695, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3475e6f5-f286-479d-8883-fba0b9c06866": {"__data__": {"id_": "3475e6f5-f286-479d-8883-fba0b9c06866", "embedding": null, "metadata": {"page_label": "312", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fe0285b1-558c-4055-8026-68c1d0603ff1", "node_type": "4", "metadata": {"page_label": "312", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "e36078321aeee38a02d65d6fe9ec32b0b94476839cb04f5608faaf0df12d9fc2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "312 Differentiating through integration\nAlgorithm 12.1Executing a stochastic program\nNodes: 1,...,K in topological order, where nodek is either a\nfunction fk or a conditional distributionpk\nInput: input s0 \u2208S0\n1: for k:= 1,...,K do\n2: Retrieve pa(k) = determ(k) \u222arandom(k)\n3: if k\u2208Vp then \u25b7 Distribution node\n4: Sk \u223cpk(\u00b7|sdeterm(k),Srandom(k))\n5: else ifk\u2208Vf then \u25b7 Function node\n6: if |random(k)|\u0338= 0 then\n7: Sk := fk(sdeterm(k),Srandom(k)) \u25b7 Output is a R.V.\n8: else if|random(k)|= 0 then\n9: sk := fk(sdeterm(k)) \u25b7 Output is deterministic\n10: Output: f(s0) := SK \u2208SK\nSpecial cases\nIf all nodes are function nodes, we recover computation graphs, reviewed\nin Section 4.1.3. If all nodes are distribution nodes, we recover Bayesian\nnetworks, reviewed in Section 10.5.\n12.5.2 Examples\nWe now present several examples that illustrate our formalism. We use\nthe legend below in the following illustrations.\nFunction Sampler Deterministic\n variable\nStochastic\n variable\n\u2022 Example 1 (SFE estimator):", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 992, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9c9cfc79-e2f0-4b10-923c-12d4fb41b896": {"__data__": {"id_": "9c9cfc79-e2f0-4b10-923c-12d4fb41b896", "embedding": null, "metadata": {"page_label": "313", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b75e8a37-dbbb-4281-a233-8724b1f61791", "node_type": "4", "metadata": {"page_label": "313", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "3a034199ba475c358291726cc3640815eefd7f4a1bbadea88547ac1f87758dec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12.5. Stochastic programs 313\nS1 \u223cp1(\u00b7| s0)\nS2 := f2(S1)\nE(s0) := E[S2]\n\u2207E(s0) = ES1 [f2(S1)\u2207s0 log p1(S1 |s0)]\n\u2022 Example 2 (Pathwise estimator):\nS1 \u223cp1\nS2 := f2(S1,s0)\nE(s0) := E[S2]\n\u2207E(s0) = ES1 [\u2207s0 f2(S1,s0)]\n\u2022 Example 3 (SFE estimator + chain rule):\ns1 := f1(s0)\nS2 \u223cp2(\u00b7| s1)\nS3 := f3(S2)\nE(s0) := E[S3]\n\u2207E(s0) = \u2202f(s0)\u2217ES2 [f3(S2)\u2207s1 log p2(S2 |s1)]\n\u2022 Example 4:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 369, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "28c48ae1-fa9e-4885-a906-d939aa972f2a": {"__data__": {"id_": "28c48ae1-fa9e-4885-a906-d939aa972f2a", "embedding": null, "metadata": {"page_label": "314", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5fa421c8-c6d5-4344-8edb-d0beb5a07ff1", "node_type": "4", "metadata": {"page_label": "314", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "fa5ba8660344b7747581e95401f73eea4a43e07d6c5441415a2293f406960aa9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "314 Differentiating through integration\ns1 := f1(s0)\ns2 := f2(s0)\nS3 \u223cp3(\u00b7| s1)\nS4 \u223cp4(\u00b7| s2,S3)\nS5 := f5(S4)\nE(s0) := E[S5] = ES3 [ES4 [f5(S4)]]\n\u2207E(s0) = ES3 [\u2202f1(s0)\u2217\u2207s1 log p(S3 |s1)ES4 [f5(S4)]]\n+ ES3 [ES4 [\u2202f2(s0)\u2217\u2207s2 log p4(S4|s2,S3)f5(S4)]]\nAs can be seen, the gradient expressions can quickly become quite\ncomplicated, demonstrating the merits of automatic differentiation in\nstochastic computation graphs.\n12.5.3 Unbiased gradient estimators\nThe output of a stochastic program is a random variable\nSK := f(s0).\nIt implicitly defines a probability distributionp(\u00b7|s0) such thatSK \u223c\np(\u00b7|s0). Executing the stochastic program once gives us an i.i.d. sample\nfrom p(\u00b7|s0).\nSince derivatives are defined for deterministic variables, we need a\nway to convert a random variable to a deterministic variable. One way\nto do so is to consider the expected value (another way would be the\nmode)\nE(s0) := E[SK] = E[f(s0)] \u2208conv(SK),", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 927, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dfd16fc0-c97b-4b64-8fc3-a53b723e2e1d": {"__data__": {"id_": "dfd16fc0-c97b-4b64-8fc3-a53b723e2e1d", "embedding": null, "metadata": {"page_label": "315", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "57cb6faf-7526-4f6c-84a6-cf0d26d8b304", "node_type": "4", "metadata": {"page_label": "315", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "d2126fa5093087dfa78a08b9a147c70c3636ffca2a89c597fe532ebc040df579", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12.5. Stochastic programs 315\nwhere the expectation is overSK \u223cp(\u00b7|s0) or equivalently over the\nintermediate random variablesSk\nSk \u223cpk(\u00b7|sdeterm(k),Srandom(k)),\nfor k \u2208Vp (the distribution nodes). We then wish to compute the\ngradient or more generally the Jacobian ofE(s0).\nIf all nodes in the stochastic computation graph are function nodes,\nwe can estimate the gradient ofE(s0) using the pathwise estimator\na.k.a. reparametrization trick (Section 12.4). This is the approach taken\nby Kingma and Welling (2013) and Rezendeet al.(2014).\nIf all nodes in the stochastic computation graph are distribution\nnodes, we can use the SFE estimator (Section 12.3). Schulmanet al.\n(2015) propose a surrogate loss so that using autodiff on that loss\nproduces an unbiased gradient of the expectation, using the SFE esti-\nmator. Foersteret al.(2018) extend the approach to support high-order\ndifferentiation. Kriekenet al.(2021) further extend the approach by\nsupporting different estimators per node, as well as control variates.\nConverting distribution nodes into function nodes and vice-versa\nOur formalism uses two types of nodes: distribution nodes with asso-\nciated conditional distributionpk and function nodes with associated\nfunction fk. It is often possible to convert between node types.\nConverting a distribution node into a function node is exactly the\nreparametrization trick studied in Section 12.4. We can use transforma-\ntions such as the location-scale transform or the inverse transform.\nConverting a function node into a distribution node can be done\nusing the change-of-variables theorem, studied in Section 12.4.5, on a\npushforward distribution.\nBecause the pathwise estimator has lower variance than SFE, this is\nthe method of choice when thefk functions are available. The conversion\nfrom distribution node to function node and vice-versa is illustrated in\nFig. 12.1.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1877, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "57ea6356-774b-45c5-8997-39626d734828": {"__data__": {"id_": "57ea6356-774b-45c5-8997-39626d734828", "embedding": null, "metadata": {"page_label": "316", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7ac5b2e6-848c-436f-8f4e-4e6c7111f0fb", "node_type": "4", "metadata": {"page_label": "316", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "25b1dc3e267c59406c421b847786f4306dbc95331bfe6b92753b74268bce996d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "316 Differentiating through integration\nTransformation \n(location-scale transform, inverse transform)\nChange-of-variables theorem\nParametric (explicit) distribution\nScore Function Estimator\n(SFE)\nPushforward (implicit) distribution\nPath Gradient Estimator \n(PGE)\nFigure 12.1:It is sometimes possible to convert a distribution node to a function\nnode and vice-versa using a suitable transformation.\n12.5.4 Local vs. global expectations\nA stochastic computation graph can be seen as astochastic process,\na collection of random variablesSk, indexed byk, the position in the\ntopological order. However, random variables are incompatible with\nautodiff. Replacing random variables by their expectation can be seen\nas a way to make them compatible with autodiff. Two strategies are\nthen possible.\nAs we saw in the previous section, a strategy is to consider the\nexpectation of the last output SK. This strategy corresponds to a\nglobal smoothing. The two major advantages are that i) we do not\nneed to assume thatfk+1 is well-defined onconv(Sk) and ii) this induces\na probability distribution over program executions. This is for instance\nuseful to compute the variance of the program. The gradient of the\nprogram\u2019s expected value can be estimated by the reparametrization\ntrick or by the SFE, depending on the type of nodes used.\nA second strategy is to replace an intermediate random variable\nSk \u2208Sk, fork\u2208{1,...,K }, by its expectationE[Sk] \u2208conv(Sk). This\nstrategy corresponds to alocal smoothing. A potential drawback of\nthis approach is thatE[Sk] belongs toconv(Sk), the convex hull ofSk.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1586, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8dfb559a-c7fd-4402-ac5c-7153ca21ebf7": {"__data__": {"id_": "8dfb559a-c7fd-4402-ac5c-7153ca21ebf7", "embedding": null, "metadata": {"page_label": "317", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "545407a9-a8ee-4f42-baed-db62cd6406c0", "node_type": "4", "metadata": {"page_label": "317", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "cfbbde0c6093353b4dabef9324427363c4befa5615e78fed9e20c68a8f014b63", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12.6. Differential equations 317\nTherefore, the functionfk+1 in whichE[Sk] is fed must be well-defined\non conv(Sk), which may not always be the case. In the case of control\nflows, another disadvantage is computational. We saw in Section 5.6 and\nSection 5.7 that using a soft comparison operator within a conditional\nstatement induces a distribution on a binary or categorical random\nvariable, corresponding to the branch to be selected. A conditional\nstatement can then be locally smoothed out by replacing the random\nvariable by its expectation i.e., aconvex combination of all the\nbranches. This means that, unless the distribution has sparse support,\nall branches must be evaluated.\n12.6 Differential equations\n12.6.1 Parameterized differential equations\nFrom residual networks to neural ODEs\nStarting froms0 := x, residual networks, reviewed in Section 4.6, iterate\nfor k\u2208{1,...,K }\nsk := sk\u22121 + hk(sk\u22121,wk).\nA residual network can be seen as parameterizing incremental discrete-\ntime input changes (hence the name \u201cresidual\u201d)\nsk \u2212sk\u22121 = hk(sk,wk).\nChen et al.(2018) proposed to parameterize continuous-time (instan-\ntaneous) changes instead. They considered the evolutions(t) of the\ninputs in continuous time driven by a functionh(t,s,w) parameterized\nby w, starting fromx. Formally, the evolutions(t) is the solution of\nthe ordinary differential equation(ODE)\ns(0) = x\ns\u2032(t) = h(t,s(t),w) t\u2208[0,T] (12.5)\nHere, s\u2032(t) is the vector of derivatives ofsas defined in Remark 2.4, and\nT denotes a final time for the trajectory. The output of such aneural\nODE (Chen et al., 2018) is thenf(x,w) := s(T). Alternatively, the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1619, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "24e0b27e-b861-42f7-a043-d07221a9c371": {"__data__": {"id_": "24e0b27e-b861-42f7-a043-d07221a9c371", "embedding": null, "metadata": {"page_label": "318", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7a23b51c-4c00-4a6f-9e18-7acbc5a17aef", "node_type": "4", "metadata": {"page_label": "318", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "7e3dc90adb8c8ca149dfdacc80534bfc292de5fff096bcc05a3a7f23a4c6d6a3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "318 Differentiating through integration\noutput can be seen as the solution of anintegration problem\nf(x,w) = s(T) = x+\n\u222bT\n0\nh(t,s(t),w)dt. (12.6)\nDifferential equations like Eq. (12.5) arise in many contexts beyond neu-\nral ODEs, ranging from modeling physical systems to pandemics (Braun\nand Golubitsky, 1983). Moreover, the differential equation presented\nin Eq. (12.5) is just an example of an ordinary differential equation,\nwhile controlled differential equations or stochastic differential equations\ncan also be considered.\nExistence of a solution\nFirst and foremost, the question is whethers(t) is well-defined. For-\ntunately, the answer is positive under mild conditions, as shown by\nPicard-Lindel\u00f6f\u2019s theorem recalled below (Butcher, 2016, Theorem 16).\nTheorem 12.1(Exsistence and uniqueness of ODE solutions). If h:\n[0,T] \u00d7S \u2192S is continuous in its first variable and Lipschitz-\ncontinuous in its second variable, then there exists a unique differ-\nentiable maps: [0,T] \u2192S satisfying\ns(0) = s0\ns\u2032(t) = h(t,s(t)) t\u2208[0,T],\nfor some givens0 \u2208S.\nFor time-independent linear functionsh(t,s) = As, the integral\nin Eq. (12.6) can be computed in closed form as\nst = exp(tA)(s0),\nwhere exp(A) is the matrix exponential. Hence, the outputs(T) can\nbe expressed as a simple function of the parameters (Ain this case).\nHowever, generally, we do not have access to such analytical solutions,\nand, just as for solving optimization problems in Chapter 11, we need\nto resort to some iterative algorithms.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1497, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ee9e141f-5aa8-4ff4-afc0-d2103cd486fc": {"__data__": {"id_": "ee9e141f-5aa8-4ff4-afc0-d2103cd486fc", "embedding": null, "metadata": {"page_label": "319", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2a46676e-03eb-4e0a-8478-fd9a79f16f84", "node_type": "4", "metadata": {"page_label": "319", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "2743363c63bc2276125dc5a801e4b8b469bbd810df19cb91e8bbf18da481845e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12.6. Differential equations 319\nIntegration methods\nTo numerically solve an ODE, we can useintegration methods,\nwhose goal is to build a sequencesk that approximates the solution\ns(t) at timestk. The simplest integration method is theexplicit Euler\nmethod, that approximates the solutions between timestk\u22121 and tk as\ns(tk\u22121) \u2212s(tk) =\n\u222btk\ntk\u22121\nh(t,s(t),w)dt\n\u2248\u03b4kh(tk\u22121,s(tk\u22121),w),\nfor a time-step\n\u03b4k := tk \u2212tk\u22121.\nThe resulting integration scheme consists in computing starting from\ns0 = x, fork\u2208{1,...,K },\nsk := sk\u22121 + \u03b4kh(tk\u22121,sk\u22121,w).\nAssimilating \u03b4kh(tk\u22121,sk\u22121,w) with hk(sk\u22121,wk), we find that residual\nnetworks are essentially the discretization of a neural ODE by an\nexplicit Euler method; more precisely, a non-autonomous neural ODEs,\nsee e.g. (Daviset al., 2020).\nEuler\u2019s forward method is only one integration method among\nmany. To cite a few, there are implicit Euler methods, semi-implicit\nmethods, Runge-Kutta methods, linear multistep methods, etc. See,\ne.g., Gautschi (2011) for a detailed review. The quality of an integration\nmethod is measured by its consistency and its stability (Gautschi, 2011).\nThese concepts naturally influence the development of evaluation and\ndifferentiation techniques for ODEs. We briefly summarize them below.\nGiven a fixed time interval\u03b4k = \u03b4 and K = \u2308T/\u03b4\u2309points, an\nintegration method isconsistent of orderk if \u2225sk \u2212s(k\u03b4)\u2225= O(\u03b4k)\nas\u03b4\u21920 andtherefore k\u2192+\u221e.Thehighertheorder k,thefewerpoints\nwe need to reach an approximation error\u03b5 on the points considered.\nThe term\u2225sk \u2212s(k\u03b4)\u2225= O(\u03b4k) is reminiscent of the error encountered\nin finite differences (Chapter 7) and is called thetruncation error.\nThe (absolute)stability of a method is defined by the set of time-\nsteps such that the integration method can integrates\u2032(t) = \u03bbs(t) for\nsome \u03bb\u2208C without blowing up ast\u2192+\u221e.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1811, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b05ed1d3-a11c-4187-9c68-4f828de0020a": {"__data__": {"id_": "b05ed1d3-a11c-4187-9c68-4f828de0020a", "embedding": null, "metadata": {"page_label": "320", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "62e45bc0-2093-4f6c-97fe-2e80ac03579c", "node_type": "4", "metadata": {"page_label": "320", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "0fdb4b80b12d7e02b3ba5dde58ebcb74a0d43b6a3deb3df33e617caa233a8210", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "320 Differentiating through integration\n12.6.2 Continuous adjoint method\nSince different parameterswinduce different trajectories associated to\nh(t,s,w) in Eq. (12.5), we may want to select one of these trajectories\nby minimizing some criterion. For example, we may consider selecting\nw\u2208W by minimizing a lossL on the final point of the trajectory,\nmin\nw\u2208W\nL(f(x,w),y), (12.7)\nwhere\nf(x,w) := s(T) = x+\n\u222bT\n0\nh(t,s(t),w)dt.\nTo solve such problems, we need to access gradients of\u2113composed with\nf through VJPs of the solution of the ODE. The VJPs can actually\nbe characterized as solutions of an ODE themselves thanks to the\ncontinuous time adjoint method(Pontryagin, 1985), presented\nbelow, and whose proof is postponed to Section 12.6.6.\nProposition 12.8(Continuous-time adjoint method). Considerafunc-\ntionh: [0,T]\u00d7S\u00d7W\u2192S , continuous in its first variable, Lipschitz-\ncontinuous and continuously differentiable in its second variable.\nAssume that\u22023h(t,s,w) exists for anyt,s,w, and is also continu-\nous in its first variable, Lipschitz-continuous in its second variable.\nDenote s: S\u2192S the solution of the ODE\ns(0) = x\ns\u2032(t) = h(t,s(t),w) t\u2208[0,T],\nand f(x,w) = s(T) the final state of the ODE at timeT.\nThen, the functionf is differentiable, and for an output direction\nu\u2208S, its VJP alonguis given by\n\u2202f(x,w)\u2217u= (r(0),g)\nfor\ng=\n\u222bT\n0\n\u22023h(t,s(t),w)\u2217r(t)dt", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1352, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c6d4e588-eb4d-4985-882a-c698c70baa89": {"__data__": {"id_": "c6d4e588-eb4d-4985-882a-c698c70baa89", "embedding": null, "metadata": {"page_label": "321", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bdce32a2-766b-45f1-9cd8-417aefbaa106", "node_type": "4", "metadata": {"page_label": "321", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "07dbe7c20e25e9e399781f1757773d6cb2e201835c857531a4e42d7cb1c3d6c4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12.6. Differential equations 321\nand forrsolving theadjoint (backward) ODE\nr\u2032(t) = \u2212\u22022h(t,s(t),w)\u2217r(t)\nr(T) = u.\nIn particular, the gradient \u2207(L \u25e6f)(x,w) for L : S \u2192R a\ndifferentiable loss is obtained by solving the adjoint ODE with\nr(T) = \u2207L(s(T)).\nExample 12.5(Fitting data through the solution of an ODE). Asan\nillustrative example, we can consider optimizing the parameters of\nan ODE to fit some data points. Namely, we may seek a continuous\ntime solutionz(t; w) of a modified Lotka Volterra ODE\nz\u2032(t; w) =\n(\n\u03b1z1(t; w) \u2212\u03b2z1(t; w)z2(t; w)\n\u2212\u03b3z2(t; w) + \u03b4z1(t; w)z2(t; w)\n)\n+ c,\nfor w= (\u03b1,\u03b2,\u03b3,\u03b4, c), that fits some observationsz1,..., zT. The\noptimization problem consists then of\nminw\nT\u2211\n\u03c4=1\n(z(tj; w) \u2212zj)2,\nand requires backpropagating through the solutionz(\u00b7; w) of the\nODE w.r.t. to its candidate parametersw. Fig. 12.2 illustrates such\na problem with varying candidate parameters\n12.6.3 Gradients via the continuous adjoint method\nProposition 12.8 gives a formal definition of the gradient. However, just\nas computing the mappingf(x,w) itself, computing its VJP or the\ngradient ofL\u25e6f requires solving an integration problem. Note that\nthe integration ofr(t) in Proposition 12.8 requires also values ofs(t).\nTherefore, we need to integrate bothr(t) and s(t). Such an approach\nis generally referred asoptimize-then-discretize because we first\nformulate the gradient in continuous time (the \u201coptimize part\u201d) and\nthen discretize the resulting ODE.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1450, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5b45845e-0527-456f-bc9c-e457ca84ba5f": {"__data__": {"id_": "5b45845e-0527-456f-bc9c-e457ca84ba5f", "embedding": null, "metadata": {"page_label": "322", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5850738f-70fb-4cb6-806a-e520a42e91a4", "node_type": "4", "metadata": {"page_label": "322", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "51bbd0b79e1b17fccc4ac884830a4eb78396738beb9fe3b90ec043116db9a533", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "322 Differentiating through integration\n0.5 1.0 1.5\n0.0\n0.5\n1.0 s(t; w1)\ns(t; w2)\ns(t; w* )\nFigure 12.2:Finding the optimal parameters of an ODE to fit some observed data.\nThe dots represent the trajectories of a dynamical system observed at regular times\n(time is represented here by a gradient color, the lighter the color, the larger the\ntime). Each line represents the solution of an ODE given by some hyperparameters\nw. The objective is to find the hyperparameters of the ODE such that its solution\nfits the data points. Green and orange lines fail to do so while the blue line fits\nthe data. To compute such parametersw, we need to backpropagate through the\nsolution of the ODE.\nSimple discretization scheme\nA first approach consists in defining a backward discretization scheme\nthat can approximate s(t) backward in time. Namely, by defining\n\u03c3(t) = s(T \u2212t),\u03c1(t) = r(T \u2212t), and\u03b3(t) =\n\u222bT\nt \u22023h(\u03c4,s(\u03c4),w)\u2217r(\u03c4)d\u03c4,\nthe derivative ofL\u25e6f is given by(\u03c1(T),\u03b3(T)). The functions\u03c3,\u03c1,\u03b3\nare solutions of a standard ODE\n\u03c3(0) = s(T), \u03c3\u2032(t) = \u2212h(T \u2212t,\u03c3(t),w),\n\u03c1(0) = \u2207L(s(T)), \u03c1\u2032(t) = \u22022h(T \u2212t,\u03c3(t),w)\u2217\u03c1(t),\n\u03b3(0) = 0, \u03b3\u2032(t) = \u22023h(T \u2212t,\u03c3(t),w)\u2217\u03c1(t).\nThe above ODE can then be solved by any integration method. Note,\nhowever, that it requires first computings(T) and \u2207L(s(T)) by an\nintegration method. The overall computation of the gradient using\nan explicit Euler method to solve forward and backward ODEs is\nsummarized in Algorithm 12.2.\nAlgorithm 12.2 naturally looks like the reverse mode of autodiff for\na residual neural networks withshared weights. A striking difference", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1567, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5b96f6e2-87c4-4f5b-a61d-dbe474baf79d": {"__data__": {"id_": "5b96f6e2-87c4-4f5b-a61d-dbe474baf79d", "embedding": null, "metadata": {"page_label": "323", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "329909d7-18a8-4a55-9067-1f234411dee2", "node_type": "4", "metadata": {"page_label": "323", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "b9f734eca8e529aa6a0b55800a0ddf2e6ebca81562bf95581fb35830fc5df4b4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12.6. Differential equations 323\nAlgorithm 12.2Gradient computation via continuous adjoint method\nwith Euler explicit discretization\n1: Functions:h: [0,T] \u00d7S\u00d7W\u2192 R, L: S\u2192 R\n2: Inputs: input x, parametersw, number of discretization stepsK.\n3: Set discretization step\u03b4= T/K, denotehk(s,w) = h(k\u03b4,s,w).\n4: Set s0 := x\n5: for k:= 1,...,K do \u25b7 Forward discretization\n6: Compute sk := sk\u22121 + \u03b4hk\u22121(sk\u22121,w).\n7: Compute u:= \u2207L(sK).\n8: Initialize rK := u, \u02c6sK = sK, gK = 0\n9: for k:= K,..., 1 do \u25b7 Backward discretization\n10: Compute \u02c6sk\u22121 := \u02c6sk \u2212\u03b4hk(\u02c6sk,w)\n11: Compute rk\u22121 := rk + \u03b4\u22022hk(\u02c6sk,w)\u2217rk\n12: Compute gk\u22121 := gk + \u03b4\u22023hk(\u02c6sk,w)\u2217rk\n13: Output: (r0,g0) \u2248\u2207(L\u25e6f)(x,w)\nis that the intermediate computationssk are not kept in memory and,\ninstead, new variables\u02c6sk are computed along the backward ODE. One\nmay believe that by switching to continuous time, we solved the memory\nissues encountered in reverse-mode autodiff. Unfortunately, this comes\nat the cost of numerical stability. As we use a discretization scheme\nto recompute the intermediate states backward in time through\u02c6sk in\nAlgorithm 12.2, we accumulate some truncation errors.\nTo understand the issue here, consider applying Algorithm 12.2\nrepeatedly on the same parameters but using\u02c6s0 instead ofs0 = xeach\ntime. In the continuous realm,\u03c3(T) = s(0). But after discretization,\n\u02c6s0 \u2248\u03c3(T) does not matchs0. Therefore, by applying Algorithm 12.2\nwith s0 = \u02c6s0, we would not get the same output even if in continu-\nous time we naturally should have. This phenomenon is illustrated in\nFig. 12.3. It intuitively shows why Algorithm 12.2 induces some noise\nin the estimation of the gradient.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1640, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6c617d4c-fc8c-4040-9917-62349c0a8339": {"__data__": {"id_": "6c617d4c-fc8c-4040-9917-62349c0a8339", "embedding": null, "metadata": {"page_label": "324", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "11f0b836-c8a9-4817-a40f-c14145370e6e", "node_type": "4", "metadata": {"page_label": "324", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "37b1a7eb5d6edcdddfc9324284986bc4192cbe9126b5ebddcc78e5003d9840c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "324 Differentiating through integration\n...\nForward approx. \nof the ODEForward \ndiscretization. \nerror\nSolution \nof ODE\nTotal forward\ndiscretization \nerror\n... Local forward \ndiscretization. \nerrorBackward approx. \nof the ODE\nTotal backward\ndiscretization \nerror\nFigure 12.3: Forward and backward discretizations when using the continuous\nadjoint method.\nMultiple shooting scheme\nAn alternative approach consists in integrating both the forward and\nbackward ODEs jointly. Namely, we may solve an ODE with boundary\nvalues\ns\u2032(t) = h(t,s(t),w), s(0) = x,\nr\u2032(t) = \u2212\u22022h(t,s(t),w)\u2217r(t), r(T) = \u2207L(s(T))\ng\u2032(t) = \u2212\u22023h(t,s(t),w)\u2217r(t), g(T) = 0,\nby means of a multiple shooting method or a collocation method (Stoer\net al., 1980). This approach still requires\u2207L(s(T)) to be approximated\nfirst.\n12.6.4 Gradients via reverse-mode on discretization\nA simpler approach consists in replacing the objective in Eq. (12.7) by\nits version discretized using some numerical method, such as an Euler\nforward discretization scheme. That is, we seek to solve\nmin\nw\u2208W\nL(sK) where sk = sk\u22121 + \u03b4h(k\u03b4,sk\u22121,w) k\u2208{1,...,K },", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1094, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6d530190-db2d-439f-be0f-e5a9bbb6ef8f": {"__data__": {"id_": "6d530190-db2d-439f-be0f-e5a9bbb6ef8f", "embedding": null, "metadata": {"page_label": "325", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25e22f8d-1312-45cf-814e-fc06cd2fb982", "node_type": "4", "metadata": {"page_label": "325", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "e5b6e05ae211724352a79c44a4344ca1aa0fa5544b2cdee2882b40699b6cb525", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12.6. Differential equations 325\nwith s0 = 0 and \u03b4 some discretization step. Gradients of the objective\ncan be computed by automatic differentiation. That approach is often\nreferred to asdiscretize-then-optimize. At first glance, this approach\nmay suffer from very high memory requirements. Indeed, to get an\naccurate solution of the ODE, a numerical integration method may\nrequire Kto be very large. Since a naive implementation of reverse-mode\nautomatic differentiation has a memory that scales linearly withK,\ncomputing the gradient by a discretize-then-optimize method could be\nprohibitive. However, the memory requirements may easily be amortized\nusing checkpointing, as explained in Section 8.5; see also (Gholaminejad\net al., 2019).\nAs for theoptimize-then-discretize method, we still accumulate\nsome truncation errors in the forward discretization process. This dis-\ncretization error occurs when computing the gradient in reverse-mode\ntoo. The discretize-then-optimize method can be seen as computing\ngradients of a surrogate objective. For that objective, the gradients are\ncorrect and well-defined. However, they may not match the gradients of\nthe true ODE formulation.\nTocomparethediscretize-then-optimizeandoptimize-then-discretize\napproaches, Gholaminejadet al.(2019) compared their performance on\nan ODE whose solution can be computed analytically by selectingh\nto be linear ins. The authors observed that discretize-then-optimize\ngenerally outperformed optimize-then-discretize. A middle ground can\nactually be found by using reversible differentiation schemes.\n12.6.5 Reversible discretization schemes\nOurexpositionoftheoptimize-then-discretizeordiscretize-then-optimize\napproaches used a simple Euler explicit discretization scheme. However,\nfor both approaches, we could have used other discretization schemes\ninstead, such as reversible discretization schemes.\nA reversible discretization scheme is a discretization scheme such\nthat we have access to a closed-form formula for the inverse of its\ndiscretization step. Formally, a discretization methodMbuilds an\napproximation (sk)K\nk=1 of the solution of an ODEs\u2032(t) = h(t,s(t)) on", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2150, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8f7be33e-6211-429b-a1b1-7c636c62827c": {"__data__": {"id_": "8f7be33e-6211-429b-a1b1-7c636c62827c", "embedding": null, "metadata": {"page_label": "326", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b3fb3ac-e5ec-4a71-a452-e4231825f348", "node_type": "4", "metadata": {"page_label": "326", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "b9e8e0f3b67ef1eff7f707d05ecc21f83cf9455a77beb3bd0e45bc8acabe60dc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "326 Differentiating through integration\nan interval[0,T] by computing fork\u2208(1,...,K )\ntk,sk,ck = M(tk\u22121,sk\u22121,ck\u22121; h,\u03b4), (12.8)\nwhere\u03b4 >0 issomefixeddiscretizationstep, tk isthetimestep(typically\ntk = tk\u22121+\u03b4),sk is the approximation ofs(tk), andck is some additional\ncontext variables used by the discretization method to build the iterates.\nAn explicit Euler method does not have a context, but just as an\noptimization method may update some internal states, a discretization\nmethod can update some context variable. The discretization scheme\nin Eq. (12.8) is a forward discretization scheme as we took a positive\ndiscretization step. By taking a negative discretization step, we obtain\nthe corresponding backward discretization scheme, fork\u2208(K,..., 1),\ntk\u22121,sk\u22121,ck\u22121 = M(tk,sk,ck; h,\u2212\u03b4).\nA discretization method isreversible if we have access toM\u22121 to\nrecompute the inputs of the discretization step from its outputs,\ntk\u22121,sk\u22121,ck = M\u22121(tk,sk,ck; h,\u03b4).\nA reversible discretization method issymmetric if the backward dis-\ncretization scheme is exactly the inverse of the forward discretization\nscheme, i.e.,\nM(tk,sk,ck; h,\u2212\u03b4) = M\u22121(tk,sk,ck; h,\u03b4).\nThe explicit Euler method is clearly not symmetric and a priori not\nreversible, unless we can solve foryk\u22121, the equation yk = yk\u22121 \u2212\n\u03b4f(yk\u22121).\nLeapfrog method\nThe (asynchronous)leapfrog method(Zhuang et al., 2021; Mutze,\n2013) on the other hand is an example of symmetric reversible discretiza-\ntion method. For a constant discretization step\u03b4, giventk\u22121,sk\u22121,ck\u22121", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1515, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "00b075d3-c19b-4814-b46a-ca5297b5a448": {"__data__": {"id_": "00b075d3-c19b-4814-b46a-ca5297b5a448", "embedding": null, "metadata": {"page_label": "327", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d854e55-f328-4526-b771-6cd53939327a", "node_type": "4", "metadata": {"page_label": "327", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "bd3be97da01269447f4f5124034f306755ddba124524075a13a6a5a5694e19a0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12.6. Differential equations 327\nand a functionh, it computes\n\u00aftk\u22121 := tk\u22121 + \u03b4\n2\n\u00afsk\u22121 := sk\u22121 + \u03b4\n2ck\u22121\n\u00afck\u22121 := h(\u00aftk\u22121,\u00afsk\u22121)\ntk := \u00aftk\u22121 + \u03b4\n2\nsk := \u00afsk\u22121 + \u03b4\n2 \u00afck\u22121\nck := 2\u00afck\u22121 \u2212ck\u22121\nM(tk\u22121,sk\u22121,ck\u22121; h,\u03b4) := (tk,sk,ck).\nOne can verify that we indeed haveM(tk,sk,ck; h,\u2212\u03b4) = (tk\u22121,sk,ck).\nByusingareversiblesymmetricdiscretizationschemeintheoptimize-\nthen-discretize approach, we ensure that, at the end of the backward\ndiscretization pass, we recover exactly the original input. Therefore, by\nrepeating forward and backward discretization schemes we always get\nthe same gradient, which was not the case for an Euler explicit scheme.\nBy using a reversible discretization scheme in the discretize-then-\noptimize method, we address the memory issues of reverse mode autodiff.\nAsexplainedinSection8.6,wecanrecomputeintermediatevaluesduring\nthe backward pass rather than storing them.\nMomentum residual networks\nIn the leapfrog method, the additional variablesck may actually be\ninterpreted as velocities of a system whose acceleration is driven by\nthe given function, that is,s\u2032\u2032(t) = h(t,s(t),w). Such an interpretation\nsuggests alternatives to the usual neural ODE paradigm. For instance,\nmomentum neural networks(Sander et al., 2021b), can be inter-\npreted as the discretization of asecond-order ordinary differential\nequations, which are naturally amenable to reversible differentiation\nschemes with a low memory footprint.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1432, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9faad3e8-6034-4c93-aa05-c4bd6603f3cb": {"__data__": {"id_": "9faad3e8-6034-4c93-aa05-c4bd6603f3cb", "embedding": null, "metadata": {"page_label": "328", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c79e5777-7648-4a6f-b184-baf3dd16dae7", "node_type": "4", "metadata": {"page_label": "328", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "d03ccdb1568409f98c12c66ce6a5ae910bda9da1c3762356d281344fc1435a36", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "328 Differentiating through integration\n12.6.6 Proof of the continuous adjoint method\nIn the following, we denotes(t,x,w) the solution of the ODE at timet\ngiven the inputxand the parametersw. We focus here on the formula-\ntion of the VJP. The proof relies on the existence of partial derivatives\nof s(t,x,w), which we do not cover here and refer to, e.g., Pontryagin\n(1985) for a complete proof of such facts given the assumptions.\nWe use the ODE constraint to introduce adjoint variables, this time\nin the form of a continuously differentiable functionr. For any such a\nfunction r, we have\n\u27e8f(x,w),u\u27e9= \u27e8s(T,x,w),u\u27e9\n+\n\u222bT\n0\n\u27e8r(t),h(t,s(t,x,w),w) \u2212\u2202ts(t,x,w)\u27e9dt,\nusing Leibniz notations such as\u2202ts(t,x,w) = \u22021s(t,x,w). The VJPs\nthen decompose as\n\u2202wf(x,w)\u2217[u]\n= \u2202ws(T,x,w)\u2217u\n+\n\u222bT\n0\n(\u2202ws(t,x,w)\u2217\u2202\u2217\nsh(t,s(t,x,w),w)\u2217\u2212\u22022\nwts(t,x,w)\u2217)r(t)dt\n+\n\u222bT\n0\n\u2202wh(t,s(t,x,w),w)\u2217r(t)dt,\n\u2202xf(x,w)\u2217[u]\n= \u2202xs(T,x,w)\u2217u\n+\n\u222bT\n0\n(\u2202xs(t,x,w)\u2217\u2202\u2217\nsh(t,s(t,x,w),w)\u2217\u2212\u22022\nxts(t,x,w)\u2217)r(t)dt\nHere the second derivative terms\u22022\nwts(t,x,w)\u2217r,\u22022\nxts(t,x,w)\u2217r cor-\nrespond to second derivatives of\u27e8s(t,x,w),r\u27e9. Since the Hessian is\nsymmetric (Schwartz\u2019s theorem presented in Proposition 2.10), we can\nswap the derivatives int and w or x. Then, to express the gradient\nuniquely in terms of first derivatives ofs, we use an integration by part", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1302, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a8af6e7d-eb08-4d41-bff5-06b70c702284": {"__data__": {"id_": "a8af6e7d-eb08-4d41-bff5-06b70c702284", "embedding": null, "metadata": {"page_label": "329", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8b1022c-6c6c-45c4-8259-bf4118122e14", "node_type": "4", "metadata": {"page_label": "329", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "b4977ccb886cbaf5a2e5319185b8c91f214ce77ad103c71067fcc2ac8c1bbd80", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12.7. Summary 329\nto have for example\n\u222bT\n0\n\u22022\nwts(t,x,w)\u2217r(t)dt=\n\u222bT\n0\n\u22022\ntws(t,x,w)\u2217r(t)dt\n= (\u2202ws(T,x,w)\u2217r(T) \u2212\u2202ws(0,x,w)\u2217r(0))\n\u2212\n\u222bT\n0\n\u2202ws(t,x,w)\u2217r(t)\u2217\u2202tr(t)dt.\nSince s(0) = x, we have\u2202ws(0,x,w)\u2217r(0) = 0. The VJP w.r.t.wcan\nthen be written as\n\u2202wf(x,w)\u2217[u]\n= \u2202ws(T,x,w)\u2217[u\u2212r(T)]\n+\n\u222bT\n0\n\u2202ws(t,x,w)\u2217[\u2202sh(t,s(t,x,w),w)\u2217r(t) + \u2202tr(t)]dt\n+\n\u222bT\n0\n\u2202wh(t,s(t,x,w),w)\u2217r(t)dt.\nBy choosingr(t) to satisfy the adjoint ODE\n\u2202tr(t) = \u2212\u2202sh(t,s(t,x,w),w)\u2217r(t), r(T) = u,\nthe expression of the VJP simplifies as\n\u2202wf(x,w)\u2217[u] =\n\u222bT\n0\n\u2202wh(t,s(t,x,w),w)\u2217r(t)dt.\nFor the VJP w.r.t. tox, we can proceed similarly. Using an integration\nby part, we have, this time,\u2202xs(0,x,w)\u2217r(0) = r(0) since s(0) = x.\nChoosing the same curver(t) satisfying the adjoint ODE we get\n\u2202xf(x,w)\u2217[u] = r(0).\nThe existence of a curversolution of the backward ODE can easily be\nshown from Picard Lindel\u00f6f\u2019s theorem and the assumptions.\n12.7 Summary\n\u2022 We studied how to differentiate integrals, with a focus on expec-\ntations and solutions of a differential equation.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1015, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bcee9886-8073-4903-832e-20a2f590e3b8": {"__data__": {"id_": "bcee9886-8073-4903-832e-20a2f590e3b8", "embedding": null, "metadata": {"page_label": "330", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "416b4138-a42c-4040-96a9-51f2e2879dca", "node_type": "4", "metadata": {"page_label": "330", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "c619bee52d887a0c920f4cc1ddcb9c3ceae840ec40b164cbf9ed9a2368382e97", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "330 Differentiating through integration\n\u2022 For differentiating through expectations, we studied two main\nmethods: the score function estimator (SFE, a.k.a. REINFORCE)\nand the path gradient estimator (PGE, a.k.a. reparametrization\ntrick).\n\u2022 The SFE is suitable when it is easy to sample from the distribution\nand its log-PDF isexplicitlyavailable. It is an unbiased estimator,\nbut is known to suffer from high variance.\n\u2022 The PGE is suitable for pushforward distributions, distributions\nthat are implicitly defined through a transformation, or a se-\nquence of them. These distributions can be easily sampled from,\nby injecting a source of randomness (such as noise) through the\ntransformations. An unbiased, low-variance estimator of the gra-\ndient of their expectation is easily obtained, provided that we can\ninterchange integration and differentiation.\n\u2022 If we have an explicit distribution, we can sometimes convert it\nto an implicit distribution, thanks to thelocation-scale trans-\nformation or theinverse transformation.\n\u2022 Conversely, if we have an implicit distribution, we can convert it to\nan explicit distribution using thechange-of-variables theorem.\nHowever, this formula requires to compute the determinant of\nan inverse Jacobian, and is computationally expensive in general.\nNormalizingflowsuseinvertibletransformationssothattheinverse\nJacobian is cheap to compute, by design.\n\u2022 Stochastic computation graphscan use a mix of explicit and\nimplicit distributions at each node.\n\u2022 For differentiating through the solution of a differential equation,\ntwo approaches can be considered.\n\u2022 We can express the gradient as the solution of a differential equa-\ntion thanks to thecontinuous adjoint method. We may then\ndiscretize backwards in time the differential equation that the gra-\ndient satisfies. This is theoptimize-then-discretize approach.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1850, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f45a8841-bb1c-4a57-9fbc-3fa3cd9bb6e1": {"__data__": {"id_": "f45a8841-bb1c-4a57-9fbc-3fa3cd9bb6e1", "embedding": null, "metadata": {"page_label": "331", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b3ce64d9-37dd-4ad8-a31b-3fbc935f5705", "node_type": "4", "metadata": {"page_label": "331", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "4df3956694765794fe4914abf8a1203e15fd3e0c67ce697acd4ae3659d2f3393", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12.7. Summary 331\n\u2022 We can also first discretize the problem in such a way that the\ngradient can simply be computed by reverse mode auto-diff, ap-\nplied on the discretization steps. This is thediscretize-then-\noptimize approach. The optimize-then-discretize approach has\nno memory cost, but discrepancies between the forward and back-\nward discretization passes often lead to numerical errors. The\ndiscretize-then-optimize introduces no such discrepancies but may\ncome at a large memory cost.\n\u2022 Reversible discretization schemescan circumvent the memory\ncost, as they enable the recomputation of intermediate discretiza-\ntion steps backwards in time.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 650, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0a691d3a-b760-40e5-a929-5f65bcb3f0a9": {"__data__": {"id_": "0a691d3a-b760-40e5-a929-5f65bcb3f0a9", "embedding": null, "metadata": {"page_label": "332", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4dc5a2be-2636-4920-a92a-a8d3cccb44e5", "node_type": "4", "metadata": {"page_label": "332", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "fdcb0d0f80a9133d54302c2ef2a4d89c928c177d326bccd99357023a55a63115", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Part IV\nSmoothing programs", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 26, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "81ab8765-d87c-4cbb-8973-ae7a5a0014c2": {"__data__": {"id_": "81ab8765-d87c-4cbb-8973-ae7a5a0014c2", "embedding": null, "metadata": {"page_label": "333", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d36a0a4d-5441-46da-8604-223bea69548d", "node_type": "4", "metadata": {"page_label": "333", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "758412026abc41067bb4200e936f69eef9b470ada31c691551a8fe60ecd2d078", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13\nSmoothing by optimization\nWhen a function is non-differentiable (or worse, discontinuous), a rea-\nsonable approach is to replace it by a differentiable approximation\n(or at least, by a continuous relaxation). We refer to the process of\ntransforming a non-differentiable function into a differentiable one as\n\u201csmoothing\u201d the original function. In this chapter, we begin by review-\ning a smoothing technique based oninfimal convolution. We then\nreview an equivalent dual approach, based on theLegendre-Fenchel\ntransform. We illustrate how to apply these techniques to compute\nsmoothed ReLUs and smoothed max operators, as well as continuous\nrelaxations of step functions and argmax operators.\n13.1 Primal approach\nWe first review how to smooth functions in the original, primal space\nof the function, using the infimal convolution and more particularly the\nMoreau envelope, a.k.a. Moreau-Yoshida regularization. In this chapter,\nwe consider functions taking potentially infinite positive values, that is,\nfunctions taking values in the half-extended real lineR \u222a{\u221e}. For a\n333", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1077, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8012441c-4b6d-4fb0-9c6c-92c659695c8d": {"__data__": {"id_": "8012441c-4b6d-4fb0-9c6c-92c659695c8d", "embedding": null, "metadata": {"page_label": "334", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b66d1eae-a309-4856-b8ba-7089e7bcf29d", "node_type": "4", "metadata": {"page_label": "334", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "6a93d5ffa0ed333c06f63e28944f3cc79162f501a5ae85218bc23d78719659fd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "334 Smoothing by optimization\nfunction f : RM \u2192R \u222a\u221e, we define its domain as\ndom(f) = {u\u2208RM : f(u) <\u221e}.\n13.1.1 Infimal convolution\nSometimes abbreviated inf-conv, the infimal convolution between two\nfunctions f and g creates a new functionf\u25a1g. It is defined as follows.\nDefinition 13.1(Infimal convolution). The infimal convolution be-\ntween two functionsf: RM \u2192R \u222a{\u221e} and g: RM \u2192R \u222a{\u221e} is\ndefined by\n(f\u25a1g)(\u00b5) := inf\nu\u2208RM\nf(u) + g(\u00b5\u2212u)\n= inf\nz\u2208RM\nf(\u00b5+ z) + g(z)\n= inf\nu,z\u2208RM\nf(u) + g(z) s.t. u= \u00b5+ z.\nIt is easy to check that the three definitions are indeed equivalent,\nby using the change of variableu:= \u00b5+ z, which is a location-scale\ntransform; see Section 12.4.1.\nThe infimal convolution can be seen as a counterpart of the usual con-\nvolution, in which integration has been replaced by minimization (hence\nits name). Similarly to the classical convolution, it iscommutative,\nmeaning that for all\u00b5\u2208RM, we have\n(f\u25a1g)(\u00b5) = (g\u25a1f)(\u00b5).\nComputing the infimal convolution involves the resolution of a\nminimization problem, that may or may not enjoy an analytical solution.\nSome examples are given in Table 13.1.\nExistence\nThe infimal convolution(f\u25a1g)(\u00b5) exists if the infimuminfu\u2208RM f(u) +\ng(\u00b5\u2212u) is finite (Bauschke and Combettes, 2017, Proposition 12.6).\nA sufficient condition to achieve this is thatu\u21a6\u2192f(u) + g(\u00b5\u2212u) is\nconvex for all\u00b5\u2208RM. However, this is not a necessary condition. For", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1388, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e7decadf-1315-4b79-a20e-528b5e3b8160": {"__data__": {"id_": "e7decadf-1315-4b79-a20e-528b5e3b8160", "embedding": null, "metadata": {"page_label": "335", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6745a5a7-0c40-413f-afe3-b36c0797dae7", "node_type": "4", "metadata": {"page_label": "335", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "7f9b7b10e78f2506fd747aa60742bc89cb8af4cf651528c34fb415fc21f93653", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13.1. Primal approach 335\nTable 13.1:Examples of infimal convolutions. We use\u03b9C to denote the indicator\nfunction of the setC.\nf(u) g(z) ( f\u25a1g)(\u00b5)\nf(u) 0 inf u\u2208RM f(u)\nf(u) \u03b9{v}(z) f(\u00b5\u2212v)\n\u03b9C(u) \u03b9D(z) \u03b9C+D(\u00b5)\n\u03b9C(u) \u2225z\u22252 dC(\u00b5) = infu\u2208C\u2225\u00b5\u2212u\u22252\nf(u) 1\n2 \u2225z\u22252\n2 envf(\u00b5) = infu\u2208RM 1\n2 \u2225\u00b5\u2212u\u22252\n2 + f(u)\nexample, the infimum can be finite even iff or g are nonconvex, for\nexample if their domain is a compact set.\nInfimal convolution with a regularization function\nWhen a functionf is non-differentiable, a commonly-used technique is\nto replace it by its infimal convolutionf\u25a1R, with some regularization\nR. The most used regularization is the squared2-norm, leading to the\nMoreau envelope, as we now review.\n13.1.2 Moreau envelope\nWhen R(z) := 1\n2 \u2225z\u22252\n2, the infimal convolutionf\u25a1R gives the so-called\nMoreau envelopeof f, which is also known as Moreau-Yoshida regu-\nlarization off.\nDefinition 13.2(Moreau envelope). Given a function f: RM \u2192\nR \u222a{\u221e}, its Moreau envelope is defined as\nenvf(\u00b5) :=\n(\nf\u25a11\n2\u2225\u00b7\u22252\n2\n)\n(\u00b5)\n= inf\nu\u2208RM\nf(u) + 1\n2\u2225\u00b5\u2212u\u22252\n2\n= inf\nz\u2208RM\nf(\u00b5+ z) + 1\n2\u2225z\u22252\n2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1066, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "406a1db5-83c9-464a-a878-345d6556056f": {"__data__": {"id_": "406a1db5-83c9-464a-a878-345d6556056f", "embedding": null, "metadata": {"page_label": "336", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "75cf121a-b58f-47c5-8978-0155ebe436e5", "node_type": "4", "metadata": {"page_label": "336", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "20d8471691c4480abc811213f9d9e928c9303152b5ae98aa1d3dbd4992436f81", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "336 Smoothing by optimization\nIntuitively, the Moreau envelope is the minimal value overu\u2208RM\nof a trade-off between staying close to the input\u00b5 according to the\nproximity term1\n2 \u2225\u00b5\u2212u\u22252\n2 and minimizingf(u). Provided that the\nminimizer exists and is unique, we can define the associatedproximal\noperator of f as\nproxf(\u00b5) := arg min\nu\u2208RM\n1\n2\u2225\u00b5\u2212u\u22252\n2 + f(u),\nIn other words, we have forproxf(\u00b5) well defined,\nenvf(\u00b5) = f(proxf(\u00b5)) + 1\n2\u2225\u00b5\u2212proxf(\u00b5)\u22252\n2. (13.1)\nProperties\nA crucial property of the Moreau envelopeenvf is that for any convex\nfunction f, it is always a smooth function, even whenf itself is not\nsmooth. By smooth, we formally mean that the resulting functionenvf\nis differentiable everywhere with Lipschitz-continuous gradients. We say\nL-smooth, if the gradients areL-Lipshcitz continuous. Such a property\ncan determine the efficiency of optimization algorithms as reviewed in\nSection 15.4. We recap below useful properties of the Moreau envelope.\nProposition 13.1(Properties of Moreau envelope). Let f: RM \u2192\nR \u222a{\u221e}.\n1. Smoothness: Iff is convex, the functionenvf is 1-smooth.\n2. Gradient: Provided thatproxf(\u00b5) is well-defined on\u00b5\u2208RM,\nthe gradient of the Moreau envelope can be expressed in terms\nof the proximal operator as\n\u2207envf(\u00b5) = \u00b5\u2212proxf(\u00b5).\n3. Moreau decomposition: Iff is convex, then for any\u00b5\u2208\nRM, we have the following identity\nproxf(\u00b5) + proxf\u2217(\u00b5) = \u00b5,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1376, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fe5d1c15-add9-48f0-8d0b-97b4fd8b4c7a": {"__data__": {"id_": "fe5d1c15-add9-48f0-8d0b-97b4fd8b4c7a", "embedding": null, "metadata": {"page_label": "337", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "67110e7d-6565-4200-805a-df95b4678c5a", "node_type": "4", "metadata": {"page_label": "337", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "ba06ad7422a28f878f534a89c7bc8750836c5590fb158fd0e3f64bf0def1294a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13.1. Primal approach 337\nwhere f\u2217is the convex conjugate off, detailed in Section 13.2.\nIn particular, we get\n\u2207envf\u2217(\u00b5) = proxf(\u00b5)\n4. Convexity: envf is convex iff is convex.\n5. Infimums coincideenvf has the same infimum as the origi-\nnal functionf:\nmin\n\u00b5\u2208RM\nenvf(\u00b5) = min\nu\u2208RM\nf(u).\nProof.\n1. This is best seen using the dual approach detailed in Section 13.3.\n2. This follows from Danskin\u2019s theorem, reviewed in Section 11.2.\n3. See, e.g., Bauschke and Combettes (2017, Theorem 14.3).\n4. This follows from the fact that the infimum of a jointly convex\nfunction is convex.\n5. We have\ninf\n\u00b5\u2208RM\nenvf(\u00b5) = inf\n\u00b5\u2208RM\ninf\nu\u2208RM\n1\n2\u2225\u00b5\u2212u\u22252\n2 + f(u)\n= inf\nu\u2208RM\ninf\n\u00b5\u2208RM\n1\n2\u2225\u00b5\u2212u\u22252\n2 + f(u)\n= inf\nu\u2208RM\nf(u).\nExamples\nTo illustrate smoothing from the Moreau envelope perspective, we\nshow how to smooth the1-norm. In this case, we obtain an analytical\nexpression for the Moreau envelope.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 875, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7fd15a2f-82a1-4c1c-9dc3-8ab2c3537c99": {"__data__": {"id_": "7fd15a2f-82a1-4c1c-9dc3-8ab2c3537c99", "embedding": null, "metadata": {"page_label": "338", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "abfcf9b2-4c8f-4095-b8e2-2258c90d947f", "node_type": "4", "metadata": {"page_label": "338", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "0f7e418910c1ae7692a29fe2ac49e7b1a5b644033e2e668e31d5cf90db5c05ac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "338 Smoothing by optimization\n3\n 2\n 1\n 0 1 2 3\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nHuber loss\nAbsolute loss\nFigure 13.1:The Huber loss is the Moreau envelope of the absolute loss.\nExample 13.1(Smoothing the1-norm via infimal convolution). We\nwish to smoothf(u) := \u2225u\u22251 = \u2211M\nj=1 |uj|. The corresponding prox-\nimal operator is the soft-thresholding operator (see Section 16.4),\nproxf(\u00b5) = arg min\nu\u2208RM\n1\n2\u2225\u00b5\u2212u\u22252\n2 + \u2225u\u22251\n= sign(\u00b5) \u00b7max(|\u00b5|\u22121,0).\nUsing Eq. (13.1) and after some algebraic manipulations, we obtain\nenvf(\u00b5) =\nM\u2211\nj=1\nhuber(\u00b5j) \u2248\nM\u2211\nj=1\n|\u00b5j|,\nwhere we defined theHuber loss\nhuber(\u00b5j) :=\n\uf8f1\n\uf8f2\n\uf8f3\n\u00b52\nj\n2 if |\u00b5j|\u2264 1\n|\u00b5j|\u2212 1\n2 if |\u00b5j|>1\n.\nThis is illustrated in Fig. 13.1 withM = 1.\nWe also illustrate in Fig. 13.2 that the Moreau envelope of nonconvex\nfunctions can be approximately computed numerically.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 801, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6bd84bd7-7ced-4cae-b2f5-9ac95b50a33e": {"__data__": {"id_": "6bd84bd7-7ced-4cae-b2f5-9ac95b50a33e", "embedding": null, "metadata": {"page_label": "339", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "12a25404-e56d-45e4-85d7-ee8724ce8943", "node_type": "4", "metadata": {"page_label": "339", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "69092420fec2ad284e1ad9e8e55aa847c77f0d94d394f1f505e7f122d1777260", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13.1. Primal approach 339\n2\n 0 2\n0\n1\n2\n3\nReLU\nOriginal\nMoreau env\n2\n 0 2\n0.00\n0.25\n0.50\n0.75\n1.00\nRamp\n2\n 0 2\n0.00\n0.25\n0.50\n0.75\n1.00\nStep\nFigure 13.2:The Moreau envelope is not limited to convex functions. For instance,\nthe ramp function is continuous but nonconvex, and the step function is not only\nnonconvex but also discontinuous. In this figure, we approximately computed the\ninfimum overu\u2208R in Definition 13.2 by restricting the search on a finite grid, in a\nclosed interval.\n13.1.3 Vector-valued functions\nThe Moreau envelope is defined byenvf(\u00b5) := infu\u2208RM f(u)+ 1\n2 \u2225\u00b5\u2212u\u22252\n2.\nAs such, it is limited to scalar-valued functionsf: RM \u2192R. To extend\nthe Moreau envelope to vector-valued functionsf: RM \u2192RT, where\nf(u) = (f1(u),...,f T(u)) and fi: RM \u2192R for i\u2208[T], we may choose\nto smooth eachfj separately to define\nenvf(\u00b5) := (envf1 (\u00b5),..., envfT(\u00b5)),\nwhere\nenvfi(\u00b5) = inf\nui\u2208RM\nfi(ui) + 1\n2\u2225\u00b5\u2212ui\u22252\n2.\nThis approach requires to solveT separate minimization problems and\nperforms the smoothing of each output coordinatei\u2208[T] independently.\nFrom Proposition 2.9, we then have that the VJP ofenvf(u) with any\ndirection d\u2208RT is\n\u2202envf(\u00b5)\u2217[d] =\nT\u2211\ni=1\n\u2202envfi(\u00b5)\u2217[di]\n=\nT\u2211\ni=1\ndi\u2207envfi(\u00b5).\nIn the particular casef(u) = (f1(u1),...,f T(uT)), we obtain\n\u2202envf(\u00b5)\u2217[d] =\nT\u2211\ni=1\ndienvfi(\u00b5i).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1286, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2e1bdd5b-00fe-479b-a33f-ad2371a9be4f": {"__data__": {"id_": "2e1bdd5b-00fe-479b-a33f-ad2371a9be4f", "embedding": null, "metadata": {"page_label": "340", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "683e0d30-7a68-40e9-a1d4-5f0ca0230cc6", "node_type": "4", "metadata": {"page_label": "340", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "505d84dc77ad33c3eba5c2c7bddcb4599dd79a5d55e74448692dadbae843279c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "340 Smoothing by optimization\nAn alternative was proposed by Roulet and Harchaoui (2022). For a\ndifferentiable functionf: RM \u2192RT, we recall that the VJP off with\na directiond\u2208RT reads\n\u2202f(u)\u2217[d] = \u2207\u27e8f,d\u27e9(u),\nwhere we defined the scalar-valued function\u27e8f,d\u27e9(u) := \u27e8f(u),d\u27e9. As a\nresult, iff is non differentiable, a natural idea is to approximate its VJP\n\u2202f(u)\u2217[d] (had it existed) by the gradient\u2207env\u27e8f,d\u27e9(\u00b5) of the Moreau\nenvelope\nenv\u27e8f,d\u27e9(\u00b5) = inf\nu\u2208RM\n\u27e8f(u),d\u27e9+ 1\n2\u2225\u00b5\u2212u\u22252\n2. (13.2)\nThis requires a single optimization problem to solve, independently of\nthe number of outputsT. Moreover, ford= ei, this recoversenvfi(\u00b5)\nas a special case.\nThis approach allows in principle to perform reverse-mode autodiff\n(gradient backpropagation) on a neural network whose layers use the\nMoreau envelope. Indeed, following Proposition 13.1, the approximate\nVJP off with a directiondis given by\n\u2202f(\u00b5)\u2217[d] \u2248\u2207env\u27e8f,d\u27e9(\u00b5) = \u00b5\u2212u\u22c6,\nwhere u\u22c6 is the solution of the minimization problem in Eq. (13.2).\nHowever, we emphasize that this minimization problem could be difficult\nto solve in general. Indeed, when performing gradient backpropagation,\nthe directiondis not necessarily non-negative, therefore the function\nbeing minimized in Eq. (13.2) could be nonconvex, even if eachfi is\nconvex. Another potential caveat is that the directiondinfluences the\nsmoothing strength, while in principle we should be able to smooth a\nfunction independently of whether we compute its VJP or not. To see\nthat, for example in the particular casef(u) = ( f1(u1),...,f T(uT)),\none easily checks that ford= (d1,...,d T), we get\nenv\u27e8f,d\u27e9(\u00b5) =\nT\u2211\ni=1\nenvdifi(\u00b5i).\nSmoothing vector-valued functions by Moreau envelope (or more gen-\nerally, by infimal convolution) remains an open area of research. We\nwill see in Chapter 14 that smoothing by convolution more naturally\nsupports vector-valued functions.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1860, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "322a319a-84ce-41ce-8a01-6df685f601e7": {"__data__": {"id_": "322a319a-84ce-41ce-8a01-6df685f601e7", "embedding": null, "metadata": {"page_label": "341", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "005133bf-6381-401f-abfb-cbccead6fdd4", "node_type": "4", "metadata": {"page_label": "341", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "aa2fe93020599275e9766bb0080f2b0ee797542f91d490d681dae27f3d93f2c8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13.2. Legendre\u2013Fenchel transforms, convex conjugates 341\n13.2 Legendre\u2013Fenchel transforms, convex conjugates\nThe Legendre-Fenchel transform, a.k.a. convex conjugate, is a way to\nturn a functionf into a new function, denotedf\u2217. We now review it in\ndetail, as it plays a major role for the dual approach to smoothing.\n13.2.1 Definition\nConsider the class of affine functions of the form\nu\u21a6\u2192\u27e8u,v\u27e9\u2212b.\nThese functions are parametrized by their slopev \u2208RM and their\nintercept \u2212b\u2208R. Now, suppose we fixv. Given a functionf(u), affine\nlower bounds off(u) are all the functions ofusuch thatb satisfies for\nall u\u2208RM,\n\u27e8u,v\u27e9\u2212b\u2264f(u) \u21d0\u21d2 \u27e8u,v\u27e9\u2212f(u) \u2264b.\nThe tightest lower bound is then defined byb such that\nb:= sup\nu\u2208dom(f)\n\u27e8u,v\u27e9\u2212f(u),\nwhere we recall that the domain off is defined by\ndom(f) := {u\u2208RM: f(u) <\u221e}.\nThis leads to the definition ofLegendre-Fenchel transform, a.k.a.\nconvex conjugate.\nDefinition 13.3(Legendre-Fenchel transform, convex conjugate). Given\na functionf: RM \u2192R \u222a{\u221e}, its convex conjugate is defined by\nf\u2217(v) := sup\nu\u2208dom(f)\n\u27e8u,v\u27e9\u2212f(u).\nWe use asup rather than amax to indicate thatf\u2217(v) is potentially\n\u221e. Following the previous discussion,\u2212f\u2217(v) is the intercept of the\ntightest affine lower bound with slopev of f(u). This is illustrated\nFig. 13.3.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1259, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "93e2b73d-8b0f-4c5f-8ef2-6ce049e42325": {"__data__": {"id_": "93e2b73d-8b0f-4c5f-8ef2-6ce049e42325", "embedding": null, "metadata": {"page_label": "342", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "81056aa8-30d4-446e-a281-b5e7d2d7373f", "node_type": "4", "metadata": {"page_label": "342", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "324383540a97020ec1d417b2c8a0e1558574c3f8661dcfc9602170c7606fc39c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "342 Smoothing by optimization\n0 1\nu\nf (u)\nf * (v)\nv\nFigure 13.3:For a fixed slopev, the functionu\u21a6\u2192uv\u2212f\u2217(v) is the tighest affine\nlower bound off with slopev.\nThe Legendre-Fenchel transform is a function transformation, as it\nproduces a new functionf\u2217. It can be seen as a dual representation\nof a function: instead of representing a convex functionf by its graph\n(u,f(u)) for u\u2208dom(f), we can represent it by the set of tangents\nwith slope v and intercept \u2212f\u2217(v) for v \u2208dom(f\u2217), as illustrated\nin Fig. 13.4. As the name \u201cconvex conjugate\u201d indicates, it is convex,\neven if the original function is not.\n13.2.2 Closed-form examples\nComputing f\u2217(v) involves the resolution of a maximization problem,\nwhich could be difficult in general without assumption onf. In some\ncases, however, we can compute an analytical expression, as we now\nillustrate.\nExample 13.2(Analytical conjugate examples). Whenf(u) = 1\n2 \u2225u\u22252\n2,\nwith dom(f) = RM, the conjugate is\nf\u2217(v) = max\nu\u2208RM\n\u27e8u,v\u27e9\u2212 1\n2\u2225u\u22252\n2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 982, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9486f255-38da-4734-90ba-34209394dffb": {"__data__": {"id_": "9486f255-38da-4734-90ba-34209394dffb", "embedding": null, "metadata": {"page_label": "343", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa8f088a-df23-498a-b721-18f4b314afe9", "node_type": "4", "metadata": {"page_label": "343", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "2263a9a4ccc7156c1ed8c43630637328cff70207eb396242453a05b53cffaa0e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13.2. Legendre\u2013Fenchel transforms, convex conjugates 343\n0.0 0.5 1.0\nu\n1.0\n0.5\n0.0\nf (u)\nf * (v)\nv\n2\n 1\n 0 1\nv\n0.0\n0.5\n1.0f * (v)\nFigure 13.4: Left:instead of representing a convex functionf by its graph(u,f(u))\nfor u\u2208dom(f), we can represent it by the set of tangents with slopevand intercept\n\u2212f\u2217(v) for v\u2208dom(f\u2217). Right: by varying the slopev of all possible tangents, we\nobtain a function of the slopev rather than of the original inputu. The colors of the\ntangents on the left are chosen to match the colors of the vertical lines on the right.\nSetting the gradientu\u21a6\u2192\u27e8u,v\u27e9\u22121\n2 \u2225u\u22252\n2 to zero, we obtainu\u22c6 = v.\nPlugging u\u22c6 back, we therefore obtain\nf\u2217(v) = \u27e8u\u22c6,v\u27e9\u2212 1\n2\u2225u\u22c6\u22252\n2 = 1\n2\u2225v\u22252\n2.\nTherefore, f = f\u2217in this case.\nWhen f(u) = \u27e8u,log u\u27e9, withdom(f) = RM\n+ , the minimizer of\nu\u21a6\u2192\u27e8u,v\u27e9\u2212\u27e8u,log u\u27e9is u\u22c6 = exp(v\u22121) and the conjugate is\nf\u2217(v) =\nM\u2211\nj=1\nexp(vj \u22121).\nSee for instance Boyd and Vandenberghe (2004) or Beck (2017) for\nmany more examples.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 950, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cb235361-ba69-409e-8d79-29a042146674": {"__data__": {"id_": "cb235361-ba69-409e-8d79-29a042146674", "embedding": null, "metadata": {"page_label": "344", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eec33efe-81f1-4ccd-8e9e-c2abd15b9bcb", "node_type": "4", "metadata": {"page_label": "344", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "50d17c0be143a4d5476ef671953545c6be958d1e27c0dc30d76c92d7d07141d7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "344 Smoothing by optimization\nConstraining the domain\nWe can incorporate constraints using anindicator function with\nvalues in the extended real lineR \u222a{\u221e},\n\u03b9C(u) :=\n\uf8f1\n\uf8f2\n\uf8f3\n0 if u\u2208C\n+\u221e otherwise\n.\nExample 13.3(Incorporating constraints). If f(u) = \u03b9C(u), where\nCis a convex set, then\nf\u2217(v) = sup\nu\u2208dom(f)\n\u27e8u,v\u27e9\u2212f(u) = sup\nu\u2208C\n\u27e8u,v\u27e9:= \u03c3C(v),\nwhich is known as thesupport functionof C. The corresponding\nargmax (assuming that it exists),\nv\u21a6\u2192arg max\nu\u2208C\n\u27e8u,v\u27e9,\nis known as thelinear maximization oracle(LMO) of C. As\nanother example, iff(u) = \u27e8u,log u\u27e9+ \u03b9\u25b3M(u) then\nf\u2217(v) = logsumexp(v) = log\nM\u2211\ni=1\nexp(vj).\nWe postpone a proof to Proposition 13.9.\n13.2.3 Properties\nThe conjugate enjoys several useful properties, that we now summarize.\nProposition 13.2(Convex conjugate properties).\n1. Convexity: f\u2217(v) is a convex function for all f: RM \u2192\nR \u222a{\u221e} (even iff is nonconvex).\n2. Fenchel-Young inequality:for allu,v\u2208RM\nf(u) + f\u2217(v) \u2212\u27e8u,v\u27e9\u2265 0.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 936, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "021b96dc-f6d3-45d6-84f3-01554c196bd7": {"__data__": {"id_": "021b96dc-f6d3-45d6-84f3-01554c196bd7", "embedding": null, "metadata": {"page_label": "345", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d4f313f2-f6d8-414a-8570-b0d87ea84660", "node_type": "4", "metadata": {"page_label": "345", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "0cf0c3b672f74a796524a7da9c662efcada5bda3f389bebede7c9ce5f7b1ef89", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13.2. Legendre\u2013Fenchel transforms, convex conjugates 345\n3. Gradient: if the supremum in Definition 13.3 is uniquely\nachieved, thenf\u2217(v) is differentiable atvand its gradient is\n\u2207f\u2217(v) = arg max\nu\u2208dom(f)\n\u27e8u,v\u27e9\u2212f(u).\nOtherwise, f\u2217(v) is sub-differentiable atvand we get a sub-\ngradient instead.\n4. Maps: If f and f\u2217are differentiable, then\nv= \u2207f(u) \u21d0\u21d2u= \u2207f\u2217(v) \u21d0\u21d2f\u2217(v)+f(u)\u2212\u27e8u,v\u27e9= 0.\n5. Biconjugate: f = f\u2217\u2217if and only iff is convex and closed\n(i.e., its sublevel sets form a closed set), otherwisef\u2217\u2217\u2264f.\nProof.\n1. This follows from the fact thatv\u21a6\u2192supu\u2208Cg(u,v) is convex ifg\nis convex inv. Note that this is true even ifg is nonconvex inu.\nHere, g(u,v) = \u27e8u,v\u27e9\u2212f(u), which is affine invand therefore\nconvex inv.\n2. This follows immediately from Definition 13.3.\n3. This follows from Danskin\u2019s theorem, reviewed in Section 11.2.\nAnother way to see this is by observing that\nf\u2217(v) = \u27e8g,v\u27e9\u2212f(g)\nf\u2217(v\u2032) \u2265\u27e8g,v\u2032\u27e9\u2212f(g),\nwhere g:= arg max\nu\u2208dom(f)\n\u27e8u,v\u27e9\u2212f(u). Subtracting the two, we obtain\nf\u2217(v\u2032) \u2265f\u2217(v) + \u27e8g,v\u2032\u2212v\u27e9.\nNow, using thatf\u2217is convex and Definition 15.6, we obtain that\ng= \u2207f\u2217(v).\n4. See, e.g., Bauschke and Combettes (2017, Proposition 16.10).\n5. See Boyd and Vandenberghe (2004, Section 3.3).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1196, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "985d9edc-4c2a-4105-9a58-433de98bd3c5": {"__data__": {"id_": "985d9edc-4c2a-4105-9a58-433de98bd3c5", "embedding": null, "metadata": {"page_label": "346", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "daa736cc-f5d7-4dc1-bd38-be113d2b0cf3", "node_type": "4", "metadata": {"page_label": "346", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "2f5d1543e602f6112b7398cd8d29375eabca9860c32a071524d523878f59e96b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "346 Smoothing by optimization\n13.2.4 Conjugate calculus\nWhile deriving a convex conjugate expression can be difficult in general,\nin some cases, it is possible to use simple rules to derive conjugates in\nterms of other conjugates.\nProposition 13.3(Conjugate calculus rules).\n1. Separable sum of functions:if f(u) = \u2211M\nj=1 fj(uj), then\nf\u2217(v) =\nM\u2211\nj=1\nf\u2217\nj(vj).\n2. Scalar multiplication:if f(u) = c\u00b7g(u), forc> 0, then\nf\u2217(v) = c\u00b7g\u2217(v/c).\n3. Additiontoanaffinefunctionandtranslation: iff(u) =\ng(u) + \u27e8\u03b1,u\u27e9+ \u03b2, then\nf\u2217(v) = g\u2217(v\u2212\u03b1) \u2212\u03b2.\n4. Composition with an invertible linear map:if f(u) =\ng(Mu), wherex\u21a6\u2192Mxis an invertible linear map, then\nf\u2217(v) = g\u2217(M\u2212Tv).\n5. Non-separable sum of functions:if h1 and h2 are convex\nfunctions, then(h1 + h2)\u2217= h\u2217\n1\u25a1h\u2217\n2, where\u25a1 is the infimal\nconvolution operator.\n13.2.5 Fast Legendre transform\nWhen an analytical expression is not available, we can resort to numeri-\ncal schemes to approximately compute the transform / conjugate. When\nf is convex, because\u2212f is concave, the maximization in Definition 13.3\nis that of a concave function. Therefore, the conjugate can be computed\nto arbitrary precision in polynomial time using classical iterative algo-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1185, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "994c67d6-052c-4463-b39c-b983ab06794e": {"__data__": {"id_": "994c67d6-052c-4463-b39c-b983ab06794e", "embedding": null, "metadata": {"page_label": "347", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "abadf77c-9f91-4456-8c2d-e07f73c0cd70", "node_type": "4", "metadata": {"page_label": "347", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "62b74c93991ea3bfa30f6822085a15830b0efb6ff13cb4185825ae08c4477346", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13.3. Dual approach 347\nrithms for constrained optimization such as projected gradient descent\n(Section 16.3) or conditional gradient a.k.a. Frank-Wolfe (Jaggi, 2013).\nWithout convexity assumption onf, f\u2217(v) can be approximated by\nf\u2217(v) \u2248sup\nu\u2208U\n\u27e8u,v\u27e9\u2212f(u),\nwhere U\u2286 dom(f) is a discrete grid of values. We can then compute\nf\u2217(v) for several inputsv\u2208V using the linear-time Legendre transform\nalgorithm (Lucet, 1997), whereV\u2286 dom(f\u2217) is another discrete grid.\nThe complexity isO(|U|\u00b7|V| ), which is linear in the grid sizes. However,\nthegridsizesaretypically |U|= |V|= O(NM),for N equally-distributed\npoints in each of theM dimensions. Therefore, this approach is limited\nto small-dimensional settings, e.g.,M \u2208{1,2,3}.\n13.3 Dual approach\nPreviously, we presented how to smooth a function by performing\nits infimal convolution with a primal-space regularizationR. We now\npresent how to smooth a function by regularizing its Legendre-Fenchel\ntransform (convex conjugate) instead. This dual, equivalent approach,\nis often mathematically more convenient.\n13.3.1 Duality between strong convexity and smoothness\nWe begin by stating a well-known result that will underpin this whole\nsection: smoothness and strong convexity are dual to each other (Hiriart-\nUrruty and Lemar\u00e9chal, 1993; Kakadeet al., 2009; Beck, 2017; Zhou,\n2018).\nProposition 13.4(Duality between strong convexity and smoothness).\nf is 1\n\u00b5-strongly convex w.r.t. the norm\u2225\u00b7\u2225 over dom(f) if and only\nif f\u2217is \u00b5-smooth w.r.t. the dual norm\u2225\u00b7\u2225\u2217over dom(f\u2217).\nFor a review of the notions of smoothness and strong convexity,\nsee Section 15.4. We give two examples of strongly-convex and smooth\nconjugate pairs in Table 13.2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1678, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e41b5dce-b645-456e-91d1-fa78c7754a59": {"__data__": {"id_": "e41b5dce-b645-456e-91d1-fa78c7754a59", "embedding": null, "metadata": {"page_label": "348", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c86d817-b5d0-4530-8ef0-105d266f8067", "node_type": "4", "metadata": {"page_label": "348", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "c0add4c2c042daf12b61a3d911fe111545f516c8f5b2e04ea1c933264a931341", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "348 Smoothing by optimization\nTable 13.2:Examples of strongly-convex and smooth conjugate pairs.\nFunction Norm Domain Conjugate Dual norm Dual domain\n1\n2 \u2225u\u22252\n2 \u2225\u00b7\u22252 RM 1\n2 \u2225v\u22252\n2 \u2225\u00b7\u22252 RM\n\u27e8u,log u\u27e9 \u2225\u00b7\u2225 1 \u25b3M logsumexp(v) \u2225\u00b7\u2225\u221e RM\n13.3.2 Smoothing by dual regularization\nThe duality between smoothness and strong convexity suggests a generic\napproach in order to smooth a functionf: RM \u2192R, by going through\nthe dual space.\n1. Compute the conjugatef\u2217:\nf\u2217(v) := sup\nu\u2208dom(f)\n\u27e8u,v\u27e9\u2212f(u).\n2. Add strongly-convex regularization\u2126 to the conjugate:\nf\u2217\n\u2126(v) := f\u2217(v) + \u2126(v). (13.3)\n3. Go back to the primal space, by computing the conjugate off\u2217\n\u2126:\nf\u2126(u) := f\u2217\u2217\n\u2126 (u) = max\nv\u2208RM\n\u27e8u,v\u27e9\u2212f\u2217\n\u2126(v).\nNote that u and v belong to different spaces, i.e.,u \u2208dom(f) and\nv \u2208dom(f\u2217). Following Proposition 13.4, if\u2126 is \u00b5-strongly convex,\nthen f\u2126(u) is 1\n\u00b5-smooth. Furthermore, following Proposition 13.2,f\u2126(u)\nis convex, even iff is nonconvex. Therefore,f\u2126(u) is asmooth and\nconvex relaxationof f(u).\nSteps 1 and 3 are the most challenging, as they both require the\nderivation of a conjugate. While an analytical solution may not exist in\ngeneral, in some simple cases, there is, as we now illustrate.\nExample 13.4(Smoothing the1-norm via dual regularization). Were-\nvisit Example 13.1, this time from the dual perspective. We wish\nto smooth out the1-norm f(u) := \u2225u\u22251 = \u2211M\nj=1 |uj|.\n1. Compute the conjugate.The conjugate of any norm\u2225\u00b7\u2225", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1413, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "509a2db2-3311-42dc-a583-29bfbd3ca711": {"__data__": {"id_": "509a2db2-3311-42dc-a583-29bfbd3ca711", "embedding": null, "metadata": {"page_label": "349", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b479bc95-ece2-4c3b-a6a2-2dc65c02c4af", "node_type": "4", "metadata": {"page_label": "349", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "aaf8879badecf13797c83cee39bb8366791d92e2d70884ff99f81deaf53951f6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13.3. Dual approach 349\nis the indicator function of the dual norm\u2019s unit ball{v \u2208\nRM: \u2225v\u2225\u2217 \u2264 1}(see e.g. Boyd and Vandenberghe (2004,\nExample 3.26)). The dual norm of\u2225u\u22251 is \u2225v\u2225\u221e. Moreover,\n{v\u2208RM: \u2225v\u2225\u221e\u22641}= [\u22121,1]M.\nRecalling that\u03b9C is the indicator function ofC, we obtain\nf\u2217(v) = \u03b9[\u22121,1]M(v).\n2. Addingstrongly-convexregularization. Weaddquadratic\nregularization \u2126(v) := 1\n2 \u2225v\u22252\n2 to define\nf\u2217\n\u2126(v) := \u03b9[\u22121,1]M(v) + \u2126(v).\n3. Going back to the primal.\nf\u2126(u) = f\u2217\u2217\n\u2126 (u) = \u27e8u,v\u22c6\u27e9\u2212\u2126(v\u22c6) =\nM\u2211\ni=1\nhuber(ui),\nwhere v\u22c6 = clip (u) := max (min (u,1) ,\u22121).\nWe therefore indeed recover the Huber loss from Example 13.1.\nReLU functions can be smoothed out in a similar way, as we see in\nmore details in Section 13.4.\nThe dual approach allows us to easily bound the smoothed function\nin terms of the original function.\nProposition 13.5(Bounds). IfL\u2126 \u2264\u2126(v) \u2264U\u2126 forall v\u2208dom(\u2126),\nthen for allu\u2208RM,\nf(u) \u2212U\u2126 \u2264f\u2126(u) \u2264f(u) \u2212L\u2126.\nProof. Let us define\nv\u22c6 := arg max\nv\u2208RM\n\u27e8u,v\u27e9\u2212f\u2217(v)\nv\u22c6\n\u2126 := arg max\nv\u2208RM\n\u27e8u,v\u27e9\u2212f\u2217\n\u2126(v),", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1000, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b11698ce-a3c8-43c3-8949-2eeb8fe99857": {"__data__": {"id_": "b11698ce-a3c8-43c3-8949-2eeb8fe99857", "embedding": null, "metadata": {"page_label": "350", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7ad29451-98c8-4226-a133-9c4570004c6a", "node_type": "4", "metadata": {"page_label": "350", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "b93f23b8e1d1ea5f862523c77b358261914c2832f792d89473a578883792d5c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "350 Smoothing by optimization\nwhere we recall thatf\u2217\n\u2126 := f\u2217+ \u2126. We then have for allu\u2208RM\nf\u2126(u) = \u27e8u,v\u22c6\n\u2126\u27e9\u2212f\u2217\n\u2126(v\u22c6\n\u2126) \u2265\u27e8u,v\u22c6\u27e9\u2212f\u2217\n\u2126(v\u22c6) = f(u) \u2212\u2126(v\u22c6)\nand similarly\nf(u) \u2212\u2126(v\u22c6\n\u2126) = \u27e8u,v\u22c6\u27e9\u2212f\u2217(v\u22c6) \u2212\u2126(v\u22c6\n\u2126) \u2265\u27e8u,v\u22c6\n\u2126\u27e9\u2212f\u2217\n\u2126(v\u22c6\n\u2126) = f\u2126(u).\nCombining the two withL\u2126 \u2264\u2126(v) \u2264U\u2126 for allv\u2208dom(\u2126), we obtain\nf(u) \u2212U\u2126 \u2264f(u) \u2212\u2126(v\u22c6) \u2264f\u2126(u) \u2264f(u) \u2212\u2126(v\u22c6\n\u2126) \u2264f(u) \u2212L\u2126.\nRemark 13.1(The gradient is differentiable almost everywhere). From\nProposition 13.2, the gradient off\u2126(u) equals\n\u2207f\u2126(u) = arg max\nv\u2208RM\n\u27e8u,v\u27e9\u2212f\u2217\n\u2126(v).\nIf \u2126 is strongly convex, thenf\u2126 is smooth, meaning that \u2207f\u2126\nis Lipschitz continuous. From Rademacher\u2019s theorem reviewed in\nSection 2.7.1,\u2207f\u2126 is then differentiable almost everywhere (that is,\nf\u2126 is twice differentiable almost everywhere). We use this property\nin the sequel to define continuous differentiable almost everywhere\nrelaxations of step functions and argmax operators.\n13.3.3 Equivalence between primal and dual regularizations\nSo far, we saw two approches to obtain a smooth approximation of\na functionf. The first approach is based on the infimal convolution\nf\u25a1R, whereR: dom (f) \u2192R denotes primal regularization. The sec-\nond approach is based on regularizing the Legendre-Fenchel transform\n(convex conjugate)f\u2217of f with some dual regularization\u2126, to define\nf\u2126 = (f\u2217+ \u2126)\u2217. It turns out that both approaches are equivalent.\nProposition 13.6(Equivalence between primal and dual regularizations).\nLet f: RM \u2192R \u222a{\u221e} and R: RM \u2192R \u222a{\u221e}, both convex and\nclosed. Then,f\u2126 = (f\u2217+ \u2126)\u2217= f\u25a1R with \u2126 = R\u2217.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1505, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "121aa955-d908-49b0-beb7-b844f39a8889": {"__data__": {"id_": "121aa955-d908-49b0-beb7-b844f39a8889", "embedding": null, "metadata": {"page_label": "351", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a8ba6e53-d2e7-4ec6-b201-f82740ffd64b", "node_type": "4", "metadata": {"page_label": "351", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "2b4dbbc01bb5389691005705df7d64442f624ee264f1a8b3567b614bbef5649a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13.3. Dual approach 351\nProof. We have\nf\u2126(u) = (f\u2217+ \u2126)\u2217(u) = sup\nv\u2208dom(f\u2217)\n\u27e8u,v\u27e9\u2212f\u2217(v) \u2212\u2126(v).\nIf h1 and h2 are convex, we have(h1 + h2)\u2217 = h\u2217\n1\u25a1h\u2217\n2 (Beck, 2017,\nTheorem 4.17). Usingh1 = f\u2217and h2 = \u2126 = R\u2217gives the desired result\nusing thatf\u2217\u2217= f and R\u2217\u2217= R since both are convex and closed (see\nProposition 13.2).\nIn particular, with\u2126 = 1\n2 \u2225\u00b7\u22252\n2 = \u2126\u2217, this shows that the Moreau\nenvelope can equivalently be written as\nenvf = f\u2126 = f\u2126\u2217.\nGiven the equivalence between the primal and dual approaches, using\none approach or the other is mainly a matter of mathematical or\nalgorithmic convenience, depending on the case.\nIn this book, we focus on applications of smoothing techniques to dif-\nferentiable programming. For applications to non-smooth optimization,\nsee Nesterov (2005) and Beck and Teboulle (2012).\n13.3.4 Regularization scaling\nDual approach\nIf \u2126 is 1-strongly convex, thenf\u2126 is a1-smooth approximation of the\noriginal functionf. To control the smoothness of the approximation,\nit suffices to regularize with\u03b5\u2126 for \u03b5 >0, leading to a1/\u03b5-smooth\napproximation f\u03b5\u2126 of f. Moreover, one can check that\nf\u03b5\u2126(v) = \u03b5f\u2126(v/\u03b5)\n\u2207f\u03b5\u2126(v) = \u2207f\u2126(v/\u03b5).\nTherefore, if we know how to computef\u2126, we can also computef\u03b5\u2126 and\nits gradient easily. Furthermore, the approximation error induced by\nthe smoothing can be quantified using Proposition 13.5 as we then have\nf(u) \u2212\u03b5U\u2126 \u2264f\u2126(u) \u2264f(u) \u2212\u03b5L\u2126,\nprovided thatL\u2126 \u2264\u2126(v) \u2264U\u2126 for allv\u2208dom(\u2126).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1424, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "35abc1c2-7d75-4d40-8b61-d0bed1127e2c": {"__data__": {"id_": "35abc1c2-7d75-4d40-8b61-d0bed1127e2c", "embedding": null, "metadata": {"page_label": "352", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e2138144-35e1-4f8a-a96b-911c96d8c45b", "node_type": "4", "metadata": {"page_label": "352", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "619d071b0e954a1e220d572e1854f22091fc8b9a014fcce41893a538295210dd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "352 Smoothing by optimization\nPrimal approach\nFollowing Definition 13.2, if we use dual regularization\u03b5\u2126, where\u03b5> 0\ncontrols the regularization strength, the corresponding primal regular-\nization isR= \u03b5\u2126\u2217(\u00b7/\u03b5). That is, we have\nf\u03b5\u2126 = f\u25a1\u03b5\u2126\u2217(\u00b7/\u03b5).\nIn the particular case\u2126(v) = 1\n2 \u2225v\u22252\n2, we have\nR(u) = \u03b5\n2\u2225u/\u03b5\u22252\n2 = 1\n2\u03b5\u2225u\u22252\n2 = 1\n\u03b5\u2126(u).\nWe therefore get\nf\u03b5\u2126 = f\u25a11\n\u03b5\u2126 = 1\n\u03b5(\u03b5f\u25a1\u2126) = 1\n\u03b5env\u03b5f.\n13.3.5 Generalized entropies\nA natural choice of dual regularization\u2126(\u03c0), when\u03c0\u2208\u25b3M is a discrete\nprobability distribution, is a negative entropy function, also known as\nnegentropy. Since negentropies play a major role in smoothed max\noperators, we discuss them in detail here.\nInformation content and entropy\nAn entropy function measures the amount of \u201csuprise\u201d of a random\nvariable or equivalently of a distribution. To define an entropy, we must\nfirst define theinformation contentI(E) of an eventE. The value\nreturned by such a function should be0 if the probability of the event is\n1, as there is no surprise. Conversely, information content should attain\nits maximal value if the probability of the event is0, as it is maximally\nsurprising. Furthermore, the more probable an eventE is, the less\nsurprising it is. Therefore, whenp(E) increases, I(E) should decrease.\nOverloading the notation, we also write the information content of the\noutcome y of a random variableY as the information content of the\nevent {Y = y},\nI(y) := I({Y = y}).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1434, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d49d06af-418b-4b00-814f-e76d6f5496dd": {"__data__": {"id_": "d49d06af-418b-4b00-814f-e76d6f5496dd", "embedding": null, "metadata": {"page_label": "353", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a0f3163-9c73-4fd7-a425-4a4e1fdee66c", "node_type": "4", "metadata": {"page_label": "353", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "1c411a9654e970e6f5ec196b3f0a18fdd72f6d1238876ee8f7e96388ff41e282", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13.3. Dual approach 353\nGiven an information content function, we can then define the\nentropyH(Y) of a random variableY \u2208Y as the expected information\ncontent,\nH(Y) := E[I(Y)].\nDifferent definitions of information content lead to different definitions\nof entropy.\nShannon\u2019s entropy\nA definition of information content satisfying the criteria above is\nI(E) := log\n( 1\np(E)\n)\n= \u2212log p(E).\nIndeed, \u2212log 1 = 0, \u2212log 0 = \u221eand \u2212log is a decreasing function over\n(0,1]. Using this information content definition leads toShannon\u2019s\nentropy(Shannon, 1948)\nH(Y) = E[I(Y)] = \u2212\n\u2211\ny\u2208Y\np(y) logp(y).\nWe can therefore define the Shannon entropy of a discrete probability\ndistribution \u03c0\u2208\u25b3M as\nH(\u03c0) = \u2212\nM\u2211\ni=1\n\u03c0ilog \u03c0i = \u2212\u27e8\u03c0,log \u03c0\u27e9\nand use the corresponding negentropy as regularization\n\u2126(\u03c0) = \u2212H(\u03c0) = \u27e8\u03c0,log \u03c0\u27e9.\nThe function is strongly convex w.r.t.\u2225\u00b7\u22251 over \u25b3M. However, it is not\nstrongly convex overRM\n+ , since this is not a bounded set; see for instance\n(Blondel, 2019, Proposition 2). Since\u2126 is added tof\u2217 in Eq. (13.3),\nwe can therefore use this choice of\u2126 to smooth out a functionf if\ndom(f\u2217) \u2286\u25b3M.\nGini\u2019s entropy\nAs an alternative, we can define information content as\nI(E) = 1\n2(1 \u2212p(E)).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1181, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c8e13854-d4b6-46cd-81d7-9effc6891b1e": {"__data__": {"id_": "c8e13854-d4b6-46cd-81d7-9effc6891b1e", "embedding": null, "metadata": {"page_label": "354", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "214b6204-23d7-4e73-b275-87aa7aeba666", "node_type": "4", "metadata": {"page_label": "354", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "4b21fe2984c95fbbb45c4d51fc60e4ba1f437b74806c89c3c11810d54412a159", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "354 Smoothing by optimization\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nTsallis 1 (Shannon)\nTsallis = 1.5\nTsallis = 2 (Gini)\nFigure 13.5:Tsallis entropies of the distribution\u03c0= (\u03c0,1 \u2212\u03c0) \u2208\u25b32, for\u03c0\u2208[0,1].\nAn entropy is a non-negative concave function that attains its maximum at the\nuniformdistribution,here (0.5,0.5).Anegativeentropy,a.k.a.negentropy,canbeused\nas a dual regularization function\u2126 to smooth out a functionf when dom(f\u2217) \u2286\u25b3M.\nThe 1\n2 factor is for later mathematical convenience. This again satisfies\nthe criteria of an information content function. Indeed, i) whenp(E) = 1,\nI(E) = 0 ii) when p(E) = 0 , I(E) attains its maximum of 1\n2 iii)\nthe function is decreasing w.r.t.p(E). Using this information content\ndefinition leads toGini\u2019s entropya.k.a. Gini index (Gini, 1912)\nH(Y) = E[I(Y)] = 1\n2\n\u2211\ny\u2208Y\np(y)(1 \u2212p(y)).\nWe can use Gini\u2019s negative entropy to define for all\u03c0\u2208\u25b3M\n\u2126(\u03c0) = 1\n2\u27e8\u03c0,\u03c0\u22121\u27e9= 1\n2(\u2225\u03c0\u22252\n2 \u22121).\nThe function is strongly convex w.r.t.\u2225\u00b7\u22252 over RM. We can therefore\nuse this choice of\u2126 to smooth out a functionf if dom(f\u2217) \u2286RM. This\nmeans that the set of functions that we can smooth out with Gini\nentropy is larger than the set of functions we can smooth out with\nShannon entropy.\nTsallis entropies\nGiven \u03b1\u22651, a more general information content definition is\nI(E) = 1\n\u03b1(\u03b1\u22121)(1 \u2212p(E)\u03b1\u22121).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1327, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "96b4197b-6836-4dc6-ad10-dcfc4916fc62": {"__data__": {"id_": "96b4197b-6836-4dc6-ad10-dcfc4916fc62", "embedding": null, "metadata": {"page_label": "355", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9af93a94-6b06-4d3f-92b0-7f083b70eba4", "node_type": "4", "metadata": {"page_label": "355", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "cb9729d4e06447a032c808ec97db23211b2ee80755e7ed8be4f7580b5c63b265", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13.3. Dual approach 355\n(1, 0, 0) (0, 1, 0)\n(0, 0, 1)\nTsallis 1 (Shannon)\n(1, 0, 0) (0, 1, 0)\n(0, 0, 1)\nTsallis = 1.5\n(1, 0, 0) (0, 1, 0)\n(0, 0, 1)\nTsallis = 2 (Gini)\nFigure 13.6:Contours of Tsallis entropies on the probability simplex.\nUsing this definition leads to theTsallis entropy(Tsallis, 1988)\nH(Y) = E[I(Y)] = 1\n\u03b1(\u03b1\u22121)\n\u2211\ny\u2208Y\np(y)(1 \u2212p\u03b1\u22121(y)).\nThe Tsallis entropy recovers the Shannon entropy in the limit\u03b1\u21921\nand the Gini entropy when\u03b1 = 2. We can use the Tsallis negative\nentropy to define for all\u03c0\u2208\u25b3M\n\u2126(\u03c0) = 1\n\u03b1(\u03b1\u22121)\u27e8\u03c0,\u03c0\u03b1\u22121 \u22121\u27e9= 1\n\u03b1(\u03b1\u22121)(\u2225\u03c0\u2225\u03b1\n\u03b1 \u22121),\nwhere \u2225v\u2225p is thep-norm for (p\u22651)\n\u2225v\u2225p :=\n(M\u2211\ni=1\nvp\ni\n)1\np\n,\nso that\n\u2225v\u2225p\np =\nM\u2211\ni=1\nvp\ni.\nTsallis entropies for\u03b1\u21921 (Shannon entropy),\u03b1= 1.5 and \u03b1= 2 (Gini\nentropy) are illustrated in Fig. 13.5 and Fig. 13.6.\nDefinition and properties of generalized entropies\nSo far, we saw how to define an entropy as the expected information\ncontent. However, generalized entropies (DeGroot, 1962; Gr\u00fcnwald and\nDawid, 2004) do not necessarily need to take this form. We follow the\ndefinition of Blondelet al.(2020).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1062, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d6a80241-51aa-4fd5-8b06-4dcd76ba92e8": {"__data__": {"id_": "d6a80241-51aa-4fd5-8b06-4dcd76ba92e8", "embedding": null, "metadata": {"page_label": "356", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6428f0c8-9897-439a-946b-3395e511bff1", "node_type": "4", "metadata": {"page_label": "356", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "3a25a1f7628794c2082d7a9a82b4bc078f9924fe2d97656b7e9a6aa8cdfb4ed7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "356 Smoothing by optimization\nDefinition 13.4(Entropy function). A function H: \u25b3M \u2192R+ is\nan entropy if\n1. H(\u03c0) = 0 if \u03c0\u2208{e1,..., eM},\n2. H is strictly concave,\n3. H(P\u03c0) = H(\u03c0) for any permutation matrixP.\nThis definition implies thatH is non-negative and is uniquely maxi-\nmized by the uniform distribution (Blondelet al., 2020, Proposition 4).\nThis is indeed what we expect from an entropy function. An example is\nthe squaredp-norm entropy (Blondelet al., 2020)\nH(\u03c0) = 1\n2 \u22121\n2\u2225\u03c0\u22252\np.\nSince the squaredp-norm is strongly convex forp\u2208(1,2] (Ball et al.,\n2002), this entropy is strongly concave forp\u2208(1,2] and can therefore\nbe used to smooth out functions.\nWenowillustratehowtoapplythesetechniquestocomputesmoothed\nReLUs and smoothed max operators, as well as continuous relaxations\nof step functions and argmax operators.\n13.4 Smoothed ReLU functions\nTo demonstrate the application of the smoothing techniques discussed in\nthis chapter, we begin by explaining how to smooth the ReLU function.\nThe ReLU function is defined by\nrelu(u) :=\n\uf8f1\n\uf8f2\n\uf8f3\nu if u\u22650\n0 otherwise\n= max(u,0).\nWe recall that in order to smooth a functionf by the dual approach,\nwe calculate its conjugate f\u2217, add regularization \u2126 to it to obtain\nf\u2217\n\u2126 := f\u2217+ \u2126 and then obtainf\u2126 by computingf\u2217\u2217\n\u2126 .\nHere, we wish to smooth outf = relu. Its convex conjugate is\nrelu\u2217(\u03c0) = \u03b9[0,1](\u03c0) =\n\uf8f1\n\uf8f2\n\uf8f3\n0 if \u03c0\u2208[0,1]\n\u221e if \u03c0\u0338\u2208[0,1]\n.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1381, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cb89fe76-e678-4608-99bc-bcd4c90d3273": {"__data__": {"id_": "cb89fe76-e678-4608-99bc-bcd4c90d3273", "embedding": null, "metadata": {"page_label": "357", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e9ccb713-9c28-4d0f-be76-423f1336899d", "node_type": "4", "metadata": {"page_label": "357", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "08046afbcdc848e2cd7758405ea3805b447d388f25dba7a57649d44ca4ba98c8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13.4. Smoothed ReLU functions 357\nTo notice why, we observe that\nrelu(u) = max\n\u03c0\u2208[0,1]\nu\u00b7\u03c0= max\n\u03c0\u2208{0,1}\nu\u00b7\u03c0=\n\uf8f1\n\uf8f2\n\uf8f3\nu if u\u22650\n0 otherwise\n. (13.4)\nIndeed, since the objective is linear in\u03c0, the maximum is attained at\none of the extreme points of[0,1], so that we can replace the constraint\n\u03c0 \u2208[0,1] with \u03c0 \u2208{0,1}. This shows that the ReLU is exactly the\nsupport function of[0,1]. Since the conjugate of the support function\nis the indicator function, we indeed obtainrelu\u2217= \u03b9[0,1]. We therefore\nhave\nrelu\u2217\n\u2126(\u03c0) = relu\u2217(\u03c0) + \u2126(\u03c0) = \u03b9[0,1](\u03c0) + \u2126(\u03c0)\nand for some choice of\u2126, we need to be able to compute\nrelu\u2126(u) = max\n\u03c0\u2208R\nu\u00b7\u03c0\u2212(\u03b9[0,1](\u03c0) + \u2126(\u03c0))\n= max\n\u03c0\u2208[0,1]\nu\u00b7\u03c0\u2212\u2126(\u03c0).\nThe softplus\nIf we use the regularizer\u2126(\u03c0) = \u03c0log \u03c0+ (1 \u2212\u03c0) log(1 \u2212\u03c0), which\ncomes from using Shannon\u2019s negentropy\u27e8\u03c0,log \u03c0\u27e9with \u03c0= (\u03c0,1 \u2212\u03c0),\nwe obtain\nrelu\u2126(u) = softplus(u) = log(1 + exp(u)).\nThis result is a special case of Proposition 13.9.\nThe sparseplus\nIf we use the regularizer\u2126(\u03c0) = \u03c0(\u03c0\u22121), which comes from using\nGini\u2019s negentropy with1\n2 \u27e8\u03c0,\u03c0\u22121\u27e9with \u03c0= (\u03c0,1 \u2212\u03c0), we obtain\nrelu\u2126(u) = sparseplus(u) =\n\uf8f1\n\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3\n0, u \u2264\u22121\n1\n4 (u+ 1)2, \u22121 <u< 1\nu, u \u22651\n.\nSee Fig. 13.8 (left figure) for a comparison of softplus and sparseplus.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1200, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "95d94a5c-e569-49de-896f-a12ab8bf49a5": {"__data__": {"id_": "95d94a5c-e569-49de-896f-a12ab8bf49a5", "embedding": null, "metadata": {"page_label": "358", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d4e714b7-7159-4342-9a52-768799217228", "node_type": "4", "metadata": {"page_label": "358", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "a44a874a5c41944669bc9e75f05abff22fb9e131f093c3edf67ac63462b16ab8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "358 Smoothing by optimization\n13.5 Smoothed max operators\nAs a more elaborate application of the smoothing techniques discussed\nin this chapter, we explain how to smooth max operators. Smoothed\nmax operators include smoothed ReLU functions as a special case.\n13.5.1 Definition and properties\nWith a slight notation overloading, given a vectoru= (u1,...,u M) \u2208\nRM, we define its maximum as\nmax(u) := max\nj\u2208[M]\nuj.\nTo obtain a smooth approximationmax\u2126 of max, we again apply the\ndual approach. The conjugate ofmax is\nmax\u2217(\u03c0) = \u03b9\u25b3M(\u03c0).\nTo notice why, we observe that the vertices of the probability simplex\n\u25b3M are the standard basis vectorse1,..., eM. Since the objective is\nlinear, we then have\nmax(u) = max\n\u03c0\u2208\u25b3M\n\u27e8u,\u03c0\u27e9= max\n\u03c0\u2208{e1,...,eM}\n\u27e8u,\u03c0\u27e9.\nIn other words, the maximum operator is exactly the support function\nof \u25b3M. Since the conjugate of the support function is the indicator\nfunction, we indeed obtainmax\u2217= \u03b9\u25b3M. We can therefore write\nmax\u2217\n\u2126(\u03c0) = max\u2217(\u03c0) + \u2126(\u03c0) = \u2126(\u03c0) + \u03b9\u25b3M(\u03c0)\nand\nmax\u2126(u) = (\u2126 + \u03b9\u25b3M)\u2217(u)\n= max\n\u03c0\u2208RM\n\u27e8u,\u03c0\u27e9\u2212(\u2126(\u03c0) + \u03b9\u25b3M(\u03c0))\n= max\n\u03c0\u2208\u25b3M\n\u27e8u,\u03c0\u27e9\u2212\u2126(\u03c0).\nThe smoothed max operatormax\u2126 can be useful in a neural network,\nfor example as a smoothed max pooling layer. Its properties have been\nstudied in (Mensch and Blondel, 2018, Lemma 1), as we recall here for\nconvenience.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1284, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cce783bd-b444-4075-9995-5a60fa8a2f35": {"__data__": {"id_": "cce783bd-b444-4075-9995-5a60fa8a2f35", "embedding": null, "metadata": {"page_label": "359", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5719b3c3-4da1-47dd-9280-729d6cc7dfd1", "node_type": "4", "metadata": {"page_label": "359", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "6dd7c9a6cf853ee11bfbb608b3813eaa9f093ae61507d5f8dd72725c23b909c6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13.5. Smoothed max operators 359\nProposition 13.7(Properties ofmax\u2126). The following properties\nhold.\n1. Bounds: if L\u2126 \u2264\u2126(\u03c0) \u2264U\u2126 for all\u03c0\u2208\u25b3M, thenmax(u)\u2212\nU\u2126 \u2264max\u2126(u) \u2264max(u) \u2212L\u2126 for allu\u2208RM.\n2. Monotonicity: if u\u2264v (element-wise), thenmax\u2126(u) \u2264\nmax\u2126(v).\n3. Commutativity: if \u2126(P\u03c0) = \u2126( \u03c0) for any permutation\nmatrix P and any\u03c0\u2208\u25b3M, thenmax\u2126(Pu) = max\u2126(u) for\nany permutation matrixP.\n4. Distributivity of+: max\u2126(u+ c1) = max\u2126(u) + c for all\nu\u2208RM and allc\u2208R.\nThese properties are leveraged in (Mensch and Blondel, 2018) to\ncreate differentiable dynamic programs. We consider in the following two\npossible choices of\u2126 leading to the softmax and sparsemax operators\nillustrated in Fig. 13.7.\nSmoothed min operators\nThe minimum operator can be expressed in terms of the maximum\noperator, since for allu\u2208RM,\nmin(u) = \u2212max(\u2212u).\nGiven a smoothed max operatormax\u2126, we can therefore easily define a\nsmoothed min operator as\nmin\u2126(u) := \u2212max\u2126(\u2212u).\n13.5.2 Reduction to root finding\nComputing max\u2126(u) for a general strongly-convex regularization\u2126 in-\nvolves the resolution of a maximum over probability simplex constraints.\nFor convenience, let us define the notation\n\u03b4\u2126(u) := (\u2126 + \u03b9RM\n+\n)\u2217(u) = max\nv\u2208RM\n+\n\u27e8u,v\u27e9\u2212\u2126(v).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1203, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "64c1d2f0-b753-4a43-95c8-5282fa898a71": {"__data__": {"id_": "64c1d2f0-b753-4a43-95c8-5282fa898a71", "embedding": null, "metadata": {"page_label": "360", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "05d6ff72-00a3-414e-9223-c65ccc9af63e", "node_type": "4", "metadata": {"page_label": "360", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "ca0ae72bc60d5e7cb943d534d96a1f7d0df865d4f46446f07f6f2b3beb16f897", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "360 Smoothing by optimization\nThe following proposition shows that we can reduce computingmax\u2126\nto solving a root equation involving\u03b4\u2126.\nProposition 13.8(Computing max\u2126 as root finding). Suppose \u2126 is\nstrongly convex. For allu\u2208RM,\nmax\u2126(u) = min\n\u03c4\u2208R\n\u03c4 + \u03b4\u2126(u\u2212\u03c41)\n= \u03c4\u22c6 + \u03b4\u2126(u\u2212\u03c4\u22c61)\nand\n\u2207max\u2126(u) = \u2207\u03b4\u2126(u\u2212\u03c4\u22c61),\nwhere \u03c4\u22c6 is the solution w.r.t.\u03c4 of the abovemin, which satisfies\nthe root equation\n\u27e8\u2207\u03b4\u2126(u\u2212\u03c4\u22c61),1\u27e9= 1.\nProof. The idea is to keep the non-negativity constraint explicit, but to\nuse a Lagrange multiplier for the equality constraint of the probability\nsimplex. We then have\nmax\u2126(u) = max\nv\u2208\u25b3M\n\u27e8u,v\u27e9\u2212\u2126(v)\n= max\nv\u2208RM\n+\nmin\n\u03c4\u2208R\n\u27e8u,v\u27e9\u2212\u2126(v) \u2212\u03c4(\u27e8v,1\u27e9\u22121)\n= min\n\u03c4\u2208R\n\u03c4 + max\nv\u2208RM\n+\n\u27e8u\u2212\u03c41,v\u27e9\u2212\u2126(v)\n= min\n\u03c4\u2208R\n\u03c4 + \u03b4\u2126(u\u2212\u03c41),\nwhere we used that we can swap themin and themax, since(u,v) \u21a6\u2192\n\u27e8u,v\u27e9\u2212\u2126(v) is convex-concave andv\u2208\u25b3M is an affine constraint. The\ngradient \u2207\u03b4\u2126(u) follows from Danskin\u2019s theorem. The root equation\nfollows from computing the derivative of\u03c4 \u21a6\u2192\u03c4+\u03b4\u2126(u\u2212\u03c41) and setting\nit to zero.\n13.5.3 The softmax\nWhen \u2126 is Shannon\u2019s negentropy, we obtain thatmax\u2126 is the softmax,\nalready briefly discussed in Section 4.4.2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1116, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "301fc0ea-c54d-4927-ad95-6e35e6a407d5": {"__data__": {"id_": "301fc0ea-c54d-4927-ad95-6e35e6a407d5", "embedding": null, "metadata": {"page_label": "361", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "030fefb5-c3bc-4a5d-8dfe-3bf991ed6e6e", "node_type": "4", "metadata": {"page_label": "361", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "82100a12dec8f96fa2c37fe4e8bf0835db712b9a0cd74fe690f5009c3cfd04e1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13.5. Smoothed max operators 361\nProposition 13.9(Analytical expression of the softmax). When\u2126(\u03c0) =\n\u27e8\u03c0,log \u03c0\u27e9, we get\nsoftmax(u) := max\u2126(u)\n= max\n\u03c0\u2208\u25b3M\n\u27e8u,\u03c0\u27e9\u2212\u2126(\u03c0)\n= logsumexp(u)\n= log\nM\u2211\nj=1\neuj.\nProof. Since dom(\u2126) = RM\n+ , we have\u03b4\u2126 = \u2126\u2217(i.e., the non-negativity\nconstraint is redundant). From Example 13.2, we therefore have\u03b4\u2126(u) =\u2211M\nj=1 exp(uj \u22121). From Proposition 13.8,max\u2126(u) = \u03c4\u22c6+ \u03b4\u2126(u\u2212\u03c4\u22c61)\nwhere \u03c4\u22c6 satisfies \u27e8\u2207\u03b4\u2126(u\u2212\u03c4\u22c61),1\u27e9= 1. Since \u2207\u03b4\u2126(u) = exp(u\u2212\n1), we need to solve \u2211M\nj=1 exp(uj \u22121 \u2212\u03c4) = 1 . We therefore get\n\u03c4\u22c6 + 1 = logsumexp(u) and therefore max\u2126(u) = logsumexp(u) \u2212\n1 + \u2211M\nj=1 exp(uj \u2212logsumexp(u)) = logsumexp(u).\nSince \u2212log M \u2264\u2126(\u03c0) \u22640 for all \u03c0 \u2208 \u25b3M, following Proposi-\ntion 13.7, we get for allu\u2208RM\nmax(u) \u2264softmax(u) \u2264max(u) + logM.\nA unique property of the softmax, which is not the case of allmax\u2126\noperators, is that it supportsassociativity.\nProposition 13.10(Associativity of the softmax). For all a,b,c \u2208\nR,\nsoftmax(softmax(a,b),c) = softmax(a,softmax(b,c)).\n13.5.4 The sparsemax\nAlternatively, choosing\u2126 to be Gini\u2019s negentropy leads to the sparsemax\n(Martins and Astudillo, 2016; Mensch and Blondel, 2018).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1135, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7e42b0ee-890e-4b13-aa80-ee973100cca2": {"__data__": {"id_": "7e42b0ee-890e-4b13-aa80-ee973100cca2", "embedding": null, "metadata": {"page_label": "362", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "44fce5fe-120c-461e-aeb2-ad10856241ab", "node_type": "4", "metadata": {"page_label": "362", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "79a98e6ad6d7a7ebe4fdbaf9201f87b1e71da341dc7fa0e5fb4c9fa93c19160f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "362 Smoothing by optimization\nProposition 13.11(Variational formulation of sparsemax). When\u2126(\u03c0) =\n1\n2 \u27e8\u03c0,\u03c0\u22121\u27e9, we have\nsparsemax(u) := max\u2126(u)\n= max\n\u03c0\u2208\u25b3M\n\u27e8u,\u03c0\u27e9\u2212\u2126(\u03c0)\n= \u27e8u,\u03c0\u22c6\u27e9\u2212\u2126(\u03c0\u22c6)\nwhere\n\u03c0\u22c6 = sparseargmax(u) := arg min\n\u03c0\u2208\u25b3M\n\u2225u\u2212\u03c0\u22252\n2.\nProof. This follows from the fact that\u2126(\u03c0) is up to a constant equal\nto 1\n2 \u2225\u03c0\u22252\n2 and completing the square.\nTherefore, computing the sparsemax can use the sparseargmax (the\nEuclidean projection onto the probability simplex) as a building block.\nWe discuss how to compute it in more detail in Section 13.7. Applying\nProposition 13.8 gives an alternative formulation.\nProposition 13.12(Sparsemax as root finding). When\u2126(\u03c0) = 1\n2 \u27e8\u03c0,\u03c0\u2212\n1\u27e9, we have\nsparsemax(u) = max\u2126(u) = min\n\u03c4\u2208R\n\u03c4 + 1\n2\nM\u2211\ni=1\n[ui \u2212\u03c4]2\n+\nand \u03c4\u22c6 satisfies\nM\u2211\ni=1\n[ui \u2212\u03c4]+ = 1.\nProof. First, we compute the expression of\u03b4\u2126(u) = maxv\u2208RM\n+\n\u27e8u,v\u27e9\u2212\n\u2126(v). Setting the gradient of v \u21a6\u2192 \u27e8u,v\u27e9\u2212 \u2126(v) and clipping, we\nobtain v\u22c6 = [u]+. Pluggingv\u22c6 back, we obtain\u03b4\u2126(u) = 1\n2\n\u2211M\ni=1[ui]2\n+.\nUsing Proposition 13.8 proves the proposition\u2019s first part. Setting the\nderivative w.r.t.\u03c4 to zero gives the second part.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1100, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "29ad7a9c-e236-4707-ad16-f446455f31f9": {"__data__": {"id_": "29ad7a9c-e236-4707-ad16-f446455f31f9", "embedding": null, "metadata": {"page_label": "363", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2b732d2f-c767-42d7-a3a9-35756260baa1", "node_type": "4", "metadata": {"page_label": "363", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "2d98b4f9b4a74e95bc0d34127dc924cc05b4ebeaa41b60f42ab7c8eb8970fd54", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13.5. Smoothed max operators 363\n2.5\n 0.0 2.5\nu1\n2.5\n0.0\n2.5\nu2\nmax(u1, u2, 0)\n0 2 4\nValue\n2.5\n 0.0 2.5\nu1\n2.5\n0.0\n2.5\nu2\nsoftmax(u1, u2, 0)\n2 4\nValue\n2.5\n 0.0 2.5\nu1\n2.5\n0.0\n2.5\nu2\nsparsemax(u1, u2, 0)\n0 2 4\nValue\nFigure 13.7:Max, softmax and sparsemax functions. The max function has non-\nsmooth contour lines (set of points {u \u2208 R3 : f(u) = c}for some constant c\nrepresented by dashed gray lines). So the gradient along these contour lines switch\nsuddenly at the corners of the contour lines switch. This shows that the max function\nis not differentiable everywhere, namely, non-differentiable on the set of points\n{u\u2208R3 : ui = uj for anyi\u0338= j}. The contour lines of the softmax and sparsemax\nfunctions on the other hand are smooth illustrating that these functions are smooth\ncounterpart of the max function.\nIt can be shown (Duchiet al., 2008; Condat, 2016) that the exact\nsolution \u03c4\u22c6 is obtained by\n\u03c4\u22c6 = 1\nj\u22c6\n\uf8eb\n\uf8ed\nj\u22c6\n\u2211\ni=1\nu[i] \u22121\n\uf8f6\n\uf8f8, (13.5)\nwhere j\u22c6 is the largestj \u2208[M] such that\nuj \u22121\nj\n\uf8eb\n\uf8ed\nj\u2211\ni=1\nu[i] \u22121\n\uf8f6\n\uf8f8>0,\nand where we used the notation u[1] \u2265 u[2] \u2265 \u00b7\u00b7\u00b7 \u2265u[M]. As an\nalternative, we can also compute\u03c4\u22c6 approximately using a bisection or\nby gradient descent w.r.t.\u03c4.\nSince 1\n2M \u2264\u2225\u03c0\u22252\n2 \u22641\n2 , we get\u2212M\u22121\n2M \u2264\u2225\u03c0\u22252\n2 \u22640 for all\u03c0\u2208\u25b3M.\nFollowing Proposition 13.7, we therefore get for allu\u2208RM\nmax(u) \u2264sparsemax(u) \u2264max(u) + M \u22121\n2M .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1344, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "afbbad1b-2b65-44b6-bebc-8e118185d080": {"__data__": {"id_": "afbbad1b-2b65-44b6-bebc-8e118185d080", "embedding": null, "metadata": {"page_label": "364", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ee120089-22d3-4dbf-a9eb-ccf775aab4b3", "node_type": "4", "metadata": {"page_label": "364", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "d8a00e3d38999ac267212ceff0e053f4e7b87a400c28f124ab289e97bdb6c273", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "364 Smoothing by optimization\n13.5.5 Recovering smoothed ReLU functions\nUsing the vectoru= (u,0) \u2208R2 as input, the smoothed max operator\nrecovers the smoothed ReLU:\nmax\u2126((u,0)) = relu\u03a8(u),\nwhere we defined \u03a8(\u03c0) := \u2126(( \u03c0,1 \u2212\u03c0)). With \u2126 being Shannon\u2019s\nnegentropy, we recover\u03a8(\u03c0) = \u03c0log \u03c0+ (1\u2212\u03c0) log(1 \u2212\u03c0); with\u2126 being\nGini\u2019s negentropy, we recover\u03a8(\u03c0) = \u03c0(\u03c0\u22121), that we used to smooth\nthe ReLU.\n13.6 Relaxed step functions (sigmoids)\nWe now turn to creating continuous relaxations of step functions. The\nbinary step function, a.k.a. Heaviside step function, is defined by\nstep(u) :=\n\uf8f1\n\uf8f2\n\uf8f3\n1 if u\u22650\n0 otherwise\n.\nFrom Eq. (13.4), its variational form is\nstep(u) = arg max\n\u03c0\u2208[0,1]\nu\u00b7\u03c0.\nWe can therefore define the relaxation\nstep\u2126(u) := arg max\n\u03c0\u2208[0,1]\nu\u00b7\u03c0\u2212\u2126(\u03c0).\nNotice that, unlike the case of the smoothed ReLU, it is a regularized\nargmax, not a regularized max. Following Remark 13.1, strongly convex\nregularization \u2126 ensures thatstep\u2126(u) is a Lipschitz continuous function\nof u and is therefore, at least, differentiable almost everywhere, unlike\nstep(u).\nThe logistic function\nIf we use the regularizer\u2126(\u03c0) = \u03c0log \u03c0+ (1 \u2212\u03c0) log(1 \u2212\u03c0), we obtain\nthe closed form\nstep\u2126(u) = logistic(u) := 1\n1 + e\u2212u = eu\n1 + eu.\nThis function is differentiable everywhere.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1255, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0fd7690c-f313-4333-a4bf-b98b68dfbad9": {"__data__": {"id_": "0fd7690c-f313-4333-a4bf-b98b68dfbad9", "embedding": null, "metadata": {"page_label": "365", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b0f2cafb-422c-4582-bd63-9d4ed41150f5", "node_type": "4", "metadata": {"page_label": "365", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "34a0d5c1fe1b8e8666bddb9b2c21706a23c62e247082401af2fb611ed6f2eb19", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13.7. Relaxed argmax operators 365\nThe sparse sigmoid\nAs an alternative, if we use\u2126(\u03c0) = \u03c0(\u03c0\u22121), we obtain a piecewise\nlinear sigmoid,\nstep\u2126(u) = sparsesigmoid(u) :=\n\uf8f1\n\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3\n0, u \u2264\u22121\n1\n2 (u+ 1), \u22121 <u< 1\n1, u \u22651\n.\nUnlike the logistic function, it can reach the exact values0 or 1. However,\nthe function has two kinks, where the function is non-differentiable.\nLink between smoothed ReLU functions and sigmoids\nIt turns out that the three sigmoids we presented above (step, logistic,\nsparsesigmoid) are all equal to the derivative of their corresponding\nsmoothed ReLU function:\nstep(u) = relu\u2032(u)\nlogistic(u) = softplus\u2032(u)\nsparsesigmoid(u) = sparseplus\u2032(u)\nand more generally\nrelu\u2032\n\u2126(u) = step\u2126(u).\nThis is a consequence of Danskin\u2019s theorem; see Example 11.2. We\nillustrate the smoothed ReLU functions and relaxed step functions\n(sigmoids) in Fig. 13.8.\n13.7 Relaxed argmax operators\nWe now turn to argmax operators, which are a generalization of step\nfunctions. With a slight notation overloading, let us now define\nargmax(u) := \u03d5(arg max\nj\u2208[M]\nuj),\nwhere \u03d5(j) = onehot(j) = ej is used to embed any integerj \u2208[M] into\nRM. Following the previous discussion, we have the variational form\nargmax(u) = arg max\n\u03c0\u2208\u25b3M\n\u27e8u,\u03c0\u27e9= arg max\n\u03c0\u2208{e1,...,eM}\n\u27e8u,\u03c0\u27e9,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1250, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9b52aea1-9677-4a5f-a2c2-b4c2f150ef97": {"__data__": {"id_": "9b52aea1-9677-4a5f-a2c2-b4c2f150ef97", "embedding": null, "metadata": {"page_label": "366", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "103a701f-14c9-4c2d-91cf-e8a9355ed1d8", "node_type": "4", "metadata": {"page_label": "366", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "0fe41a9d842334e1525a9b0a809992a72ee6c24ef55391b92dec2cc3653e1e42", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "366 Smoothing by optimization\n2\n 1\n 0 1 2\n0\n1\n2\nActivations\nReLU\nSoftPlus\nSparsePlus\n2\n 1\n 0 1 2\n0.0\n0.5\n1.0\nSigmoids\nHeaviside\nLogistic\nSparseSigmoid\nFigure 13.8: Smoothed ReLU functions and relaxed step functions (sigmoids).\nDifferentiating the left functions gives the right functions.\nwhere the second equality uses that a linear function is maximized at\none of the vertices of the simplex. This variational form suggests to\ndefine the relaxation\nargmax\u2126(u) := arg max\n\u03c0\u2208\u25b3M\n\u27e8u,\u03c0\u27e9\u2212\u2126(\u03c0).\nAgain, following Remark 13.1,argmax\u2126(u) is guaranteed to be, at least,\na differentiable almost everywhere function ofuif \u2126 is strongly convex.\nSimilarly to sigmoids, it turns out that these mappings are equal to\nthe gradient of their corresponding smoothed max operator:\nargmax\u2126(u) = \u2207max\u2126(u).\nThis is again a consequence of Danskin\u2019s theorem.\nThe softargmax\nWhen using Shannon\u2019s entropy\u2126(\u03c0) = \u27e8\u03c0,log \u03c0\u27e9, we obtain\nargmax\u2126(u) = softargmax(u) = exp(u)\n\u2211M\nj=1 exp(uj),\nwhich is differentiable everywhere.\nProof. We know thatmax\u2126(u) = logsumexp(u) and that\u2207max\u2126(u) =\nargmax\u2126(u). Differentiatinglogsumexp(u) gives softargmax(u).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1114, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "681767ae-7bbf-4bf5-985d-f9aa9884dda6": {"__data__": {"id_": "681767ae-7bbf-4bf5-985d-f9aa9884dda6", "embedding": null, "metadata": {"page_label": "367", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a181963f-518f-4a3b-ae2e-d20c0808acf1", "node_type": "4", "metadata": {"page_label": "367", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "1bf689007d064afef618e13a75a28491beb8498b00fdc7c671ff5a3f7965f6c6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13.7. Relaxed argmax operators 367\nThe sparseargmax\nWhen using Gini\u2019s entropy\u2126(\u03c0) = 1\n2 \u27e8\u03c0,\u03c0\u22121\u27e9, which is up to a constant\nequal to 1\n2 \u2225\u03c0\u22252\n2, we obtain the sparseargmax (Martins and Astudillo,\n2016)\nargmax\u2126(u) = sparseargmax(u)\n:= arg max\n\u03c0\u2208\u25b3M\n\u27e8u,\u03c0\u27e9\u2212 1\n2\u27e8\u03c0,\u03c0\u22121\u27e9\n= arg max\n\u03c0\u2208\u25b3M\n\u27e8u,\u03c0\u27e9\u2212 1\n2\u2225\u03c0\u22252\n2\n= arg min\n\u03c0\u2208\u25b3M\n\u2225u\u2212\u03c0\u22252\n2,\nwhich is nothing but the Euclidean projection onto the probability\nsimplex (see also Section 16.3). The Euclidean projection onto the\nprobability simplex \u25b3M can be computed exactly using a median-\nfinding-like algorithm. The complexity isO(M) expected time and\nO(Mlog M) worst-case time (Brucker, 1984; Michelot, 1986; Duchi\net al., 2008; Condat, 2016). Computing the Euclidean projection onto\nthe probability simplex boils down to computing\u03c4\u22c6 given in Eq. (13.5).\nOnce we computed it, we have\nsparseargmax(u) = [u\u2212\u03c4\u22c6]+,\nAt its name indicates, and as the above equation shows, sparseargmax\nis sparse, but it is only differentiable almost everywhere. Note that\nthe operator is originally known as sparsemax (Martins and Astudillo,\n2016), but this is a misnomer, as it is really an approximation of the\nargmax. Therefore, in analogy with the softargmax, we use the name\nsparseargmax. We compare the argmax, softmax and sparseargmax in\nFig. 13.9 and Fig. 13.10.\nRelaxed argmin operators\nThe argmin operator can be expressed in terms of the argmax operator,\narg min(u) = arg max(\u2212u).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1400, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a5a7d257-0bfd-41f9-8786-fdf8751bee82": {"__data__": {"id_": "a5a7d257-0bfd-41f9-8786-fdf8751bee82", "embedding": null, "metadata": {"page_label": "368", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c8a71c8e-e7f7-4fd8-879a-f2104b11f2cd", "node_type": "4", "metadata": {"page_label": "368", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "18b78a5e7f5a6b078b45b602fdd3ea26e8131f55f50a71983426c45663363a9d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "368 Smoothing by optimization\n2.5\n 0.0 2.5\nu1\n2.5\n0.0\n2.5\nu2\n1\n2.5\n 0.0 2.5\nu1\n2.5\n0.0\n2.5\nu2\n2\n2.5\n 0.0 2.5\nu1\n2.5\n0.0\n2.5\nu2\n3\n0.0 0.5 1.0\nValue\n0.0 0.5 1.0\nValue\n0.0 0.5 1.0\nValue\n= argmax(u1, u2, 0)\n2.5\n 0.0 2.5\nu1\n2.5\n0.0\n2.5\nu2\n1\n2.5\n 0.0 2.5\nu1\n2.5\n0.0\n2.5\nu2\n2\n2.5\n 0.0 2.5\nu1\n2.5\n0.0\n2.5\nu2\n3\n0.0 0.5 1.0\nValue\n0.0 0.5 1.0\nValue\n0.0 0.5 1.0\nValue\n= softargmax(u1u2, 0)\n2.5\n 0.0 2.5\nu1\n2.5\n0.0\n2.5\nu2\n1\n2.5\n 0.0 2.5\nu1\n2.5\n0.0\n2.5\nu2\n2\n2.5\n 0.0 2.5\nu1\n2.5\n0.0\n2.5\nu2\n3\n0.0 0.5 1.0\nValue\n0.0 0.5 1.0\nValue\n0.0 0.5 1.0\nValue\n= sparseargmax(u1u2, 0)\nFigure 13.9:Values ofargmax(u), softargmax(u), andsparseargmax(u) for u=\n(u1,u2,0), when varyingu1 and u2. The argmax is a piecewise constant, discontinuous\nfunction. The softargmax is a continuous and differentiable everywhere function, but\nit is always strictly positive and therefore dense. The sparseargmax is a continuous\nfunction and its output can be sparse, but it is only a differentiable almost everywhere\nfunction.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 980, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5134b113-4f47-4722-9356-af47ad709402": {"__data__": {"id_": "5134b113-4f47-4722-9356-af47ad709402", "embedding": null, "metadata": {"page_label": "369", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dc4c8148-7e73-47fa-97bf-86749ca3234f", "node_type": "4", "metadata": {"page_label": "369", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "a70acb8ef648b90f5d6b84f2a2227ea3fa26a7af9440861104012b449b50b9be", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13.8. Summary 369\nFigure 13.10:Same as Fig. 13.9 but using a 3D plot.\nGiven a relaxed argmax operatorargmax\u2126, we can therefore define a\nrelaxed argmin by\nargmin\u2126(u) := argmax\u2126(\u2212u).\nWe then have for allu\u2208RM\nargmin\u2126(u) = \u2207min\u2126(u).\n13.8 Summary\n\u2022 When a functionf is non-differentiable (or worse, discontinuous),\na reasonable approach is to replace it by its smooth approximation\n(or continuous relaxation).\n\u2022 The first approach we reviewed is infimal convolution betweenf\nand primal regularizationR. The Moreau envelope is a special\ncase, obtained by usingR= 1\n2 \u2225\u00b7\u22252\n2.\n\u2022 The second approach we reviewed is regularizing the convex con-\njugate f\u2217of f with some dual regularization\u2126. We saw that the\nprimal and dual approaches are equivalent whenR= \u2126\u2217.\n\u2022 The Legendre-Fenchel transformation, a.k.a. convex conjugate,\ncan be seen as a dual representation of a function: instead of\nrepresenting f by its graph(u,f(u)) for u \u2208dom(f), we can\nrepresent it by the set of tangents with slopev and intercept", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 996, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "56b089b0-3c18-48b2-b64a-f884817618bc": {"__data__": {"id_": "56b089b0-3c18-48b2-b64a-f884817618bc", "embedding": null, "metadata": {"page_label": "370", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "873dc934-8f63-4168-a833-aad5b89804f2", "node_type": "4", "metadata": {"page_label": "370", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "648879bd0331136ca389ef8ae8e18beb3aa09481cd08637792256059dda16b7e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "370 Smoothing by optimization\n\u2212f\u2217(v) for v\u2208dom(f\u2217) As its name indicates, it is convex, even\nif the original function is not.\n\u2022 We showed how to apply smoothing techniques to create smoothed\nReLU functions and smoothed max operators. We also showed that\ntaking their gradients allowed us to obtain generalized sigmoid\nfunctions and argmax operators.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 349, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "57d4e818-a59f-47f2-a671-f7e6129386e4": {"__data__": {"id_": "57d4e818-a59f-47f2-a671-f7e6129386e4", "embedding": null, "metadata": {"page_label": "371", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8686e99-b64f-4c80-9ddd-3ec2a9a74adc", "node_type": "4", "metadata": {"page_label": "371", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "fe3ee2c21c07eea8073bcdc77aea9a0c98c147f79cd082ca664e454258dbc5d0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14\nSmoothing by integration\nIn this chapter, we review smoothing techniques based onconvolution.\n14.1 Convolution\n14.1.1 Convolution operators\nThe convolution between two functions f and g produces another\nfunction, denotedf \u2217g. It is defined by\n(f \u2217g)(\u00b5) :=\n\u222b\u221e\n\u2212\u221e\nf(u)g(\u00b5\u2212u) du, (14.1)\nassuming that the integral is well defined. It is therefore the integral of\nthe product off and g after g is reflected about they-axis and shifted.\nIt can be seen as a generalization of themoving average. Using\nthe change of variableu:= \u00b5+ z, which is again thelocation-scale\ntransform, we can also write\n(f \u2217g)(\u00b5) =\n\u222b\u221e\n\u2212\u221e\nf(\u00b5\u2212z)g(z) dz= (g\u2217f)(\u00b5). (14.2)\nThe convolution operator is thereforecommutative.\n371", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 695, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2f4fd679-df67-41cb-bc63-de3891d43084": {"__data__": {"id_": "2f4fd679-df67-41cb-bc63-de3891d43084", "embedding": null, "metadata": {"page_label": "372", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "98c32d68-879c-43be-abc9-afc38d096054", "node_type": "4", "metadata": {"page_label": "372", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "90b2d7f0430c511854266f28ccd866e52d11aa5728e093283976b900dc998b3b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "372 Smoothing by integration\n14.1.2 Convolution with a kernel\nThe convolution is frequently used together with akernel \u03bato create a\nsmooth approximationf \u2217\u03ba of f. The most frequently used kernel is\nthe Gaussian kernelwith width\u03c3, defined by\n\u03ba\u03c3(z) := 1\u221a\n2\u03c0\u03c3e\u22121\n2 ( z\n\u03c3)2\n.\nThis is the probability density function (PDF) of the normal distribution\nwith zero mean and variance\u03c32. The term 1\u221a\n2\u03c0\u03c3 is a normalization\nconstant, ensuring that the kernel sums to1 for all\u03c3. We therefore say\nthat \u03ba\u03c3 is anormalized kernel.\nAveraging perspective\nApplying the definition of the convolution in Eq. (14.1), we obtain\n(f \u2217\u03ba\u03c3)(\u00b5) := 1\u221a\n2\u03c0\u03c3\n\u222b\u221e\n\u2212\u221e\nf(u)e\u22121\n2 ( \u00b5\u2212u\n\u03c3 )2\ndu\n= EU\u223cp\u00b5,\u03c3[f(U)],\nwhere\np\u00b5,\u03c3(u) := \u03ba\u03c3(\u00b5\u2212u) = 1\u221a\n2\u03c0\u03c3e\u22121\n2 ( \u00b5\u2212u\n\u03c3 )2\nis the PDF of the Gaussian distribution with mean\u00b5 and variance\n\u03c32. Therefore, we can seef \u2217\u03ba\u03c3 as the expectation of f(u) over a\nGaussian centered around\u00b5. This property is true for all translation-\ninvariant kernels, that correspond to a location-scale family distribution\n(e.g., the Laplace distribution). The convolution therefore performs an\naveraging with all points, with points nearby\u00b5 given more weight by\nthe distribution. The parameter\u03c3 controls the importance we want to\ngive to farther points. We call this viewpoint averaging, as we replace\nf(u) by E[f(U)].\nPerturbation perspective\nConversely, using the alternative definition of the convolution operator\nin Eq. (14.2), which stems from the commutativity of the convolution,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1460, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a05c1a32-07fc-4bb9-9eb2-2b4149eaf92f": {"__data__": {"id_": "a05c1a32-07fc-4bb9-9eb2-2b4149eaf92f", "embedding": null, "metadata": {"page_label": "373", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dc390077-cb7c-4cf2-8f9b-15aa027719f8", "node_type": "4", "metadata": {"page_label": "373", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "09e7f745a9dd08a299a4a5965739519bebd2494f91101ea1e49be8ac3bf0fff5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14.1. Convolution 373\nwe have\n(f \u2217\u03ba\u03c3)(\u00b5) :=\n\u222b\u221e\n\u2212\u221e\nf(\u00b5\u2212z)e\u22121\n2 ( z\n\u03c3)2\ndz\n= EZ\u223cp0,\u03c3[f(\u00b5\u2212Z)]\n= EZ\u223cp0,\u03c3[f(\u00b5+ Z)],\nwhere, in the third line, we used thatp0,\u03c3 is sign invariant, i.e.,p0,\u03c3(z) =\np0,\u03c3(\u2212z). This viewpoint shows that smoothing by convolution with\na Gaussian kernel can also be seen as injecting Gaussiannoise or\nperturbations to the function\u2019s input.\nLimit case\nWhen \u03c3\u21920, the kernel\u03ba\u03c3 converges to a Dirac delta function,\nlim\n\u03c3\u21920\n\u03ba\u03c3(z) = \u03b4(z).\nSince the Dirac delta is the multiplicative identity of the convolution\nalgebra (this is also known as the sifting property), when\u03c3\u21920, f\u2217\u03ba\u03c3\nconverges tof, i.e.,\nlim\n\u03c3\u21920\n(f \u2217\u03ba\u03c3)(u) = f(u).\n14.1.3 Discrete convolution\nMany times, we work with functions whose convolution does not have\nan analytical form. In these cases, we can use a discrete convolution on\na grid of values. For two functionsf and g defined overZ, the discrete\nconvolution is defined by\n(f \u2217g)[i] :=\n\u221e\u2211\nj=\u2212\u221e\nf[j]g[i\u2212j].\nAs for its continuous counterpart, the discrete convolution is commuta-\ntive, namely,\n(f \u2217g)[i] =\n\u221e\u2211\nj=\u2212\u221e\nf[i\u2212j]g[j] = (g\u2217f)[i].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1065, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2cc60300-0467-4fc7-af20-a8cde500438a": {"__data__": {"id_": "2cc60300-0467-4fc7-af20-a8cde500438a", "embedding": null, "metadata": {"page_label": "374", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3d37decc-2d4c-4627-b50f-234f99aa6bc2", "node_type": "4", "metadata": {"page_label": "374", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "3c4b177f069af07633c97eeb81d3a040d7aefd8bf34056e7a53cfd24d7131660", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "374 Smoothing by integration\n3\n 2\n 1\n 0 1 2 3\n0\n2\n4\n6\n8\n10\n= 0.25\n= 0.5\n= 1.0\nFigure 14.1:Smoothing of the signalf[t] := t2 + 0.3 sin(6\u03c0t) with a sampled and\nrenormalized Gaussian kernel.\nWhenghasfinitesupportovertheset S := {\u2212M,\u2212M+1,..., 0,...,M \u2212\n1,M}, meaning thatg[i] = 0 for alli\u0338\u2208S, a finite summation may be\nused instead, i.e.,\n(f \u2217g)[i] =\nM\u2211\nj=\u2212M\nf[i\u2212j]g[j] = (g\u2217f)[i].\nIn practice, convolution between a discrete signalf: Z \u2192R and a\ncontinuous kernel\u03ba: R \u2192R is implemented by discretizing the kernel.\nOne of the simplest approaches consists in sampling points on an interval,\nevaluating the kernel at these points and renormalizing the obtained\nvalues, so that the sampled kernel sums to1. This is illustrated with\nthe Gaussian kernel in Fig. 14.1. Since the Gaussian kernel decays\nexponentially fast, we can choose a small interval around0. For a survey\nof other possible discretizations of the Gaussian kernel, see Getreuer\n(2013).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 942, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b9eafb9b-4fca-4e88-9442-7d95f4abf729": {"__data__": {"id_": "b9eafb9b-4fca-4e88-9442-7d95f4abf729", "embedding": null, "metadata": {"page_label": "375", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85f6ea14-fbdb-4b68-9147-9b21663b6679", "node_type": "4", "metadata": {"page_label": "375", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "6a3118c34251e80e21cc20885c8c04ecd233754e3a96c35048a23cac08bd34ad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14.1. Convolution 375\n14.1.4 Differentiation\nRemarkably, provided that the two functions are integrable with inte-\ngrable derivatives, the derivative of the convolution satisfies\n(f \u2217g)\u2032= (f\u2032\u2217g) = (f \u2217g\u2032),\nwhich simply stems from switching derivative and integral in the defini-\ntion of the convolution. Moreover, we have the following proposition.\nProposition 14.1(Differentiability of the convolution). Ifgisn-times\ndifferentiable with compact support overR and f is locally inte-\ngrable overR, thenf \u2217g is n-times differentiable overR.\n14.1.5 Multidimensional convolution\nSo far, we studied the convolution of one-dimensional functions. The\ndefinition can be naturally extended to multidimensional functions\nf: RM \u2192R and g: RM \u2192R as\n(f \u2217g)(\u00b5) :=\n\u222b\nRM\nf(u)g(\u00b5\u2212u) du,\nassuming again that the integral exists. Typically, a Gaussian kernel\nwith diagonal covariance matrix is used\n\u03ba\u03c3(z) :=\nM\u220f\nj=1\n1\u221a\n2\u03c0\u03c3j\ne\n\u22121\n2 (\nzj\n\u03c3j\n)2\n= 1\n\u221a\n2\u03c0\nM\n\u03c3M\ne\u22121\n2\n\u2225z\u22252\n2\n\u03c32 , (14.3)\nwhere, in the second equality, we assumed\u03c31 = \u00b7\u00b7\u00b7 = \u03c3M. In an image\nprocessing context, whereM = 2, it is approximated using a discrete\nconvolution and it is called aGaussian blur.\n14.1.6 Link between convolution and infimal convolution\nThe infimal convolution we studied in Section 13.1 takes the form\n(f\u25a1g)(\u00b5) := inf\nu\u2208RM\nf(u) + g(\u00b5\u2212u).\nIn comparison, the classical convolution takes the form\n(F \u2217G)(\u00b5) :=\n\u222b\nRM\nF(u)G(\u00b5\u2212u) du.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1387, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5a82fed2-5b0a-4323-bf81-17fd8a830ad2": {"__data__": {"id_": "5a82fed2-5b0a-4323-bf81-17fd8a830ad2", "embedding": null, "metadata": {"page_label": "376", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3da80f2b-1a8f-4655-86c4-4635063958c1", "node_type": "4", "metadata": {"page_label": "376", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "bfbf1944f8b3e4d7fce561ce2631469751ae0371d11af4bca2f94c085312137e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "376 Smoothing by integration\nThe two forms of convolution are clearly related. Infimal convolution\nperforms an infimum and uses the sum off and g: it uses a min-\nplus algebra. Classical convolution performs an integral and uses the\nproduct ofF and G: it uses asum-product algebra.\n14.1.7 The soft infimal convolution\nThe link between the infimal convolution and the classical convolution\ncan be further elucidated if we replace the infimum with a soft minimum\nin the definition of the infimal convolution.\nDefinition 14.1(Soft infimal convolution). The soft infimal convo-\nlution betweenf: RM \u2192R and g: RM \u2192R is\n(f\u25a1\u03b5g)(\u00b5) := softmin\u03b5\nu\u2208RM\nf(u) + g(\u00b5\u2212u),\nwhere we defined the soft minimum (assuming that it exists) over\nSof any functionh: S\u2192 R as\nsoftmin\u03b5\nu\u2208S\nh(u) := \u2212\u03b5log\n\u222b\nS\nexp (\u2212h(u)/\u03b5) du.\nWe recover the infimal convolution as\u03b5\u21920.\nComputation using a convolution\nWe now show that we can rewrite the soft infimal convolution using\na classical convolution. Indeed, by using the exponential change of\nvariable (sometimes referred to asCole-Hopf transformationin a\npartial differential equation context)\nC\u03b5{f}(u) := exp(\u2212f(u)/\u03b5)\nC\u22121\n\u03b5 {F}(v) = \u2212\u03b5log F(v),\nwe can define each function in the exponential domain,\nF\u03b5 := C\u03b5{f}\nG\u03b5 := C\u03b5{g}\nH\u03b5 := C\u03b5{h\u03b5}.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1250, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "96b5fad8-fb83-4ff4-8cbd-3aa041c3e20c": {"__data__": {"id_": "96b5fad8-fb83-4ff4-8cbd-3aa041c3e20c", "embedding": null, "metadata": {"page_label": "377", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "28a07de2-28c9-48b4-bbc1-0753d324eaae", "node_type": "4", "metadata": {"page_label": "377", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "918e7219d18d705b3a1b4a9580cd3f0c446ed9a021bc8e649cee937fe0f1987b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14.1. Convolution 377\nIt is easy to check that we then have\nH\u03b5(\u00b5) = (F\u03b5 \u2217G\u03b5)(\u00b5).\nBack to log domain, we obtain\nh\u03b5(\u00b5) = C\u22121\n\u03b5 {H\u03b5}(\u00b5).\nCombining the transformation and its inverse, we can write\nh\u03b5(\u00b5) = C\u22121\n\u03b5 {C\u03b5{f}\u2217C\u03b5{g}}(\u00b5).\nWhat we have shown is that, after an exponential change of variable,\nthe soft infimal convolution can be reduced to the computation of a\nconvolution. This is useful as a discrete convolution on a grid of sizen\ncan be computed inO(nlog n).\n14.1.8 The soft Moreau envelope\nWe saw in Section 13.1.2 that the infimal convolution betweenf and\nR(z) = 1\n2 z2 is the Moreau envelope,\nMf(\u00b5) := (f\u25a1R)(\u00b5) = inf\nu\u2208RM\nf(u) + 1\n2\u2225\u00b5\u2212u\u22252\n2.\nReplacing the infimal convolution with a soft infimal convolution, we\ncan define the \u201csoft\u201d Moreau envelope,\nM\u03b5\nf(\u00b5) := (f\u25a1\u03b5R)(\u00b5) = softmin\u03b5\nu\u2208RM\nf(u) + 1\n2\u2225\u00b5\u2212u\u22252\n2.\nWe emphasize that this is operation isnot the same as the convolution\nof f with a Gaussian kernel. Indeed, we have\nM\u03b5\nf(\u00b5) = \u2212\u03b5log\n\u222b\nRM\nexp\n(\n(\u2212f(u) \u22121\n2\u2225\u00b5\u2212u\u22252\n2)/\u03b5\n)\ndu.\nwhile\n(f \u2217\u03ba\u03c3)(\u00b5) :=\n\u222b\nRM\nf(u)\u03ba\u03c3(\u00b5\u2212u) du,\nwhere \u03ba\u03c3 is for instance defined in Eq. (14.3).\nWe saw that the Moreau envelope is a smooth function. One may\ntherefore ask what do we gain from using a soft Moreau envelope. The\nbenefit can be computational, as the latter can be approximated using\na discrete convolution.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1300, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e303dc33-daba-4743-a90b-cc5d70de8589": {"__data__": {"id_": "e303dc33-daba-4743-a90b-cc5d70de8589", "embedding": null, "metadata": {"page_label": "378", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18e4bacd-621d-40f0-8793-a8fa107defb8", "node_type": "4", "metadata": {"page_label": "378", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "3fe0976faf2f63dd7ae7a2713e7cbcc7c67f22375f8efb9f93ccabde2d68160a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "378 Smoothing by integration\n14.2 Fourier and Laplace transforms\nLet us define theFourier transformof f by\nF(s) := F{f}(s) :=\n\u222b\u221e\n\u2212\u221e\nf(t)e\u2212i2\u03c0stdt, s \u2208R.\nNote that F{f}is a function transformation: it transforms f into\nanother functionF.\n14.2.1 Convolution theorem\nNow, consider the convolution\nh(t) := (f \u2217g)(t).\nIf we define the three transformations\nF := F{f}, G:= F{g}, H:= F{h},\nthe convolution theoremstates that\nH(s) = F(s) \u00b7G(s), s \u2208R.\nWritten differently, we have\nF{f \u2217g}= F{f}\u00b7F{ g}.\nIn words, in the Fourier domain, the convolution operation becomes a\nmultiplication. Conversely,\nh(t) = (f \u2217g)(t) = F\u22121{F \u00b7G}(t), t \u2208R.\nThe convolution theorem also holds if we replace the Fourier transform\nwith the Laplace transform or with the two-sided (bilateral) Laplace\ntransform.\n14.2.2 Link between Fourier and Legendre transforms\nIn Section 13.2, we studied another function transformation: the convex\nconjugate, also known as Legendre-Fenchel transform. We recap the\nanalogies between these transforms in Table 14.1. In particular, the\ncounterpart of\nF{f \u2217g}= F{f}\u00b7F{ g}.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1074, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4dfbab53-1279-4754-ac68-22842d91dfea": {"__data__": {"id_": "4dfbab53-1279-4754-ac68-22842d91dfea", "embedding": null, "metadata": {"page_label": "379", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a766100b-6ed6-4a4f-a28a-cabb15ea4820", "node_type": "4", "metadata": {"page_label": "379", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "b96e4321107434ad6fc3312130c76bbd741bcf1b09a6a65332cacee40d029b7d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14.2. Fourier and Laplace transforms 379\nTable 14.1:Analogy between Fourier and Legendre transforms. See Proposition 13.3\nfor more conjugate calculus rules.\nFourierF{f} Legendre f\u2217\nSemiring (+,\u00b7) (min ,+)\nScaling (a> 0) f(t) = g(t/a) f(t) = ag(t/a)\nF{f}(t) = aF{g}(as) f\u2217(s) = ag\u2217(s)\nTranslation f(t) = g(t\u2212t0) f(t) = g(t\u2212t0)\nF{f}(s) = e\u2212i2\u03c0t0sF{g}(s) f\u2217(s) = g\u2217(s) + t0\nConvolution h= f \u2217g h = f\u25a1g\nF{h}= F{f}\u00b7F{ g} h\u2217= f\u2217+ g\u2217\nGaussian / quadratic f(t) = e\u2212at2\nf(t) = a\n2 t2\nF{f}(s) =\n\u221a\n\u03c0\nae\u2212\u03c02s2/a f\u2217(s) = 1\n2as2\nSmoothing f \u2217\u03ba\u03c3 f\u25a1 1\n2\u03b5\u2225\u00b7\u22252\n2\nfor the infimal convolution is\n(f\u25a1g)\u2217= f\u2217+ g\u2217.\nIn words, the Legendre-Fenchel transform is to the infimal convolution\nwhat the Fourier transform is to the convolution.\n14.2.3 The soft Legendre-Fenchel transform\nWe saw in Section 13.2 that the Legendre-Fenchel transform (convex\nconjugate) of a functionf: RM \u2192R is\nf\u2217(v) := max\nu\u2208RM\n\u27e8u,v\u27e9\u2212f(u).\nIf necessary, we can support constraints by including an indicator\nfunction in the definition off. The conjugate can be smoothed out using\na log-sum-exp, which plays the role of a soft maximum (Section 13.5).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1096, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "430663ae-fff1-4cdb-9d95-44a2ca025992": {"__data__": {"id_": "430663ae-fff1-4cdb-9d95-44a2ca025992", "embedding": null, "metadata": {"page_label": "380", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37725764-f5dc-464e-b305-686052c2b92a", "node_type": "4", "metadata": {"page_label": "380", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "bf01eacee104db98553540d614f7915550d596c266de296e469a552bd1d09a30", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "380 Smoothing by integration\nDefinition 14.2(Soft convex conjugate).\nf\u2217\n\u03b5(v) := softmax\u03b5\nu\u2208RM\n\u27e8u,v\u27e9\u2212f(u),\nwhere we defined the soft maximum (assuming that it exists) over\nSof any functiong: S\u2192 R as\nsoftmax\u03b5\nu\u2208S\ng(u) := \u03b5log\n\u222b\nS\nexp (g(u)/\u03b5) du.\nIn the limit\u03b5\u21920, we recover the convex conjugate.\nComputation using a convolution\nWe now show that this smoothed conjugate can be rewritten using a\nconvolution if we apply a bijective transformation tof.\nProposition 14.2(Smoothed convex conjugate as convolution). The\nsmoothed conjugate can be rewritten as\nf\u2217\n\u03b5(v) = Q\u22121\n\u03b5\n{ 1\nQ\u03b5{f}\u2217G\u03b5\n}\n(v)\nwhere\nG\u03b5 := C\u03b5\n{ 1\n2\u2225\u00b7\u22252\n2\n}\n= exp\n(\n\u22121\n2\n\u2225\u00b7\u22252\n2\n\u03b5\n)\nQ\u03b5{f}:= C\u03b5\n{\nf(\u00b7) \u22121\n2\u2225\u00b7\u22252\n2\n}\n= exp\n(1\n2\u03b5\u2225\u00b7\u22252\n2 \u22121\n\u03b5f(\u00b7)\n)\nQ\u22121\n\u03b5 {F}:= 1\n2\u2225\u00b7\u22252\n2 \u2212\u03b5log(F(\u00b7)).\nThis insight was tweeted by Gabriel Peyr\u00e9 in April 2020.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 791, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9cab3cef-d1b4-4fbb-9104-f9c7a9020b22": {"__data__": {"id_": "9cab3cef-d1b4-4fbb-9104-f9c7a9020b22", "embedding": null, "metadata": {"page_label": "381", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "230fc89a-906b-4925-abbc-29a1e46db99a", "node_type": "4", "metadata": {"page_label": "381", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "b0d6ec376f9b1bf1c1d1feb5b6110db291fb90e333aec8464478b222c302d47f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14.2. Fourier and Laplace transforms 381\n3\n 2\n 1\n 0 1 2 3\n0\n2\n4\n6\n8\nFigure 14.2:Applying the smoothed conjugate twice gives a smoothed biconjugate\n(convex envelope) of the function.\nProof.\nf\u03b5(v) := \u03b5log\n\u222b\nexp\n(1\n\u03b5\u27e8u,v\u27e9\u2212 1\n\u03b5f(u))\n)\ndu\n= \u03b5log\n\u222b\nexp\n(\n\u22121\n2\u03b5\u2225u\u2212v\u22252\n2 + 1\n2\u03b5\u2225u\u22252\n2 + 1\n2\u03b5\u2225v\u22252\n2 \u22121\n\u03b5f(u))\n)\ndu\n= \u03b5log\n\u222b\nexp\n(\n\u22121\n2\u03b5\u2225u\u2212v\u22252\n2 + 1\n2\u03b5\u2225u\u22252\n2 \u22121\n\u03b5f(u))\n)\ndu+ 1\n2\u2225v\u22252\n2\n= \u03b5log\n\u222b\nG\u03b5(v\u2212u)Q\u03b5{f}(u)du+ 1\n2\u2225v\u22252\n2\n= \u03b5log(Q\u03b5{f}\u2217G\u03b5)(v) + 1\n2\u2225v\u22252\n= 1\n2\u2225v\u22252 \u2212\u03b5log\n( 1\nQ\u03b5{f}\u2217G\u03b5\n)\n(v)\n= Q\u22121\n\u03b5\n{ 1\nQ\u03b5{f}\u2217G\u03b5\n}\n(v)\nWhat did we gain from this viewpoint? The convex conjugate can\noften be difficult to compute in closed form. If we replaceRM with a dis-\ncrete setS(i.e., a grid), we can then approximate the smoothed convex", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 708, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6864e3f1-480e-4da7-9363-39a427f32ff0": {"__data__": {"id_": "6864e3f1-480e-4da7-9363-39a427f32ff0", "embedding": null, "metadata": {"page_label": "382", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "64f551ba-4e02-41c2-b816-0a4a009496c7", "node_type": "4", "metadata": {"page_label": "382", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "07416bcd915a8cfcba0e24ff43985dcc37a43061e56b6dd59c7ae36b568005b7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "382 Smoothing by integration\nconjugate inO(nlog n), wheren= |S|, using a discrete convolution,\n(Q\u03b5{f}\u2217G\u03b5)(v) \u2248\n\u2211\nu\u2208S\nG\u03b5(v\u2212u)Q\u03b5{f}(u)\n= Kq,\nwhere Kis then\u00d7nGaussian kernel matrix whose entries correspond to\nexp(\u22121\n2\u03b5\u2225u\u2212u\u2032\u22252\n2) for u,u\u2032\u2208S and qis then-dimensional vector whose\nentries correspond toexp(1\n\u03b5(1\n2 \u2225v\u22252\n2 \u2212f(u)) for u\u2208S. This provides\na GPU-friendly alternative to the fast Legendre transform algorithm,\ndiscussed in Section 13.2. Of course, due to the curse of dimensionality,\nthe technique is limited to functions defined on low-dimensional sets.\nWe illustrate in Fig. 14.2 the application of the technique to computing\nan approximate biconjugate (convex envelope) of a function.\nRemark 14.1(Link with the two-sided Laplace transform). For\none-dimensional functions, instead of using a convolution, we can\nalso write the soft convex conjugate as\nf\u2217\n\u03b5(v) = \u03b5log\n\u222b\u221e\n\u2212\u221e\nexp\n(1\n\u03b5[uv\u2212f(u)])\n)\ndu\n= \u03b5log B\n{\ne\u2212f\n\u03b5\n} (\n\u2212v\n\u03b5\n)\n= \u2212C\u22121\n\u03b5 {B{C\u03b5{f}}}\n(\n\u2212v\n\u03b5\n)\nwhere we defined the two-sided (bilateral) Laplace transform\nB{g}(v) :=\n\u222b\u221e\n\u2212\u221e\ne\u2212uvg(u)du\nand where we assumed that the integral exists.\n14.3 Examples\nIn this section, we review practical examples for which the convolution\nwith a Gaussian kernel enjoys an analytical solution.\n14.3.1 Smoothed step function", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1265, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "44dcec57-d2e9-477b-bd3e-b1b6dba924c1": {"__data__": {"id_": "44dcec57-d2e9-477b-bd3e-b1b6dba924c1", "embedding": null, "metadata": {"page_label": "383", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ab3ed68a-8da5-4b23-8357-753ac8ff28e6", "node_type": "4", "metadata": {"page_label": "383", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "ad225af4346d9ee5368dc8fdc3569a5679ef105eaaddf73b47fdf99626eb2c02", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14.3. Examples 383\nExample 14.1(Smoothed Heaviside). The Heaviside step function\nis defined by\nstep(u) := h(u) :=\n\uf8f1\n\uf8f2\n\uf8f3\n1 if u\u22650\n0 otherwise\n.\nWith the Gaussian kernel, we therefore obtain\n(h\u2217\u03ba\u03c3)(\u00b5) =\n\u222b\u00b5\n\u2212\u221e\n\u03ba\u03c3(z)h(\u00b5\u2212z)dz+\n\u222b\u221e\n\u00b5\n\u03ba\u03c3(z)h(\u00b5\u2212z)dz\n=\n\u222b\u00b5\n\u2212\u221e\n\u03ba\u03c3(z)dz\n= \u03a6\u03c3(\u00b5)\n= 1\n2\n[\n1 + erf\n( \u00b5\u221a\n2\u03c3\n)]\n,\nwhere \u03a6\u03c3(\u00b5) is the CDF of the Gaussian distribution with zero\nmean and variance\u03c32, and where we used the error function\nerf(z) := 2\u221a\u03c0\n\u222bz\n0\ne\u2212t2\ndt,\nthat we both already encountered in Chapter 3. Although there is\nno closed form for the error function, it is commonly available in\nnumerical analysis software, such as SciPy.\n14.3.2 Smoothed ReLU function\nExample 14.2(Smoothed ReLU). The ReLU is defined by\nr(u) :=\n\uf8f1\n\uf8f2\n\uf8f3\nu if u\u22650\n0 otherwise\n= u\u00b7h(u).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 744, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "12a88d13-871c-44d8-9626-d4e6cbe21f07": {"__data__": {"id_": "12a88d13-871c-44d8-9626-d4e6cbe21f07", "embedding": null, "metadata": {"page_label": "384", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8b2725dc-2582-4a04-add5-60fcd229c176", "node_type": "4", "metadata": {"page_label": "384", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "7d566b1482bdd64b193e292334e0c3fbb4c69d5657f7ef058ea2ceffb88f82e4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "384 Smoothing by integration\n3\n 2\n 1\n 0 1 2 3\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n= 0.5\n= 1.0\n= 2.0\n3\n 2\n 1\n 0 1 2 3\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 14.3:Smoothing of the ReLU and Heaviside functions by convolution with\na Gaussian kernel, for three values of the width\u03c3.\nSimilarly to the previous example, we obtain\n(r\u2217\u03ba\u03c3)(\u00b5) =\n\u222b\u00b5\n\u2212\u221e\n\u03ba\u03c3(z)r(\u00b5\u2212z)dz\n=\n\u222b\u00b5\n\u2212\u221e\n\u03ba\u03c3(z)(\u00b5\u2212z)dz\n= \u00b5\n\u222b\u00b5\n\u2212\u221e\n\u03ba\u03c3(z)z\u2212\n\u222b\u00b5\n\u2212\u221e\n\u03ba\u03c3(z)zdz\n= \u00b5\u03a6\u03c3(\u00b5) + \u03c32\u03ba\u03c3(\u00b5).\nIn the second integral, settinga:= 1\n2\u03c32 , we used\n\u222b\nze\u2212az2\ndz= \u22121\n2a\n\u222b\netdt= \u22121\n2aet + C = \u22121\n2ae\u2212az2\n+ C\nand t:= \u2212az2 \u21d2zdz = \u22121\n2adt.\nTo illustrate differentiation of the convolution, we show how to\ndifferentiate the smoothed ReLu.\nExample 14.3(Differentiating the smoothed ReLU). Differentiating\nthe smoothed ReLU from Example 14.2, we obtain\n(r\u2217\u03ba\u03c3)\u2032= (r\u2032\u2217\u03ba\u03c3) = h\u2217\u03ba\u03c3 = \u03a6\u03c3.\nTherefore, unsurprisingly, the derivative of the smoothed ReLU is\nthe smoothed Heaviside step function. Differentiating once again,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 920, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0b83b8f6-4ef7-4fb0-91bb-208be322e234": {"__data__": {"id_": "0b83b8f6-4ef7-4fb0-91bb-208be322e234", "embedding": null, "metadata": {"page_label": "385", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d366da98-37f9-4ca6-a7fa-329d3aaeaed5", "node_type": "4", "metadata": {"page_label": "385", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "34c5d592001b58b83e39c17526c98e4fbdb17566ee5c0a96ae53b60092d101c7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14.4. Perturbation of blackbox functions 385\nwe obtain,\n(r\u2217\u03ba\u03c3)\u2032\u2032= (h\u2217\u03ba\u03c3)\u2032= (h\u2032\u2217\u03ba\u03c3) = \u03b4\u2217\u03ba\u03c3 = \u03ba\u03c3,\nwhere the derivativeh\u2032is well-defined almost everywhere. We can\narrive at the same result by using thath\u2217\u03ba\u03c3 = \u03a6\u03c3 and \u03a6\u2032\n\u03c3 = \u03ba\u03c3,\nsince \u03a6\u03c3 and \u03ba\u03c3 are the CDF and PDF of the Gaussian with zero\nmean and\u03c32 variance.\n14.4 Perturbation of blackbox functions\nIn this section, we review how to approximately compute a convolution\nwith a kernel and its gradient using Monte-Carlo estimation.\n14.4.1 Expectation in a location-scale family\nA rather intuitive approach to smooth a functionf : RM \u2192R is to\naverage its values on an input\u00b5, perturbed by some additive noise\nZ \u223cp, for some noise distributionp. This defines the surrogate\nf\u03c3(\u00b5) := EZ\u223cp[f(\u00b5+ \u03c3Z)].\nThe parameter \u03c3 controls the perturbation strength: as\u03c3 \u21920, we\nnaturally recoverf. An equivalent viewpoint is obtained by defining\nthe transformation (change of variables)\nU := \u00b5+ \u03c3Z.\nWe then have\nU \u223cp\u00b5,\u03c3,\nwhere p\u00b5,\u03c3 is thelocation-family distributiongenerated by the noise\ndistribution p. It is the pushforward distribution ofZ through the\ntransformation (see Section 12.4.4). In this notation, the initial noise\ndistribution pis then simplyp= p0,1. The perturbed function can then\nbe expressed from these two perspectives as\nf\u03c3(\u00b5) = EZ\u223cp0,1 [f(\u00b5+ \u03c3\u00b7Z)]\n= EU\u223cp\u00b5,\u03c3[f(U)]. (14.4)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1320, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a6475ee6-c201-4b01-8076-b207ebc81dbd": {"__data__": {"id_": "a6475ee6-c201-4b01-8076-b207ebc81dbd", "embedding": null, "metadata": {"page_label": "386", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4ba84d37-a2dc-4bd3-965f-9f25d4bb7179", "node_type": "4", "metadata": {"page_label": "386", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "fe3898e3d06e7c0e336d0ed105469d0e0c0779899d8b9acd599af8a4faf985ea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "386 Smoothing by integration\nWriting the expectation as the integral of a p.d.f, we naturally recover\nthe smoothing by convolution presented earlier,\nf\u03c3(\u00b5) =\n\u222b\nf(\u00b5+ \u03c3z)p0,1(z)dz\n= f \u2217\u03ba\u03c3(\u00b5),\nwhere we defined the kernel\n\u03ba\u03c3(z) := p0,\u03c3(\u2212z).\nIn the sequel, we assume that the noise distribution decomposes as\np0,1(z) := exp(\u2212\u03bd(z))/C,\nwhere \u03bd(z) is the log-density of the noise distribution andC is a\nnormalization constant. For instance, the Gaussian distribution with\ndiagonal covariance matrix and the correspondingGaussian kernel\nare obtained with\u03bd(z) = 1\n2 \u2225z\u22252\n2 and C =\n\u221a\n2\u03c0\nM\n.\nApproximation by Monte-Carlo estimation\nInstead of approximating the integral above (continuous convolution)\nwith a discrete convolution on a grid, as we did in Section 14.1.3,\nthe expectation perspective suggests that we can estimatef\u03c3(\u00b5) by\nMonte-Carlo estimation: we simply draw samples from the distribution,\nevaluate the function at these samples and average. Beyond mere Monte-\nCarlo estimation, more elaborate approximation schemes are studied in\n(Chaudhuri and Solar-Lezama, 2010).\n14.4.2 Gradient estimation by reparametrization\nProvided that the conditions for swapping differentiation and integration\nhold (see Section 12.1), we have\n\u2207f\u03c3(\u00b5) = EZ\u223cp0,1 [\u2207f(\u00b5+ \u03c3\u00b7Z)]. (14.5)\nNote that iff is only differentiable almost everywhere, the formula may\nstill hold. For example, iff is the ReLU, then\u2207f is the Heaviside step\nfunction, and we obtain the correct gradient off\u03c3 using the formula", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1473, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "18d28e50-c19d-43c7-ae5c-dfb4ab5853e4": {"__data__": {"id_": "18d28e50-c19d-43c7-ae5c-dfb4ab5853e4", "embedding": null, "metadata": {"page_label": "387", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "08f0afe3-6c02-4ac3-96f8-6b9faf8d6693", "node_type": "4", "metadata": {"page_label": "387", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "deb0893730c359618a01dccd2a5bb5c7ec0a79ae827879bd38b3004159bbe1a0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14.4. Perturbation of blackbox functions 387\nabove; see Example 14.1. However, iff is not absolutely continuous,\nthe formula may not hold. For example, iff is the Heaviside function,\nthe right-hand side of(14.5) is 0 which does not match the gradient of\nf\u03c3; see again Example 14.1.\nFrom the second expression off\u03c3 in (14.4), we can see the formula\nof the gradient in (14.5) as a reparametrization trickU = \u00b5+ \u03c3Z; see\nSection 12.4. Namely, we have\n\u2207f\u03c3(\u00b5) = \u2207\u00b5EU\u223cp\u00b5,\u03c3[f(U)]\n= \u2207\u00b5EZ\u223cp0,1 [f(\u00b5+ \u03c3\u00b7Z)]\n= EZ\u223cp0,1 [\u2207\u00b5f(\u00b5+ \u03c3\u00b7Z)]\n= EZ\u223cp0,1 [\u2207f(\u00b5+ \u03c3\u00b7Z)]. (14.6)\n14.4.3 Gradient estimation by SFE, Stein\u2019s lemma\nIn some cases, we may not have access to\u2207f or f may not be absolutely\ncontinuous and therefore the formula in(14.5) cannot apply. For these\ncases, we can use the score function estimator (SFE) from Section 12.3.\nHere, forf\u03c3(\u00b5) = EU\u223cp\u00b5,\u03c3[f(U)], we obtain\n\u2207f\u03c3(\u00b5) = EU\u223cp\u00b5,\u03c3[f(U)\u2207\u00b5log p\u00b5,\u03c3(U)].\nSince the PDF can be written as\np\u00b5,\u03c3(u) = 1\n\u03c3p0,1((u\u2212\u00b5)/\u03c3),\nwhere\np0,1(z) := exp(\u2212\u03bd(z))/C,\nwe obtain\n\u2207\u00b5log p\u00b5,\u03c3(u) = \u2207\u03bd((u\u2212\u00b5)/\u03c3)/\u03c3.\nTo summarize, we have shown that\n\u2207f\u03c3(\u00b5) = EU\u223cp\u00b5,\u03c3[f(U)\u2207\u03bd((U \u2212\u00b5)/\u03c3)/\u03c3]\n= EZ\u223cp0,1 [f(\u00b5+ \u03c3\u00b7Z)\u2207\u03bd(Z)/\u03c3], (14.7)\nwhereweusedthechangeofvariable Z = (U\u2212\u00b5)/\u03c3.Thesametechnique\ncan also be used if we want to estimate the gradient w.r.t.\u03b8= (\u00b5,\u03c3) or", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1260, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "de1ee434-c695-4da5-9903-c380535a64dc": {"__data__": {"id_": "de1ee434-c695-4da5-9903-c380535a64dc", "embedding": null, "metadata": {"page_label": "388", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f2914984-93d6-4064-8052-405569b03054", "node_type": "4", "metadata": {"page_label": "388", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "54ac40088cdf2b553b8cd506ecffa4f12c4e5c0a7209ec3af6ceb3b03ad4da8c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "388 Smoothing by integration\nif we want to estimate the Jacobian of the expectation of a vector-valued\nfunction.\nIn the particular case of Gaussian noise, since\u2207\u03bd(z) = z, we obtain\n\u2207f\u03c3(\u00b5) = EZ\u223cp0,1 [f(\u00b5+ \u03c3\u00b7Z)Z/\u03c3].\nThis is known asStein\u2019s lemma. It should be noted that the above is\nan unbiased estimator of the gradient of the smoothed functionf\u03c3, but\na biased estimator of the gradient of the original functionf (assuming\nthat it exists). However, smoothing is usually a good thing, as it can\naccelerate the convergence of gradient-based algorithms. Computing\nthe gradient of perturbed general programs is studied in detail in\n(Kreikemeyer and Andelfinger, 2023).\n14.4.4 Link between reparametrization and SFE\nUsing the log-derivative identity, we have for any distribution with\ndifferentiable densityp\nEZ\u223cp[h(Z)\u2207log p(Z)] =\n\u222b\nRM\nh(z)\n(\u2207p(z)\np(z)\n)\np(z)dz\n=\n\u222b\nRM\nh(z)\u2207p(z)dz.\nUsing integration by partsand assuming thath(z)p(z) goes to zero\nwhen \u2225z\u2225\u2192\u221e , we have\n\u222b\nRM\nh(z)\u2207p(z)dz= \u2212\n\u222b\nRM\np(z)\u2207h(z)dz.\nWe have therefore the identity\nEZ\u223cp[h(Z)\u2207log p(Z)] = \u2212EZ\u223cp[\u2207h(Z)].\nImportantly, contrary to the SFE estimator from Section 12.3, this\nidentity uses gradients with respect to z, not with respect to the\nparameters of the distribution. Nevertheless, using the reparametrization", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1275, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ddba2cc6-75ff-485a-991a-4b2608a6ee62": {"__data__": {"id_": "ddba2cc6-75ff-485a-991a-4b2608a6ee62", "embedding": null, "metadata": {"page_label": "389", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f0f3ea6e-8cd6-4549-95b4-c927e37cab2f", "node_type": "4", "metadata": {"page_label": "389", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "e6be2b8ebba6ac222ded129c1e36d0ed2124820fd5bd9e9ee61f1780b945cc87", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14.4. Perturbation of blackbox functions 389\nh(z) := f(\u00b5+ \u03c3\u00b7z), we have\u2207h(z) = \u2207f(\u00b5+ \u03c3\u00b7z) \u00b7\u03c3 so that\n\u2207f\u03c3(\u00b5) = \u2207\u00b5EU\u223cp\u00b5,\u03c3[f(U)]\n= EZ\u223cp0,1 [\u2207f(\u00b5+ \u03c3\u00b7Z)] (reparametrization trick)\n= EZ\u223cp0,1 [\u2207h(Z)/\u03c3]\n= \u2212EZ\u223cp0,1 [h(Z)\u2207log p(Z)/\u03c3]\n= EZ\u223cp0,1 [h(Z)\u2207\u03bd(Z)/\u03c3]\n= EZ\u223cp0,1 [f(\u00b5+ \u03c3\u00b7Z)\u2207\u03bd(Z)/\u03c3] (score function estimator)\nEssentially, integration by parts allowed us to convert the reparametriza-\ntion trick estimator into the SFE estimator. For more applications of\nintegration by parts in machine learning, see Francis Bach\u2019s excellent\nblog post.\n14.4.5 Variance reduction and evolution strategies\nAs discussed in Chapter 12, the SFE suffers from high variance. We\nnow apply variance reduction techniques to it. To do so, we assume\nthat \u2207\u03bd(Z) has zero mean forZ \u223cp0,1. This assumption for example\nholds for Gaussian noise. This assumption implies that\nEZ\u223cp0,1 [f(\u00b5)\u2207\u03bd(Z)/\u03c3] = f(\u00b5)EZ\u223cp0,1 [\u2207\u03bd(Z)/\u03c3] = 0\nand therefore\n\u2207f\u03c3(\u00b5) = EZ\u223cp0,1 [(f(\u00b5+ \u03c3\u00b7Z) \u2212f(\u00b5))\u2207\u03bd(Z)/\u03c3] . (14.8)\nThis is an example ofcontrol variatediscussed in Section 12.3. This\ncan be interpreted as using afinite differencefor computing a direc-\ntional derivative in therandom directionZ (see \u201climit case\u201d below).\nInspired by acentral finite difference, we can also use\n\u2207f\u03c3(\u00b5) = EZ\u223cp0,1 [(f(\u00b5+ \u03c3\u00b7Z) \u2212f(\u00b5\u2212\u03c3\u00b7Z))\u2207\u03bd(Z)/(2\u03c3)] . (14.9)\nThese estimators have been used as part of blackbox (zero-order) op-\ntimization algorithms, such as evolution strategies (Salimans et\nal., 2017) or random gradient-free optimization(Nesterov and\nSpokoiny, 2017). For quadratic functions, it is easy to show that the\nsecond estimator achieves lower variance (Recht and Frostig, 2017). The", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1610, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "265dc105-c5a8-40b5-96d4-612fe0879569": {"__data__": {"id_": "265dc105-c5a8-40b5-96d4-612fe0879569", "embedding": null, "metadata": {"page_label": "390", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60dfb41b-bcfa-418b-b594-7e9526eb6264", "node_type": "4", "metadata": {"page_label": "390", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "d7fb8d0342aa53cd2bdbbeb979072f40ce4a2d2b704ff5d324d03b800a1a2af4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "390 Smoothing by integration\n100 101 102 103 104 105 106 107\nNumber of samples\n10 1\n100\n101\n102\n103\nGradient error\nSFE\nSFE with forward difference\nSFE with central difference\nFigure 14.4:Comparison of the score function estimator (SFE) with or without\nvariance reduction for blackbox gradient estimation. We show the error|\u2207f(\u00b5) \u2212\n\u2207f\u03c3(\u00b5)|for f(u) := u3 and f\u03c3(\u00b5) := E[f(\u00b5+ \u03c3Z)], where Z \u223cNormal(0,1) and\n\u03c3:= 0.1.Toestimate \u2207f\u03c3(\u00b5),wecomparethreeestimators:thevanillaSFEEq.(14.7),\nthe SFE estimator with forward difference (variance reduced) Eq. (14.8), and the SFE\nestimator with central difference (varianced reduced) Eq. (14.9). In all three cases,\nwe approximate the expectation by Monte-Carlo estimation using some number of\nsamples. The variance-reduced estimators not only achieve smaller error, they are\nalso more numerically stable as\u03c3 gets smaller.\nidea of sampling bothZ and \u2212Z simultaneously is called antithetic\n(Geweke, 1988) or mirrored sampling (Brockhoffet al., 2010). Evolution\nstrategies have also been used to obtain unbiased gradient estimators\nof partially unrolled computational graphs (Vicolet al., 2021). We\nempirically compare the SFE with or without variance reduction for\nblackbox gradient estimation in Fig. 14.4.\n14.4.6 Zero-temperature limit\nWe now discuss the limit case\u03c3 \u21920. That is, we assume that we do\nnot want to perform smoothing and that\u2207f exists. We recall that the\ndirectional derivative off at \u00b5in the directionzis\n\u2202f(\u00b5)[z] = \u27e8\u2207f(\u00b5),z\u27e9\n= lim\n\u03c3\u21920\n[f(\u00b5+ \u03c3\u00b7z) \u2212f(\u00b5)] /\u03c3.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1507, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c8daa24f-8184-456f-8e9f-942d193ac94f": {"__data__": {"id_": "c8daa24f-8184-456f-8e9f-942d193ac94f", "embedding": null, "metadata": {"page_label": "391", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d13ee4fe-7734-43f4-8184-57bbcf381caa", "node_type": "4", "metadata": {"page_label": "391", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "e88956a8bc7a8182d6a9d5034cad17bdeba92d990217ee82e8f9cb03e2c5b003", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14.5. Gumbel tricks 391\nWhen \u03c3\u21920 and Z follows the standard Gaussian distribution, meaning\nthat \u2207\u03bd(z) = z, Eq. (14.8) therefore becomes\n\u2207f\u03c3(\u00b5) = EZ\u223cp0,1 [\u2202f(\u00b5)[Z]\u2207\u03bd(Z)]\n= EZ\u223cp0,1 [\u2202f(\u00b5)[Z]Z]\n= EZ\u223cp0,1 [\u27e8\u2207f(\u00b5),Z\u27e9Z]\n= EZ\u223cp0,1\n[\n\u2207f(\u00b5)ZZ\u22a4\n]\n= \u2207f(\u00b5).\nThis should not be too surprising, as we already know from the convo-\nlution perspective thatf\u03c3(\u00b5) = (f \u2217\u03ba\u03c3)(\u00b5) \u2192f(\u00b5) when \u03c3\u21920. This\nrecovers the randomized forward-mode estimator already presented in\nSection 8.7.\n14.5 Gumbel tricks\n14.5.1 The Gumbel distribution\nThe Gumbel distribution is a distribution frequently used in extreme\nvalue theory. As illustrated in Fig. 14.5, we consider the shifted standard\nGumbel distribution, whose PDF is defined by\np(z) := exp(\u2212\u03bd(z)),\nwhere\n\u03bd(z) := z+ \u03b3+ exp(\u2212(z+ \u03b3)),\nand where\u03b3 \u22480.577 is Euler\u2019s constant. Note that in some formulations,\nthe distribution is not shifted, i.e.,\u03b3 is not added. IfZ is distributed\naccording to the shifted standard Gumbel distribution, we writeZ \u223c\nGumbel(0,1).\nTo obtain a multivariate extension with location-scale parameters\n\u00b5and \u03c3, we takeM independent random variablesZ := (Z1,...,Z m)\nand apply the location-scale transform (Section 12.4.1). That is,\nU \u223cGumbel(\u00b5,\u03c3) \u21d0\u21d2U = \u00b5+ \u03c3Z, Zi \u223cGumbel(0,1).\nAs we used shifted standard Gumbel distributions, we naturally get that\nE[U] = \u00b5and Var(U) = \u03c32. We can use Gumbel noise as an alternative", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1356, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7707894d-c3fb-4cbd-8a35-9864c41c0e5f": {"__data__": {"id_": "7707894d-c3fb-4cbd-8a35-9864c41c0e5f", "embedding": null, "metadata": {"page_label": "392", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "31aaf4d1-e581-4b33-81ee-c639c0b305af", "node_type": "4", "metadata": {"page_label": "392", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "ce7532fb74cca9d797961e454863dd3ef2d2fa2dacc205bbf15826f74547518f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "392 Smoothing by integration\n4\n 3\n 2\n 1\n 0 1 2 3 4\nz\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\np(z)\nShifted standard Gumbel distribution\nMean\nMode\nFigure 14.5:We use a shifted definition of the standard Gumbel distribution so\nthat the mean is achieved atz = 0, and the mode atz = \u2212\u03b3. With an unshifted\ndefinition, the mean and the mode would be achieved at\u03b3 and 0, respectively.\nto the Gaussian noise used in Section 14.4. Thankfully, in particular\ncases, we can compute closed-form expressions of the expectation of\nperturbed functions.\nRemark 14.2(Link between Gumbel and exponential distribution).\nA random variableZ is distributed asGumbel(\u00b5,1) if and only if\nexp(\u2212Z) is distributed as an exponential distributionExp(exp(\u00b5\u2212\n\u03b3)). To see this, one can simply compute the CDF ofexp(\u2212Z) and\nrecognize the CDF ofExp(exp(\u00b5\u2212\u03b3)). Therefore, when compar-\ning Gumbel distributions, we can use standard properties of the\nexponential distribution.\nRemark 14.3(Sampling Gumbel noise). IfU \u223cUniform(0,1),then\nZ := \u2212log(\u2212log(U)) \u2212\u03b3 satisfies Z \u223cGumbel(0,1), where we\nrecall that we useGumbel(0,1) to denote the shifted standard\nGumbel distribution. To see this, note thatP(Z \u2264t) = P(U \u2264\nexp(exp(\u2212(\u03b3+t)))) = exp(exp(\u2212(\u03b3+t))), where the last expression\nmatches the CDF ofGumbel(0,1).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1268, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "06c7ae4e-65c1-4f81-96b1-9f578d0f5955": {"__data__": {"id_": "06c7ae4e-65c1-4f81-96b1-9f578d0f5955", "embedding": null, "metadata": {"page_label": "393", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9d77d323-e287-47c2-9301-d17f17c1919e", "node_type": "4", "metadata": {"page_label": "393", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "8832bf404aef6f405f9a020b1aa2c257b5ae87cbae71dfb43ba992cc2ca1eda0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14.5. Gumbel tricks 393\n14.5.2 Perturbed comparison\nTo start with, the Gumbel distribution can be used to smooth a binary\ncomparison like the greater than or equal operators. Recall that the\nlatter is defined for any\u00b51,\u00b52 \u2208R as\ngt(\u00b51,\u00b52) :=\n\uf8f1\n\uf8f2\n\uf8f3\n1 if \u00b51 \u2265\u00b52\n0 if \u00b51 <\u00b52\n= step(\u00b51 \u2212\u00b52),\nwherestep is the Heaviside function. As shown below, by perturbing each\nvariable with Gumbel noise, we recoverlogistic(a\u2212b) = 1/(1 +e\u2212(a\u2212b))\nas an approximation ofstep(a\u2212b).\nProposition 14.3(Gumbel trick for binary variables). Let\nZ1,Z2 \u223cGumbel(0,1) be two independent random variables. The\ndifference of their location-scale transform (Section 12.4.1) is dis-\ntributed according to a logistic distribution (Remark 3.1), i.e.,\n\u00b51 + \u03c3Z1 \u2212(\u00b52 + \u03c3Z2) \u223cLogistic(\u00b51 \u2212\u00b52,\u03c3),\nfor \u00b51,\u00b52 \u2208R and \u03c3 >0. In particular, we have\nEZ1,Z2 [gt(\u00b51 + \u03c3Z1,\u00b52 + \u03c3Z2)] = 1\n1 + e\u2212(\u00b51\u2212\u00b52)/\u03c3.\nProof. We first derive the CDF of\u00b51 + \u03c3Z1 \u2212(\u00b52 + \u03c3Z2) as\nP(\u00b51 + \u03c3Z1 \u2212(\u00b52 + \u03c3Z2) \u2264t) = P(\u00b51/\u03c3+ Z1 \u2264(\u00b52 + t)/\u03c3+ Z2)\n= P\n(\ne\u2212(\u00b51/\u03c3+Z1) \u2265e\u2212((\u00b52+t)/\u03c3+Z2)\n)\n.\nBy Remark 14.2,e\u2212(\u00b51/\u03c3+Z1) \u223cExp(exp(\u00b51/\u03c3\u2212\u03b3)), and similarly for\ne\u2212(\u00b52+t)/\u03c3+Z2 . Now one easily shows that ifU \u223cExp(u),V \u223cExp(v)\nindependent, thenP(U \u2264V) = u/(u+ v). Hence, we get\nP(\u00b51 + \u03c3Z1 \u2212(\u00b52 + \u03c3Z2) \u2264t) = e(\u00b52+t)/\u03c3\u2212\u03b3\ne(\u00b52+t)/\u03c3\u2212\u03b3 + e\u00b51/\u03c3\u2212\u03b3\n= 1\n1 + e\u2212(t\u2212(\u00b51\u2212\u00b52))/\u03c3.\nWe recognize the CDF of the logistic distribution with mean\u00b51 \u2212\u00b52\nand scale\u03c3, denotedLogistic(\u00b51 \u2212\u00b52,\u03c3). For the last claim, we simply", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1408, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ef5ae5d8-046a-4cd3-9e62-d25d161c1d31": {"__data__": {"id_": "ef5ae5d8-046a-4cd3-9e62-d25d161c1d31", "embedding": null, "metadata": {"page_label": "394", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fe18d9c2-64b3-48e4-ad32-d186e47115b5", "node_type": "4", "metadata": {"page_label": "394", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "da232d9996fa47216ab3060e10ea38eec30a1d26534fd8329e34bab0c2fdbef6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "394 Smoothing by integration\nhave that\nE[gt(\u00b51 + \u03c3Z1,\u00b52 + \u03c3Z2)] = E[step(\u00b51 + \u03c3Z1 \u2212(\u00b52 + \u03c3Z2)]\n= P(\u00b51 + \u03c3Z1 \u2212(\u00b52 + \u03c3Z2) \u22650)\n= 1\n1 + e\u2212(\u00b51\u2212\u00b52)/\u03c3.\n14.5.3 Perturbed argmax\nSuppose we want to smooth\ny(u) := arg max\ny\u2208{e1,...,eM}\n\u27e8y,u\u27e9= \u03d5(i(u)),\nwhere\ni(u) := arg max\ni\u2208[M]\nui\n\u03d5(i) := ei\nwith \u03d5(i) is the one-hot encoding ofi \u2208[M]. It turns out that the\nfunction y(u) perturbed using Gumbel noise enjoys a closed form\nexpectation, which is nothing else than the softargmax.\nProposition 14.4(Gumbel trick for categorical variables). Letusde-\nfine M independent random variablesZ \u223cGumbel(0,1) \u2208RM.\nThen, for\u00b5\u2208RM and \u03c3 >0,\nY := i(\u00b5+ \u03c3\u00b7Z) \u21d0\u21d2Y \u223cCategorical(softargmax(\u00b5/\u03c3)).\nMoreover, we have\ny\u03c3(\u00b5) = EZ[y(\u00b5+ \u03c3\u00b7Z)]\n= EY[\u03d5(Y)]\n= softargmax(\u00b5/\u03c3).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 734, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4dcffb83-de21-437f-b229-ded6fc244bad": {"__data__": {"id_": "4dcffb83-de21-437f-b229-ded6fc244bad", "embedding": null, "metadata": {"page_label": "395", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f12f874-6da4-49cb-97ba-54897d0ff17f", "node_type": "4", "metadata": {"page_label": "395", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "1c3ca53a1955ea8801bd86d9ec68c13b1c4659a7c28dc1a600e1edfaffbea139", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14.5. Gumbel tricks 395\nProof. Fork\u2208[M], we have that\nP(Y = k) = P\n(\narg max\ni\u2208[M]\n{\u00b5i + \u03c3Zi}= k\n)\n= P\n(\narg min\ni\u2208[M]\n{e\u2212\u00b5i/\u03c3\u2212Zi}= k\n)\nBy Remark 14.2, we have thate\u2212\u00b5i/\u03c3\u2212Zi \u223cExp(exp(\u00b5i/\u03c3\u2212\u03b3)). One eas-\nily verifies as an exercise, that, forU1,...,U M independent exponential\nvariables with parametersu1,...,u m, we haveP(arg mini\u2208[M]{Ui}=\nk) = uk/\u2211M\ni=1 ui. Hence, we get\nP(Y = k) = exp(\u00b5k/\u03c3)\n\u2211M\ni=1 exp(\u00b5i/\u03c3),\nthat is,\nY \u223cCategorical(softargmax(\u00b5/\u03c3)).\nThe last claim follows from the distribution ofY and the definition of\n\u03d5.\n14.5.4 Perturbed max\nA similar result holds if we now wish to perturb the max instead of the\nargmax.\nProposition 14.5(Link to log-sum-exp). Let us defineM indepen-\ndent random variablesZ \u223cGumbel(0,1) \u2208RM and\nf(u) := max\ni\u2208[M]\nui.\nThen, for\u00b5\u2208RM and \u03c3 >0,\nV := f(\u00b5+ \u03c3\u00b7Z) \u21d0\u21d2V \u223cGumbel(\u03c3LSE(\u00b5/\u03c3),\u03c3).\nMoreover, we have\nf\u03c3(\u00b5) := EZ[f(\u00b5+ \u03c3\u00b7Z)] = EV[V] = \u03c3\u00b7LSE(\u00b5/\u03c3).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 884, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "974fb3f9-6a18-43e4-8eb6-b33cb15d9a7d": {"__data__": {"id_": "974fb3f9-6a18-43e4-8eb6-b33cb15d9a7d", "embedding": null, "metadata": {"page_label": "396", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1296e427-d73e-402e-934d-cf7b8d5d995b", "node_type": "4", "metadata": {"page_label": "396", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "8f8de0f67a62a14f1e96917dbb09de7d1d6101e5f637d8b3b8b3b8bfd973bbe8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "396 Smoothing by integration\nProof. We derive the CDF off(\u00b5+ \u03c3\u00b7Z) as\nP(max\ni\u2208[M]\n{\u00b5i + \u03c3Zi}\u2264 t) = P\n(\nmin\ni\u2208[M]\n{e\u2212(\u00b5i/\u03c3\u2212Zi)}\u2265 e\u2212t/\u03c3\n)\nWe havee\u2212(\u00b5i/\u03c3\u2212Zi) \u223cExp(exp(\u00b5i/\u03c3i) \u2212\u03b3) and forU1,...,U M inde-\npendent exponential random variables with parametersui, we have\nmini\u2208[M] Ui \u223cExp(\u2211M\ni=1 ui). Hence,\nP\n(\nmax\ni\u2208[M]\n{\u00b5i + \u03c3Zi}\u2264 t\n)\n= exp\n(\n\u2212\nM\u2211\ni=1\n(exp(\u00b5i/\u03c3\u2212\u03b3)) exp(\u2212t/\u03c3)\n)\n= exp(\u2212exp(\u2212(t\u2212\u03c3LSE(\u00b5/\u03c3))/\u03c3\u2212\u03b3)).\nWe recognize the CDF of the shifted Gumbel distribution with location-\nscale parameters\u03c3LSE(\u00b5/\u03c3) and \u03c3.\nFor further reading on the Gumbel trick, see Tim Vieira\u2019s great\nblog.\n14.5.5 Gumbel trick for sampling\nThe Gumbel trick is also useful in its own right forsampling with-\nout computing the normalization constant of the softargmax. Indeed,\nProposition 14.4 ensures that if Z is Gumbel noise, then Y is dis-\ntributed according toCategorical(softargmax(\u00b5/\u03c3)). Computing the\narg-maximum, as required to computeY, can be done in one pass. There-\nfore, we obtain a one-pass algorithm to sample directly from the logits\n\u00b5, without explicitly computing the probabilitiessoftargmax(\u00b5/\u03c3).\nOne may wonder whether such trick could also be used with the\nnormal distribution. Unfortunately, there is no closed form in this case\nbecause it would require integrating the CDF of the maximum ofM\u22121\nGaussian distributions. However, other tricks can be defined such as\nusing Weibull distributions, see Baloget al.(2017).\n14.5.6 Perturb-and-MAP\nPreviously, we discussed the Gumbel trick in the classification setting,\nwhere Y = [ M]. In the structured prediction setting, outputs are", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1569, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c88079db-3e0d-4a2e-9b61-8a5c6437bdf8": {"__data__": {"id_": "c88079db-3e0d-4a2e-9b61-8a5c6437bdf8", "embedding": null, "metadata": {"page_label": "397", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2cc3c423-fdee-4eca-a002-c3755d61dc79", "node_type": "4", "metadata": {"page_label": "397", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "68ce1a6aad1dc6790d7ac0c73c5e146fe2bca992162ff87582c2b5897220eb8b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14.5. Gumbel tricks 397\ntypically embedded inRM but the output space is very large. That is,\nY\u2286 RM but |Y|\u226b M. Structured outputs are then decoded using a\nmaximum a-posteriori (MAP) oracle\nf(u) := max\ny\u2208Y\n\u27e8y,u\u27e9\ny(u) := arg max\ny\u2208Y\n\u27e8y,u\u27e9.\nFor this setting, the perturbed versions off and y,\nf\u03c3(\u00b5) := EZ[f(\u00b5+ \u03c3\u00b7Z)]\ny\u03c3(\u00b5) := EZ[y(\u00b5+ \u03c3\u00b7Z)],\nno longer enjoy a closed form in general. However, we can approximate\nthem using Monte-carlo estimation. For the gradient of\u2207f\u03c3(\u00b5), two\nestimators exist (Abernethyet al., 2016; Berthetet al., 2020).\nProposition 14.6(Gradient of perturbed max). Let Y \u2286RM and\nZ be noise with density\np0,1(z) := exp(\u2212\u03bd(z))/C.\nThen, f\u03c3(\u00b5) is smooth, and its gradient is given by\n\u2207f\u03c3(\u00b5) = EZ[y(\u00b5+ \u03c3\u00b7Z)]\n= EZ[f(\u00b5+ \u03c3\u00b7Z)\u2207\u03bd(Z)\u03c3]\n\u2208conv(Y).\nWe therefore have\u2207f\u03c3(\u00b5) = y\u03c3(\u00b5).\nThe first estimator is simply a consequence of the reparametrization\ntrick seen in Eq. (14.6) and ofy= \u2207f, which follows from Danskin\u2019s\ntheorem (see Section 11.2). The second estimator is just SFE seen in\nEq. (14.7). The first estimator usually has lower variance, as it uses\nmore information, namely thaty= \u2207f.\nThe Jacobian ofy\u03c3(\u00b5) also has two estimators (Abernethyet al.,\n2016; Berthetet al., 2020).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1186, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ff44a9fa-a8f6-47c1-b09f-07d2638ea6f2": {"__data__": {"id_": "ff44a9fa-a8f6-47c1-b09f-07d2638ea6f2", "embedding": null, "metadata": {"page_label": "398", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b1a62608-225e-4b81-abaf-156355da975e", "node_type": "4", "metadata": {"page_label": "398", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "5637aa50176f4d05c98b25ac212a3f0a2294199acf2d0f24f768fb4c517dec9b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "398 Smoothing by integration\nProposition 14.7(Jacobian of perturbed argmax). Under the same\nnotation as in Proposition 14.6, we have\n\u2202y\u03c3(\u00b5) = EZ\n[\ny(\u00b5+ \u03c3Z)\u2207\u03bd(Z)\u22a4/\u03c3\n]\n= EZ\n[\nf(\u00b5+ \u03c3Z)\n(\n\u2207\u03bd(Z)\u2207\u03bd(Z)\u22a4\u2212\u22072\u03bd(Z)\n)\n/\u03c32\n]\n.\nThe first estimator uses SFE. The second estimator is obtained by\ndifferentiating through\ny\u03c3(\u00b5) = \u2207f\u03c3(\u00b5) = EZ[f(\u00b5+ \u03c3\u00b7Z)\u2207\u03bd(Z)/\u03c3].\nThe first estimator usually has lower variance. Note that we cannot use\nthe reparametrization trick this time, sinceyis discontinuous, contrary\nto f.\nLink between perturbation and regularization\nAs shown in (Berthetet al., 2020, Proposition 2.2), assumingYis a\nconvex polytope with non-empty interior andp has a strictly positive\ndensity, the function\nf\u03c3(\u00b5) := EZ[f(\u00b5+ \u03c3\u00b7Z)] = EZ[max\ny\u2208Y\n\u27e8\u00b5+ \u03c3\u00b7Z,y\u27e9]\nis strictly convex and its convex conjugatef\u2217\n\u03c3(y) is Legendre-type. We\ncan therefore rewritef\u03c3(\u00b5) from the regularization perspective as\nf\u03c3(\u00b5) = max\ny\u2208Y\n\u27e8\u00b5,y\u27e9\u2212f\u2217\n\u03c3(y).\nand \u2207f\u03c3(\u00b5) = y\u03c3(\u00b5) is a mirror map, a one-to-one mapping from\nRM to the interior ofY. Unfortunately,f\u2217\n\u03c3(y) does not enjoy a closed\nform in general. Conversely, does any regularization has a corresponding\nnoise distribution? The reciprocal is not true.\n14.5.7 Gumbel-softargmax\nSuppose we want to smooth out thecomposition h(u) := g(y(u))\nbetween some functiong: {e1,..., eM}\u2192 R and the argmax\ny(u) := arg max\ny\u2208{e1,...,eM}\n\u27e8y,u\u27e9.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1341, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ec705e36-42ca-4767-8279-8170b3875a9a": {"__data__": {"id_": "ec705e36-42ca-4767-8279-8170b3875a9a", "embedding": null, "metadata": {"page_label": "399", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ce859bd8-19f0-46d1-b8d7-1cad32382376", "node_type": "4", "metadata": {"page_label": "399", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "8340012d302c67a527900547fec5ab0f93514b57efab3566293f62b88ccfe189", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14.5. Gumbel tricks 399\nWe can do so by\nh\u03c3(\u00b5) := EZ [g(y(\u00b5+ \u03c3Z))] .\nThis is useful for instance to compute the expectation of a loss (instead\nof the loss of an expectation). To compute the gradient ofh\u03c3(\u00b5), we can\nreadily use the SFE. However, we saw that it suffers from high variance.\nUnfortunately, we cannot swap differentiation and integration (expecta-\ntion) here, sincey(u) is a discontinuous function. See Section 12.1 for\nmore details regarding differentiation under the integral sign.\nThe key idea of the Gumbel-softargmax (Janget al., 2016; Maddison\net al., 2016) is to replacey(u) with a softargmax (with temperature\nparameter \u03c4) to define\nh\u03c3,\u03c4(\u00b5) := EZ [g(softargmax\u03c4(\u00b5+ \u03c3Z))] .\nSince the softargmax is a regularized argmax, we can see the Gumbel-\nsoftargmax approach as usingboth regularization and perturbation.\nNote that the approach is also known as Gumbel-softmax. We use the\nname Gumbel-softargmax for consistency with the terminology of this\nbook.\nThe key benefit is that we can now swap differentiation and integra-\ntion (expectation) to get an unbiased estimator of\u2207h\u03c3,\u03c4(\u00b5). However,\nthis will be abiased estimator of\u2207h\u03c3(\u00b5), the amount of bias being\ncontrolled by the temperature\u03c4. In particular, in the limit case\u03c4 \u21920,\nwe haveh\u03c3,\u03c4(\u00b5) \u2192h\u03c3(\u00b5). One caveat, however, is that the functiong\nneeds to be well defined on\u25b3M, instead of{e1,..., eM}.\nThe use of the softargmax transformation defines a continuous\ndistribution (Jang et al., 2016; Maddisonet al., 2016), that we now\nexplain with\u03c3= 1.\nProposition 14.8(Gumbel-softargmax / Concrete distributions). Let\nus define thecontinuousrandom variable\nT := softargmax\u03c4(\u00b5+ Z) \u2208\u25b3M,\nwhere Z is a Gumbel random variable. Then T is distributed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1702, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b3dc535a-6e31-41f5-bec0-d747421a859c": {"__data__": {"id_": "b3dc535a-6e31-41f5-bec0-d747421a859c", "embedding": null, "metadata": {"page_label": "400", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "42fcd292-59de-4dcc-aa75-a73d0fefc07b", "node_type": "4", "metadata": {"page_label": "400", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "976694cb6d3c719fb66aca87159631a7b5f11e6de3fca2f016dab38d4c511e06", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "400 Smoothing by integration\naccording to a distribution with density\np\u00b5,\u03c4(t) := \u0393(M)\u03c4M\u22121\n(M\u2211\ni=1\n\u03c0i\nt\u03c4\ni\n)\u2212M M\u220f\ni=1\n\u03c0i\nt\u03c4+1\ni\n,\nwhere \u03c0:= softargmax(\u00b5).\nWe can extend the Gumbel softargmax to the structured setting by\nreplacing\ny(u) := arg max\ny\u2208Y\n\u27e8y,u\u27e9,\nwith its regularized variant (Pauluset al., 2020). Similarly as before,\none caveat is thatg needs to be well defined onconv(Y) instead ofY.\nMoreover, regularizingyis not always easy computationally.\n14.6 Summary\n\u2022 We studied smoothing techniques based on function convolution\nwith a kernel. Due to the commutativity of the convolution, we\ncan alternatively see these as the expectation of the function,\nperturbed with noise, assuming the kernel corresponds to the\nPDF of some noise distribution.\n\u2022 Their gradients can be estimated using the path gradient estimator\n(PGE) or score function estimator (SFE), depending on whether\nthe gradient of the original function is available or not.\n\u2022 We saw that Stein\u2019s lemma is a special case of SFE used with\nGaussian noise. The so-called \u201cevolution strategies\u201d are just a\nvariant of that with variance reduction and can be interpreted as\nrandomized finite difference.\n\u2022 When using Gumbel noise, we were able to derive closed-form\nexpressions for the expectation in specific cases: perturbed com-\nparison, perturbed argmax and perturbed max.\n\u2022 We also studied the connections between smoothing by optimiza-\ntion and smoothing by integration. Infimal convolution is the\ncounterpart of convolution, and the Legendre-Fenchel transform", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1527, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5b1048d4-aa4e-4bf8-8cc8-87501bda2585": {"__data__": {"id_": "5b1048d4-aa4e-4bf8-8cc8-87501bda2585", "embedding": null, "metadata": {"page_label": "401", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dd79b24e-a810-4def-8a40-d9c7f67fb015", "node_type": "4", "metadata": {"page_label": "401", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "856ffbb4a531c8694bd78cfc29cc934fb40c59e5a6bf6474fca77d5ae67a0892", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14.6. Summary 401\nis the counterpart of Fourier and Laplace\u2019s transforms. Infimal\nconvolution uses a min-plus algebra in the log domain, while\nthe convolution uses a sum-product algebra in the exponential\ndomain.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 212, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f53c02dc-2894-4d51-a8df-a0e1aa24be64": {"__data__": {"id_": "f53c02dc-2894-4d51-a8df-a0e1aa24be64", "embedding": null, "metadata": {"page_label": "402", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9a391930-3324-4cae-bbf8-653b6acf4753", "node_type": "4", "metadata": {"page_label": "402", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "d016dc209269e33e9d1a73256f719a0759a6cccb58278977a636ac3652512e48", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Part V\nOptimizing differentiable\nprograms", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 41, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8171804d-5ab3-4c5a-bed2-430fbf98b126": {"__data__": {"id_": "8171804d-5ab3-4c5a-bed2-430fbf98b126", "embedding": null, "metadata": {"page_label": "403", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bd54a1b5-0b94-47ac-8f86-afe20a648369", "node_type": "4", "metadata": {"page_label": "403", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "bf9870ad408101720a659f577ed947fdbf5e585e4de93619352ec20c8f6f4a43", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "15\nOptimization basics\n15.1 Objective functions\nConsider a functionL, for example evaluating the error or \u201closs\u201dL(w)\nachieved by a model with parametersw\u2208W, whereW= RP. To find\nthe best possible model parameterization, we seek to minimizeL(w),\nthat is, to compute approximately\nL\u22c6 := inf\nw\u2208W\nL(w),\nassuming that the infimum exists (i.e.,L(w) is lower bounded). We will\ndenote a solution, if it exists, by\nw\u22c6 \u2208arg min\nw\u2208W\nL(w) :=\n{\nw\u2208W : L(w) = min\nw\u2032\u2208W\nL(w\u2032)\n}\n.\nIn general, an analytical solution is not available and computing such\na minimum approximately requires an optimization algorithm. An\noptimization algorithm is an iterative procedure, which, starting from an\ninitial pointw0, outputs aftertiterations a pointwt that approximates\nthe minimum ofL up to some accuracy\u03b5, i.e.,\nL(wt) \u2212L\u22c6 \u2264\u03b5. (15.1)\n403", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 809, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2e260656-243f-41c3-b670-f04111a5d0d4": {"__data__": {"id_": "2e260656-243f-41c3-b670-f04111a5d0d4", "embedding": null, "metadata": {"page_label": "404", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "96b5a6d7-c83d-4479-8fe1-bf88a7a63700", "node_type": "4", "metadata": {"page_label": "404", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "1c63168f66000bab38e980d0fd5c1fbbcf74a91572036b495472c284ee7f18af", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "404 Optimization basics\n15.2 Oracles\nTo produce iteratesw1,w2,... that converge to a minimum, the al-\ngorithm naturally needs to have access to information aboutL. For\nexample, the algorithm needs a priori to be able to evaluateL to know\nif it decreased its value or not. Such information about the function\nis formalized by the notion oforacles (Nemirovski and Yudin, 1983).\nFormally, oracles are procedures that an algorithm can call to access\ninformation about the objectiveL(w) at any given pointw\u2208W. We\nusually mainly consider the following three oracles.\n\u2022 Zero-order oracle:evaluating the functionL(w) \u2208R.\n\u2022 First-order oracle:evaluating the gradient\u2207L(w) \u2208W for L\ndifferentiable.\n\u2022 Second-order oracle:evaluating the Hessian matrix\u22072L(w),\nor evaluating the Hessian-vector product (HVP)\u22072L(w)v\u2208W,\nfor L twice differentiable and any vectorv\u2208W.\nGiven an oracleOfor a functionL, we can formally define an optimiza-\ntion algorithm as a procedure which computes the next iterate as a\nfunction of all past and current information. Formally, an algorithmA\nbuilds a sequencew1,..., wt from a starting pointw0 as\nwt+1 := A(w0,..., wt,O(w0),..., O(wt),\u03bb),\nwhere \u03bb\u2208\u039b \u2286RQ encapsulates some hyperparameters of the algorithm,\nsuch as the stepsize. Oftentimes, algorithms build the next iterate simply\nfrom the information collected at the current iterate, without using all\npast iterates. That is, they take the formwt+1 = A(wt,O(wt),\u03bb). A\nclassical example is the gradient descent algorithm, that uses a first-order\noracle to compute iterates of the form\nwt+1 := wt \u2212\u03b3\u2207L(wt),\nwhere the stepsize\u03b3 is a hyperparameter of the algorithm. The notion of\noracle therefore delineates different classes of algorithms. For instance,\nwe may consider zero-order algorithms or first-order algorithms.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1781, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3b2d3d32-bc2f-4455-acc2-747593064965": {"__data__": {"id_": "3b2d3d32-bc2f-4455-acc2-747593064965", "embedding": null, "metadata": {"page_label": "405", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "385ccbfc-d3a8-4df8-a6ef-5c3c0ccb579f", "node_type": "4", "metadata": {"page_label": "405", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "f83911ddbfe932c077dc64a4b72e240bdb414c28662b8d4d95e96f10071e0d3f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "15.3. Variational perspective of optimization algorithms 405\n15.3 Variational perspective of optimization algorithms\nOne of the most basic optimization algorithms is theproximal point\nmethod, which produceswt+1 from wt by\nwt+1 := arg min\nw\u2208W\nL(w) + 1\n2\u03b3\u2225w\u2212wt\u22252\n2.\nIn words, the next iterate is produces by solving a trade-off between\nminimizing the functionL and staying close towt. Unfortunately, the\noptimization problem involved in performing this parameter update is\nas difficult as the original optimization problem, making the proximal\npoint method impractical.\nAs we shall see in Chapter 16 and Chapter 17, many optimization\nalgorithms can be seen as an approximation of the proximal point\nmethod, in the sense that they solve\nwt+1 := arg min\nw\u2208W\n\u02dcL(w,wt) + 1\n2\u03b3\u2225w\u2212wt\u22252\n2.\nor more generally\nwt+1 := arg min\nw\u2208W\n\u02dcL(w,wt) + 1\n\u03b3d(w,wt),\nwhere \u02dcL(w,wt) is an approximation ofL(w) around wt and d(w,w\u2032)\nis some form of distance betweenwand w\u2032. Different choices of\u02dcL and\nd lead to different optimization algorithms, and to different trade-offs.\n15.4 Classes of functions\nWhen studying algorithms theoretically, stronger results can often be\nstated by restricting to certain classes of functions. We already covered\ncontinuous and differentiable functions in Chapter 2. We review a few\nimportant other classes in this section.\n15.4.1 Lipschitz functions\nLipschitz continuity is a stronger form of continuity. Intuitively, a\nLipschitz continuous function is limited in how fast it can change.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1492, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e72545b6-f514-44bb-966b-4f770a5be1a9": {"__data__": {"id_": "e72545b6-f514-44bb-966b-4f770a5be1a9", "embedding": null, "metadata": {"page_label": "406", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "34698dc2-68d0-4af2-9f49-a0688b3dd878", "node_type": "4", "metadata": {"page_label": "406", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "97e2e1a8afb3b3c158ee681e0faab8b2aeef22802142209abacdf26af3a793c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "406 Optimization basics\nDefinition 15.1(Lipschitz-continuous functions). Afunction g: W\u2192\nFis \u03b2-Lipschitz continuousif for allw,v\u2208W\n\u2225g(w) \u2212g(v)\u22252 \u2264\u03b2\u2225w\u2212v\u22252.\nNote that the definition is valid even for vector-valued functions.\nWith respect to arbitrary norms\nThanks to dual norms reviewed in Section 18.1, we can state a more\ngeneral definition of Lipschitz continuity based on arbitrary norms,\ninstead of the2-norm. Moreover, we may consider Lipschitz-continuity\nover a subset of the input domain.\nDefinition 15.2(Lipschitz continuous functions w.r.t. a norm). Afunc-\ntion g: W\u2192F is said to be\u03b2-Lipschitz w.r.t. a norm\u2225\u00b7\u2225 over a\nset C\u2286W if for allw,v\u2208C\n\u2225g(w) \u2212g(v)\u2225\u2217\u2264\u03b2\u2225w\u2212v\u2225.\nWhen \u2225\u00b7\u2225 = \u2225\u00b7\u22252, we recover Definition 15.1, since the2-norm is\ndual to itself.\n15.4.2 Smooth functions\nA differentiable functionL is said to be\u03b2-smooth if its gradients are\n\u03b2-Lipschitz continuous. Settingg(w) = \u2207L(w) in Definition 15.1, we\nobtain the following definition.\nDefinition 15.3(Smooth functions). A differentiable function\nL: W\u2192 R is \u03b2-smooth for \u03b2 >0 if for allw,v\u2208W\n\u2225\u2207L(w) \u2212\u2207L(v)\u22252 \u2264\u03b2\u2225w\u2212v\u22252.\nSmoothness ensures that the information provided by the gradient\nat somewis meaningful in a neighborhood ofw, since its variations are\nupper-bounded. If the variations were not bounded, the gradient atv", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1280, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3b94ba16-6c47-4cce-baef-1df4232f69f2": {"__data__": {"id_": "3b94ba16-6c47-4cce-baef-1df4232f69f2", "embedding": null, "metadata": {"page_label": "407", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f3ff06ce-ac51-448e-a7ba-5e199aa99211", "node_type": "4", "metadata": {"page_label": "407", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "a8078a582124ebe6404e73e521842cd0c8943566ae69e0534ae4631654786c2d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "15.4. Classes of functions 407\narbitrarily close towcould drastically change, rendering the information\nprovided by a first-order oracle potentially useless.\nSmoothness of a function can be interpreted as having a quadratic\nupper bound on the function as formalized below.\nProposition 15.1(Smooth functions). If a differentiable function\nL: W\u2192 R is \u03b2-smooth then for allw,v\u2208W,\n|L(w) \u2212L(v) + \u27e8\u2207L(v),w\u2212v\u27e9|\u2264 \u03b2\n2 \u2225w\u2212v\u22252\n2.\nIn particular, we have\nL(w) \u2264L(v) + \u27e8\u2207L(v),w\u2212v\u27e9+ \u03b2\n2 \u2225w\u2212v\u22252\n2.\nProof. This is shown by bounding|L(v) \u2212L(w) \u2212\u27e8\u2207L(w),v\u2212w\u27e9|\nusing the integral representation of the objective alongw\u2212v, i.e.,\n|L(v) \u2212L(w) \u2212\u27e8\u2207L(w),v\u2212w\u27e9|= |\n\u222b1\n0 \u27e8\u2207L(w+ s(v\u2212w)),v\u2212w\u27e9ds\u2212\n\u27e8\u2207L(w),v\u2212w)\u27e9|\u2264\n\u222b1\n0 \u2225\u2207L(w+ s(v\u2212w))ds\u2212\u2207L(w)\u22252\u2225v\u2212w\u22252 \u2264\nL\u2225w\u2212v\u22252\n2/2, where the last inequality follows from the smoothness\nassumption and standard integration.\nIn other words,L(w) is upper-bounded and lower-bounded around\nv by a quadratic function ofw. We will see in Section 16.1 that this\ncharacterization gives rise to a variational perspective on gradient\ndescent.\nWith respect to arbitrary norms\nWe can generalize the definition of smoothness in Definition 15.3 to\narbitrary norms.\nDefinition 15.4(Smooth functions w.r.t. a norm). A function L :\nW \u2192R is \u03b2-smooth w.r.t. a norm \u2225\u00b7\u2225 over a set C if for all\nw,v\u2208C\n\u2225\u2207L(w) \u2212\u2207L(v)\u2225\u2217\u2264\u03b2\n2 \u2225w\u2212v\u2225.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1303, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c7a18bf3-168b-4df6-acca-4a7276851237": {"__data__": {"id_": "c7a18bf3-168b-4df6-acca-4a7276851237", "embedding": null, "metadata": {"page_label": "408", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "41c78e09-a863-4f35-9f27-b23f8c164a12", "node_type": "4", "metadata": {"page_label": "408", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "f5beb9406423fa32ad0d981228ae1f904859792d14706340a5963fc5b62b83f1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "408 Optimization basics\nAn equivalent characterization, generalizing Proposition 15.1 to\narbitrary norms, is given below (see, e.g. Beck (2017, Theorem 5.8)).\nProposition 15.2(Smooth functions w.r.t. a norm). Ifadifferentiable\nfunction L: W\u2192 R is \u03b2-smooth w.r.t. a norm\u2225\u00b7\u2225 over a setC,\nthen for allw,v\u2208C\n|L(w) \u2212L(v) \u2212\u27e8\u2207L(v),w\u2212v\u27e9\ued19 \ued18\ued17 \ued1a\nBL(w,v)\n|\u2264 \u03b2\n2 \u2225w\u2212v\u22252,\nwhereBf istheBregmandivergencegeneratedby f (Definition18.2).\n15.4.3 Convex functions\nA convex function is a function such that its value on the average of two\nor more points is smaller than the average of the values of the functions\nat these points. This is illustrated in Figure 15.2 and formalized below.\nDefinition 15.5(Convex functions). A functionL: W\u2192 R is said\nto beconvexif for allw,v\u2208W and \u03c4 \u2208[0,1]\nL(\u03c4w+ (1 \u2212\u03c4)v) \u2264\u03c4L(w) + (1\u2212\u03c4)L(v).\nThe functionL is strictly convexif the above inequality is strict\nfor allw\u0338= v.\nThe above characterization can easily be generalized to multiple\npoints. Namely, for w1,..., wn \u2208 Wand \u03c41,...\u03c4 n \u2265 0 such that\u2211n\ni=1 \u03c4i = 1 (that is,\u03c41,...,\u03c4 n defines a probability distribution over\n[n]), we have ifL is convex that\nL\n( n\u2211\ni=1\n\u03c4iwi\n)\n\u2264\nn\u2211\ni=1\n\u03c4iL(wi).\nThe point\u2211n\ni=1 \u03c4iwi is called a convex combination. This can be seen\nas comparing the function at the average point to the average of the\nvalues at theses points and can further be generalized to any random\nvariable.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1368, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ed6e6547-185c-4685-9abc-2765f6ab3a83": {"__data__": {"id_": "ed6e6547-185c-4685-9abc-2765f6ab3a83", "embedding": null, "metadata": {"page_label": "409", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e8587ffe-8f61-4299-9015-9abb78d599e1", "node_type": "4", "metadata": {"page_label": "409", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "20832f5b5f02a9667bc1b5752f8a9ee43cff9585929481867bedbddd1cfe1d3d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "15.4. Classes of functions 409\nProposition 15.3(Jensen\u2019s inequality). A function L: W \u2192R is\nconvex if it satisfiesJensen\u2019s inequality, that is, for any random\nvariable W on W,\nL(E[W]) \u2264E[L(W)],\nprovided that the expectations are well-defined.\nIf the function considered is differentiable, an alternative charac-\nterization of convexity is to observe how linear approximations of the\nfunction lower bound the function. This is illustrated in Figure 15.2\nand formalized below.\nDefinition 15.6(Convex differentiable functions). Adifferentiablefunc-\ntion L: W\u2192 R is convex if and only if for allw,v\u2208W\nL(v) \u2265L(w) + \u27e8\u2207L(w),v\u2212w\u27e9.\nThe functionLis strictly convex if and only if the above inequality\nis strict for anyw\u0338= v.\nThe above characterization pinpoints the relevance of convex func-\ntion in optimization: if we can find a point\u02c6wwith null gradient, then\nwe know that we have found the minimum as we have\n\u2207L( \u02c6w) = 0 =\u21d2 \u2200v\u2208RP, L(v) \u2265L( \u02c6w) =\u21d2 L( \u02c6w) = L\u22c6.\nThis means that by having access to the gradient of the function or an\napproximation thereof, we have access to a sufficient criterion to know\nwhether we found a global minimum. In the case of a gradient descent\non a smooth function, convexity ensures convergence to a minimum at\na sublinear rate as detailed below.\nFinally, if the function is twice differentiable, convexity of a function\ncan be characterized in terms of the Hessian of the function.\nProposition 15.4(Convex twice differentiable functions). Atwicedif-\nferentiable functionL: W\u2192 R is convex if and only if its Hessian\nis positive semi-definite,\n\u2200w\u2208W, \u22072L(w) \u2ab00, i.e., \u2200w,v\u2208W, \u27e8v,\u22072L(w)v\u27e9\u2265 0.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1612, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "65175249-a0fb-4a8f-aa75-5ae0fffb71a3": {"__data__": {"id_": "65175249-a0fb-4a8f-aa75-5ae0fffb71a3", "embedding": null, "metadata": {"page_label": "410", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa8379e5-5515-4c22-b2f7-9077fe880f19", "node_type": "4", "metadata": {"page_label": "410", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "579b9caecae1eab39656061d875f09291d1aa3967315da91413ccedee6835aa9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "410 Optimization basics\nThe functionLis strictly convex if and only if the Hessian is positive-\ndefinite, \u2200w\u2208W, \u22072L(w) \u227b0, i.e.,\u2200w,v\u2208W, \u27e8v,\u22072L(w)v\u27e9>\n0.\n15.4.4 Strongly-convex functions\nConvexity can also be strengthened by considering\u00b5-strongly convex\nfunctions.\nDefinition 15.7(Strongly-convex functions). A functionL : W\u2192\nR is \u00b5-strongly convexfor \u00b5> 0 if for allw,v\u2208W and \u03c4 \u2208[0,1]\nL(\u03c4w+ (1 \u2212\u03c4)v) \u2264\u03c4L(w) + (1\u2212\u03c4)L(v) \u2212\u00b5\n2 \u03c4(1 \u2212\u03c4)\u2225w\u2212v\u22252\n2.\nA differentiable functionL is \u00b5-strongly convex if and only if for\nall w,v\u2208W\nL(v) \u2265L(w) + \u27e8\u2207L(w),w\u2212v\u27e9+ \u00b5\n2 \u2225w\u2212v\u22252\n2.\nA twice differentiable function is\u00b5-strongly convex if and only if\nits Hessian satisfies\n\u2200w\u2208W, \u22072L(w) \u2ab0\u00b5I, i.e., \u2200w,v\u2208W, \u27e8v,\u22072L(w)v\u27e9\u2265 \u00b5\u2225v\u22252\n2.\nThe characterization of strong convexity for differentiable functions\nstates that L(w) is lower-bounded by a quadratic. This enables the\ndesign of linearly convergent algorithms as explained later. We naturally\nhave the implications\nL strongly convex =\u21d2 L strictly convex =\u21d2 L convex.\nWith respect to arbitrary norms\nA function can be strongly convex w.r.t. an arbitrary norm, simply\nby replacing the2-norm in Definition 15.7 with that norm. For differ-\nentiable strongly convex functions, we have the following alternative\ncharacterization, generalizing Definition 15.7 to arbitrary norms.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1290, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "303bf54d-4eef-49b4-87d6-490ab4fe9b29": {"__data__": {"id_": "303bf54d-4eef-49b4-87d6-490ab4fe9b29", "embedding": null, "metadata": {"page_label": "411", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c16255e-c467-436a-8722-1f3772903cbf", "node_type": "4", "metadata": {"page_label": "411", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "838b5dd8a89d66f866c27250d978cd5d457600ad43f2f22078deb4987913efb0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "15.4. Classes of functions 411\nProposition 15.5(Differentiable strongly-convex functions). Ifadif-\nferentiable functionL: W\u2192 R is \u00b5-strongly convex w.r.t. a norm\n\u2225\u00b7\u2225 over a setC, then for allw,v\u2208C\n\u00b5\n2 \u2225w\u2212v\u22252 \u2264L(w) \u2212L(v) \u2212\u27e8\u2207L(v),w\u2212v\u27e9\ued19 \ued18\ued17 \ued1a\nBL(w,v)\n.\nObviously, if a functionL is \u00b5-strongly convex, then,\u03bbL is (\u00b5\u03bb)-\nstrongly convex. Because all norms are equivalent, if a function is\nstrongly convex w.r.t. a norm, it is also strongly-convex w.r.t. another\nnorm. However, stating the norm w.r.t. which strong convexity holds can\nlead to better constant\u00b5(the higher, the better in terms of convergence\nrates of, e.g., a gradient descent). We also emphasize that it is important\nto mention over which set strong convexity holds. We give examples\nbelow.\nExample 15.1(Strongly convex functions). The function f(u) =\n1\n2 \u2225u\u22252\n2 is 1-strongly convex w.r.t.\u2225\u00b7\u22252 over RM.\nThe functionf(u) = \u27e8u,log u\u27e9is 1-strongly convex w.r.t.\u2225\u00b7\u22251\nover \u25b3M. Applying Proposition 15.5, we obtain for allp,q\u2208\u25b3M\n1\n2\u2225p\u2212q\u22252\n1 \u2264Bf(p,q) = KL(p,q),\nwhich is known asPinsker\u2019s inequality. We empirically verify\nthe inequality in Fig. 15.1.\nMore generally,f(u) is 1\n\u00b5-strongly convex w.r.t.\u2225\u00b7\u22251 over any\nbounded setC\u2282 RM\n+ such that\u00b5= supu\u2208C\u2225u\u22251 (Blondel, 2019).\nHowever, it is not strongly convex overRM\n+ , as it is not bounded.\n15.4.5 Nonconvex functions\nIn general, the minimum of a function necessarily has a null gradient,\nthat is,\nw\u22c6 \u2208arg min\nw\u2208W\nL(w) =\u21d2 \u2207L(w\u22c6) = 0.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1436, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7b8b244e-d99a-46d5-b853-8f09b495380d": {"__data__": {"id_": "7b8b244e-d99a-46d5-b853-8f09b495380d", "embedding": null, "metadata": {"page_label": "412", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3e0ccae5-53b5-47f8-b89e-458c985215a9", "node_type": "4", "metadata": {"page_label": "412", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "de29430e7417f7aff1d31f96453fc59d5dc53955a5477de33e86253bf42d8e7a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "412 Optimization basics\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nKL(p, q)\n0.5||p q||2\n1\nFigure 15.1:Graphical verification of Pinsker\u2019s inequality,1\n2 \u2225p\u2212q\u22252\n1 \u2264KL(p,q),\nwith p:= (\u03c0,1 \u2212\u03c0) and q:= (0.3,0.7).\nTo see this, consider the function F : t \u2192 L(w\u22c6 \u2212t\u2207L(w\u2217)). If\n\u2207L(w\u22c6) \u0338= 0, thenF\u2032(0) = \u2212\u2225\u2207L(w\u22c6)\u22252\n2 \u0338= 0. Therefore, there exists\na smallt> 0 such thatF(t) <F (0), i.e.,L(w\u22c6) is not the minimum.\nHowever, if the function is not convex, the converse is a priori not true:\nfinding a point that has a null gradient does not ensure that we have\nfound a global minimum as illustrated in Figure 15.3.\nFor non-convex functions, a point with null gradient is called a\nstationary point. A stationary point may define alocal maximum\nor alocal minimum. Formally, \u02c6wis a local minimum if\n\u2203r> 0, s.t. \u2200v\u2208W satisfying \u2225v\u2212\u02c6w\u2225\u2264 r, we haveL(v) \u2265L( \u02c6w).\nA local maximum is defined similarly, except thatL(v) \u2264L( \u02c6w) in a\nneighborhood of \u02c6w. For non-convex functions, convergence rates are\ntherefore generally expressed in terms of convergence of the norm of the\ngradient \u2225\u2207f(wt)\u22252 towards 0. Such theoretical results do not ensure\nconvergence to the global minimum but rather convergence to a point\nwhere no further progress may a priori be possible with just gradient\ninformation.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1280, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "06fb0e3e-ed03-43b2-9b68-10ad687cdcd7": {"__data__": {"id_": "06fb0e3e-ed03-43b2-9b68-10ad687cdcd7", "embedding": null, "metadata": {"page_label": "413", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "61a18081-2ac9-4ed5-8015-f0616ca0e6c0", "node_type": "4", "metadata": {"page_label": "413", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "f83f330294e2f2315aa192bf74b3f1fc0fe51ecd016c2d00b12f7909c11a2303", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "15.5. Performance guarantees 413\nFigure 15.2: Convex function: any secant is\nabove the function, any tangent is below the func-\ntion, a point with zero gradient is a minimum.\nGlobal minimumLocal minimum\nFigure 15.3:Non-convex func-\ntion: a point with zero gradient\nis not necessarily the global min-\nimum.\n15.5 Performance guarantees\nFor a given class of functions, we can define the performance of an\nalgorithm as the number of iterations the algorithm would need to find\nan\u03b5-accuratesolutionasinEq.(15.1).Thisiscalledthe computational\ncomplexity of the algorithm, denoted\nt= T(\u03b5).\nAlternatively, the performance of an algorithm can be stated in terms\nof convergence rate, i.e., the accuracy that the algorithm reaches\nafter t iterations,\n\u03b5= R(t),\nwhere R is a decreasing positive function vanishing ast\u2192+\u221e. Usu-\nally, R incorporates properties of the function minimized, such as its\nsmoothness constant\u03b2 and information on the initial point, such as its\nfunction value. The corresponding computational complexityT(\u03b5) is\nthen given as the minimum number of iterationst such thatR(t) \u2264\u03b5,\nT(\u03b5) = min{t\u2208N: R(t) \u2264\u03b5}.\nConvergence rates can generally be classified by considering the\nprogress ratio on iterationt, defined by\n\u03c1t := R(t)\nR(t\u22121).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1238, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5f8e2533-990e-4e66-b17b-37654cbdcab3": {"__data__": {"id_": "5f8e2533-990e-4e66-b17b-37654cbdcab3", "embedding": null, "metadata": {"page_label": "414", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2975c766-d4de-46f4-8217-9ec77d5e5694", "node_type": "4", "metadata": {"page_label": "414", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "83f439b3f7736246e436e6036dcb27af8013f394ebc77ce3941385df914b329d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "414 Optimization basics\nThe asymptotic convergence rate is then defined by\n\u03c1\u221e:= lim\nt\u2192+\u221e\n\u03c1t.\nWe can classify the rates as follows.\n1. Sublinear convergence rates,\u03c1\u221e= 1:the longer the algorithm\nruns, the slower it makes progress. That is, the relative progress\neventually tends to stall ast \u2192+\u221e. Examples ofR(t) in this\ncategory includeO(1/t),O(1/t2) or more generallyO(1/t\u03b1) for\nsome \u03b1> 0. This is equivalent toT(\u03b5) = O(\u03b5\u22121/\u03b1).\n2. Linear convergence rates, \u03c1\u221e = c \u2208(0,1): the algorithm\neventually reaches a state of constant relative progress at each\niteration, leading to an overall rateR(t) = O(exp(\u2212ct)) for c\ndepending on the properties of the objective. This corresponds to\nT(\u03b5) = O(c\u22121 ln \u03b5\u22121).\n3. Superlinear convergence rates,\u03c1\u221e= 0: the relative progress\nis better at each new iteration. This can happen for, e.g.,R(t) =\nO(exp(\u2212t2)), leading toT(\u03b5) = O(\n\u221a\nln \u03b5\u22121) or\nR(t) = O(exp(\u2212exp(t))), also called a quadratic rate, leading to\nT(\u03b5) = O(ln ln\u03b5\u22121).\nThis is illustrated in Fig. 15.4.\nNote that the term \u201clinear\u201d may be misleading as the rates are in\nfact exponential. They are called \u201clinear\u201d because of their behavior in\nlog scale.\nUpper and lower bounds\nThe best performance of a class of algorithms equipped with a given\noracle (e.g. first-order oracle) can be upper-bounded or lower-bounded.\nThis allows to show that an algorithm with access limited to a certain\ntype of oracle cannot theoretically do better than a certain number. For\nexample, the computational complexity to minimize\u03b2-smooth functions\nrestricted on [0,1]P with first-order oracles is lower bounded byc\n\u03b5P\n(Nemirovski and Yudin, 1983, p. 1.1.7). For example, withP = 10\nand \u03b5 = 10\u22123, this gives1030 iterations. Note that these results are\npessimistic by construction. The actual performance of an algorithm\non a specific instance of this function class may be much better than", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1858, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dc3ce791-7aa6-4a24-b3c4-6b4a48de93a3": {"__data__": {"id_": "dc3ce791-7aa6-4a24-b3c4-6b4a48de93a3", "embedding": null, "metadata": {"page_label": "415", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "16cfd766-7ce8-485b-bce6-4f485d6ba437", "node_type": "4", "metadata": {"page_label": "415", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "4261d55ff0f2efffe95e4c77418ea5538f284d970cce817f92f3db613251266a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "15.5. Performance guarantees 415\n0 20 40 60 80 100\nIteration t\n10 20\n10 17\n10 14\n10 11\n10 8\n10 5\n10 2\nConvergence rate R(t) (log scale)\nR(t) = 1/ t (sublinear)\nR(t) = 1/t (sublinear)\nR(t) = 1/t2 (sublinear)\nR(t) = e t (linear)\nR(t) = e t2\n (superlinear)\n0 20 40 60 80 100\nIteration t\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProgress ratio t = R(t)\nR(t 1)\nFigure 15.4: Left:convergence rates.Right: progress ratios. An algorithm with\nsublinearconvergencerateseventuallyeventuallystopsmakingprogress.Analgorithm\nwith linear convergence rate eventually reaches a state of constant progress. An\nalgorithm with superlinear convergence rate makes faster progress after each iteration.\nthis worst-case scenario, as it is the case with popular algorithms such\nas quasi-Newton methods. Better computational complexities can be\nachieved by further restricting the class of functions to the set of convex\nfunctions, which play a central role in optimization and many other\nfields.\nZero-order vs. first-order\nFor the class of smooth strongly convex functions, the computational\ncomplexity of the best first-order algorithm is (up to constant and\nlogarithmic factors)P times better than that of the best zero-order al-\ngorithm (Nesterov, 2018; Nesterov and Spokoiny, 2017). This theoretical\ncomparison shows that, while zero-order optimization algorithms may\nperform on par with first-order optimization algorithms for problems\nwith a low dimensionP, they can be much slower for high dimensional\nproblems, i.e.,P \u226b1.\nIn different settings, for example with stochastic oracles (Duchi\net al., 2015) or for different classes of functions, slightly different com-\nparisons may be achieved, such as a\n\u221a\nP factor instead ofP. However,\nthe same conclusion holds in the current frameworks considered: first-\norder optimization algorithms can provide fast rates that are dimension\nindependent while the rates of zero-order optimization algorithms gen-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1913, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fca15130-a494-4d2e-a92f-a7972eece9f8": {"__data__": {"id_": "fca15130-a494-4d2e-a92f-a7972eece9f8", "embedding": null, "metadata": {"page_label": "416", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c0f2746e-34f6-47ab-bf69-8c1160dd163e", "node_type": "4", "metadata": {"page_label": "416", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "6c1b6fffea3a994dfdaa67ff4d6f00bed649a40ec0748fc289f629892d0ec673", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "416 Optimization basics\nerally depend on the dimension of the problem, making them unfit for\nhigh-dimensional problems.\nThis explains the immense success of first-order algorithms for train-\ning neural networks. Fortunately, using reverse-mode autodiff, as studied\nin Chapter 8, it can be shown that computing a gradient has roughly\nthe same complexity as evaluating the function itself Section 8.3.3\n15.6 Summary\n\u2022 The information available to us on a function can be formalized\nby the notion oforacle. Zero-order oracles can only evaluate the\nfunction; first-order oracles can also compute the gradient; second-\norder oracles can also compute the Hessian or the Hessian-vector\nproduct (HVP).\n\u2022 Most optimization algorithms reviewed in this book can be viewed\nfrom avariational perspective, in which the next iteration is\nproduced by optimizing a trade-off between an approximation of\nthe function and a proximity term. Different approximations and\ndifferent proximity terms lead to different algorithms.\n\u2022 We also reviewed different classes of functions, and performance\nguarantees.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1084, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3fc877f8-e3d2-4030-9ccb-4a6904c18040": {"__data__": {"id_": "3fc877f8-e3d2-4030-9ccb-4a6904c18040", "embedding": null, "metadata": {"page_label": "417", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "91943f55-eb5a-4882-9722-e58d971b0b11", "node_type": "4", "metadata": {"page_label": "417", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "66b3855aa58045f8bcc57874adce4ea07dd6afc3019e41360603984574022ced", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "16\nFirst-order optimization\n16.1 Gradient descent\nGradient descent is one of the simplest algorithms in our toolbox to\nminimize a function. At each iteration, it moves along the negative\ngradient direction, scaled by a stepsize\u03b3:\nwt+1 = wt \u2212\u03b3\u2207L(wt). (16.1)\nThe path taken by a gradient descent on a simple quadratic is illustrated\nin Fig. 16.1 for different choices of the stepsize.\n16.1.1 Variational perspective\nConsider the linear approximation ofL(w) around wt,\nL(w) \u2248L(wt) + \u27e8\u2207L(wt),w\u2212wt\u27e9.\nOne can easily check that the gradient descent update in Eq. (16.1) can\nbe rewritten as the solution of a minimization problem, namely,\nwt+1 = arg min\nw\u2208W\nL(wt) + \u27e8\u2207L(wt),w\u2212wt\u27e9+ 1\n2\u03b3\u2225w\u2212wt\u22252\n2. (16.2)\nInwords,agradientdescentupdateoptimizesatrade-offbetweenstaying\nclose to the currentwt, thanks to the proximity term1\n2\u03b3\u2225w\u2212wt\u22252\n2, and\n417", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 833, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "77084eb2-36c0-4697-abaa-876f02972645": {"__data__": {"id_": "77084eb2-36c0-4697-abaa-876f02972645", "embedding": null, "metadata": {"page_label": "418", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7ac8c78d-33c4-4ff3-aa6c-cc529a712235", "node_type": "4", "metadata": {"page_label": "418", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "eba09e0ce370a7ea021a4b6de2af37469d27772497595161219ca2651a365bfb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "418 First-order optimization\n1\n 0 1\nw1\n0.5\n0.0\n0.5\nw2\nw0\nw1\nw2\nw *\nGradient descent\n Stepsize 0.5\n1\n 0 1\nw1\n0.5\n0.0\n0.5\nw2\nw0\nw1\nw2\nw *\nGradient descent\n Stepsize 1.8\nFigure 16.1:Trajectory taken by gradient descent on the objectivef(w) := 0.05w2\n1 +\n0.5w2\n2 with a small (left) or large (right) stepsize. In each case, the iterates follow the\nnormal vectors to the contour lines (dashed lines): the negative gradients. A small\nstepsize leads to slow convergence but a larger stepsize induces oscillations.\nminimizing the linearization ofL around wt. Intuitively, by choosing\u03b3\nsufficiently small, we ensure that the minimizer of the regularized linear\napproximation stays in a neighborhood where the linear approximation\nis valid. This viewpoint is useful to motivate gradient descent extensions.\n16.1.2 Convergence for smooth functions\nAs long as\u2207L(wt) \u0338= 0, the functionLt(\u03b3) := L(wt \u2212\u03b3\u2207L(wt)) has\na negative derivative at 0, i.e.,L\u2032\nt(0) = \u2212\u2225\u2207L(wt)\u22252\n2. Hence, as long\nas \u2207L(wt) \u0338= 0, there exists a stepsize ensuring a decrease in objective\nvalues at each iterate. However, without further assumptions, such a\nstepsize may depend on each iterate and may be infinitesimally small.\nTo quantify the convergence of gradient descent with a constant stepsize,\nwe restrict to the class of smooth functions. By applying Proposition 15.1\non the iterate of gradient descent, we obtain that\nL(wt+1) \u2264L(wt) \u2212\u03b3\u2225\u2207L(wt)\u22252\n2 + \u03b2\u03b32\n2 \u2225\u2207L(wt)\u22252\n2.\nTherefore, for\u03b2-smooth functions, by selecting\u03b3 \u22641\n\u03b2, we get that\nL(wt+1) \u2212L(wt) \u2264\u2212\u03b3\n2 \u2225\u2207L(wt)\u22252\n2,\nwhich illustrates the main mechanism behind gradient descent: each\niteration decreases the objective by a constant times the norm of the\ngradient of the current iterate. This equation can further be summed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1739, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "283a87a3-04d4-4f59-aae6-6561e0d68448": {"__data__": {"id_": "283a87a3-04d4-4f59-aae6-6561e0d68448", "embedding": null, "metadata": {"page_label": "419", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37ecb6f0-0c17-46aa-ad50-59d15b0d5925", "node_type": "4", "metadata": {"page_label": "419", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "a28e14872437e369a1fcb56ce4e71aff2bbb34d9e3be0e5d1b89dce282c35803", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "16.1. Gradient descent 419\nover all iterates up toT. This telescopes the objective values, leading to\nmin\nt\u2208{0,...,T\u22121}\n\u2225\u2207L(wt)\u22252\n2 \u22641\nT\nT\u22121\u2211\nt=0\n\u2225\u2207L(wt)\u22252\n2\n\u2264 2\n\u03b3T\n(\nL(w0) \u2212L(wT)\n)\n\u2264 2\n\u03b3T\n(\nL(w0) \u2212L\u22c6\n)\n,\nwhere we recall thatL\u22c6 is the infimum ofL. Therefore, after sufficiently\nmany iterations, gradient descent finds a point whose gradient norm is\narbitrarily small.\nNon-convex case\nWithout further assumptions, i.e., in the non-convex case, the above\nresult (i.e., convergence to a stationary point, measured by the gradient\nnorm) is the best we may get in theory. DenotingTs(\u03b5) the number\nof iterations needed for a gradient descent to output a point that is\n\u03b5-stationary, i.e.,\u2225\u2207L( \u02c6w)\u22252 \u2264\u03b5, we haveTs(\u03b5) \u2264O(\u03b5\u22122).\nConvex case\nBy adding a convexity assumption on the objective, we can use the lower\nbound provided by the convexity assumption to ensure convergence to\na minimum. Namely, for a\u03b2-smooth and convex functionf, and with\nstepsize \u03b3 \u22641/\u03b2, we have that (Nesterov, 2018)\nL(wT) \u2212L\u22c6 \u2264 1\n\u03b3T\u2225w0 \u2212w\u22c6\u22252\n2.\nThat is, we get a sublinear convergence rate, and the associated compu-\ntational complexity to find a minimum isT(\u03b5) = O(1/\u03b5).\nStrongly convex case\nIf we further strengthen the assumptions by considering\u03b2-smooth, \u00b5-\nstrongly convex functions, the convergence rate of a gradient descent", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1295, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e6e74a44-6074-44ff-8e3b-eb1107a1a44c": {"__data__": {"id_": "e6e74a44-6074-44ff-8e3b-eb1107a1a44c", "embedding": null, "metadata": {"page_label": "420", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6116c9-2cb9-4eb0-be46-952399e626f6", "node_type": "4", "metadata": {"page_label": "420", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "e5677dfd2ac60e3b17f04ebdefb4a24e818061cb560e1e222016e3b294dd9d5f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "420 First-order optimization\ncan be shown to be (Nesterov, 2018), for any stepsize\u03b3 \u22641/\u03b2,\nL(wT) \u2212L\u22c6 \u2264(1 \u2212\u03b3\u00b5)T (\nL(w0) \u2212L\u22c6\n)\n\u2264exp (\u2212\u03b3\u00b5T)\n(\nL(w0) \u2212L\u22c6\n)\n.\nThat is, we obtain a linear convergence rate and the associated computa-\ntional complexity isT(\u03b5) = O(ln \u03b5\u22121). The above convergence rates may\nbe further refined (Nesterov, 2018); we focused above on the simplest\nresult for clarity.\nStrong convexity can also be replaced by a weaker assumption,\ngradient-dominating property (Polyak, 1963), i.e.,\u2225\u2207L(v)\u22252\n2 \u2265c(L(v)\u2212\nL\u22c6) for some constantcand anyv\u2208W. A convex, gradient-dominating\nfunction can also be minimized at a linear rate.\n16.1.3 Momentum and accelerated variants\nWe started with gradient descent as a simple example of first-order\noptimization algorithm. However, different optimization algorithms can\nbe designed from the access to first-order oracles and the knowledge\nof the class of functions considered. For example, consider quadratic\nconvex functionsw \u21a6\u21921\n2 w\u22a4Aw+ b\u22a4w, that are a basic example of\nsmooth strongly convex functions ifAis positive definite. An optimal\nmethod in this case is the heavy-ball method of Polyak (1964), that can\nbe written as\nvt+1 := \u03bdvt \u2212\u03b3\u2207L(wt)\nwt+1 := wt + vt+1.\nThe heavy-ball method uses an additional variablevt, that can be\ninterpreted as the velocity of a ball driven by the negative gradient\nto converge towards a minimum. Intuitively, this additional velocity\ncircumvents the oscillations that a gradient descent may present as\nillustrated in Fig. 16.2 compared to Fig. 16.1. For\u03bd = 0, we recover\nusual gradient descent. For\u03bd >0, the velocities accumulate a form\nof an inertia momentum, where\u03bd is interpreted as the \u201cmass\u201d of the\nball. In terms of convergence rates, the heavy-ball method can be\nshown to converge linearly similarly to gradient descent, but with a rate", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1820, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "36f0fed9-7f45-4d15-bdd1-3c82ac2e292d": {"__data__": {"id_": "36f0fed9-7f45-4d15-bdd1-3c82ac2e292d", "embedding": null, "metadata": {"page_label": "421", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0195fa02-c772-48e5-b18f-ffa0963851da", "node_type": "4", "metadata": {"page_label": "421", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "87c103c61eff1cd58079b69ec30591ac8bb0b65734d4b1bb2bf758294a0913a6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "16.2. Stochastic gradient descent 421\n1\n 0 1\nw1\n0.5\n0.0\n0.5\nw2\nw0\nw1\nw2\nw *\nGradient descent with momentum\n Stepsize 1.8 Momentum 0.2\nFigure 16.2:Trajectory taken by gradient descent with momentum. Compared to\ngradient descent without momentum, for the same stepsize, the oscillations previously\nobserved in Fig. 16.1 are no longer present, and the algorithm converges faster to\nthe minimum.\nO(exp(\u2212T\n\u221a\n\u00b5/\u03b2)) for appropriate choices of\u03bd,\u03b3. In comparison, by\nchoosing an optimal stepsize for the gradient descent, its convergence\nrate is O(exp(\u2212T\u00b5/\u03b2)) which is provably worse, as we always have\n\u00b5/\u03b2 \u22641.\nBeyond the case of quadratic functions, accelerated variants of\ngradient descent for convex or strongly convex functions have been\ndeveloped by Nesterov (2018). Such variants have inspired the design\nof optimization algorithms in stochastic settings presented below.\n16.2 Stochastic gradient descent\nIn machine learning, we are usually interested in minimizing theex-\npected lossof the model over the data distribution\u03c1:\nmin\nw\u2208W\nL(w) := ES\u223c\u03c1[L(w; S)] .\nFor example,L is often set toL(w; S) := \u2113(Y,f (X,w)), where\u2113 is a\nloss function,f is a neural network andS = (X,Y ) is a random pair,\ncomposed of an inputX and an associated targetY, sampled from\u03c1.\nIn this setting, since the data distribution\u03c1 is generally unknown and\nmay be infinite, we cannot exactly evaluate the expected lossL(w) or\nits gradient\u2207L(w).\nIn practice, we are often given a fixed dataset ofnpairs si = (xi,yi).\nThis is a special case of the expected loss setting, since this can be seen", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1558, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a598cd99-fb4e-45f8-81fb-02b83e3821f2": {"__data__": {"id_": "a598cd99-fb4e-45f8-81fb-02b83e3821f2", "embedding": null, "metadata": {"page_label": "422", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4aef79b3-9b44-4fbf-93a2-49760af82b5a", "node_type": "4", "metadata": {"page_label": "422", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "c0c66d0b175d8063fd1e55700ccac7b7612a1875f2baa9da0c0bb1ccbac04429", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "422 First-order optimization\nas a empirical distribution\u03c1= \u03c1n\nL(w) = ES\u223c\u03c1n [L(w; S)] = 1\nn\nn\u2211\ni=1\nL(w; (Xi,Yi)).\nThe gradient ofL(w) is then\n\u2207L(w) := 1\nn\nn\u2211\ni=1\n\u2207L(w; (xi,yi)).\nIn this case, we see that thefull gradient \u2207L(w), as needed by gradient\ndescent, is the average of theindividual gradients. That is, the cost of\ncomputing\u2207L(w) isproportionaltothenumberoftrainingpoints n.For\nnvery large, that is a very large amount of samples, this computational\ncost can be prohibitive. Stochastic gradients circumvent this issue.\n16.2.1 Stochastic gradients\nUsually, even if we do not know\u03c1, we can sample from it, i.e., we have\naccess to samplesS \u223c\u03c1. We can then use astochastic gradientof\nthe form\u2207L(w; S) as a random estimate of\u2207L(w). This may look like\na rough estimate but, on average, this is a valid approximation since\nES\u223c\u03c1[\u2207L(w; S)] = \u2207L(w).\nWe say that\u2207L(w; S) is anunbiased estimatorof \u2207L(w). To fur-\nther improve the approximation, we may also considermini-batch\nestimates by samplingm \u226an data points Si := ( Xi,Yi) and using\n1\nm\n\u2211m\ni=1 \u2207L(w; Si), whose expectation still matches\u2207L(w), while po-\ntentially reducing the approximation error by averaging multiple stochas-\ntic gradients. Computationally, the main advantage is that the cost is\nnow proportional tom instead ofn.\nIn whole generality, one can consider stochastic first-order oracles\ndefined below.\nDefinition 16.1(Stochastic first-order oracles). Astochasticfirst-\norder oracleof an expected objectiveL(w) is a random estimate\ng(w; S) of \u2207L(w) with S sampled according to some distribution", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1558, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a4eb0b79-f647-4bea-a572-b28e53b85800": {"__data__": {"id_": "a4eb0b79-f647-4bea-a572-b28e53b85800", "embedding": null, "metadata": {"page_label": "423", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "88c043bb-73e7-43f4-882b-254312d41907", "node_type": "4", "metadata": {"page_label": "423", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "f7002a97dd759d1b4baa62fbbf971e4138b0b4763556880b3a094537cb9d3f6e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "16.2. Stochastic gradient descent 423\nq. A stochastic gradient is said to be anunbiased estimatorif\nES\u223cq[g(w; S)] = \u2207L(w).\nThe varianceof a stochastic gradient is\nES\u223cq\n[\n\u2225g(w; S) \u2212\u2207L(w)\u22252\n2\n]\n.\nWhen q= \u03c1, we recover stochastic gradients. Whenq is the product\nof mindependent samples according top, we recover mini-batch stochas-\ntic gradients. First-order stochastic optimization algorithms build upon\nstochastic first-order oracles to approximately find the minimum of the\nexpected objective. In such a setting, the iterates of the algorithm are\nby definition random. Convergence rates therefore need to be expressed\nin probabilistic terms by considering for example the expected objective\nvalue according to the randomness of the oracles.\n16.2.2 Vanilla SGD\nEquipped with a stochastic first-order oracle, such as (mini-batch)\nstochastic gradients, we can definestochastic gradient descentas\nwt+1 = wt \u2212\u03b3g(wt; St) where St \u223cq.\nWe assume thatSt is independent ofwt. Compared to the usual gradient\ndescent, the main impediment of the stochastic setting is the additional\nnoise induced by the stochastic estimates: their variance.\nFor example, consider applying a stochastic gradient descent on the\nexpectation of\u03b2-smooth convex functionsL(w; s) with unbiased oracles.\nTo harness the randomness of the iterates, consider afterT iterations\noutputting the average of the firstT iterates, that is \u00afwT := 1\nT\n\u2211T\nt=1 wt.\nMoreover, suppose that the variance of the stochastic first-order oracles\nis bounded by\u03c32 for all minimizersw\u22c6 of L. Denoting byES0,...,ST\u22121\nthe randomness associated to the stochastic oracles, we have then that\nfor a stepsize\u03b3 \u22641/(4\u03b2), (Lan, 2012),\nES0,...,ST\u22121 [L( \u00afwT)] \u2212L\u22c6 \u2264 1\n\u03b3T\u2225w0 \u2212w\u22c6\u22252\n2 + 2\u03b3\u03c32.\nThe resulting convergence rate illustrates that a stochastic gradient\ndescent converges to the minimum of the expected objective up to a", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1853, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d68c6ae0-0009-4c6f-82ba-a24ec545d851": {"__data__": {"id_": "d68c6ae0-0009-4c6f-82ba-a24ec545d851", "embedding": null, "metadata": {"page_label": "424", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ae002d4b-d816-41c4-aa97-a67e92dd8b95", "node_type": "4", "metadata": {"page_label": "424", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "b000481315edae965f8c4fbf96f21a8fc794c1798ff080635e0a6b20118b3c08", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "424 First-order optimization\nconstant term depending on the variance of the oracle and the stepsize.\nOne can diminish the variance by considering mini-batches: if the\nvariance of a single stochastic gradient is\u03c32\n1, considering a mini-batch\nof m gradients reduces the variance of the corresponding oracle to\n\u03c3m = \u03c32\n1/m. To decrease the additional term, one may also decrease\nthe stepsizes over the iterations. For example, by choosing a decreasing\nstepsize like \u03b3t = t\u22121/2, the convergence rate is then of the order\nO((\u2225w0 \u2212w\u22c6\u22252\n2 + \u03c32 ln t)/\n\u221a\n(t)). The stepsize can also be selected as\na constant \u03b30 that decreases the average objective for the first T0\niterations and reduced by a multiplicative factor at regular intervals\nlike \u03b3j = \u03c1\u03b3j\u22121 for \u03c1 \u2208(0,1) to handle iterations betweenTj,Tj+1.\nAlternative stepsize schedules such as a cosine decay (Loshchilov and\nHutter, 2016) have recently become popular.\nThe literature on alternative optimization schemes for stochastic\noptimization is still rapidly evolving, with new heuristics regularly\nproposed. We present below two popular techniques.\n16.2.3 Momentum variants\nAccelerated optimization algorithms developed in the deterministic\nsetting may be extended to the stochastic setting. For example, the\nheavy-ball method can be adapted to the stochastic setting, leading to\nstochastic gradient descent withmomentum (Sutskever et al., 2013)\ngenerally implemented as\nvt+1 := \u03bdvt + g(wt; St)\nwt+1 := wt \u2212\u03b3vt+1.\nAs mentioned earlier the momentum method can be modified to han-\ndle non-quadratic smooth strongly convex functions. This leads to\nNesterov\u2019s accelerated method in the deterministic setting. This has\nbeen adapted to the stochastic with a so-calledNesterov momen-\ntum (Sutskever et al., 2013)\nvt+1 := \u03bdvt + g(wt + \u03bdvt; St)\nwt+1 := wt \u2212\u03b3vt+1.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1800, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5a8f5b51-340e-4be4-9dbe-adb1c2bfb080": {"__data__": {"id_": "5a8f5b51-340e-4be4-9dbe-adb1c2bfb080", "embedding": null, "metadata": {"page_label": "425", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0706fc31-0438-4e89-84ee-9540c254dc80", "node_type": "4", "metadata": {"page_label": "425", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "3e4cddfdb035e116002eb37b49e2795e17d2efc5285e1141595363053fac1fb5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "16.3. Projected gradient descent 425\n16.2.4 Adaptive variants\nIn any gradient descent-like algorithm, selecting the stepsize is key for\ngood performance. While a constant stepsize may be used if the function\nis smooth, we may not know in advance the smoothness constant of the\nobjective, which means that additional procedures may be required to\nselect appropriately the stepsize. In the deterministic case, line-searches\nsuch as the Armijo or Wolfe\u2019s rules (Wright and Nocedal, 1999) can be\nused to check whether the selected stepsize decreases sufficiently the\nobjective at each iteration. Such rules have be adapted in the stochastic\nsetting (Vaswaniet al., 2019).\nAnother way to decrease the sensitivity of the algorithm with respect\nto the stepsize has been to estimate first and second-order moments\nof the gradients and use the latter as a form of preconditioning to\nsmooth the trajectory of the iterates. This led to the popularAdam\noptimizer (Kingma and Ba, 2014). It takes the form,\nmt+1 := \u03bd1mt + (1 \u2212\u03bd1)gt\nvt+1 := \u03bd2vt + (1 \u2212\u03bd2)(gt)2\n\u02c6mt+1 := mt+1/(1 \u2212\u03bdt\n1)\n\u02c6vt+1 := vt+1/(1 \u2212\u03bdt\n2)\nwt+1 := wt \u2212\u03b3 \u02c6mt+1/\n(\u221a\n\u02c6vt+1 + \u03b5\n)\n,\nwhere gt := g(wt; St), (gt)2 denotes the element-wise square ofgt and\n\u03bd1,\u03bd2,\u03b3,\u03b5 are hyper-parameters of the algorithm. Numerous variants\nexist, such as varying the stepsize\u03b3 above along the iterations.\n16.3 Projected gradient descent\nOftentimes, we seek to find the solution of a minimization problem\nsubject toconstraints on the variables, of the form\nmin\nw\u2208C\nL(w), (16.3)\nwhere C\u2286W = RP is a set of constraints. We say that an approximate\nsolution \u02c6w to Eq. (16.3) isfeasible if \u02c6w \u2208C. Naturally, the design", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1641, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "586d0211-ab58-4df0-bd21-1f1827425de6": {"__data__": {"id_": "586d0211-ab58-4df0-bd21-1f1827425de6", "embedding": null, "metadata": {"page_label": "426", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85df79ec-9b2d-4146-bc90-3d3243546817", "node_type": "4", "metadata": {"page_label": "426", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "270f32da88ab10bec499955ba0b4b852fa1f01d5380ef536860d0ab9f6ccf664", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "426 First-order optimization\nof algorithms for the constrained setting now depends, not only on\ninformation aboutL, but also on information aboutC.\nSimilarly toL, differentoracles can be considered aboutC. One of\nthe most commonly used oracle is theEuclidean projection\nDefinition 16.2(Euclidean projection). The Euclidean projection\nonto the setCis defined by\nprojC(w) := arg min\nv\u2208C\n\u2225w\u2212v\u22252\n2.\nThis projection, which is well-defined whenCis a convex set, can\nbe used in projected gradient descent, that we briefly review below.\nTypically, the projection on a particular setCrequires a dedicated\nalgorithm to compute it.\nOther possible oracles arelinear maximization oracles(LMO)\nused in Frank-Wolfe algorithms andBregman projection oracles,\nused in mirror descent algorithms. The algorithm choice can be dictated\nby what oracle aboutCis available.\n16.3.1 Variational perspective\nProjected gradient descent is a natural generalization of gradient descent,\nbased on the Euclidean projection oracle. Its iterates read\nwt+1 := projC(wt \u2212\u03b3\u2207L(wt)).\nAt each iteration, we attempt to decrease the objective by moving along\nthe negative gradient direction, while ensuring that the next iterate\nremains feasible, thanks to the projection step.\nSimilarlytothevariationalperspectiveofgradientdescentinEq.(16.2),\nthe projected gradient descent update is equivalent to\nwt+1 = arg min\nw\u2208C\nL(wt) + \u27e8\u2207L(wt),w\u2212wt\u27e9+ 1\n2\u03b3\u2225w\u2212wt\u22252\n2.\nThisshowsthatprojectedgradientdescentminimizesatrade-offbetween\nstaying close towt and minimizing the linearization ofL around wt,\nwhile staying inC.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1562, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7c4a3087-0c51-4ebc-a2f8-b76a2ecf7a0a": {"__data__": {"id_": "7c4a3087-0c51-4ebc-a2f8-b76a2ecf7a0a", "embedding": null, "metadata": {"page_label": "427", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a0504346-f2c4-4851-b0b0-57bcff4f9ad8", "node_type": "4", "metadata": {"page_label": "427", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "0f11faa856ec07c527ac38bc79e9a03df26ca92a476ae1291145f534e699b37f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "16.3. Projected gradient descent 427\nIn terms of convergence rates, they remain the same as gradient\ndescent (Nesterov, 2018). For example, projected gradient descent on a\nsmooth convex function still converges at a rateR(T) = O(1/T).\nThere are numerous extensions of vanilla projected gradient descent.\nSimilarly to gradient descent, the stepsize can be automatically adjusted\nusing linesearch techniques and there exists accelerated variants. If\nwe replace\u2207L(w) with a stochastic gradient\u2207L(w; S), we obtain a\nstochastic projected gradient descent.\n16.3.2 Optimality conditions\nIn the unconstrained case, a minimum necessarily has a zero gradient.\nIn the constrained setting, there may not be any feasible parameters\nwith zero gradient. Instead, the optimality of a point is characterized\nby the fact that no better solution can be found by moving along the\ngradient at that point, while staying in the constraints. Formally, it\nmeans that for any\u03b3 >0, a minimizerw\u22c6 of L on Csatisfies\nw\u22c6 = projC(w\u22c6 \u2212\u03b3\u2207L(w\u22c6)).\nIt can be shown that this condition is equivalent (Nesterov, 2018) to\n\u27e8\u2207L(w\u22c6),w\u2212w\u22c6\u27e9\u2265 0 \u2200w\u2208C.\n16.3.3 Commonly-used projections\nWe now briefly review a few useful Euclidean projections.\n\u2022 If C= RP, we obviously have\nprojC(w) = w.\nTherefore, in the unconstrained setting, projected gradient descent\nindeed recovers gradient descent.\n\u2022 If C= [a,b]P (box constraints), we have\nprojC(w) = clip(w,a,b ) := min{max{w,a},b}.\nwhere themin and max are applied coordinate-wise.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1478, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7f0adfa2-06fb-48de-a85c-c8ac37d766e1": {"__data__": {"id_": "7f0adfa2-06fb-48de-a85c-c8ac37d766e1", "embedding": null, "metadata": {"page_label": "428", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b820238f-9868-44db-8ab2-4f1d7764aefc", "node_type": "4", "metadata": {"page_label": "428", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "3fcb8488abc7e09f4108ae962a9288bc715d8b301def50f789162e1b5518b1ec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "428 First-order optimization\n\u2022 As a special case of the above, ifC= RP\n+ (non-negative orthant),\nprojC(w) = max{w,0},\nalso known as non-negative part or ReLu.\n\u2022 If C= \u25b3P (unit probability simplex),\nprojC(w) = max{w\u2212\u03c41,0},\nwhere \u03c4 \u2208R is a constant ensuring thatprojC(w) normalizes to1.\nIt is known that\u03c4 can be found inO(Plog P) using a sort. This\ncan be improved toO(P) using a median-finding like algorithm.\n16.4 Proximal gradient method\nThe constrained setting (withCa convex set) can be recast as uncon-\nstrained optimization, by extending our analysis to functions taking\ninfinite values. Let us denote the indicator function of the setCby\n\u03b9C(w) :=\n\uf8f1\n\uf8f2\n\uf8f3\n0 if w\u2208C\n+\u221e otherwise\n.\nClearly, the constrained problem in Eq. (16.3) can then be rewritten as\nmin\nw\u2208W\nL(w) + \u03b9C(w).\nThis suggests that constrained optimization is a special case ofcom-\nposite objectivesof the form\nmin\nw\u2208W\nL(w) + \u2126(w),\nwhere \u2126 is a convex but potentially non-differentiable function. We\nassume that we have access to an oracle associated with\u2126 called the\nproximal operator.\nDefinition 16.3(Proximal operator). The proximal operator asso-\nciated with\u2126: W\u2192 R is\nprox\u2126(w) := arg min\nv\u2208W\n1\n2\u2225w\u2212v\u22252\n2 + \u2126(v).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1180, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9d15f657-3f6a-433b-a1a7-c0e2e8f95456": {"__data__": {"id_": "9d15f657-3f6a-433b-a1a7-c0e2e8f95456", "embedding": null, "metadata": {"page_label": "429", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e8646b04-b0c9-40ab-9626-fb4ac1efbce4", "node_type": "4", "metadata": {"page_label": "429", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "656e5bc6dab042d9eea004fbce9641fce82b30307c8bd8a2a58e295ebfa983e0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "16.4. Proximal gradient method 429\nThis leads to the proximal gradient method, reviewed below.\n16.4.1 Variational perspective\nWith this method, the update reads\nwt+1 = prox\u03b3\u2126(wt \u2212\u03b3\u2207L(wt)).\nThis update again enjoys an intuitive variational perspective, namely,\nwt+1 = arg min\nw\u2208W\nL(wt) + \u27e8\u2207L(wt),w\u2212wt\u27e9+ 1\n2\u03b3\u2225w\u2212wt\u22252\n2 + \u2126(w).\nThat is, we linearizeL around wt, but keep\u2126 as is.\nThe proximal gradient method is popularly used when the objective\nfunction contains a sparsity-inducing regularizer\u2126. For example, for\nthe LASSO (Tibshirani, 1996), which aims at predicting targetsy=\n(y1,...,y n)\u22a4\u2208RN from observationsX= (x1,..., xn)\u22a4\u2208RN\u00d7P, we\nset L(w) = 1\n2 \u2225Xw\u2212y\u22252\n2 and \u2126(w) = \u03bb\u2225w\u22251, where \u03bb >0 controls\nthe regularization strength. In this case,prox\u2126 is the so-called soft-\nthresholding operator (see below).\nConvergence guarantees of the proximal gradient method remain\nthe same as for gradient descent, such as aO(1/T) rate for smooth\nconvex functions.\n16.4.2 Optimality conditions\nAn optimal solution of the problem is characterized by thefixed point\nequation\nw\u22c6 = prox\u03b3\u2126(w\u22c6 \u2212\u03b3\u2207L(w\u22c6)),\nfor all\u03b3 >0 (Nesterov, 2018). In other words, the proximal gradient\nmethod (which includes gradient descent and projected gradient descent\nas special cases), can be seen as fixed point iteration schemes. Such\na viewpoint suggests using acceleration methods from the fixed point\nliterature such as Anderson acceleration (Pollock and Rebholz, 2021).\nIt is also useful when designing implicit differentiation schemes as\npresented in Chapter 8.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1524, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "316aff54-5d5f-4891-986e-2c13042a2545": {"__data__": {"id_": "316aff54-5d5f-4891-986e-2c13042a2545", "embedding": null, "metadata": {"page_label": "430", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2512d278-3030-4650-b0cb-8e1bcf542f31", "node_type": "4", "metadata": {"page_label": "430", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "90f529b77b1b7b48eb02720f780767e2476afc35671691318bf9ced229dad6bd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "430 First-order optimization\n16.4.3 Commonly-used proximal operators\nWe now briefly review a few useful proximal operators.\n\u2022 If \u2126(w) = 0, we have\nprox\u03b3\u2126(w) = w.\nTherefore, with this proximal operator, the proximal gradient\nmethod recovers gradient descent.\n\u2022 If \u2126(w) = \u03b9C(w), we have\nprox\u03b3\u2126(w) = projC(w).\nTherefore, with this proximal operator, the proximal gradient\nmethod recovers projected gradient descent.\n\u2022 If \u2126(w) = \u03bb\u2225w\u22251, we have\nprox\u03b3\u2126(w) = (sign(w) \u00b7max(|w|\u2212\u03b3\u03bb,0)),\nwhere the operations are applied coordinate-wise. This is the\nso-called soft-thresholding operator.\n\u2022 \u2126(w) = \u03bb\u2211\ng\u2208G\u2225wg\u22252 where G is a partition of [P] and wg\ndenotes the subvector restricted tog, then we have\n[\nprox\u03b3\u2126(w)\n]\ng\n= max(1 \u2212\u03bb\u00b7\u03b3/\u2225wg\u22252,0)wg,\nwhich is used in the group lasso (Yuan and Lin, 2006) and can be\nused to encourage group sparsity.\nFor a review of more proximal operators, see for instance (Bachet al.,\n2012; Parikh, Boyd,et al., 2014).\n16.5 Summary\n\u2022 From a variational perspective, gradient descent is the algorithm\nobtained when linearizing the objective function and using a\nquadratic regularization term.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1104, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "04bb4681-4d9e-4f9b-8d02-a438f5e321cb": {"__data__": {"id_": "04bb4681-4d9e-4f9b-8d02-a438f5e321cb", "embedding": null, "metadata": {"page_label": "431", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c016a968-aad6-4f94-b4ce-d44df22c743a", "node_type": "4", "metadata": {"page_label": "431", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "867a4d9ed35ac1a10128944a5533fd7807ebdae5d20c11fcf81f0dd977122f5d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "16.5. Summary 431\n\u2022 Projected gradient descent is the algorithm obtained when there\nis an additional constraint (the Euclidean projection naturally\nappearing, due to the quadratic regularization term).\n\u2022 When the objective is the sum of a differentiable function and\na non-differentiable function, proximal gradient is the algorithm\nobtained when the differentiable function is linearized but the\nnon-differentiable function is kept as is.\n\u2022 We also reviewed various stochastic gradient based algorithms,\nincluding vanilla SGD, SGD with momentum and Adam.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 555, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "92717293-a186-4a76-a8d0-c0c8069e2101": {"__data__": {"id_": "92717293-a186-4a76-a8d0-c0c8069e2101", "embedding": null, "metadata": {"page_label": "432", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "83d8c6bb-9ba7-462e-92ec-ead4307b3bcb", "node_type": "4", "metadata": {"page_label": "432", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "3f40b4484c1fb5067066e164937581d82a43b65d7cd600a7633f5288f8cb3f51", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "17\nSecond-order optimization\nWe review in this chapter methods whose iterations take the form\nwt+1 := wt \u2212\u03b3tBt\u2207L(wt),\nwhere \u03b3t is a stepsize andBt is a pre-conditioning matrix involving\nsecond-order derivatives.\n17.1 Newton\u2019s method\n17.1.1 Variational perspective\nWe saw in Eq. (16.2) that gradient descent can be motivated from\na variational perspective, in which we use a linear approximation of\nthe objective around the current iterate, obtained from the current\ngradient. Similarly, if we have access not only to the gradient but also\nto the Hessian of the objective, we can use a quadratic approximation\nof the objective around the current iterate. More precisely, given a\nfunction L(w), we may consider minimizing the second-order Taylor\napproximation ofL(w) around the current iteratewt,\nL(w) \u2248L(wt) + \u27e8\u2207L(wt),w\u2212wt\u27e9+ 1\n2\u27e8w\u2212wt,\u22072L(wt)(w\u2212wt)\u27e9.\n432", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 852, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "39450837-1b63-4b44-93f5-2509da2a82be": {"__data__": {"id_": "39450837-1b63-4b44-93f5-2509da2a82be", "embedding": null, "metadata": {"page_label": "433", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37752dae-6e4d-4fba-98da-5d18c7b373c2", "node_type": "4", "metadata": {"page_label": "433", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "b6bf143b493d8071b655cdcdda1e173b7638721f54469c0e6c85a1a68df2f3ba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "17.1. Newton\u2019s method 433\nNewton\u2019s methodsimply iteratively minimizes this quadratic approx-\nimation around the current iterationwt, namely,\nwt+1 = arg min\nw\u2208W\nL(wt) + \u27e8\u2207L(wt),w\u2212wt\u27e9+ 1\n2\u27e8w\u2212wt,\u22072L(wt)(w\u2212wt)\u27e9.\n(17.1)\nIf the Hessian is positive definite atwt, which we denote by\u22072L(wt) \u227b\n0, then the minimum is well-defined and unique (this is for example the\ncase ifLis strictly convex). The iterates can then be written analytically\nas\nwt+1 = wt \u2212\u22072L(wt)\u22121\u2207L(wt).\nIf the Hessian is not positive definite, the minimum may not be defined.\nIgnoring this issue and taking the analytical formulation could be\ndangerous, as it could amount to computing the maximum of the\nquadratic instead if, for example, the quadratic was strictly concave\n(i.e., \u22072L(w) \u227a0).\n17.1.2 Regularized Newton method\nA simple technique to circumvent this issue consists in adding a regu-\nlarization term to the Hessian. Namely, from a variational viewpoint,\nwe can add a proximity term1\n2 \u2225w\u2212wt\u22252\n2, encouraging to stay close to\nthe currentwt. The iterates of this regularized Newton method then\ntake the form\nwt+1 = arg min\nw\u2208W\nL(wt) + \u27e8\u2207L(wt),w\u2212wt\u27e91\n2\u27e8w\u2212wt,\u22072L(wt)(w\u2212wt)\u27e9\n+ \u03b7t\n2 \u2225w\u2212wt\u22252\n2,\nwhere \u03b7t controls the regularization strength. Assuming\u03b7t >0 is strong\nenough to make\u22072L(wt) + \u03b7tI positive-definite, we have\nwt+1 = wt \u2212dt,\nwhere we defined the direction\ndt := (\u22072L(wt) + \u03b7tI)\u22121\u2207L(wt). (17.2)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1371, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9c36e9b5-688c-4973-9090-6d366ea6f28e": {"__data__": {"id_": "9c36e9b5-688c-4973-9090-6d366ea6f28e", "embedding": null, "metadata": {"page_label": "434", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f0346e8c-7911-4514-9234-1663b08b6311", "node_type": "4", "metadata": {"page_label": "434", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "86cb27f8e366aa4a8e8011204fe2e758335c45b5cd74ba5312e874ff6f4782a3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "434 Second-order optimization\nOther techniques to circumvent this issue include using cubic regular-\nization and modifying the spectral decomposition of the Hessian, by\nthresholding the eigenvalues or taking their absolute values. We refer\nthe interested reader to, e.g., (Nesterov, 2018; Wright and Nocedal,\n1999) for more details.\n17.1.3 Approximate direction\nWe observe a main impediment for implementing such a second-order\noptimization algorithm: even if we had access to the Hessian of the\nobjective for free and this Hessian was positive definite, computing the\nexact directiondt in Eq. (17.2) requires computing an inverse-Hessian\nvector product (IHVP) with the gradient\u2207L(wt). Doing so exactly\nrequires solving a linear system\n(\u22072L(wt) + \u03b7tI)dt = \u2207L(wt),\nwhich a priori takesO(P3) time. In practice, however, we can compute\nIHVPs approximately, as explained in Section 9.4.\n17.1.4 Convergence guarantees\nWhile implementing Newton\u2019s method comes at a higher computational\ncost, it can also benefit from faster convergence rates. Briefly, if Newton\u2019s\nmethod is initialized at a pointw0 \u2208W close enough from the mini-\nmizer w\u22c6 of a\u00b5-strongly convex function withM-Lipschitz continuous\nHessian (namely\u2225w0 \u2212w\u2217\u22252 \u2264 2\u00b5\n3M), then Newton\u2019s method converges\nat a quadratic rate (Nesterov, 2018), that is,R(t) \u2264O(exp(exp(\u2212t))\n(see Section 15.5 for a brief introduction to performance guarantees).\nThis is far superior to gradient descent. Such an efficiency motivated\nthe development of interior point methods, that have been a break-\nthrough in constrained optimization, thanks to the use of log-barrier\npenalties (Nesterov, 2018).\n17.1.5 Linesearch\nIn practice, we may not have access to an initial point close enough\nfrom the minimizer. In that case, even for strictly convex functions for", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1790, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6c2f8b1e-cd1a-4504-acce-be9274172911": {"__data__": {"id_": "6c2f8b1e-cd1a-4504-acce-be9274172911", "embedding": null, "metadata": {"page_label": "435", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36f27c83-4011-435e-b9ac-bd3938e5837d", "node_type": "4", "metadata": {"page_label": "435", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "78f81ec770fcae29279876cf96c9de3c4867b3dc8dc49cb65fdc1ebd3204eb0c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "17.1. Newton\u2019s method 435\nwhich Newton\u2019s steps are well-defined, takingwt+1 = wt \u2212dt may not\nensure a decrease of the objective values. Nevertheless, the directiondt\nmay define a descent direction as defined below.\nDefinition 17.1(Descent direction). A pointd\u2208W defines ade-\nscent direction \u2212d for an objective L at w, if there exists a\npositive stepsize\u03b3 >0 such that\nL(w\u2212\u03b3d) \u2264L(w).\nIf L is differentiable,\u2212dis a descent direction if\u27e8\u2212d,\u2207L(w)\u27e9<0.\nForNewton\u2019smethodwithoutregularization, dt = \u22072L(wt)\u22121\u2207L(wt)\nis then a descent direction atwt, as long as\u2207L(wt) \u0338= 0 and \u22072L(wt) \u227b\n0. If\u22072L(wt) \u0338\u227b0, choosing\u03b7t >0 such that\u22072L(wt) + \u03b7tI \u227b0, also\nensures thatdt = \u2212(\u22072L(wt) + \u03b7tI)\u22121\u2207L(wt) is a descent direction\n(as long as\u2207L(wt) \u0338= 0). Newton\u2019s method is then generally equipped\nwith a linesearch method that attempts to take steps of the form x\nwt+1 = wt \u2212\u03b3tdt\nwith \u03b3t chosen as the largest stepsize among{\u03c1\u03c4,\u03c4 \u2208N}for \u03c1\u2208(0,1)\nuntil a sufficient decrease of the objective is satisfied such as, forc\u2208\n(0,1),\nL(wt \u2212\u03b3tdt) \u2264L(wt) \u2212c\u03b3t\u27e8\u2207L(wt),\u22072L(wt)\u22121\u2207L(wt)\u27e9.\nFor strongly convex functions, such an implementation exhibits two\nphases: a first phase during which Newton\u2019s steps are \u201cdamped\u201d by\nusing a stepsize\u03b3t <1 and a second phase of super-fast convergence\nduring which stepsizes\u03b3t = 1 are taken, and the objective decreases very\nfast. Even far from the optimum, Newton directions can advantageously\nadapt to the local geometry of the objective to speed-up convergence\ncompared to a regular gradient descent as explained below.\n17.1.6 Geometric interpretation\nTo understand the efficiency of Newton\u2019s method compared to gradient\ndescent, consider the minimization of a simple quadratic\nL(w) = 1\n2aw2\n1 + 1\n2bw2\n2", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1708, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "35c08bde-6c62-419d-b431-31432392febb": {"__data__": {"id_": "35c08bde-6c62-419d-b431-31432392febb", "embedding": null, "metadata": {"page_label": "436", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "494d1a79-cc74-4505-9308-0a491a6bb8b3", "node_type": "4", "metadata": {"page_label": "436", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "9ef354907a36b9d5936b714d1eb983014eea14cbbe555fb95ac27e778a3edbf5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "436 Second-order optimization\nfor a \u226bb \u22650, as illustrated in Fig. 17.1. A gradient descent moves\nalong the directions\u2207L(w) = (aw1,bw2)\u22a4 and its stepsize is limited\nby the variations in the first coordinate leading to some oscillations. If\nwe were simply rescaling the gradient by(a,b), i.e., taking steps of the\nform\nwt+1 = wt \u2212\u03b3diag(a\u22121,b\u22121)L(wt),\nthe variations in both coordinates would be normalized to one and the\nstepsize could simply be chosen to\u03b3 = 1 to directly getw\u22c6. In other\nwords, by adapting the geometry of the directions with the geometry\ninduced by the objective, we can circumvent the oscillations.\nThat\u2019s exactly what Newton\u2019s method does by modifying the gradi-\nent direction using the inverse of the Hessian. Formally, at iterationt,\nconsider the modified objective\n\u02dcL(v) = L(Av) for A= \u22072L(wt)\u22121/2,\nwith L strictly convex andA the inverse matrix square root of the\nHessian. One easily verifies that a Newton step is equivalent to a\ngradient step on\u02dcL, that is,\nvt+1 = vt \u2212\u2207\u02dcL(vt) \u21d0\u21d2 wt+1 = wt \u2212(\u22072L(wt))\u22121\u2207L(wt)\nwhere\nwt = Avt = \u22072L(wt)\u22121/2vt.\nIn the geometry induced byA, the objective is generally better condi-\ntioned as illustrated in Fig. 17.1. This explains the efficiency of Newton\u2019s\nmethod. In particular for any strongly convex quadratic, a Newton step\nreaches the optimum in one iteration, while a gradient step can take\nmany more iterations.\n17.1.7 Stochastic Newton\u2019s method\nConsider now an expected loss\nmin\nw\u2208W\nL(w) := ES\u223c\u03c1[L(w; S)] .\nIn that case, an estimate of the Hessian can be constructed just like for\nthe gradient using that\nES\u223c\u03c1\n[\n\u22072L(w; S)\n]\n= \u22072L(w).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1597, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aeb7fda5-aa5e-43f7-842c-70402d0edd97": {"__data__": {"id_": "aeb7fda5-aa5e-43f7-842c-70402d0edd97", "embedding": null, "metadata": {"page_label": "437", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e9f5f7c3-c675-4a3f-b33b-031873ea9dee", "node_type": "4", "metadata": {"page_label": "437", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "1cfce42d8f43e03cd65968de6d246bf8b0b06241dec84cb4fedfd26d84106f2b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "17.2. Gauss-Newton method 437\nFigure 17.1:Left: Minimization of a quadraticL(w) := 1\n2 aw2\n1 + 1\n2 bw2\n2 by gradient\ndescent. Fora\u226bb\u22650, gradient descent typically oscillates. Right: minimization\nby Newton\u2019s method amounts to change the geometry of the problem to avoid\noscillations.\nDenote then\ng(w; S) \u2248\u2207L(w), H (w; S\u2032) \u2248\u22072L(w)\nsome stochastic estimates of respectively of the gradient and the Hessian\nwith S,S\u2032independently drawn fromp or from mini-batch approaxima-\ntions with varying mini-batch sizes. One implementation of astochastic\nNewton methodcan then be\nwt+1 = wt \u2212\u03b3t(H(wt; S\u2032) + \u03b7tI)\u22121g(wt; S),\nfor \u03b7t \u22650 such that(H(wt; S\u2032) + \u03b7t)\u22121 \u227b0 and \u03b3t fixed or chosen to\nsatisfy some sufficient decrease condition. We refer the interested reader\nto, e.g., (Xuet al., 2020), for more details and variants.\n17.2 Gauss-Newton method\nNewton\u2019s method (17.1) is usually not properly defined for non-convex\nobjective functions, since the Hessian may not be positive definite at the\ncurrent iterate. We saw in Section 9.2 that the Gauss-Newton matrix can\nbe used to define a positive-semidefinite approximation of the Hessian.\nHere, we revisit the Gauss-Newton method from a variational and\npartial linearizationperspective. While the original Gauss-Newton\nmethod originates from nonlinear least-squares, we will first describe\nan extension to arbitrary convex loss functions, since it is both more\ngeneral and easier to explain.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1424, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "320d9f0b-dc69-49c8-93a9-66921d93ccb4": {"__data__": {"id_": "320d9f0b-dc69-49c8-93a9-66921d93ccb4", "embedding": null, "metadata": {"page_label": "438", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "24ae9e8e-a48a-4171-95c2-f354f172fba6", "node_type": "4", "metadata": {"page_label": "438", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "14a818d60c8d8c465aed7dad74a35e9f45bdca4d2ab1079dfd0021a3914f654b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "438 Second-order optimization\n17.2.1 With exact outer function\nConsider a composite objective of the form\nL(w) := \u2113(f(w)),\nwhere \u2113: M\u2192 R is aconvexfunction, such as a convex loss function\napplied on a given sample, andf : W\u2192M is anonlinear function,\nsuch as a neural network with parametersw\u2208W, evaluated on the\nsame sample. We saw that gradient descent and Newton\u2019s method\namount to usinglinear and quadratic approximations ofL(w) around\nthe current iteratewt, respectively. As a middle ground between the\ntwo, the Gauss-Newton method uses the linearization off around wt\nf(w) \u2248f(wt) + \u2202f(wt)(w\u2212wt)\nbut keeps\u2113 as is to obtain the objective\nwt+1 := arg min\nw\u2208W\n\u2113(f(wt) + \u2202f(wt)(w\u2212wt))\n= arg min\nw\u2208W\n\u2113(\u2202f(wt)w+ f(wt) \u2212\u2202f(wt)wt)\n= arg min\nw\u2208W\n\u2113(Jtw+ \u03b4t),\nwhere we defined the shorthandsJt := \u2202f(wt) and \u03b4t := f(wt) \u2212\n\u2202f(wt)wt. We call\u2113(Jtw+ \u03b4t) the partial linearizationof L= \u2113\u25e6f\nat wt, as opposed to the full linearization ofLused in gradient descent.\nSince the composition of a convex function and of linear function is\nconvex, this objective isconvexeven ifL(w) is nonconvex. In practice,\nwe often add a proximity term as regularization to define\nwt+1 := arg min\nw\u2208W\n\u2113(Jtw+ \u03b4t) + \u03b7t\n2 \u2225w\u2212wt\u22252\n2. (17.3)\nWe can see this update as an approximation of theproximal point\nupdate\narg min\nw\u2208W\nL(w) + \u03b7t\n2 \u2225w\u2212wt\u22252\n2,\nwhere L(w) has been replaced by its partial linearization. Solving\nEq. (17.3) using gradient-based solvers requires to compute the gradient", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1449, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f2ad224a-fd52-4853-b7ea-ed54f79081c6": {"__data__": {"id_": "f2ad224a-fd52-4853-b7ea-ed54f79081c6", "embedding": null, "metadata": {"page_label": "439", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "65fad22e-7ffc-460e-8c4d-bb24313d80c0", "node_type": "4", "metadata": {"page_label": "439", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "887dc552494d258c4d788fc9d5b37f91eb2967faf826effabdafe54d3426012f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "17.2. Gauss-Newton method 439\nof w \u21a6\u2192\u2113(Jtw+ \u03b4t), which isw \u21a6\u2192(Jt)\u2217\u2207\u2113(Jtw+ \u03b4t). Computing\nthis gradient by autodiff therefore requires to perform a forward pass\nto compute the JVPJtw and a backward pass to compute the VJP\n(Jt)\u2217\u2207\u2113(z). See Section 2.3 for an introduction to these operators and\nChapter 8 for an introduction to autodiff.\nThe Gauss-Newton method with arbitrary convex outer loss is\noften called modified Gauss-Newton (Nesterov, 2007) or prox-linear\n(Drusvyatskiy and Paquette, 2019). The classical Gauss-Newton and\nLevenberg-Marquardt (Levenberg, 1944; Marquardt, 1963) methods\noriginate from nonlinear least-squares and are recovered when\u2113(z) is\nquadratic (Kelley, 1995), such as\u2113(z) := 1\n2 \u2225z\u2212y\u22252\n2, forysome reference\ntarget. The Gauss-Newton method corresponds classically to not using\nregularization (i.e.,\u03b7t = 0) and the Levenberg-Marquardt method uses\nregularization (usually called damping, potentially changing\u03b7t across\niterations). See e.g., (Messereret al., 2021), for a survey of different\nvariants.\n17.2.2 With approximate outer function\nAnother variant of the Gauss-Newton method consists in replacing the\nconvex loss\u2113 with its quadratic approximation aroundzt := f(wt),\nqt(z) := \u2113(zt) + \u27e8\u2207\u2113(zt),z\u2212zt\u27e9+ 1\n2\u27e8z\u2212zt,\u22072\u2113(zt)(z\u2212zt)\u27e9\u2248 \u2113(z)\nto define the update\nwt+1 := arg min\nw\u2208W\nqt(Jtw+ \u03b4t) + \u03b7t\n2 \u2225w\u2212wt\u22252\n2.\nNotice that \u2113 has been replaced by its quadratic approximationqt.\nThis objective is always aconvex quadratic, unlike the objective\nof the Newton method in Eq. (17.1), which is a priori anonconvex\nquadratic, iff is nonlinear. Simple calculations show that\nwt+1 = arg min\nw\u2208W\nL(wt) + \u27e8qt,Jt(w\u2212wt)\u27e9\n+ 1\n2\u27e8w\u2212wt,(Jt)\u2217QtJt(w\u2212wt)\u27e9+ \u03b7t\n2 \u2225w\u2212wt\u22252\n2", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1670, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e9124981-0080-4951-b4a7-57aad50de1ca": {"__data__": {"id_": "e9124981-0080-4951-b4a7-57aad50de1ca", "embedding": null, "metadata": {"page_label": "440", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b2cfd4ee-de55-46fa-bf3c-fcdba6af709c", "node_type": "4", "metadata": {"page_label": "440", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "7e3115a8f7fedf8f3d6657f81d139f3a1c20df740e3a2ec1e6c1db71e4624b41", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "440 Second-order optimization\nwhere qt := \u2207\u2113(f(wt)) \u2208M = RZ,Qt := \u22072\u2113(f(wt)) \u2208RZ\u00d7Z. The\nclosed form solution is\nwt+1 = wt \u2212((Jt)\u2217QtJt + \u03b7tI)\u22121(Jt)\u2217qt\n= wt \u2212(\u22072\nGN(\u2113\u25e6f)(wt) + \u03b7tI)\u22121\u2207L(wt),\nwhere we used the (generalized)Gauss-Newton matrixof L= \u2113\u25e6f,\ndefined in Section 9.2.\n17.2.3 Linesearch\nSimilarly to Newton\u2019s method, the iterates of a Gauss-Newton method\nmay diverge when used alone. However, the direction\u2212(\u22072\nGN(\u2113\u25e6f)(wt)+\n\u03b7tI)\u22121\u2207L(wt) defines a descent direction for any\u03b7t > 0 and can be\ncombined with a stepsize\u03b3t (typically chosen using a linesearch) to\nobtain iterates of the form\nwt+1 = wt \u2212\u03b3t(\u22072\nGN(\u2113\u25e6f)(wt) + \u03b7tI)\u22121\u2207L(wt).\n17.2.4 Stochastic Gauss-Newton\nIn deep learning, the objective generally consists in an expectation\nover samples of the composition between a loss function and a network\nfunction:\nL(w) = ES\u223c\u03c1[L(w; S)] = E(X,Y)\u223c\u03c1[\u2113(f(w; X); Y)]\nwhere S = (X,Y ) denotes a sample pair of inputX with associated\nlabel Y. In that case, as already studied in Section 9.2, the Gauss-\nNewton matrix\u22072\nGNLis the expectation of the individual Gauss-Newton\nmatrices\n\u22072\nGNL(w; x,y) := \u2202f(w; x)\u22a4\u22072\u2113(f(w; x))\u2202f(w; x),\n\u22072\nGNL(w) := E(X,Y)\u223c\u03c1[\u22072\nGNL(w; x,y)].\nWe can estimate the gradient and the Gauss-Newton matrix by, respec-\ntively, g(w; S) \u2248\u2207L(w), andG(w; S\u2032) \u2248\u22072\nGNL(w) for S,S\u2032 \u223c\u03c1 or\nusing mini-batch approximations. Astochastic Gauss-Newton method\ntherefore performs iterates\nwt+1 := wt \u2212\u03b3t(G(w; S\u2032) + \u03b7tI)\u22121g(w,S),\nfor \u03b7t \u22650 and \u03b3t fixed or selected to satisfy some criterion.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1487, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2ca6cfcd-6fa9-4410-bb72-df22d4954021": {"__data__": {"id_": "2ca6cfcd-6fa9-4410-bb72-df22d4954021", "embedding": null, "metadata": {"page_label": "441", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b2e1bb14-49eb-45f0-8667-04e87fc85a62", "node_type": "4", "metadata": {"page_label": "441", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "e8880d4ca373dd97d842ef929b0cbe2f73fddc66aac26579748c55afc86c713c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "17.3. Natural gradient descent 441\n17.3 Natural gradient descent\nNatural gradient descent (Amari, 1998) follows a similar principle as\ngradient descent: linearize the objective around the current iterate and\nminimize this approximation together with a proximity term. It differs\nfrom gradient descent in the choice of the proximity term: rather than\nusing a squared Euclidean distance between theparameters, it uses a\nKullback-Leibler divergence between theprobability distributions\nthese parameters define.\nNegative log-likelihood\nWe consider objectives of the form\nmin\nw\u2208W\nL(w) = ES\u223c\u03c1[L(w; S)] = ES\u223c\u03c1[\u2212log qw(S)] ,\nwhere \u03c1 is an unknown data distribution (but from which we can sam-\nple) and whereqw is a probability distribution parameterized byw. As\nreviewed in Chapter 3, the negative log-likelihood can be used as a loss\nfunction (many loss functions can be seen from this perspective, includ-\ning the squared and logistic loss functions). In the unsupervised setting,\nwhere S = Y, we simply useqw(Y) as is. In the supervised setting,\nwhere S = (X,Y ), we use the product ruleP(X,Y ) = P(X)P(Y|X) to\nparameterize qw(S) as\nqw(x,y) := \u03c1X(x)p\u03b8(y),\nwhere \u03c1X is the marginal distribution forX, p\u03b8(y) is the PMF/PDF\nof a probability distribution and\u03b8= f(w; x) is for instance a neural\nnetwork with parametersw\u2208W and inputx\u2208X.\n17.3.1 Variational perspective\nNatural gradient descent is motivated by updates of the form\nwt+1 = arg min\nw\u2208W\nL(wt) + \u27e8\u2207L(wt),w\u2212wt\u27e9+ KL(qwt,qw),\nwhere KL(p,q) :=\n\u222b\np(z) log p(z)\nq(z) dzis the Kullback-Leibler (KL) diver-\ngence. Unlike gradient descent, the proximity term is therefore between", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1619, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "81abb764-db72-4a06-ad32-ca2eed3e88a0": {"__data__": {"id_": "81abb764-db72-4a06-ad32-ca2eed3e88a0", "embedding": null, "metadata": {"page_label": "442", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "98c1edf5-6380-4f4f-8134-cff7519a4b8f", "node_type": "4", "metadata": {"page_label": "442", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "5ad73669445e2d4c5e595a98fc80a5996e4e8b130c8814861af37f7c99727b80", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "442 Second-order optimization\nthe current distributionqwt and a candidate probability distribution\nqw. The above problem is intractable in general, as the KL may not\nhave a closed form. Nevertheless, its quadratic approximation can be\nshown (Amari, 1998) to admit a simple form,\nKL(qwt,qw) \u22481\n2\u27e8w\u2212wt,\u22072\nFL(wt)(w\u2212wt)\u27e9\nwhere we used the Fisher information matrix \u22072\nFL(w), studied in\nSection 9.3. Equipped with this quadratic approximation of the KL\ndivergence, natural gradient descent amounts to compute iterates as\nwt+1 := arg min\nw\u2208W\nL(wt) + \u27e8\u2207L(wt),w\u2212wt\u27e9\n+ 1\n2\u27e8w\u2212wt,\u22072\nFL(wt)(w\u2212wt)\u27e9+ \u03b7t\n2 \u2225w\u2212wt\u22252\n2,\nwhere a quadratic proximity-term was added to ensure a unique solution.\nThis is a srictly convex problem as\u22072\nFL(wt) is positive semi-definite.\nThe closed-form solution is\nwt+1 = wt \u2212(\u22072\nFL(wt) + \u03b7tI)\u22121\u2207L(wt).\nBecause the Gauss-Newton and Fisher information matrices are equiv-\nalent whenp\u03b8 is an exponential family distribution (Proposition 9.6),\nthe Gauss-Newton and natural gradient methods coincide in this case.\n17.3.2 Stochastic natural gradient descent\nIn practice, we may not have access to\u2207L(wt) in closed form as it is\nan expectation over\u03c1. Moreover,\u22072\nFL(wt) may not be computable in\nclosed form either. To estimate the Fisher information matrix, we can\nuse that (see Section 9.3) using the shorthand\u03b8:= f(w,X),\n\u22072\nFL(w) = EX\u223c\u03c1XEY\u223cp\u03b8[\u2207L(w; X,Y ) \u2297\u2207L(w; X,Y )].\nWe can then build estimatesg(wt; S) \u2248\u2207L(wt,S) and F(wt; S\u2032) \u2248\n\u22072\nFL(wt) for S sampled from \u03c1 and S\u2032 sampled from qwt(x,y) =\npX(x)\u03c1\u03b8(y). A stochastic natural gradient descent can then be imple-\nmented as\nwt+1 = wt \u2212\u03b3t(F(wt; S\u2032) + \u03b7tI)\u22121g(wt; S),", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1619, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d172ee5d-3250-4b91-a02e-4dd76d851b97": {"__data__": {"id_": "d172ee5d-3250-4b91-a02e-4dd76d851b97", "embedding": null, "metadata": {"page_label": "443", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "669c54db-c164-42d9-928b-2d495aa437fb", "node_type": "4", "metadata": {"page_label": "443", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "36da18cc903fddba8391d7f343456a8cb0e927af6dfcc328f2cbe1b3a12f9926", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "17.4. Quasi-Newton methods 443\nwhere \u03b3t is a stepsize, possibly chosen by linesearch.\nIn deep learning, the product with the inverse Fisher or Gauss-\nNewton matrices can remain costly to compute. Several approximations\nhave been proposed, such as KFAC (Martens and Grosse, 2015; Botev\net al., 2017), which uses a computationally efficient structural approxi-\nmation to these matrices.\n17.4 Quasi-Newton methods\n17.4.1 BFGS\nA celebrated example ofquasi-Newton method is the BFGS method\n(Broyden, 1970; Fletcher, 1970; Goldfarb, 1970; Shanno, 1970), whose\nacronym follows from its author names. The rationale of the BFGS\nupdate stems once again from a variational viewpoint. We wish to\nbuild a simple quadratic model of the objective ht(w) = L(wt) +\n\u27e8\u2207L(wt),w\u2212wt\u27e9+ 1\n2 \u27e8w\u2212wt,Qt(w\u2212wt)\u27e9for someQt built along\nthe iterations rather than taken as\u22072L(wt). One desirable property of\nsuch quadratic model would be that its gradients at consecutive iterates\nmatch the gradients of the original function, i.e.,\u2207ht(wt) = \u2207L(wt)\nand \u2207ht(wt\u22121) = \u2207L(wt\u22121). A simpler condition, called thesecant\ncondition consists in considering the differences of these vectors, that\nis, ensuring that\n\u2207ht(wt) \u2212\u2207ht(wt\u22121) = \u2207L(wt) \u2212\u2207L(wt\u22121)\n\u21d0\u21d2Qt(wt \u2212wt\u22121) = \u2207L(wt) \u2212\u2207L(wt\u22121)\n\u21d0\u21d2wt \u2212wt\u22121 = Bt(\u2207L(wt) \u2212\u2207L(wt\u22121)),\nfor Bt = (Qt)\u22121. BuildingBt, a surrogate of the inverse of the Hessian\nsatisfying the secant equation, can then be done as\nBt+1 :=\n(\nI \u2212\u03c1tst(yt)\u22a4\n)\nBt\n(\nI \u2212\u03c1tst(yt)\u22a4\n)\n+ \u03c1tst(st)\u22a4\nwhere\nst := wt+1 \u2212wt\nyt := \u2207L(wt+1) \u2212\u2207L(wt)\n\u03c1t := 1\n\u27e8st,yt\u27e9.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1518, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8d5b5769-384c-43ac-b0db-472ecbf6c8bf": {"__data__": {"id_": "8d5b5769-384c-43ac-b0db-472ecbf6c8bf", "embedding": null, "metadata": {"page_label": "444", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3f261dca-43a5-491d-8476-7ff12d1c63b6", "node_type": "4", "metadata": {"page_label": "444", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "548427947862a1def6820a73ac98fd9fb1e1beb26ca4194df5b7256554c58fba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "444 Second-order optimization\nA typical implementation of BFGS storesBt \u2208RP\u00d7P in memory, which\nis prohibitive whenP is large.\n17.4.2 Limited-memory BFGS\nInpractice,thelimited-memorycounterpartofBFGS,calledLBFGS(Liu\nand Nocedal, 1989), is often preferred. The key observation of LBFGS\nis that we do not need to materializeBt in memory: we only need\nto multiply it with the gradient\u2207L(wt). That is, we can seeBt as\na linear map. Fortunately, the product betweenBt and any vectorv\ncan be computed efficiently if we store(s1,y1,\u03c11),..., (st,yt,\u03c1t) in\nmemory. In practice, a small history of past values is used to reduce\nmemory and computational cost. Because LBFGS has the benefits of\nsecond-order-like methods with much reduced cost, it has become a de-\nfacto algorithm, outperforming most other algorithms for medium-scale\nproblems without particular structure (Liu and Nocedal, 1989).\n17.5 Approximate Hessian diagonal inverse preconditionners\nOne application of the approximations of the Hessian diagonal developed\nin Section 9.7 is to obtain cheap approximations of the Hessian diagonal\ninverse,\nBt := diag(|Ht\n11|\u22121,..., |Ht\nPP |\u22121).\nSuch a scaling would for instance be sufficient to make the quadratic\nexample presented in Fig. 17.1 work. Many optimization algorithms,\nincluding the popular ADAM, can be viewed as using a preconditioner\nthat approximates the inverse of the Hessian\u2019s diagonal.\n17.6 Summary\n\u2022 We reviewed Newton\u2019s method, the Gauss-Newton method, natu-\nral gradient descent, quasi-Newton methods and preconditioning\nmethods.\n\u2022 We adopted a variational viewpoint, where the method\u2019s next\niterate is computed as the solution of a trade-off between mini-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1672, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "980b0587-0eab-4684-8eac-1abf8ee5c29e": {"__data__": {"id_": "980b0587-0eab-4684-8eac-1abf8ee5c29e", "embedding": null, "metadata": {"page_label": "445", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "10f96c88-23d0-448a-af3f-769d08394940", "node_type": "4", "metadata": {"page_label": "445", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "b8820c5998beb826453c5f250632294b3432298d4f5008ec26baa99f060c63f3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "17.6. Summary 445\nmizing an approximation of the function (linear, partially linear,\nquadratic) and a proximity term (squared Euclidean, KL).\n\u2022 All methods were shown to use iterates of the form\nwt+1 := wt \u2212\u03b3tBt\u2207L(wt)\nbut have different trade-offs between the cost it takes to evaluate\nBt\u2207L(wt) and the richness of the information used aboutL.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 343, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "565f6e2f-909d-4c64-8339-5ab5178bad59": {"__data__": {"id_": "565f6e2f-909d-4c64-8339-5ab5178bad59", "embedding": null, "metadata": {"page_label": "446", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "57ab765a-d484-4a1d-9aa7-65e553c5eeb3", "node_type": "4", "metadata": {"page_label": "446", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "268a22a3c3a00eff307cff9d8ca7329372bf7570c7c96473b8cd7b3dd7b91e42", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "18\nDuality\nIn this chapter, we review duality principles in optimization.\n18.1 Dual norms\nWe introduce in this section dual norms, since they are useful in this\nbook.\nDefinition 18.1(Dual norms). Given a norm\u2225u\u2225, its dual is\n\u2225v\u2225\u2217:= max\n\u2225u\u2225\u22641\n\u27e8u,v\u27e9.\nTherefore, the dual norm of\u2225\u00b7\u2225 is thesupport functionof the\nunit ball induced by the norm\u2225\u00b7\u2225,\nB\u2225\u00b7\u2225:= {u\u2208RD: \u2225u\u2225\u2264 1}.\nWe give examples of pairs of dual norms below.\n446", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 416, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7a7f1f85-bcdf-4975-98d7-943ddff9cc2c": {"__data__": {"id_": "7a7f1f85-bcdf-4975-98d7-943ddff9cc2c", "embedding": null, "metadata": {"page_label": "447", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8c7ddde4-4805-4527-8c8c-5c653cccca41", "node_type": "4", "metadata": {"page_label": "447", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "90e6c5bd6d05e4c8048f134e9c90cf01d1ef9ca16ee6e356a55a215a14897fca", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "18.2. Fenchel duality 447\nExample 18.1(Dual norm ofp-norms). The p-norm is defined by\n\u2225u\u2225p :=\n\uf8eb\n\uf8ed\nD\u2211\nj=1\n|uj|p\n\uf8f6\n\uf8f8\n1/p\n.\nIts dual is\u2225v\u2225q where q is such that1\np + 1\nq = 1. For instance, the\ndual norm of the2-norm is itself, since1\n2 + 1\n2 = 1. The1-norm and\nthe \u221e-norm are dual of each other, since1\n1 + 1\n\u221e= 1.\nThedefinitionofdualnormimpliesageneralizationof Cauchy\u2013Schwarz\u2019s\ninequality: for allu,v\u2208RD\n|\u27e8u,v\u27e9|\u2264\u2225 u\u2225\u2217\u2225v\u2225.\nSee, e.g., Beck (2017, Lemma 1.4).\nProposition 18.1(Conjugate of norms and squared norms). Weknow\nthat the conjugate of the support function is the indicator function.\nTherefore, iff(u) = \u2225u\u2225, then\nf\u2217(v) = \u03b9B\u2225\u00b7\u2225(v) =\n\uf8f1\n\uf8f2\n\uf8f3\n0 if \u2225v\u2225\u2217\u22641\n\u221e otherwise\n.\nOn the other hand, iff(u) = 1\n2 \u2225u\u22252, then\nf\u2217(v) = 1\n2\u2225v\u22252\n\u2217.\n18.2 Fenchel duality\nWe consider in this section standard objectives of the form\nmin\nw\u2208W\nL(w) := min\nw\u2208W\n\u2113(f(w)) + R(w),\nwhere f: W \u2192M, \u2113: M\u2192 R and R: W \u2192R. We first show that\nthe minimization of this objective, called theprimal, can be lower\nbounded by aconcavemaximization objective, called thedual, even\nif the primal is nonconvex.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1066, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1e25043c-a641-4f97-8e26-9f06b5b8d9fa": {"__data__": {"id_": "1e25043c-a641-4f97-8e26-9f06b5b8d9fa", "embedding": null, "metadata": {"page_label": "448", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "05a6188a-f8fb-4187-9dfb-c234789f9564", "node_type": "4", "metadata": {"page_label": "448", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "93eb63e4490a28d7ace7c20ac44c2c9bc1e431f89f30ba246a7e8521adb47bb0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "448 Duality\nProposition 18.2(Weak duality). Letf: W\u2192M (potentiallynon-\nlinear), \u2113: M\u2192 R (potentially nonconvex) andR: W\u2192 R (poten-\ntially nonconvex). Then\nmin\nw\u2208W\n\u2113(f(w)) + R(w) \u2265max\n\u03b1\u2208M\n\u2212Rf(\u03b1) \u2212\u2113\u2217(\u2212\u03b1),\nwhere we used the conjugate\n\u2113\u2217(\u2212\u03b1) := max\n\u03b8\u2208M\n\u27e8\u2212\u03b1,\u03b8\u27e9\u2212\u2113(\u03b8)\nand the \u201cgeneralized conjugate\u201d\nRf(\u03b1) := max\nw\u2208W\n\u27e8\u03b1,f(w)\u27e9\u2212R(w).\nMoreover, \u2113\u2217and Rf are both convex functions.\nWe emphasize that the result in Proposition 18.2 is fully general, in\nthe sense that it does not assume the linearity off or the convexity of\u2113\nand R. The caveat, of course, is thatRf and \u2113\u2217are difficult to compute\nin general, iff is nonlinear, and if\u2113 and R are nonconvex.\nProof.\nmin\nw\u2208W\n\u2113(f(w)) + R(w)\n= min\nw\u2208W\n\u03b8\u2208M\n\u2113(\u03b8) + R(w) s.t. \u03b8= f(w)\n= min\nw\u2208W\n\u03b8\u2208M\nmax\n\u03b1\u2208M\n\u2113(\u03b8) + R(w) + \u27e8\u03b1,\u03b8\u2212f(w)\u27e9\n\u2265max\n\u03b1\u2208M\nmin\nw\u2208W\n\u03b8\u2208M\n\u2113(\u03b8) + R(w) + \u27e8\u03b1,\u03b8\u2212f(w)\u27e9\n= max\n\u03b1\u2208M\nmin\nw\u2208W\n\u27e8\u03b1,\u2212f(w)\u27e9+ R(w) + min\n\u03b8\u2208M\n\u2113(\u03b8) + \u27e8\u03b1,\u03b8\u27e9\n= max\n\u03b1\u2208M\n\u2212max\nw\u2208W\n\u27e8\u03b1,f(w)\u27e9\u2212R(w) \u2212max\n\u03b8\u2208M\n\u27e8\u2212\u03b1,\u03b8\u27e9\u2212\u2113(\u03b8)\n= max\n\u03b1\u2208M\n\u2212Rf(\u03b1) \u2212\u2113\u2217(\u2212\u03b1).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 942, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4c33b4c6-d531-45d3-9fb7-934f3c7c895d": {"__data__": {"id_": "4c33b4c6-d531-45d3-9fb7-934f3c7c895d", "embedding": null, "metadata": {"page_label": "449", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "08def93a-0740-4196-8b6d-3559d0da9d33", "node_type": "4", "metadata": {"page_label": "449", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "8d10ee5716d1bda8659ef54940e828aa7a8be8990c34bda10dcca31250a7f9ea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "18.2. Fenchel duality 449\nIn the case whenf(w) = Aw, whereA is a linear map, and when\nboth \u2113 and R are convex, we can state a much stronger result.\nProposition 18.3(Strong duality). Let A be a linear map fromW\nto M. Let\u2113: M\u2192 R and R: W\u2192 R be convex functions. LetA\u2217\ndenote the adjoint ofA (Section 2.3). Then,\nmin\nw\u2208W\n\u2113(Aw) + R(w) = max\n\u03b1\u2208M\n\u2212R\u2217(A\u2217\u03b1) \u2212\u2113\u2217(\u2212\u03b1).\nFurthermore, the primal solution satisfies\nw\u22c6 \u2208arg max\nw\u2208W\n\u27e8A\u03b1\u22c6,w\u27e9\u2212R(w).\nWhen R is strictly convex, the primal solution is uniquely deter-\nmined by\nw\u22c6 = \u2207R\u2217(A\u2217\u03b1\u22c6).\nProof. Since f(w) = Aw, we have\nRf(\u03b1) := max\nw\u2208W\n\u27e8\u03b1,f(w)\u27e9\u2212R(w)\n= max\nw\u2208W\n\u27e8\u03b1,Aw\u27e9\u2212R(w)\n= max\nw\u2208W\n\u27e8A\u2217\u03b1,w\u27e9\u2212R(w)\n= R\u2217(A\u2217\u03b1).\nFurthermore, the inequality in the proof of Proposition 18.2 is an\nequality, since themin max is that of a convex-concave function.\nThe maximization problem in Proposition 18.3 is called theFenchel\ndual. By strong duality, the value of the maximum and the value of the\nminimum are equal. We can therefore choose to equivalently solve the\ndual instead of the primal. This can be advantageous when the space\nMis smaller thanW.\nWe now apply the Fenchel dual to obtain the dual of regularized\nmulticlass linear classification.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1164, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0c04b51a-76c3-45d9-967d-604901b52ff3": {"__data__": {"id_": "0c04b51a-76c3-45d9-967d-604901b52ff3", "embedding": null, "metadata": {"page_label": "450", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0ae15f81-7174-4fe1-8c56-a7b4c93b62ab", "node_type": "4", "metadata": {"page_label": "450", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "118f12678f6ab3895733ccd416b23027b36fe74a1db93ab3fa211dad2d39e3bf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "450 Duality\nTable 18.1:Examples of loss conjugates. For regression losses (squared, absolute),\nwhere yi \u2208RM, we defineti = \u03d5(yi) = yi. For classification losses (logistic, per-\nceptron, hinge), where yi \u2208[M], we define ti = \u03d5(yi) = eyi . To simplify some\nexpressions, we defined the change of variable\u00b5i := yi \u2212\u03b1i.\n\u2113i(\u03b8i) \u2113\u2217\ni(\u2212\u03b1i)\nSquared 1\n2 \u2225\u03b8i \u2212ti\u22252\n2\n1\n2 \u2225\u03b1i\u22252\n2 \u2212\u27e8ti,\u03b1i\u27e9\nAbsolute \u2225\u03b8i \u2212ti\u22251 \u03b9[\u22121,1]M(\u03b1i) \u2212\u27e8ti,\u03b1i\u27e9\nLogistic LSE(\u03b8i) \u2212\u27e8\u03b8,ti\u27e9 \u27e8 \u00b5i,log \u00b5i\u27e9+ \u03b9\u25b3M(\u00b5i)\nPerceptron maxi\u2208[M] \u03b8i \u2212\u03b8y \u03b9\u25b3M(\u00b5i)\nHinge maxi\u2208[M][i\u0338= y] + \u03b8i \u2212\u03b8y \u03b9\u25b3M(\u00b5i) \u2212\u27e81 \u2212ti,\u00b5i\u27e9\nExample 18.2(Sum of separable loss functions). When the loss is\n\u2113(\u03b8) := \u2211N\ni=1 \u2113i(\u03b8i), where\u03b8= Aw= (A1w,...,A Nw) \u2208MN and\nAi is a linear map fromWto M, we obtain\nmin\nw\u2208W\nN\u2211\ni=1\n\u2113i(Aiw) + R(w) = max\n\u03b1\u2208MN\n\u2212R(A\u2217\u03b1) \u2212\nN\u2211\ni=1\n\u2113\u2217\ni(\u2212\u03b1i),\nwhere A\u2217\u03b1= (A\u2217\n1\u03b11,...,A \u2217\nN\u03b1N). Typically, we define\nAiw:= Wxi,\nwhere W \u2208RM\u00d7D is a reshaped version ofw\u2208W, xi \u2208RD is a\ntraining sample, andM is the number of classes. In this case, we\nthen have\nA\u2217\ni\u03b1i = \u03b1ix\u22a4\ni .\nExamples of loss function conjugates are given in Table 18.1.\n18.3 Bregman divergences\nBregman divergences are a measure of difference between two points.\nDefinition 18.2(Bregman divergence). TheBregmandivergencegen-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1211, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "618d8338-5b0a-4602-ae38-6c166163bdd5": {"__data__": {"id_": "618d8338-5b0a-4602-ae38-6c166163bdd5", "embedding": null, "metadata": {"page_label": "451", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "20572619-d5f2-4e6b-a1f0-0cc11ec723f8", "node_type": "4", "metadata": {"page_label": "451", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "009f8959eb7c100870cf996071800293012bee026b62ab6ff14ff020f36fe77f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "18.3. Bregman divergences 451\nerated by a differentiable convex functionf: RD \u2192R is\nBf(u,v) := f(u) \u2212f(v) \u2212\u27e8\u2207f(v),u\u2212v\u27e9\n= \u27e8\u2207f(v),v\u27e9\u2212f(v) \u2212[\u27e8\u2207f(v),u\u27e9\u2212f(u)] ,\nwhere u,v\u2208dom(f).\nIntuitively, the Bregman divergence is the difference betweenf(u)\nand its linearization u \u21a6\u2192f(v) + \u27e8\u2207f(v),u\u2212v\u27e9around v. This is\nillustrated in Fig. 18.1.\nExample 18.3(Examples of Bregman divergences). Iff(u) = 1\n2 \u2225u\u22252\n2,\nwhere dom(f) = RD, then\nBf(u,v) = 1\n2\u2225u\u2212v\u22252\n2,\nthe squared Euclidean distance. If f(u) = \u27e8u,log u\u27e9, where\ndom(f) = RD\n+, then\nBf(u,v) =\nD\u2211\nj=1\nuj log uj\nvj\n\u2212\nD\u2211\nj=1\nuj +\nD\u2211\nj=1\nvj,\nthe (generalized)Kullback-Leibler divergence.\nProperties\nBregman divergences enjoy several useful properties.\nProposition 18.4(Properties of Bregman divergences). Letf: RD \u2192\nR be a differentiable convex function.\n1. Non-negativity: Bf(u,v) \u22650 for allu,v\u2208dom(f).\n2. Positivity: Bf(u,v) = 0 if and only ifu = v (when f is\nstrictly convex).\n3. Convexity:Bf(u,v) is convex inu.\n4. Dual-spaceform: Bf(u,v) = Bf\u2217(b,a),where b= \u2207f(v) \u2208", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1005, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8b939cfa-30d5-4075-96c5-5cee90f63843": {"__data__": {"id_": "8b939cfa-30d5-4075-96c5-5cee90f63843", "embedding": null, "metadata": {"page_label": "452", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54015bf9-880e-4cd0-93ac-1230cb28da6a", "node_type": "4", "metadata": {"page_label": "452", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "f942b0884a6f06702d86f7a194ad55c84e7c1a8399a6efd86fee3614a90d6372", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "452 Duality\nu\nDf (u, v)\nv\nf(u)\nf(v) +\u27e8\u2207f(v), u\u2212v\u27e9\nFigure 18.1: The Bregman divergence generated byf is the difference between\nf(u) and its linearization aroundv.\ndom(f\u2217) and a= \u2207f(u) \u2208dom(f\u2217).\nProof. The properties follow immediately from the convexity off(u).\n1. From Definition 15.6.\n2. From the unicity of minimizers.\n3. From the fact thatu\u21a6\u2192Bf(u,v) is the sum off(u) and a linear\nfunction ofu.\nThe Bregman divergence can be used to define natural generaliza-\ntions of the Euclidean projection and proximal operators, reviewed in\nSection 16.3 and Section 16.4.\nDefinition 18.3(Bregman proximal and projection operators). Letv\u2208\ndom(f). The Bregman proximal operator is\nbproxf,g(v) := arg min\nu\u2208dom(f)\u2229dom(g)\nBf(u,v) + g(u).\nIn particular, the Bregman projection ontoC\u2286 dom(f) is\nbprojf,C(v) := arg min\nu\u2208C\nBf(u,v).\nIt turns out that these operators are intimately connected to the\ngradient mapping of the convex conjugate.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 924, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f8968ce6-e56d-4ba0-a655-d32fdd66e1d7": {"__data__": {"id_": "f8968ce6-e56d-4ba0-a655-d32fdd66e1d7", "embedding": null, "metadata": {"page_label": "453", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8915d10-a1ee-48c0-8cca-ed30acdaf313", "node_type": "4", "metadata": {"page_label": "453", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "653da6c394cabf757255595f70df500565b216959b49d1e6fc0fbf02ffe152b5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "18.4. Fenchel-Young loss functions 453\nProposition 18.5(Link with conjugate\u2019s gradient). If \u2126 = f + g,\nthen for all\u03b8\u2208dom(f\u2217)\n\u2207\u2126\u2217(\u03b8) = bproxf,g(\u2207f\u2217(\u03b8)).\nIn particular, if\u2126 = f + \u03b9C, then for all\u03b8\u2208dom(f\u2217)\n\u2207\u2126\u2217(\u03b8) = bprojf,C(\u2207f\u2217(\u03b8)).\nWe give two examples below.\nExample 18.4(Bregman projections on the simplex). Iff(u) = 1\n2 \u2225u\u22252\n2,\nthen\nbprojf,\u25b3D(v) = arg min\nu\u2208\u25b3D\n1\n2\u2225u\u2212v\u22252\n2.\nIf f(u) = \u27e8u,log u\u22121\u27e9, then\nbprojf,\u25b3D(v) = arg min\nu\u2208RD\n+\nKL(u,v) = softmax(\u03b8),\nwhere v= \u2207f\u2217(\u03b8) = exp(\u03b8).\nTherefore, the softmax can be seen as a projection onto the proba-\nbility simplex in the Kullback-Leilbler divergence sense!\n18.4 Fenchel-Young loss functions\nWe end this chapter with a brief review of the Fenchel-Young family of\nloss functions (Blondelet al., 2020), which includes all loss functions in\nTable 18.1.\nDefinition 18.4(Fenchel-Young loss). TheFenchel-Younglossfunc-\ntion generated by\u2126 is\n\u2113\u2126(\u03b8,t) := \u2126\u2217(\u03b8) + \u2126(t) \u2212\u27e8\u03b8,t\u27e9\nwhere \u03b8\u2208dom(\u2126\u2217) and t\u2208dom(\u2126).\nTypically, we set\u03b8= f(x,w), wheref is a model prediction function\nwith parameterswand t= \u03d5(y), where\u03d5: Y\u2192 dom(\u2126). For instance,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1071, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f23fe794-3ea1-4871-a851-d6bcdccfc904": {"__data__": {"id_": "f23fe794-3ea1-4871-a851-d6bcdccfc904", "embedding": null, "metadata": {"page_label": "454", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "acddec9c-c458-4f07-a33e-8ea338cfbcd4", "node_type": "4", "metadata": {"page_label": "454", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "b387ad90c61b20057d1b06e5f9524a7f11777d07fea6b19c751368efc68b0e30", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "454 Duality\nsuppose we work with categorical outputsy \u2208[M]. Then, we can set\n\u03d5(y) = ey, whereey is the one-hot encoding ofy.\nThe important point to notice is that the Fenchel-Young loss is\ndefined over arguments inmixed spaces: \u03b8belongs to the dual space,\nwhile t belongs to the primal space. In fact, the Fenchel-Young loss\nis intimately connected to the Bregman divergence, sinceB\u2126(t,v) =\n\u2126\u2217(\u03b8) + \u2126(t) \u2212\u27e8\u03b8,t\u27e9, if we set \u03b8 = \u2207\u2126(v). The key properties of\nFenchel-Young loss functions are summarized below.\nProposition 18.6(Properties of Fenchel-Young loss functions).\n1. Non-negativity: \u2113\u2126(\u03b8,t) \u2265 0 for all \u03b8 \u2208 dom(\u2126\u2217) and\nt\u2208dom(\u2126).\n2. Positivity:\u2113\u2126(\u03b8,t) = 0 if and only if\u2207\u2126\u2217(\u03b8) = t, assuming\n\u2126 is strictly convex.\n3. Convexity:\u2113\u2126(\u03b8,t) is convex in\u03b8(regardless of\u2126) and int\n(if \u2126 is convex)\n4. Relation with composite Bregman divergence:\n0 \u2264 B\u2126(t,\u2207\u2126\u2217(\u03b8))\ued19 \ued18\ued17 \ued1a\npossibly nonconvex in\u03b8\n\u2264 \u2113\u2126(\u03b8,t)\ued19 \ued18\ued17\ued1a\nconvex in\u03b8\n.\nSee Blondelet al.(2020) for an in-depth study of more properties.\n18.5 Summary\n\u2022 The convex conjugate serves as a powerful abstraction inFenchal\nduality, decoupling the dual expression and function-specific\nterms.\n\u2022 The convex conjugate is also tightly connected toBregman\ndivergences and can be used to derive the family ofFenchel-\nYoung lossfunctions, which can be seen as primal-dual Bregman\ndivergences.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1320, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b69c638a-09ae-470d-9335-a9275df7f663": {"__data__": {"id_": "b69c638a-09ae-470d-9335-a9275df7f663", "embedding": null, "metadata": {"page_label": "455", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "283339f0-9ebc-4c0e-b353-0aeb23550b71", "node_type": "4", "metadata": {"page_label": "455", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "fdadd87e23b98b4e511ca4e3f8fcd7e4203d157a2621a3436e671b7f76f0709a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "References\nAbadi, M., A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.\nCorrado, A. Davis, J. Dean, M. Devin,et al.(2016). \u201cTensorflow:\nLarge-scale machine learning on heterogeneous distributed systems\u201d.\narXiv preprint arXiv:1603.04467.\nAbernethy, J., C. Lee, and A. Tewari. (2016). \u201cPerturbation techniques\nin online learning and optimization\u201d.Perturbations, Optimization,\nand Statistics. 233.\nAji, S. M. and R. J. McEliece. (2000). \u201cThe generalized distributive\nlaw\u201d. IEEE transactions on Information Theory. 46(2): 325\u2013343.\nAmari, S.-I. (1998). \u201cNatural gradient works efficiently in learning\u201d.\nNeural computation. 10(2): 251\u2013276.\nBa, J. L., J. R. Kiros, and G. E. Hinton. (2016). \u201cLayer normalization\u201d.\narXiv preprint arXiv:1607.06450.\nBach, F., R. Jenatton, J. Mairal, G. Obozinski,et al.(2012). \u201cOptimiza-\ntion with sparsity-inducing penalties\u201d.Foundations and Trends\u00ae in\nMachine Learning. 4(1): 1\u2013106.\nBall, K., E. A. Carlen, and E. H. Lieb. (2002). \u201cSharp uniform convexity\nand smoothness inequalities for trace norms\u201d.Inequalities: Selecta\nof Elliott H. Lieb: 171\u2013190.\nBall, W. W. R. (1960).A short account of the history of mathematics.\nCourier Corporation.\n455", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1180, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "74b3b6b6-cd87-4a47-97b2-b1509ebc0263": {"__data__": {"id_": "74b3b6b6-cd87-4a47-97b2-b1509ebc0263", "embedding": null, "metadata": {"page_label": "456", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d6cd164c-85e3-4923-949f-2bd6db4a671f", "node_type": "4", "metadata": {"page_label": "456", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "9e61bbcd4663f7882ad9ea0ed85d920324a95b4ac2d5f8953e419074d39644b5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "456 References\nBalog, M., N. Tripuraneni, Z. Ghahramani, and A. Weller. (2017). \u201cLost\nrelatives of the Gumbel trick\u201d. In:International Conference on\nMachine Learning. PMLR. 371\u2013379.\nBarndorff-Nielsen, O. (2014).Information and exponential families: in\nstatistical theory. John Wiley & Sons.\nBaston, R. A. and Y. Nakatsukasa. (2022). \u201cStochastic diagonal esti-\nmation: probabilistic bounds and an improved algorithm\u201d.arXiv\npreprint arXiv:2201.10684.\nBaum, L. E. and T. Petrie. (1966). \u201cStatistical inference for probabilistic\nfunctions of finite state Markov chains\u201d.The annals of mathematical\nstatistics. 37(6): 1554\u20131563.\nBaur, W. and V. Strassen. (1983). \u201cThe complexity of partial deriva-\ntives\u201d. Theoretical computer science. 22(3): 317\u2013330.\nBauschke Heinz, H. and L. Combettes Patrick. (2017).Convex Analysis\nand Monotone Operator Theory in Hilbert Spaces, 2011. 2nd ed.\n978\u20131.\nBaydin, A. G., B. A. Pearlmutter, A. A. Radul, and J. M. Siskind.\n(2018). \u201cAutomatic differentiation in machine learning: a survey\u201d.\nJournal of Marchine Learning Research. 18: 1\u201343.\nBaydin,A.G.,B.A.Pearlmutter,D.Syme,F.Wood,andP.Torr.(2022).\n\u201cGradientswithoutbackpropagation\u201d. arXiv preprint arXiv:2202.08587.\nBeck, A. (2017).First-order methods in optimization. SIAM.\nBeck, A. and M. Teboulle. (2012). \u201cSmoothing and first order methods:\nA unified framework\u201d.SIAM Journal on Optimization. 22(2): 557\u2013\n580.\nBecker, S. and Y. Le Cun. (1988). \u201cImproving the convergence of back-\npropagation learning with second order methods\u201d. In:Proceedings\nof the 1988 connectionist models summer school. 29\u201337.\nBekas, C., E. Kokiopoulou, and Y. Saad. (2007). \u201cAn estimator for the\ndiagonal of a matrix\u201d.Applied numerical mathematics. 57(11-12):\n1214\u20131229.\nBergstra, J., O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Des-\njardins, J. Turian, D. Warde-Farley, and Y. Bengio. (2010). \u201cTheano:\na CPU and GPU math expression compiler\u201d. In:Proceedings of the\nPython for scientific computing conference (SciPy). Vol. 4. No. 3.\nAustin, TX. 1\u20137.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2010, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "75e5c486-02a0-4be2-9301-0cf8fee69db9": {"__data__": {"id_": "75e5c486-02a0-4be2-9301-0cf8fee69db9", "embedding": null, "metadata": {"page_label": "457", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "19a07a60-813a-43f2-956a-e466ed8858d7", "node_type": "4", "metadata": {"page_label": "457", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "8a898a77a1590f274e02582ce7d1c141ad7f6fe601b14de8c32cb66cbfa83ede", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "References 457\nBerthet, Q., M. Blondel, O. Teboul, M. Cuturi, J.-P. Vert, and F. Bach.\n(2020). \u201cLearning with differentiable pertubed optimizers\u201d.Advances\nin neural information processing systems. 33: 9508\u20139519.\nBlelloch, G. E. (1989). \u201cScans as primitive parallel operations\u201d.IEEE\nTransactions on computers. 38(11): 1526\u20131538.\nBlondel, M. (2019). \u201cStructured prediction with projection oracles\u201d.\nAdvances in neural information processing systems. 32.\nBlondel, M., Q. Berthet, M. Cuturi, R. Frostig, S. Hoyer, F. Llinares-\nL\u00f3pez, F. Pedregosa, and J.-P. Vert. (2021). \u201cEfficient and Modular\nImplicit Differentiation\u201d.arXiv preprint arXiv:2105.15183.\nBlondel, M., A. F. Martins, and V. Niculae. (2020). \u201cLearning with\nfenchel-young losses\u201d.The Journal of Machine Learning Research.\n21(1): 1314\u20131382.\nBolte, J., R. Boustany, E. Pauwels, and B. Pesquet-Popescu. (2022).\n\u201cOn the complexity of nonsmooth automatic differentiation\u201d. In:\nThe Eleventh International Conference on Learning Representations.\nBolte, J. and E. Pauwels. (2020). \u201cA mathematical model for automatic\ndifferentiation in machine learning\u201d.Advances in Neural Information\nProcessing Systems. 33: 10809\u201310819.\nBotev, A., H. Ritter, and D. Barber. (2017). \u201cPractical Gauss-Newton\noptimisation for deep learning\u201d. In:International Conference on\nMachine Learning. 557\u2013565.\nBoumal, N. (2023).An introduction to optimization on smooth manifolds.\nCambridge University Press.\nBoyd, S. P. and L. Vandenberghe. (2004).Convex optimization. Cam-\nbridge university press.\nBradbury, J., R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D.\nMaclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-\nMilne, and Q. Zhang. (2018).JAX: composable transformations of\nPython+NumPy programs. Version 0.3.13.url: http://github.com/\ngoogle/jax.\nBraun, M. and M. Golubitsky. (1983).Differential equations and their\napplications. Vol. 2. Springer.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1888, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "41735202-8403-4ee3-82b1-fe5c0f64a350": {"__data__": {"id_": "41735202-8403-4ee3-82b1-fe5c0f64a350", "embedding": null, "metadata": {"page_label": "458", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "de795d82-d754-4141-ae66-3b88ff19a6ce", "node_type": "4", "metadata": {"page_label": "458", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "c690dd5a246bb21017fb7a604fc9b19b99f50787df4ecdcfad24f09190a2b7f0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "458 References\nBrockhoff, D., A. Auger, N. Hansen, D. V. Arnold, and T. Hohm.\n(2010). \u201cMirrored sampling and sequential selection for evolution\nstrategies\u201d. In: Parallel Problem Solving from Nature, PPSN XI:\n11th International Conference, Krak\u00f3w, Poland, September 11-15,\n2010, Proceedings, Part I 11. Springer. 11\u201321.\nBroyden, C. G. (1970). \u201cThe convergence of a class of double-rank\nminimization algorithms 1. general considerations\u201d.IMA Journal\nof Applied Mathematics. 6(1): 76\u201390.\nBrucker, P. (1984). \u201cAnO(n) algorithm for quadratic knapsack prob-\nlems\u201d. Operations Research Letters. 3(3): 163\u2013166.\nButcher, J. C. (2016). Numerical methods for ordinary differential\nequations. John Wiley & Sons.\nCajori, F. (1993).A history of mathematical notations. Vol. 1. Courier\nCorporation.\nC\u00e9a, J. (1986). \u201cConception optimale ou identification de formes, calcul\nrapide de la d\u00e9riv\u00e9e directionnelle de la fonction co\u00fbt\u201d.M2AN-\nMod\u00e9lisation math\u00e9matique et analyse num\u00e9rique. 20(3): 371\u2013402.\nChaudhuri, S. and A. Solar-Lezama. (2010). \u201cSmooth interpretation\u201d.\nACM Sigplan Notices. 45(6): 279\u2013291.\nChen, R. T., Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. (2018).\n\u201cNeural ordinary differential equations\u201d.Advances in neural infor-\nmation processing systems. 31.\nChen, X., N. Kayal, A. Wigderson,et al.(2011). \u201cPartial derivatives in\narithmetic complexity and beyond\u201d.Foundations and Trends\u00ae in\nTheoretical Computer Science. 6(1\u20132): 1\u2013138.\nClarke, F. H., Y. S. Ledyaev, R. J. Stern, and P. R. Wolenski. (2008).\nNonsmooth analysis and control theory. Vol. 178. Springer Science\n& Business Media.\nClarke, F. H. (1975). \u201cGeneralized gradients and applications\u201d.Trans-\nactions of the American Mathematical Society. 205: 247\u2013262.\nCohn, D. L. (2013).Measure theory. Vol. 5. Springer.\nCondat, L. (2016). \u201cFast projection onto the simplex and the\u21131 ball\u201d.\nMathematical Programming. 158(1-2): 575\u2013585.\nDalrymple, D. (2016).Differentiable Programming. url: https://www.\nedge.org/response-detail/26794.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1985, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "179626e4-bbca-4d32-8957-5ff747102dbd": {"__data__": {"id_": "179626e4-bbca-4d32-8957-5ff747102dbd", "embedding": null, "metadata": {"page_label": "459", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3e02a2b0-5f09-452a-be8d-7c639cdd6dd2", "node_type": "4", "metadata": {"page_label": "459", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "06b4a22f2c1a04732a9918ef84a04983271230bff6010df46d63a24f76d8743e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "References 459\nDangel, F., F. Kunstner, and P. Hennig. (2019). \u201cBackpack: Packing\nmore into backprop\u201d.arXiv preprint arXiv:1912.10985.\nDavis, J. Q., K. Choromanski, J. Varley, H. Lee, J.-J. Slotine, V.\nLikhosterov, A. Weller, A. Makadia, and V. Sindhwani. (2020).\n\u201cTime dependence in non-autonomous neural odes\u201d.arXiv preprint\narXiv:2005.01906.\nDeGroot, M. H. (1962). \u201cUncertainty, information, and sequential ex-\nperiments\u201d. The Annals of Mathematical Statistics. 33(2): 404\u2013419.\nDehghani, M., J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer,\nA. P. Steiner, M. Caron, R. Geirhos, I. Alabdulmohsin,et al.(2023).\n\u201cScaling vision transformers to 22 billion parameters\u201d. In:Interna-\ntional Conference on Machine Learning. PMLR. 7480\u20137512.\nDeisenroth, M. P., A. A. Faisal, and C. S. Ong. (2020).Mathematics\nfor machine learning. Cambridge University Press.\nDevlin, J., M.-W. Chang, K. Lee, and K. Toutanova. (2019). \u201cBERT:\nPre-training of deep bidirectional transformers for language un-\nderstanding\u201d. In:Proceedings of the 2019 conference of the North\nAmerican chapter of the association for computational linguistics:\nhuman language technologies, volume 1 (long and short papers).\n4171\u20134186.\nDrusvyatskiy, D. and C. Paquette. (2019). \u201cEfficiency of minimizing\ncompositions of convex functions and smooth maps\u201d.Mathematical\nProgramming. 178: 503\u2013558.\nDuchi, J. C., M. I. Jordan, M. J. Wainwright, and A. Wibisono. (2015).\n\u201cOptimal rates for zero-order convex optimization: The power of two\nfunction evaluations\u201d.IEEE Transactions on Information Theory.\n61(5): 2788\u20132806.\nDuchi, J. C., S. Shalev-Shwartz, Y. Singer, and T. Chandra. (2008).\n\u201cEfficientprojectionsontothe \u21131-ballforlearninginhighdimensions\u201d.\nIn: Proc. of ICML.\nDufter, P., M. Schmitt, and H. Sch\u00fctze. (2022). \u201cPosition information\nin transformers: An overview\u201d.Computational Linguistics. 48(3):\n733\u2013763.\nEisner, J. (2016). \u201cInside-outside and forward-backward algorithms are\njust backprop (tutorial paper)\u201d. In:Proceedings of the Workshop on\nStructured Prediction for NLP. 1\u201317.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2048, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "561d0bf1-5365-4a3d-b7f1-ff1d41334ae6": {"__data__": {"id_": "561d0bf1-5365-4a3d-b7f1-ff1d41334ae6", "embedding": null, "metadata": {"page_label": "460", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "348691c7-171a-4395-a2d2-a0b9bd3685b7", "node_type": "4", "metadata": {"page_label": "460", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "e00ca1414fd5cc75036416f115842dd360cfa72b9ee5e5248e137b4c4cb460ac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "460 References\nElsayed, M. and A. R. Mahmood. (2022). \u201cHesScale: Scalable Compu-\ntation of Hessian Diagonals\u201d.arXiv preprint arXiv:2210.11639.\nEpperly, E. N., J. A. Tropp, and R. J. Webber. (2023). \u201cXTrace: Making\nthe most of every sample in stochastic trace estimation\u201d.arXiv\npreprint arXiv:2301.07825.\nFarinhas, A., W. Aziz, V. Niculae, and A. F. Martins. (2021). \u201cSparse\ncommunicationviamixeddistributions\u201d. arXiv preprint arXiv:2108.02658.\nFlanders, H. (1973). \u201cDifferentiation under the integral sign\u201d.The\nAmerican Mathematical Monthly. 80(6): 615\u2013627.\nFleming, W. H. and R. W. Rishel. (2012).Deterministic and stochastic\noptimal control. Vol. 1. Springer Science & Business Media.\nFletcher, R. (1970). \u201cA new approach to variable metric algorithms\u201d.\nThe computer journal. 13(3): 317\u2013322.\nFoerster, J., G. Farquhar, M. Al-Shedivat, T. Rockt\u00e4schel, E. Xing,\nand S. Whiteson. (2018). \u201cDice: The infinitely differentiable monte\ncarlo estimator\u201d. In:International Conference on Machine Learning.\nPMLR. 1529\u20131538.\nForney, G. D. (1973). \u201cThe viterbi algorithm\u201d.Proceedings of the IEEE.\n61(3): 268\u2013278.\nFranceschi, L., M. Donini, P. Frasconi, and M. Pontil. (2017). \u201cFor-\nward and reverse gradient-based hyperparameter optimization\u201d. In:\nInternational Conference on Machine Learning. PMLR. 1165\u20131173.\nFrey, B. J., F. R. Kschischang, H.-A. Loeliger, and N. Wiberg. (1997).\n\u201cFactor graphs and algorithms\u201d. In:Proceedings of the Annual Aller-\nton Conference on Communication Control and Computing. Vol. 35.\nCiteseer. 666\u2013680.\nFrigyik, B. A., S. Srivastava, and M. R. Gupta. (2008). \u201cAn introduction\nto functional derivatives\u201d.Dept. Electr. Eng., Univ. Washington,\nSeattle, WA, Tech. Rep. 1.\nFrostig, R., M. J. Johnson, D. Maclaurin, A. Paszke, and A. Radul.\n(2021).\u201cDecomposingreverse-modeautomaticdifferentiation\u201d. arXiv\npreprint arXiv:2105.09469.\nGautschi, W. (2011).Numerical analysis. Springer Science & Business\nMedia.\nGetreuer, P. (2013). \u201cA survey of Gaussian convolution algorithms\u201d.\nImage Processing On Line. 2013: 286\u2013310.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2027, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0ef7f2d6-1601-45b3-aa28-6cd0502a60cd": {"__data__": {"id_": "0ef7f2d6-1601-45b3-aa28-6cd0502a60cd", "embedding": null, "metadata": {"page_label": "461", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "769ae878-bd45-4a88-b877-e317e42087b9", "node_type": "4", "metadata": {"page_label": "461", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "12acd473baef0236b445e7e5d0f65b07d1235249cc1a6e7fb11de27d97fe30b0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "References 461\nGeweke, J. (1988). \u201cAntithetic acceleration of Monte Carlo integration\nin Bayesian inference\u201d.Journal of Econometrics. 38(1-2): 73\u201389.\nGholaminejad, A., K. Keutzer, and G. Biros. (2019). \u201cANODE: Uncon-\nditionally Accurate Memory-Efficient Gradients for Neural ODEs\u201d.\nIn: International Joint Conferences on Artificial Intelligence.\nGini, C. (1912). \u201cVariabilit\u00e0 e mutabilit\u00e0\u201d.Reprinted in Memorie di\nmetodologica statistica (Ed. Pizetti E, Salvemini, T). Rome: Libreria\nEredi Virgilio Veschi.\nGirard, A. (1989). \u201cA fast \u2018Monte-Carlo cross-validation\u2019procedure for\nlarge least squares problems with noisy data\u201d.Numerische Mathe-\nmatik. 56: 1\u201323.\nGoldfarb, D. (1970). \u201cA family of variable-metric methods derived by\nvariational means\u201d.Mathematics of computation. 24(109): 23\u201326.\nGomez, A. N., M. Ren, R. Urtasun, and R. B. Grosse. (2017). \u201cThe\nreversible residual network: Backpropagation without storing acti-\nvations\u201d. Advances in neural information processing systems. 30.\nGraves, A., G. Wayne, and I. Danihelka. (2014). \u201cNeural turing ma-\nchines\u201d. arXiv preprint arXiv:1410.5401.\nGreig, D. M., B. T. Porteous, and A. H. Seheult. (1989). \u201cExact max-\nimum a posteriori estimation for binary images\u201d.Journal of the\nRoyal Statistical Society Series B: Statistical Methodology. 51(2):\n271\u2013279.\nGriewank,A.(1992).\u201cAchievinglogarithmicgrowthoftemporalandspa-\ntial complexity in reverse automatic differentiation\u201d.Optimization\nMethods and Software. 1(1): 35\u201354.doi: 10.1080/10556789208805505.\nGriewank, A. (2003). \u201cA mathematical view of automatic differentia-\ntion\u201d. Acta Numerica. 12: 321\u2013398.\nGriewank, A. (2012). \u201cWho invented the reverse mode of differentiation\u201d.\nDocumenta Mathematica, Extra Volume ISMP. 389400.\nGriewank, A. and A. Walther. (2008).Evaluating derivatives: principles\nand techniques of algorithmic differentiation. SIAM.\nGrimm, J., L. Pottier, and N. Rostaing-Schmidt. (1996). \u201cOptimal time\nand minimum space-time product for reversing a certain class of\nprograms\u201d. PhD thesis. INRIA.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2014, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1301251c-b25d-4b01-a1c3-a80484dfe91f": {"__data__": {"id_": "1301251c-b25d-4b01-a1c3-a80484dfe91f", "embedding": null, "metadata": {"page_label": "462", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "86b4e1c4-9966-42e4-9878-2f7b4e96c0d8", "node_type": "4", "metadata": {"page_label": "462", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "a29a34901c8207d052f2f5a6e3a7a379726f999aa77b02ff4c1cecdb4f2fe4a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "462 References\nGr\u00fcnwald, P. D. and A. P. Dawid. (2004). \u201cGame theory, maximum\nentropy, minimum discrepancy and robust Bayesian decision theory\u201d.\nAnnals of Statistics: 1367\u20131433.\nHallman, E., I. C. Ipsen, and A. K. Saibaba. (2023). \u201cMonte Carlo\nmethods for estimating the diagonal of a real symmetric matrix\u201d.\nSIAM Journal on Matrix Analysis and Applications. 44(1): 240\u2013269.\nHe, K., X. Zhang, S. Ren, and J. Sun. (2016). \u201cDeep residual learning\nfor image recognition\u201d. In:Proceedings of the IEEE conference on\ncomputer vision and pattern recognition. 770\u2013778.\nHelfrich, K., D. Willmott, and Q. Ye. (2018). \u201cOrthogonal recurrent\nneural networks with scaled Cayley transform\u201d. In:International\nConference on Machine Learning. PMLR. 1969\u20131978.\nHestenes, M. R., E. Stiefel,et al.(1952). Methods of conjugate gradients\nfor solving linear systems. Vol. 49. No. 1. NBS Washington, DC.\nHewitt, E. (1948). \u201cRings of real-valued continuous functions. I\u201d.Trans-\nactions of the American Mathematical Society. 64(1): 45\u201399.\nHida, T. and M. Hitsuda. (1976).Gaussian processes. Vol. 120. American\nMathematical Soc.\nHiriart-Urruty, J.-B. and C. Lemar\u00e9chal. (1993).Convex analysis and\nminimization algorithms II. Vol. 305. Springer science & business\nmedia.\nHutchinson, M. F. (1989). \u201cA stochastic estimator of the trace of the\ninfluence matrix for Laplacian smoothing splines\u201d.Communications\nin Statistics-Simulation and Computation. 18(3): 1059\u20131076.\nImai, T. (2019).Where did \u201cdifferentiable programming\u201d come from?\nurl: https://medium.com/@bonotake/where-did-differentiable-\nprogramming-come-from-27b385fb6d6d.\nIoffe, S. and C. Szegedy. (2015). \u201cBatch normalization: Accelerating\ndeep network training by reducing internal covariate shift\u201d. In:\nInternational conference on machine learning. pmlr. 448\u2013456.\nJaggi, M. (2013). \u201cRevisiting Frank-Wolfe: Projection-free sparse convex\noptimization\u201d. In: International conference on machine learning.\nPMLR. 427\u2013435.\nJang, E., S. Gu, and B. Poole. (2016). \u201cCategorical reparameterization\nwith gumbel-softmax\u201d.arXiv preprint arXiv:1611.01144.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2070, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b6ba6624-0890-47c9-89d6-1bbbde8b0928": {"__data__": {"id_": "b6ba6624-0890-47c9-89d6-1bbbde8b0928", "embedding": null, "metadata": {"page_label": "463", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1e37c8a0-d7b3-4149-bd29-98a638595110", "node_type": "4", "metadata": {"page_label": "463", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "d21b5d4a4d26ff782e50bb4849b16ffb5c58325a20e36e5f055e3aac84275b76", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "References 463\nJayaram, B. and M. Baczynski. (2008).Fuzzy Implications. Vol. 231.\nSpringer Science & Business Media.\nKakade, S., S. Shalev-Shwartz, A. Tewari,et al.(2009). \u201cOn the duality\nof strong convexity and strong smoothness: Learning appl ications\nand matrix regularization\u201d.Tech report. 2(1): 35.\nKarpathy, A. (2017). \u201cSoftware 2.0\u201d.\nKelley, C. T. (1995).Iterative methods for linear and nonlinear equations.\nSIAM.\nKingma, D. P. and J. Ba. (2014). \u201cAdam: A method for stochastic\noptimization\u201d. arXiv preprint arXiv:1412.6980.\nKingma, D. P. and M. Welling. (2013). \u201cAuto-encoding variational\nbayes\u201d. arXiv preprint arXiv:1312.6114.\nKlir, G. and B. Yuan. (1995).Fuzzy sets and fuzzy logic. Vol. 4. Prentice\nhall New Jersey.\nKobyzev, I., S. Prince, and M. A. Brubaker. (2019). \u201cNormalizing flows:\nIntroduction and ideas\u201d.stat. 1050: 25.\nKreikemeyer, J. N. and P. Andelfinger. (2023). \u201cSmoothing methods\nfor automatic differentiation across conditional branches\u201d.IEEE\nAccess.\nKrieken, E., J. Tomczak, and A. Ten Teije. (2021). \u201cStorchastic: A frame-\nwork for general stochastic automatic differentiation\u201d.Advances in\nNeural Information Processing Systems. 34: 7574\u20137587.\nKunstner, F., P. Hennig, and L. Balles. (2019). \u201cLimitations of the em-\npirical Fisher approximation for natural gradient descent\u201d.Advances\nin neural information processing systems. 32.\nLafferty, J., A. McCallum, and F. C. Pereira. (2001). \u201cConditional\nrandom fields: Probabilistic models for segmenting and labeling\nsequence data\u201d.\nLan, G. (2012). \u201cAn optimal method for stochastic composite optimiza-\ntion\u201d. Mathematical Programming. 133(1-2): 365\u2013397.\nLeCun, Y. (1988). \u201cA theoretical framework for back-propagation\u201d. In:\nProceedings of the 1988 connectionist models summer school. Vol. 1.\n21\u201328.\nLeCun, Y. (2018). \u201cDeep Learning est mort. Vive Differentiable Pro-\ngramming!\u201d", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1852, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "928f207d-4d47-4c97-8a29-442612f2a7d9": {"__data__": {"id_": "928f207d-4d47-4c97-8a29-442612f2a7d9", "embedding": null, "metadata": {"page_label": "464", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b5e04a0c-f2a5-4941-9ce8-96ee6694e915", "node_type": "4", "metadata": {"page_label": "464", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "b4bb45109afcb67bc57b71d9f29cce7aaf34117bc8e54b7d425a36817d3b2c48", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "464 References\nLevenberg, K. (1944). \u201cA method for the solution of certain non-linear\nproblems in least squares\u201d.Quarterly of applied mathematics. 2(2):\n164\u2013168.\nLiu, D. C. and J. Nocedal. (1989). \u201cOn the limited memory method for\nlarge scale optimization\u201d.Mathematical Programming. 45: 503\u2013528.\nLiu, H., Z. Li, D. Hall, P. Liang, and T. Ma. (2023). \u201cSophia: A Scal-\nable Stochastic Second-order Optimizer for Language Model Pre-\ntraining\u201d. arXiv preprint arXiv:2305.14342.\nLoeliger, H.-A. (2004). \u201cAn introduction to factor graphs\u201d.IEEE Signal\nProcessing Magazine. 21(1): 28\u201341.\nLoshchilov, I. and F. Hutter. (2016). \u201cSGDR: Stochastic gradient de-\nscent with warm restarts\u201d. In:International Conference on Learning\nRepresentations.\nLucet, Y. (1997). \u201cFaster than the fast Legendre transform, the linear-\ntime Legendre transform\u201d.Numerical Algorithms. 16: 171\u2013185.\nMaclaurin, D., D. Duvenaud, and R. P. Adams. (2015). \u201cAutograd:\nEffortless gradients in numpy\u201d. In:ICML 2015 AutoML workshop.\nVol. 238. No. 5.\nMaddison, C. J., A. Mnih, and Y. W. Teh. (2016). \u201cThe concrete\ndistribution: A continuous relaxation of discrete random variables\u201d.\narXiv preprint arXiv:1611.00712.\nMarquardt, D. W. (1963). \u201cAn algorithm for least-squares estimation\nof nonlinear parameters\u201d.Journal of the society for Industrial and\nApplied Mathematics. 11(2): 431\u2013441.\nMartens, J. (2020). \u201cNew insights and perspectives on the natural\ngradient method\u201d. Journal of Machine Learning Research. 21(1):\n5776\u20135851.\nMartens, J. and R. Grosse. (2015). \u201cOptimizing neural networks with\nKronecker-factored approximate curvature\u201d. In:International con-\nference on machine learning. 2408\u20132417.\nMartins, A. and R. Astudillo. (2016). \u201cFrom softmax to sparsemax: A\nsparse model of attention and multi-label classification\u201d. In:Inter-\nnational conference on machine learning. PMLR. 1614\u20131623.\nMartins, J. R., P. Sturdza, and J. J. Alonso. (2003). \u201cThe complex-\nstep derivative approximation\u201d.ACM Transactions on Mathematical\nSoftware (TOMS). 29(3): 245\u2013262.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2017, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ecfdf064-b916-4d25-8173-465c8db9a456": {"__data__": {"id_": "ecfdf064-b916-4d25-8173-465c8db9a456", "embedding": null, "metadata": {"page_label": "465", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5697515d-9a6e-4ad4-b720-f916cf9ebc9b", "node_type": "4", "metadata": {"page_label": "465", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "4dc7027bc92a582cd0b493270c05318cfb08f930733b6a5f75177014d32fa95a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "References 465\nMeent, J.-W. van de, B. Paige, H. Yang, and F. Wood. (2018). \u201cAn intro-\nductiontoprobabilisticprogramming\u201d. arXiv preprint arXiv:1809.10756.\nMensch, A. and M. Blondel. (2018). \u201cDifferentiable dynamic program-\nming for structured prediction and attention\u201d. In:International\nConference on Machine Learning. PMLR. 3462\u20133471.\nMesserer, F., K. Baumg\u00e4rtner, and M. Diehl. (2021). \u201cSurvey of sequen-\ntial convex programming and generalized Gauss-Newton methods\u201d.\nESAIM: Proceedings and Surveys. 71: 64\u201388.\nMeyer, R. A., C. Musco, C. Musco, and D. P. Woodruff. (2021).\n\u201cHutch++: Optimal stochastic trace estimation\u201d. In:Symposium on\nSimplicity in Algorithms (SOSA). SIAM. 142\u2013155.\nMichelot, C. (1986). \u201cA finite algorithm for finding the projection of a\npoint onto the canonical simplex ofRn\u201d. Journal of Optimization\nTheory and Applications. 50(1): 195\u2013200.\nMohamed, S., M. Rosca, M. Figurnov, and A. Mnih. (2020). \u201cMonte\ncarlo gradient estimation in machine learning\u201d.The Journal of\nMachine Learning Research. 21(1): 5183\u20135244.\nMohri, M., F. Pereira, and M. Riley. (2008). \u201cSpeech recognition with\nweighted finite-state transducers\u201d. Springer Handbook of Speech\nProcessing: 559\u2013584.\nMorgenstern, J. (1985). \u201cHow to compute fast a function and all its\nderivatives: A variation on the theorem of Baur-Strassen\u201d.ACM\nSIGACT News. 16(4): 60\u201362.\nMorrey Jr, C. B. (2009).Multiple integrals in the calculus of variations.\nSpringer Science & Business Media.\nMurphy, K. P. (2022).Probabilistic Machine Learning: An introduction.\nMIT Press.url: http://probml.github.io/book1.\nMurphy, K. P. (2023).Probabilistic Machine Learning: Advanced Topics.\nMIT Press.url: http://probml.github.io/book2.\nMutze, U. (2013). \u201cAn asynchronous leapfrog method II\u201d.arXiv preprint\narXiv:1311.6602.\nNemirovski, A. and D. Yudin. (1983). \u201cProblem complexity and method\nefficiency in optimization\u201d.\nNesterov, Y. (2005). \u201cSmooth minimization of non-smooth functions\u201d.\nMathematical programming. 103: 127\u2013152.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1981, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3f40aca0-e07f-42e9-9230-9131904cc449": {"__data__": {"id_": "3f40aca0-e07f-42e9-9230-9131904cc449", "embedding": null, "metadata": {"page_label": "466", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7e619656-f68b-4ef8-8f07-4bda71a79a41", "node_type": "4", "metadata": {"page_label": "466", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "7ecfe57161c147cc3a16fdf56a6348b45637f6abd739c67b210d316d6d823b0f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "466 References\nNesterov, Y. (2007). \u201cModified Gauss\u2013Newton scheme with worst case\nguarantees for global performance\u201d.Optimisation methods and soft-\nware. 22(3): 469\u2013483.\nNesterov, Y. (2018).Lectures on convex optimization. Vol. 137. Springer.\nNesterov, Y. and V. Spokoiny. (2017). \u201cRandom gradient-free minimiza-\ntion of convex functions\u201d.Foundations of Computational Mathemat-\nics. 17: 527\u2013566.\nOlah, C. (2015).Neural networks, types, and functional programming.\nurl: https://colah.github.io/posts/2015-09-NN-Types-FP/.\nPapamakarios, G., E. Nalisnick, D. J. Rezende, S. Mohamed, and\nB. Lakshminarayanan. (2021). \u201cNormalizing flows for probabilistic\nmodeling and inference\u201d.The Journal of Machine Learning Research.\n22(1): 2617\u20132680.\nParikh, N., S. Boyd,et al.(2014). \u201cProximal algorithms\u201d.Foundations\nand trends\u00ae in Optimization. 1(3): 127\u2013239.\nPaszke, A., S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T.\nKilleen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E.\nYang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,\nL. Fang, J. Bai, and S. Chintala. (2019). \u201cPyTorch: An Imperative\nStyle, High-Performance Deep Learning Library\u201d. In:Advances in\nNeural Information Processing Systems 32. 8024\u20138035.\nPaulus, M., D. Choi, D. Tarlow, A. Krause, and C. J. Maddison. (2020).\n\u201cGradient estimation with stochastic softmax tricks\u201d.Advances in\nNeural Information Processing Systems. 33: 5691\u20135704.\nPetersen, F., C. Borgelt, H. Kuehne, and O. Deussen. (2021). \u201cLearning\nwith algorithmic supervision via continuous relaxations\u201d.Advances\nin Neural Information Processing Systems. 34: 16520\u201316531.\nPeyr\u00e9, G. (2020). \u201cMathematical foundations of data sciences\u201d.Rn. 1:\n2.\nPeyr\u00e9, G. and M. Cuturi. (2019). \u201cComputational optimal transport:\nWith applications to data science\u201d.Foundations and Trends\u00ae in\nMachine Learning. 11(5-6): 355\u2013607.\nPlotkin, G. (2018).Some Principles of Differential Programming Lan-\nguages. url: https://popl18.sigplan.org/details/POPL-2018-papers/\n76/Some-Principles-of-Differential-Programming-Languages.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2048, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6751dfd9-296f-4efe-9fb3-f43f60545431": {"__data__": {"id_": "6751dfd9-296f-4efe-9fb3-f43f60545431", "embedding": null, "metadata": {"page_label": "467", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0f34d5fa-b64d-480d-a677-3f8b015c5922", "node_type": "4", "metadata": {"page_label": "467", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "4e79899cea3fa5993c104e6473ffc92cdbb62abb13c5bc300a7198526ea7e27d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "References 467\nPollock, S. and L. G. Rebholz. (2021). \u201cAnderson acceleration for con-\ntractive and noncontractive operators\u201d.IMA Journal of Numerical\nAnalysis. 41(4): 2841\u20132872.\nPolyak, B. (1963). \u201cGradient methods for the minimisation of function-\nals\u201d. USSR Computational Mathematics and Mathematical Physics.\n3(4): 864\u2013878.\nPolyak, B. T. (1964). \u201cSome methods of speeding up the convergence\nof iteration methods\u201d.Ussr computational mathematics and mathe-\nmatical physics. 4(5): 1\u201317.\nPontryagin, L. S. (1985). \u201cThe mathematical theory of optimal processes\nand differential games\u201d.Trudy Mat. Inst. Steklov. 169: 119\u2013158.\nPress, O. and L. Wolf. (2016). \u201cUsing the output embedding to improve\nlanguage models\u201d.arXiv preprint arXiv:1608.05859.\nRabiner, L. R. (1989). \u201cA tutorial on hidden Markov models and selected\napplications in speech recognition\u201d.Proceedings of the IEEE. 77(2):\n257\u2013286.\nRademacher, H. (1919). \u201c\u00dcber partielle und totale differenzierbarkeit\nvon Funktionen mehrerer Variabeln und \u00fcber die Transformation\nder Doppelintegrale\u201d.Mathematische Annalen. 79(4): 340\u2013359.\nRadul, A., A. Paszke, R. Frostig, M. Johnson, and D. Maclaurin. (2022).\n\u201cYou only linearize once: Tangents transpose to gradients\u201d.arXiv\npreprint arXiv:2204.10923.\nRahimi, A. and B. Recht. (2007). \u201cRandom features for large-scale kernel\nmachines\u201d. Advances in neural information processing systems. 20.\nRecht, B. (2016). \u201cMates of Costate\u201d.\nRecht, B. and R. Frostig. (2017). \u201cNesterov\u2019s Punctuated Equilibrium\u201d.\nRezende, D. J., S. Mohamed, and D. Wierstra. (2014). \u201cStochastic back-\npropagation and approximate inference in deep generative models\u201d.\nIn: International conference on machine learning. PMLR. 1278\u2013\n1286.\nRockafellar, R. T. and R. J.-B. Wets. (2009). Variational analysis.\nVol. 317. Springer Science & Business Media.\nRodriguez, O. H. and J. M. Lopez Fernandez. (2010). \u201cA semiotic\nreflection on the didactics of the chain rule\u201d.The Mathematics\nEnthusiast. 7(2): 321\u2013332.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1967, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3a3c6404-e4cd-48db-a8ac-4518839b8b72": {"__data__": {"id_": "3a3c6404-e4cd-48db-a8ac-4518839b8b72", "embedding": null, "metadata": {"page_label": "468", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "44a84d35-c13e-4ce0-baea-29dbc0944781", "node_type": "4", "metadata": {"page_label": "468", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "46ed249b106f3fc5ea02677c40e19402bf420d353f5ee74de3ea551283bf8570", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "468 References\nRoulet, V. and Z. Harchaoui. (2022). \u201cDifferentiable programming \u00e0 la\nMoreau\u201d. In:ICASSP 2022-2022 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP). IEEE. 3498\u2013\n3502.\nSaad, Y. and M. H. Schultz. (1986). \u201cGMRES: A generalized minimal\nresidual algorithm for solving nonsymmetric linear systems\u201d.SIAM\nJournal on scientific and statistical computing. 7(3): 856\u2013869.\nSalimans, T., J. Ho, X. Chen, S. Sidor, and I. Sutskever. (2017). \u201cEvo-\nlution strategies as a scalable alternative to reinforcement learning\u201d.\narXiv preprint arXiv:1703.03864.\nSander, M. E., P. Ablin, M. Blondel, and G. Peyr\u00e9. (2021a). \u201cMomentum\nresidual neural networks\u201d. In:International Conference on Machine\nLearning. PMLR. 9276\u20139287.\nSander, M. E., P. Ablin, M. Blondel, and G. Peyr\u00e9. (2021b). \u201cMomentum\nresidual neural networks\u201d. In:International Conference on Machine\nLearning. PMLR. 9276\u20139287.\nSatterthwaite, F. (1942). \u201cGeneralized poisson distribution\u201d.The Annals\nof Mathematical Statistics. 13(4): 410\u2013417.\nSchlag, I., K. Irie, and J. Schmidhuber. (2021). \u201cLinear transformers\nare secretly fast weight programmers\u201d. In:International Conference\non Machine Learning. PMLR. 9355\u20139366.\nSch\u00f6lkopf, B. and A. J. Smola. (2002).Learning with kernels: support\nvector machines, regularization, optimization, and beyond. MIT\npress.\nSchulman, J., N. Heess, T. Weber, and P. Abbeel. (2015). \u201cGradient\nestimation using stochastic computation graphs\u201d.Advances in neural\ninformation processing systems. 28.\nSchwartz, J. (1954). \u201cThe formula for change in variables in a multiple\nintegral\u201d. The American Mathematical Monthly. 61(2): 81\u201385.\nSchwarz, H. (1873). \u201cCommunication\u201d.Archives des Sciences Physiques\net Naturelles. 48: 38\u201344.\nSengupta, S., M. J. Harris, M. Garland, and J. D. Owens. (2010).\n\u201cEfficient Parallel Scan Algorithms for Manycore GPUs.\u201d\nShanno, D. F. (1970). \u201cConditioning of quasi-Newton methods for\nfunction minimization\u201d.Mathematics of computation. 24(111): 647\u2013\n656.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1993, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "30d7795b-a2a7-4f91-a408-0a4d3c4b009d": {"__data__": {"id_": "30d7795b-a2a7-4f91-a408-0a4d3c4b009d", "embedding": null, "metadata": {"page_label": "469", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f5ff7f61-8596-45b4-8c6c-16a7d43ce2c6", "node_type": "4", "metadata": {"page_label": "469", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "d063dc2904cd01671a6de3137e288a1c8a4d8fb56e8cebf674bd61f0a9a32efb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "References 469\nShannon, C. E. (1948). \u201cA mathematical theory of communication\u201d.\nThe Bell system technical journal. 27(3): 379\u2013423.\nShawe-Taylor, J. and N. Cristianini. (2004).Kernel methods for pattern\nanalysis. Cambridge university press.\nSquire, W. and G. Trapp. (1998). \u201cUsing complex variables to estimate\nderivatives of real functions\u201d.SIAM review. 40(1): 110\u2013112.\nStoer, J., R. Bulirsch, R. Bartels, W. Gautschi, and C. Witzgall. (1980).\nIntroduction to numerical analysis. Vol. 1993. Springer.\nStumm, P. and A. Walther. (2010). \u201cNew algorithms for optimal online\ncheckpointing\u201d. SIAM Journal on Scientific Computing. 32(2): 836\u2013\n854.\nSu, J., M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. (2024). \u201cRo-\nformer: Enhanced transformer with rotary position embedding\u201d.\nNeurocomputing. 568: 127063.\nSutherland, D. J. and J. Schneider. (2015). \u201cOn the error of random\nFourier features\u201d.arXiv preprint arXiv:1506.02785.\nSutskever, I., J. Martens, G. Dahl, and G. Hinton. (2013). \u201cOn the\nimportance of initialization and momentum in deep learning\u201d. In:\nInternational conference on machine learning. PMLR. 1139\u20131147.\nSutton, C., A. McCallum,et al.(2012). \u201cAn introduction to conditional\nrandom fields\u201d.Foundations and Trends\u00ae in Machine Learning. 4(4):\n267\u2013373.\nSutton, R. S., D. McAllester, S. Singh, and Y. Mansour. (1999). \u201cPolicy\ngradient methods for reinforcement learning with function approxi-\nmation\u201d. Advances in neural information processing systems. 12.\nTaylor, M. (2002). \u201cDifferential forms and the change of variable for-\nmula for multiple integrals\u201d.Journal of mathematical analysis and\napplications. 268(1): 378\u2013383.\nTibshirani, R. (1996). \u201cRegression shrinkage and selection via the lasso\u201d.\nJournal of the Royal Statistical Society: Series B (Methodological).\n58(1): 267\u2013288.\nTignol, J.-P. (2015).Galois\u2019 theory of algebraic equations. World Scien-\ntific Publishing Company.\nTsallis, C. (1988). \u201cPossible generalization of Boltzmann-Gibbs statis-\ntics\u201d. Journal of statistical physics. 52: 479\u2013487.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2009, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3dbb222c-e798-408d-bf54-8f0e23bfa877": {"__data__": {"id_": "3dbb222c-e798-408d-bf54-8f0e23bfa877", "embedding": null, "metadata": {"page_label": "470", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "87b955d7-4288-4bdd-9df6-e51ef925cc0e", "node_type": "4", "metadata": {"page_label": "470", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "ed6e32efb18a0d814bf1e2f1373eb7b6365bb33eff09009b4e5f557e2358f507", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "470 References\nvan Krieken, E. (2024). \u201cOptimisation in Neurosymbolic Learning Sys-\ntems\u201d. PhD thesis. Vrije Universiteit Amsterdam.\nVaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\n\u0141. Kaiser, and I. Polosukhin. (2017). \u201cAttention is all you need\u201d.\nAdvances in neural information processing systems. 30.\nVaswani, S., A. Mishkin, I. Laradji, M. Schmidt, G. Gidel, and S.\nLacoste-Julien. (2019). \u201cPainless stochastic gradient: Interpolation,\nline-search, and convergence rates\u201d.Advances in neural information\nprocessing systems. 32.\nVerdu, S. and H. V. Poor. (1987). \u201cAbstract dynamic programming\nmodels under commutativity conditions\u201d.SIAM Journal on Control\nand Optimization. 25(4): 990\u20131006.\nVicol, P., L. Metz, and J. Sohl-Dickstein. (2021). \u201cUnbiased gradient\nestimation in unrolled computation graphs with persistent evolu-\ntion strategies\u201d. In:International Conference on Machine Learning.\nPMLR. 10553\u201310563.\nVirtanen, P., R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy,\nD. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright,\nS. J. van der Walt, M. Brett, J. Wilson, K. J. Millman, N. Mayorov,\nA. R. J. Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey, \u0130. Polat,\nY. Feng, E. W. Moore, J. VanderPlas, D. Laxalde, J. Perktold,\nR. Cimrman, I. Henriksen, E. A. Quintero, C. R. Harris, A. M.\nArchibald, A. H. Ribeiro, F. Pedregosa, P. van Mulbregt, and SciPy\n1.0 Contributors. (2020). \u201cSciPy 1.0: Fundamental Algorithms for\nScientific Computing in Python\u201d.Nature Methods. 17: 261\u2013272.\nViterbi, A. (1967). \u201cError bounds for convolutional codes and an asymp-\ntotically optimum decoding algorithm\u201d.IEEE transactions on In-\nformation Theory. 13(2): 260\u2013269.\nVorst, H. A. v. d. and H. A. van der Vorst. (1992). \u201cBi-CGSTAB: A\nFast and Smoothly Converging Variant of Bi-CG for the Solution of\nNonsymmetric Linear Systems\u201d.SIAM Journal on Scientific and\nStatistical Computing. 13(2): 631\u2013644.url: http://dx.doi.org/10.\n1137/0913035.\nWainwright, M. J. and M. I. Jordan. (2008). \u201cGraphical models, expo-\nnential families, and variational inference\u201d.Foundations and Trends\u00ae\nin Machine Learning. 1(1\u20132): 1\u2013305.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2138, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "db950bfe-06fe-4b76-8ebb-0ff02f627f54": {"__data__": {"id_": "db950bfe-06fe-4b76-8ebb-0ff02f627f54", "embedding": null, "metadata": {"page_label": "471", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2b97f61c-ee76-4d52-ac08-40afff2196cb", "node_type": "4", "metadata": {"page_label": "471", "file_name": "elemets-of-differential-programming.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\elemets-of-differential-programming.pdf", "file_type": "application/pdf", "file_size": 8684871, "creation_date": "2025-12-04", "last_modified_date": "2025-12-04"}, "hash": "b2d0ecbf59d283ebe5e9865725cdf6eaca62a2f43ee6085b8c27efa9dddcd18d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "References 471\nWang, Q., P. Moin, and G. Iaccarino. (2009). \u201cMinimal repetition\ndynamic checkpointing algorithm for unsteady adjoint calculation\u201d.\nSIAM Journal on Scientific Computing. 31(4): 2549\u20132567.\nWei, C., S. Kakade, and T. Ma. (2020). \u201cThe implicit and explicit\nregularization effects of dropout\u201d. In:International conference on\nmachine learning. PMLR. 10181\u201310192.\nWerbos, P. J. (1990). \u201cBackpropagation through time: what it does and\nhow to do it\u201d.Proceedings of the IEEE. 78(10): 1550\u20131560.\nWerbos, P. J. (1994).The roots of backpropagation: from ordered deriva-\ntives to neural networks and political forecasting. Vol. 1. John Wiley\n& Sons.\nWright, S. and J. Nocedal. (1999). \u201cNumerical optimization\u201d.Springer\nScience. 35(67-68): 7.\nXu, P., F. Roosta, and M. W. Mahoney. (2020). \u201cSecond-order opti-\nmization for non-convex machine learning: An empirical study\u201d. In:\nProceedings of the 2020 SIAM International Conference on Data\nMining. SIAM. 199\u2013207.\nYuan, M. and Y. Lin. (2006). \u201cModel selection and estimation in re-\ngression with grouped variables\u201d.Journal of the Royal Statistical\nSociety: Series B (Statistical Methodology). 68(1): 49\u201367.\nZhang, A., Z. C. Lipton, M. Li, and A. J. Smola. (2021). \u201cDive into\ndeep learning\u201d.arXiv preprint arXiv:2106.11342.\nZhou, X. (2018). \u201cOn the fenchel duality between strong convexity and\nlipschitz continuou s gradient\u201d.arXiv preprint arXiv:1803.06573.\nZhuang, J., N. C. Dvornek, S. Tatikonda, and J. S. Duncan. (2021).\n\u201cMali: A memory efficient and reverse accurate integrator for neural\nodes\u201d. arXiv preprint arXiv:2102.04668.\nZiegler, D. M., N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei,\nP. Christiano, and G. Irving. (2019). \u201cFine-tuning language models\nfrom human preferences\u201d.arXiv preprint arXiv:1909.08593.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1777, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e62e2700-b799-48e0-89cd-62af6a821370": {"__data__": {"id_": "e62e2700-b799-48e0-89cd-62af6a821370", "embedding": null, "metadata": {"page_label": "i", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0f2ad73d-88e7-4e3b-982e-bacdf3d1682c", "node_type": "4", "metadata": {"page_label": "i", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "fddc624a510895d2c88e10c92d64d5833ca60323dc33a29c04ae48e214c64d85", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\nAn introduction to\noptimization on\nsmooth manifolds\nNicolas Boumal\nThis is a pre-publication version, compiled with pdf annotations\ncorrecting an error and offering clarifications up to September 15, 2023.\nPublished version available from Cambridge University Press:\nhttps://cambridge.org/9781009166157\nAuthor\u2019s book webpage, with video lectures, exercises and more:\nhttps://www.nicolasboumal.net/book", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 659, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6beb62c9-40e3-408d-ab55-c5e1381c7c58": {"__data__": {"id_": "6beb62c9-40e3-408d-ab55-c5e1381c7c58", "embedding": null, "metadata": {"page_label": "ii", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c68a7d62-dc0f-4bb0-afa9-ce7bf196307e", "node_type": "4", "metadata": {"page_label": "ii", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "13c01c35e857f03a139a7787c6475ba4fb63e4c83974be6eda0e44529d2a76b6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 257, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d14c9939-e92b-46fb-92fe-0ba99f996112": {"__data__": {"id_": "d14c9939-e92b-46fb-92fe-0ba99f996112", "embedding": null, "metadata": {"page_label": "iii", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e46784c0-9dee-4176-a042-1b1cc5afa453", "node_type": "4", "metadata": {"page_label": "iii", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "56456b7d4c18e430f8b9b9b306d61b926c4005761e49b3aff09388c0855321cd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\nTo my family and mentors.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 283, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e257ad70-e07a-4316-8fe3-0277f98cb971": {"__data__": {"id_": "e257ad70-e07a-4316-8fe3-0277f98cb971", "embedding": null, "metadata": {"page_label": "iv", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4337020f-f90f-48ee-8a3b-d0d401c4a4ed", "node_type": "4", "metadata": {"page_label": "iv", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "aa1cba43a72dcb3c0d03ae5ebb6d882834130fa6cabce5d05146b08d76f777fd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 257, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9c524edf-f84e-4fbf-b6d2-c31053ab9ef7": {"__data__": {"id_": "9c524edf-f84e-4fbf-b6d2-c31053ab9ef7", "embedding": null, "metadata": {"page_label": "v", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c0f570d6-9e2a-4493-abd2-f0e4b74e3fa6", "node_type": "4", "metadata": {"page_label": "v", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "05a860682164964413045e782274d46fbf01bf61f06f86aa8a8db9a1b25e0aca", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\nContents\nPreface page ix\nNotation xiv\n1 Introduction 1\n2 Simple examples 4\n2.1 Sensor network localization from directions: an affine subspace 4\n2.2 Single extreme eigenvalue or singular value: spheres 5\n2.3 Dictionary learning: products of spheres 6\n2.4 Principal component analysis: Stiefel and Grassmann 8\n2.5 Synchronization of rotations: special orthogonal group 11\n2.6 Low-rank matrix completion: fixed-rank manifold 12\n2.7 Gaussian mixture models: positive definite matrices 14\n2.8 Smooth semidefinite programs 14\n3 Embedded geometry: first order 17\n3.1 Reminders of Euclidean space 21\n3.2 Embedded submanifolds of a linear space 25\n3.3 Smooth maps on embedded submanifolds 33\n3.4 The differential of a smooth map 34\n3.5 Vector fields and the tangent bundle 37\n3.6 Moving on a manifold: retractions 39\n3.7 Riemannian manifolds and submanifolds 41\n3.8 Riemannian gradients 42\n3.9 Local frames* 46\n3.10 Notes and references 50\n4 First-order optimization algorithms 53\n4.1 A first-order Taylor expansion on curves 54\n4.2 First-order optimality conditions 55\n4.3 Riemannian gradient descent 56\n4.4 Regularity conditions and iteration complexity 59\n4.5 Backtracking line-search 61\n4.6 Local convergence* 64", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1466, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4f0fd80d-101c-43b8-b918-0ad8ef951c6b": {"__data__": {"id_": "4f0fd80d-101c-43b8-b918-0ad8ef951c6b", "embedding": null, "metadata": {"page_label": "vi", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6182aab8-b995-44c1-90f3-6c8200b05ddd", "node_type": "4", "metadata": {"page_label": "vi", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "5054f02d12f400e1008935531db378be05d5c2c235226abc62cf57521dea2795", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\nvi Contents\n4.7 Computing gradients* 71\n4.8 Numerically checking a gradient* 79\n4.9 Notes and references 80\n5 Embedded geometry: second order 83\n5.1 The case for another derivative of vector fields 85\n5.2 Another look at differentials of vector fields in linear spaces 85\n5.3 Differentiating vector fields on manifolds: connections 86\n5.4 Riemannian connections 89\n5.5 Riemannian Hessians 94\n5.6 Connections as pointwise derivatives* 97\n5.7 Differentiating vector fields on curves 100\n5.8 Acceleration and geodesics 105\n5.9 A second-order Taylor expansion on curves 107\n5.10 Second-order retractions 108\n5.11 Special case: Riemannian submanifolds* 110\n5.12 Special case: metric projection retractions* 114\n5.13 Notes and references 116\n6 Second-order optimization algorithms 119\n6.1 Second-order optimality conditions 119\n6.2 Riemannian Newton\u2019s method 121\n6.3 Computing Newton steps: conjugate gradients 124\n6.4 Riemannian trust regions 131\n6.5 The trust-region subproblem: truncated CG 144\n6.6 Local convergence of RTR with tCG* 147\n6.7 Simplified assumptions for RTR with tCG* 148\n6.8 Numerically checking a Hessian* 150\n6.9 Notes and references 151\n7 Embedded submanifolds: examples 154\n7.1 Euclidean spaces as manifolds 154\n7.2 The unit sphere in a Euclidean space 157\n7.3 The Stiefel manifold: orthonormal matrices 159\n7.4 The orthogonal group and rotation matrices 163\n7.5 Fixed-rank matrices 165\n7.6 The hyperboloid model 173\n7.7 Manifolds defined by h(x) = 0 177\n7.8 Notes and references 180", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1758, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e258e30d-8a5b-48c8-ac6b-2397cb3860e1": {"__data__": {"id_": "e258e30d-8a5b-48c8-ac6b-2397cb3860e1", "embedding": null, "metadata": {"page_label": "vii", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f2e7977-7e08-4f03-95bf-cf2711350739", "node_type": "4", "metadata": {"page_label": "vii", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "22a48123919f0c3461430a3a750e2e47a1243d0d62bcd4aa6f59a4d0352a6989", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\nContents vii\n8 General manifolds 182\n8.1 A permissive definition 182\n8.2 The atlas topology, and a final definition 188\n8.3 Embedded submanifolds are manifolds 192\n8.4 Tangent vectors and tangent spaces 194\n8.5 Differentials of smooth maps 196\n8.6 Tangent bundles and vector fields 197\n8.7 Retractions and velocity of a curve 199\n8.8 Coordinate vector fields as local frames 200\n8.9 Riemannian metrics and gradients 201\n8.10 Lie brackets as vector fields 201\n8.11 Riemannian connections and Hessians 203\n8.12 Covariant derivatives and geodesics 205\n8.13 Taylor expansions and second-order retractions 205\n8.14 Submanifolds embedded in manifolds 206\n8.15 Notes and references 210\n9 Quotient manifolds 212\n9.1 A definition and a few facts 216\n9.2 Quotient manifolds through group actions 219\n9.3 Smooth maps to and from quotient manifolds 223\n9.4 Tangent, vertical and horizontal spaces 224\n9.5 Vector fields 226\n9.6 Retractions 231\n9.7 Riemannian quotient manifolds 232\n9.8 Gradients 234\n9.9 A word about Riemannian gradient descent 236\n9.10 Connections 238\n9.11 Hessians 239\n9.12 A word about Riemannian Newton\u2019s method 240\n9.13 Total space embedded in a linear space 243\n9.14 Horizontal curves and covariant derivatives 246\n9.15 Acceleration, geodesics and second-order retractions 247\n9.16 Grassmann manifold: summary* 250\n9.17 Notes and references 254\n10 Additional tools 260\n10.1 Distance, geodesics and completeness 260\n10.2 Exponential and logarithmic maps 264\n10.3 Parallel transport 270\n10.4 Lipschitz conditions and Taylor expansions 274\n10.5 Transporters 285\n10.6 Finite difference approximation of the Hessian 292\n10.7 Tensor fields and their covariant differentiation 294\n10.8 Notes and references 302", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1971, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7867fea4-e9ae-42b2-897c-b01216c83940": {"__data__": {"id_": "7867fea4-e9ae-42b2-897c-b01216c83940", "embedding": null, "metadata": {"page_label": "viii", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1e82188b-f13a-443e-8a08-633a0f859b0c", "node_type": "4", "metadata": {"page_label": "viii", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "f7ff319879722e50a9ad105cb690857304999c72ea40919ad9793373ee932ba1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\nviii REFERENCES\n11 Geodesic convexity 307\n11.1 Convex sets and functions in linear spaces 308\n11.2 Geodesically convex sets and functions 310\n11.3 Alternative definitions of geodesically convex sets* 314\n11.4 Differentiable geodesically convex functions 316\n11.5 Geodesic strong convexity and Lipschitz continuous gradients 319\n11.6 Example: positive reals and geometric programming 323\n11.7 Example: positive definite matrices 326\n11.8 Notes and references 329\nReferences 331\nIndex 347", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 744, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8d88ba50-92a7-4408-963c-48ec1ac0c710": {"__data__": {"id_": "8d88ba50-92a7-4408-963c-48ec1ac0c710", "embedding": null, "metadata": {"page_label": "ix", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "651c8b2e-96e3-440a-ba36-7e88c9108d16", "node_type": "4", "metadata": {"page_label": "ix", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "fa4c1f28b9517214bc3ed9625cfd363d9474e1c62aa8922a383dee3d30d087d7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\nPreface\nOptimization problems on smooth manifolds arise in science and engineering as\na result of natural geometry (e.g., the set of orientations of physical objects in\nspace is a manifold), latent data simplicity (e.g., high-dimensional data points lie\nclose to a low-dimensional linear subspace, leading to low-rank data matrices),\nsymmetry (e.g., observations are invariant under rotation, translation or other\ngroup actions, leading to quotients) and positivity (e.g., covariance matrices and\ndiffusion tensors are positive definite). This has led to successful applications\nnotably in machine learning, computer vision, robotics, scientific computing,\ndynamical systems and signal processing.\nAccordingly, optimization on manifolds has garnered increasing interest from\nresearchers and engineers alike. Building on fifty years of research efforts that\nhave recently intensified, it is now recognized as a wide, beautiful and effective\ngeneralization of unconstrained optimization on linear spaces.\nYet, engineering programs seldom include training in differential geometry:\nthe field of mathematics concerned with smooth manifolds. Moreover, existing\ntextbooks on this topic usually align with the interests of mathematicians more\nthan with the needs of engineers and applied mathematicians. This creates a\nsignificant but avoidable barrier to entry for optimizers.\nOne of my goals in writing this book is to offer a different, if at times unortho-\ndox, introduction to differential geometry. Definitions and tools are introduced in\na need-based order for optimization. We start with a restricted setting\u2014that of\nembedded submanifolds of linear spaces\u2014which allows us to define all necessary\nconcepts in direct reference to their usual counterparts from linear spaces. This\ncovers a wealth of applications.\nIn what is perhaps the clearest departure from standard exposition, charts and\natlases are not introduced until quite late. The reason for doing so is twofold:\npedagogically, charts and atlases are more abstract than what is needed to work\non embedded submanifolds; and pragmatically, charts are seldom if ever useful\nin practice. It would be unfortunate to give them center stage.\nOf course, charts and atlases are the right tool to provide a unified treatment\nof all smooth manifolds in an intrinsic way. They are introduced eventually,\nat which point it becomes possible to discuss quotient manifolds: a powerful\nlanguage to understand symmetry in optimization. Perhaps this abstraction is\nnecessary to fully appreciate the depth of optimization on manifolds as more", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2839, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "be33a365-87ba-4221-aaa2-c8db2f329d71": {"__data__": {"id_": "be33a365-87ba-4221-aaa2-c8db2f329d71", "embedding": null, "metadata": {"page_label": "x", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7019f3a7-a74d-4a54-a068-853be1bdf7f5", "node_type": "4", "metadata": {"page_label": "x", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "1d836e4c1e1d54eff2036ed19fbbab3574480ba093b8cf7bf24edc7b71b4d199", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\nx Preface\nthan just a fancy tool for constrained optimization in linear spaces, and truly a\nmathematically natural setting for unconstrained optimization in a wider sense.\nTime-tested optimization algorithms are introduced immediately after the\nearly chapters about embedded geometry. Crucially, the design and analysis of\nthese methods remain unchanged whether we are optimizing on a manifold which\nis embedded in a linear space or not. This makes it possible to get to algorithms\nearly on, without sacrificing generality. It also underlines the conceptual point\nthat the algorithms truly operate on the manifolds intrinsically.\nThe last two chapters visit more advanced topics that are not typically neces-\nsary for simple applications. The first one delves deeper into geometric tools. The\nsecond one introduces the basics of geodesic convexity: a broad generalization of\nconvexity, which is one of the most fruitful structures in classical optimization.\nIntended audience\nThis book is intended for students and researchers alike. The material has proved\npopular with applied mathematicians and mathematically inclined engineering\nand computer science students at the graduate and advanced undergraduate\nlevels.\nReaders are assumed to be comfortable with linear algebra and multivariable\ncalculus. Central to the raison d\u2019\u02c6 etre of this book, there are no prerequisites in\ndifferential geometry or optimization. For computational aspects, it is helpful to\nhave notions of numerical linear algebra, for which I recommend the approachable\ntextbook by Trefethen and Bau [TB97].\nBuilding on these expectations, the aim is to give full proofs and intuition for\nall concepts that are introduced, at least for submanifolds of linear spaces. The\nhope is to equip readers to pursue research projects in (or using) optimization on\nmanifolds, involving both mathematical analysis and efficient implementation.\nHow to use this book\nThe book is self-contained and should suit both self-learners and instructors.\nChapters 3 and 5 can serve as a standalone introduction to differential and\nRiemannian geometry. They focus on embedded submanifolds of linear spaces,\nwith proofs. Chapter 7 details examples of manifolds: it is meant for on-and-off\nreading in parallel with Chapters 3 and 5. These chapters do not involve charts,\nand they aim to convey the fact that geometric tools are computational tools.\nFrom there, the expected next step is to work through Chapters 4 and 6 about\noptimization algorithms. Readers may also choose to embark on Chapter 8 to\nsee how embedded manifolds fit into the general theory of smooth manifolds.\nThat is a useful (though not fully necessary) stepping stone toward Chapter 9\nabout quotient manifolds. Alternatively, they may decide to learn about further\ngeometric tools in Chapter 10 or about a Riemannian notion of convexity in\nChapter 11.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3123, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7b0bd446-eea3-4f05-b715-302b14ebbb10": {"__data__": {"id_": "7b0bd446-eea3-4f05-b715-302b14ebbb10", "embedding": null, "metadata": {"page_label": "xi", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a97ccd78-598f-47a7-8996-40ff72c0f6de", "node_type": "4", "metadata": {"page_label": "xi", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "b7d5d918920af36e19234bd9600c246666c82fd5c9a0a72a4507091e0e242e5f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\nPreface xi\nThese chapter dependencies are summarized in the diagram below, where an\narrow from A to B means it is preferable to read A before B.\nCh. 3\nstart\nCh. 7 Ch. 4\nCh. 5 Ch. 6\nCh. 8 Ch. 9\nCh. 10\nCh. 11\n1. Introduction\n2. Applications\n3. First-order geometry\n4. First-order optimization\n5. Second-order geometry\n6. Second-order optimization\n7. Examples of manifolds\n8. General manifolds\n9. Quotient manifolds\n10. Additional geometric tools\n11. Geodesic convexity\nIn a graduate course at Princeton University in 2019 and 2020 (24 lectures of\n80 minutes each), I covered much of Chapters 1\u20136 and select parts of Chapter 7\nbefore the midterm break, then much of Chapters 8\u20139 and select parts of Chap-\nters 10\u201311 after the break. At EPFL in 2021, I discussed mostly Chapters 1\u20138\nin 13 lectures of 90 minutes each supplemented with exercise sessions.\nThe numerous exercises in the book have wide-ranging difficulty levels. Some\nare included in part as a way to convey information while skipping technicalities.\nStarred sections can be skipped safely for a first encounter with the material.\nChapters end with references and notes that many readers may find relevant but\nwhich would otherwise break the flow. Did \u22c6the mark in the margin catch your\nattention? That is its purpose. You may see a couple of those in the book.\nWhat is new, or different, or hard to find elsewhere\nThe de facto reference for optimization on manifolds is the landmark 2008 book\nOptimization Algorithms on Matrix Manifolds by Pierre-Antoine Absil, Robert\nMahony and Rodolphe Sepulchre [AMS08]. It is an important source for the\npresent book as well, with significant overlap of topics. In the years since, the\nfield has evolved, and with it the need for an entry point catering to a broader\naudience. In an effort to address these needs, I aim to:\n1. Provide a different, self-contained introduction to the core concepts.\nThis includes a \u201ccharts last\u201d take on differential geometry with proofs adapted\naccordingly; a somewhat unusual yet equivalent definition of connections that\n(I believe) is more intuitive from an optimizer\u2019s point of view; and an account\nof optimization on quotient manifolds which benefits from years of hindsight.\nThis introduction is informed by the pains I had entering the field.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2538, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c0767d44-8e44-4dc1-9871-db339fc9901f": {"__data__": {"id_": "c0767d44-8e44-4dc1-9871-db339fc9901f", "embedding": null, "metadata": {"page_label": "xii", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ddca0521-04fd-4472-aecf-03b85ecec732", "node_type": "4", "metadata": {"page_label": "xii", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "bc90ea0cfd357bb3ae1177c464192a62c7fc1c6b054f86f5ac1752a348423d22", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\nxii Preface\n2. Discuss new topics that have grown in importance since 2008.\nThis includes a replacement of asymptotic convergence results in favor of\nworst-case, non-asymptotic iteration complexity results; a related take on\nLipschitz continuity for Riemannian gradients and Hessians paired with their\neffect on Taylor expansions on manifolds; an explicit construction of geometric\ntools necessary for optimization over matrices of fixed rank; a simple study\nof metric projection retractions; an extrinsic view of the Riemannian Hes-\nsian for submanifolds through the Weingarten map and second fundamental\nform; a discussion of the smooth invertibility of retractions and of the domain\nof the inverse of the exponential map; transporters as a natural alternative\nto vector and parallel transports; finite differences of gradients to approxi-\nmate Hessians; and an introduction to geodesic convexity (not restricted to\nHadamard manifolds) with a gradient algorithm for the strongly convex case.\nMany of these build on research papers referenced in text.\n3. Share tricks of the trade that are seldom, if ever, spelled out.\nThis includes several examples of manifolds worked out in full detail; prag-\nmatic instructions for how to derive expressions for gradients and Hessians\nof matrix functions, and how to check them numerically; explicit formulas\nfor geometric tools on product manifolds (mostly given as exercises); and a\nnumber of comments informed by ten years of software development in the\nfield.\nThe main differential geometry references I used are the fantastic books by\nLee [Lee12, Lee18], O\u2019Neill [O\u2019N83], and Brickell and Clark [BC70]. Definitions\nof geometric concepts in this book, though at times stated differently, are fully\ncompatible with Absil et al.\u2019s book. This is also compatible with Lee\u2019s text-\nbooks with one exception: Riemannian submanifolds to us are understood to be\nembedded submanifolds, whereas Lee also allows them to be merely immersed\nsubmanifolds. Moreover, we use the word \u201cmanifold\u201d to mean \u201csmooth mani-\nfold,\u201d that is, C\u221e. Most results extend to manifolds and functions of class Ck.\nThere is much to say about the impact of curvature on optimization. This is\nan active research topic that has not stabilized yet. Therefore, I chose to omit\ncurvature entirely from this book, save for a few brief comments in the last two\nchapters. Likewise, optimization on manifolds is proving to be a particularly\nfertile ground for benign non-convexity and related phenomena. There are only\na few hints to that effect throughout the book: the research continues.\nSoftware and online resources\nLittle to no space is devoted to existing software packages for optimization on\nmanifolds, or to numerical experiments. Yet, such packages significantly speed\nup research and development in the field. The reader may want to experiment\nwith Manopt (Matlab), PyManopt (Python) or Manopt.jl (Julia), all available\nfrom manopt.org.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3204, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5182c7cd-3f74-4378-b724-a625570fcc04": {"__data__": {"id_": "5182c7cd-3f74-4378-b724-a625570fcc04", "embedding": null, "metadata": {"page_label": "xiii", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a543476b-70d5-4405-8102-903b60de501a", "node_type": "4", "metadata": {"page_label": "xiii", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "249513ea3f5dc0003056a8a74845c73ac4a82bbf100867187ea576132c4e2902", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\nPreface xiii\nIn particular, the Matlab implementations of most manifolds discussed in this\nbook are listed in Table 7.1 on p155. Gradient descent (Algorithm 4.1) with\nbacktracking line-search (Algorithm 4.2) is available as steepestdescent. The\ntrust-region method (Algorithm 6.3) with the truncated conjugate gradient sub-\nproblem solver (Algorithm 6.4) is available as trustregions. These implemen-\ntations include a wealth of tweaks and tricks that are important in practice:\nmany are explained here, some are only documented in the code. The Python\nand Julia versions offer similar features.\nThe following webpage collects further resources related to this book:\nnicolasboumal.net/book\nIn particular, teaching and learning material will be listed there, as well as errata.\nThanks\nA number of people offered decisive comments. I thank Pierre-Antoine Absil\n(who taught me much of this) and Rodolphe Sepulchre for their input at the early\nstages of planning for this book, as well as (in no particular order) Eitan Levin,\nChris Criscitiello, Quentin Rebjock, Razvan-Octavian Radu, Joe Kileel, Bart\nVandereycken, Coralia Cartis, Bamdev Mishra, Suvrit Sra, Stephen McKeown,\nJohn M. Lee and S\u00b4 andor Z. N\u00b4 emeth for numerous conversations that led to direct\nimprovements. Likewise, reviewers offered welcome advice and suggestions for\nadditions (that I was only able to implement partially). Another big thank you\nto the people involved with the Manopt toolboxes: these efforts are led with\nBamdev for the Matlab version; by Jamie Townsend, Niklas Koep and Sebastian\nWeichwald for the Python version; and by Ronny Bergmann for the Julia version.\nI am also indebted to the mathematics departments at Princeton University\nand EPFL for supporting me while I was writing. Finally, I thank Katie Leach\nat Cambridge University Press for her enthusiasm and candid advice that helped\nshape this project into its final form.\nNicolas Boumal Lausanne, Switzerland\nnicolas.boumal@epfl.ch", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2231, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "69cbc88c-384a-4d04-9292-de2dadb30449": {"__data__": {"id_": "69cbc88c-384a-4d04-9292-de2dadb30449", "embedding": null, "metadata": {"page_label": "xiv", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d81ebb34-568a-4ca6-87e1-a42a4fa68357", "node_type": "4", "metadata": {"page_label": "xiv", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "45f95574570d63e69543d7dfbbd930643c69f01ce3917b8260d10994f879462e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\nNotation\nThe following lists typical uses of symbols. Local exceptions are documented in\nplace. For example, c typically denotes a curve, but sometimes denotes a real\nconstant. Symbols defined and used locally only are omitted.\nR, C Real and complex numbers\nR+ Positive reals (x >0)\nRm\u00d7n Real matrices of size m \u00d7 n\nRm\u00d7n\nr Real matrices of size m \u00d7 n and rank r\nSym(n), Skew(n) Symmetric and skew-symmetric real matrices of size n\nsym(M), skew(M) Symmetric and skew-symmetric parts of a matrix M\nSym(n)+ Symmetric positive definite real matrices of size n\nTr(M), det(M) Trace, determinant of a square matrix M\ndiag(M) Vector of diagonal entries of a matrix M\ndiag(u1, . . . , un) Diagonal matrix of size n with given diagonal entries\nM\u2020 Moore\u2013Penrose pseudoinverse of matrix M\nId Identity matrix of size d\nI Subset of R (often open with 0 \u2208 I) or identity matrix\nId Identity operator\n|a| Modulus of a \u2208 C (absolute value if a \u2208 R)\n|A| Cardinality of a set A\nE, E\u2032, F Linear spaces, often with a Euclidean structure\nM, M\u2032, M, N Smooth manifolds, often with a Riemannian structure\nSd\u22121 Unit sphere, in a Euclidean space of dimension d\nOB(d, n) Oblique manifold (product of S d\u22121 copied n times)\nO(d), SO(d) Orthogonal and special orthogonal groups in Rd\u00d7d\nSt(n, p) Stiefel manifold embedded in Rn\u00d7p\nGr(n, p) Grassmann manifold as the quotient St( n, p)/O(p)\nGL(n) General linear group (invertible matrices in Rn\u00d7n)\nHn Hyperbolic space as hyperboloid embedded in Rn+1\ndim M Dimension of M\nx, y, z Points on a manifold\nu, v, w, s, \u03be, \u03b6 Tangent vectors\np, q Integers or polynomials or points on a manifold\nTM Tangent bundle of M", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1881, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "baa80214-e8e6-4411-afa9-5a0c169d51e1": {"__data__": {"id_": "baa80214-e8e6-4411-afa9-5a0c169d51e1", "embedding": null, "metadata": {"page_label": "xv", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "280d1ef5-8cb3-451e-abd6-731e72030028", "node_type": "4", "metadata": {"page_label": "xv", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "fc10a00fc441b370be0f2c89c8b80baa0257347936e691423f126cbe9c5ee089", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\nNotation xv\nTxM Tangent space to M at x \u2208 M\nNxM Normal space (orthogonal complement of T xM)\nProjx, Proj\u22a5\nx Orthogonal projector to T xM, NxM\nHx, Vx Horizontal and vertical space at x for a quotient manifold\nProjH\nx , ProjV\nx Orthogonal projectors to H x, Vx\nliftx Horizontal lift operator for quotient manifolds\nRx(v) Retraction R evaluated at ( x, v) \u2208 TM\nExpx(v) Exponential map Exp evaluated at ( x, v) \u2208 TM\nLogx(y) Vector v such that Expx(v) = y (see Definition 10.20)\nexp, log Scalar or matrix exponential and logarithm\nO, Ox Domain of Exp (subset of T M), Expx (subset of TxM);\nCan also denote these domains for a non-global retraction.\ninj(M), inj(x) Injectivity radius of a manifold, at a point\n\u27e8\u00b7, \u00b7\u27e9x Riemannian inner product on T xM\n\u27e8\u00b7, \u00b7\u27e9 Euclidean inner product;\nSometimes denotes \u27e8\u00b7, \u00b7\u27e9x with subscript omitted.\n\u2225 \u00b7 \u2225, \u2225 \u00b7 \u2225x Norms associated to \u27e8\u00b7, \u00b7\u27e9 and \u27e8\u00b7, \u00b7\u27e9x\n\u2225 \u00b7 \u2225 Also denotes operator norm for linear maps\n[\u00b7, \u00b7] Lie bracket\n\u2207 Affine connection (often Riemannian) on a manifold\nd\ndt Classical derivative with respect to t\nD\ndt Covariant derivative induced by a connection \u2207\n\u2202\n\u2202xi\nPartial derivative with respect to real variable xi\nxi Often the ith coordinate of a vector x \u2208 Rn\nxk Often the kth element of a sequence x0, x1, x2, . . .\u2208 M\nf, g Real-valued functions\nflow A real number such that f(x) \u2265 flow for all x\nh Often a local defining function with values in Rk\ngradf, Hessf Riemannian gradient and Hessian of f;\nEuclidean gradient, Hessian if domain of f is Euclidean.\n\u2207f, \u22072f First and second covariant derivatives of f as tensor fields\nc, \u03b3 Curves\nc\u2032, \u03b3\u2032, \u02d9c, \u02d9\u03b3 Velocity vector fields of curves c, \u03b3\nc\u2032\u2032, \u03b3\u2032\u2032 Intrinsic acceleration vector fields of c, \u03b3\n\u00a8c, \u00a8\u03b3 Extrinsic acceleration vector fields of c, \u03b3\nL, Lg, LH Lipschitz constants (nonnegative reals)\nL(c) Length of a curve c\ndist(x, y) Distance (often Riemannian) between two points x, y\nB(x, r) Open ball {v \u2208 TxM : \u2225v\u2225x < r} or {y \u2208 E: \u2225y \u2212 x\u2225 < r}\n\u00afB(x, r) Closed ball as above\nA, L Linear maps\nA, A+ Atlas, maximal atlas\nim L, kerL Range space (image) and null space (kernel) of L", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2332, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "23d087e5-0b9a-49cd-9dd2-34661c13b414": {"__data__": {"id_": "23d087e5-0b9a-49cd-9dd2-34661c13b414", "embedding": null, "metadata": {"page_label": "xvi", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eb1d7f3e-5f5b-4470-a8a7-7f48b68b40d6", "node_type": "4", "metadata": {"page_label": "xvi", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "747764d71ca04e23e2f55e395adac6af99b87673a702c6a1c047c3cb3d2ff68c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\nxvi Notation\nrank(M), rank(L) Rank of a matrix or linear map\nM\u22a4, M\u2217 Transpose or Hermitian conjugate-transpose of matrix M\nL\u2217 Adjoint of a linear map L between Euclidean spaces\nA \u2ab00, A \u227b0 States A = A\u2217 is positive semidefinite or positive definite\nspan(u1, . . . , um) Linear subspace spanned by vectors u1, . . . , um\nF, G, H Maps, usually to and from linear spaces or manifolds\nF : A \u2192 B A map defined on the whole domain A\nF|U Restriction of the map F to the domain U\nF(\u00b7, y) For a map ( x, y) 7\u2192 F(x, y), this is the map x 7\u2192 F(x, y)\nF \u25e6 G Composition of maps: ( F \u25e6 G)(x) = F(G(x))\nDF(x)[v] Differential of F at x along v\nU, V, W, X, Y, Z Vector fields on a manifold, or\nMatrices which could be tangent vectors or points on M;\nY, Z Can also be vector fields along a curve.\nU, V, W, O Can also be open sets, usually in a linear space.\n\u00aff, \u00afF ,\u00afV , . . . Smooth extensions or lifts of f, F, V, . . .\n\u00afu, \u00afU Can also denote complex conjugation of u, U\nT Tensor field\nTs Differential of retraction DRx(s)\nU, V Open sets in a manifold\nW Weingarten map\nII Second fundamental form\nfV Vector field x 7\u2192 f(x)V (x) (with f real valued)\nV f Real function x 7\u2192 Df(x)[V (x)]\nF(M), F(E) Set of smooth real-valued functions on M, E\nX(M), X(E) Set of smooth vector fields on M, E\nX(c) Set of smooth vector fields along a curve c\nTy\u2190x Vector transport from TxM to TyM\nPTc\nt1\u2190t0 Parallel transport along curve c from c(t0) to c(t1)\nPs Parallel transport along \u03b3(t) = Expx(ts) from 0 to 1\n\u223c Equivalence relation\nA/\u223c Quotient set of A by the relation \u223c\n[x] Equivalence class of x for some equivalence relation\n\u03c0 Canonical projection \u03c0 : TM \u2192 Mor \u03c0 : M \u2192M/\u223c;\nOccasionally denotes the mathematical constant.\nA \u2282 B A is a proper subset of B (the sets are not equal)\nA \u2286 B A is a subset of B (the sets may be equal)\nA \u2229 B Intersection of sets A, B\nA \u222a B Union of sets A, B\n\u2205 Empty set.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2124, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0d3f3b07-a9a7-44a9-8100-74c4e3900561": {"__data__": {"id_": "0d3f3b07-a9a7-44a9-8100-74c4e3900561", "embedding": null, "metadata": {"page_label": "1", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a2fc76b1-5158-4836-8450-b224488c562a", "node_type": "4", "metadata": {"page_label": "1", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "978dc2251de4b38c2dd26cdf8299d5ee321db1b639a590b83bf189f6efd1c33f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n1 Introduction\nOptimization is a staple of mathematical modeling. In this rich framework, we\nconsider a set S called the search space\u2014it contains all possible answers to our\nproblem, good and bad\u2014and a cost function f : S \u2192 R which associates a cost\nf(x) to each element x of S. The goal is to find x \u2208 S such that f(x) is as small\nas possible, that is, a best answer. We write\nmin\nx\u2208S\nf(x)\nto represent both the optimization problem and the minimal cost (if it exists).\nOccasionally, we wish to denote specifically the subset ofS for which the minimal\ncost is attained; the standard notation is\narg min\nx\u2208S\nf(x),\nbearing in mind that this set might be empty. We will discuss a few simple\napplications which can be modeled in this form.\nRarely, optimization problems admit an analytical solution. Typically, we need\nnumerical algorithms to (try to) solve them. Often, the best algorithms exploit\nmathematical structure in S and f.\nAn important special case arises when S is a linear space such as Rn. Minimiz-\ning a function f in Rn is called unconstrained optimization because the variable\nx is free to move around Rn, unrestricted.\nIf f is sufficiently differentiable and Rn is endowed with an inner product (that\nis, if we make it into a Euclidean space), then we have a notion of gradient and\nperhaps even a notion of Hessian forf. These objects give us a firm understanding\nof how f behaves locally around any given point. Famous algorithms such as\ngradient descent and Newton\u2019s method exploit these objects to move around Rn\nefficiently in search of a solution.\nNotice, however, that the Euclidean structure of Rn and the smoothness of f\nare irrelevant to the definition of the optimization problem itself: they are merely\nstructures that we may (and as experience shows, we should) use algorithmically\nto our advantage.\nSubsuming linearity, we focus on smoothness as the key structure to exploit:\nwe assume the setS is a smooth manifold and the function f is smooth on S. This\ncalls for precise definitions, constructed first in Chapter 3. For a first intuition,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2327, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "da07b93c-1fa5-4e64-9f59-beee4754ff1f": {"__data__": {"id_": "da07b93c-1fa5-4e64-9f59-beee4754ff1f", "embedding": null, "metadata": {"page_label": "2", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d635c7d7-fe12-4655-b170-64a3be476383", "node_type": "4", "metadata": {"page_label": "2", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "e5e137d37776be2f5d99137ec51c29bb6fb5bc7d993645c42b18f7939ae3c622", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n2 Introduction\none can think of smooth manifolds as surfaces in Rn that do not have kinks or\nboundaries, such as a plane, a sphere, a torus, or a hyperboloid.\nWe could think of optimization over such surfaces as constrained, in the sense\nthat x is not allowed to move freely in Rn: it is constrained to remain on the\nsurface. Alternatively, and this is the viewpoint favored here, we can think of this\nas unconstrained optimization, in a world where the smooth surface is the only\nthing that exists: like an ant walking on a large ball might feel unrestricted in\nits movements, aware only of the sphere it lives on; or like the two-dimensional\ninhabitants of Flatland [Abb84] who find it hard to imagine that there exists\nsuch a thing as a third dimension, feeling thoroughly free in their own subspace.\nA natural question then is: can we generalize the Euclidean algorithms from\nunconstrained optimization to handle the broader class of optimization over\nsmooth manifolds? The answer is essentially yes, going back to the 70s [Lue72,\nLic79], the 80s [Gab82] and the 90s [Udr94, Smi94, HM96, Rap97, EAS98], and\nsparking a significant amount of research in the past two decades.\nTo generalize algorithms such as gradient descent and Newton\u2019s method, we\nneed a proper notion of gradient and Hessian on smooth manifolds. In the linear\ncase, this required the introduction of an inner product: a Euclidean structure.\nIn our more general setting, we leverage the fact that smooth manifolds can be\nlinearized locally around every point. The linearization at x is called the tangent\nspace at x. By endowing each tangent space with its own inner product (varying\nsmoothly with x, in a sense to be made precise), we construct what is called a\nRiemannian structure on the manifold: it becomes a Riemannian manifold.\nA Riemannian structure is sufficient to define gradients and Hessians on the\nmanifold, paving the way for optimization. There exist several Riemannian struc-\ntures on each manifold: our choice may impact algorithmic performance. In that\nsense, identifying a useful structure is part of the algorithm design\u2014as opposed\nto being part of the problem formulation, which ended with the definition of the\nsearch space (as a crude set) and the cost function.\nChapter 2 covers a few simple applications, mostly to give a sense of how\nmanifolds come up. We then go on to define smooth manifolds in a restricted 1\nsetting in Chapter 3, where manifolds are embedded in a linear space, much like\nthe unit sphere in three-dimensional space. In this context, we define notions of\nsmooth functions, smooth vector fields, gradients and retractions (a means to\nmove around on a manifold). These tools are sufficient to design and analyze\na first optimization algorithm in Chapter 4: Riemannian gradient descent. As\nreaders progress through these chapters, it is the intention that they also read\nbits of Chapter 7 from time to time: useful embedded manifolds are studied\nthere in detail. Chapter 5 provides more advanced geometric tools for embedded\nmanifolds, including the notions of Riemannian connections and Hessians. These\n1 Some readers may know Whitney\u2019s celebrated embedding theorems, which state that any\nsmooth manifold can be embedded in a linear space [BC70, p82]. The mere existence of an\nembedding, however, is of little use for computation.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3603, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c69f4da4-04b7-4694-b7ab-925cc7945539": {"__data__": {"id_": "c69f4da4-04b7-4694-b7ab-925cc7945539", "embedding": null, "metadata": {"page_label": "3", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d5d20651-adfb-4f5b-aa51-6e2e6485e9ac", "node_type": "4", "metadata": {"page_label": "3", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "b449e9d0c32c23079d8c012f1d5b942f47b31506793a2560afcfecca65a67be4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n1 Introduction 3\nare put to good use in Chapter 6 to design and analyze Riemannian versions of\nNewton\u2019s method and the trust-region method.\nThe linear embedding space is useful for intuition, to simplify definitions, and\nto design tools. Notwithstanding, all the tools and concepts we define in the\nrestricted setting are intrinsic, in the sense that they are well defined regardless\nof the embedding space. We make this precise much later, in Chapter 8, where all\nthe tools from Chapters 3 and 5 are redefined in the full generality of standard\ntreatments of differential geometry. This is also the time to discuss topological\nissues to some extent. Generality notably makes it possible to discuss a more\nabstract class of manifolds called quotient manifolds in Chapter 9. They offer a\nbeautiful way to harness symmetry, so common in applications.\nIn closing, Chapter 10 offers a limited treatment of more advanced geometric\ntools such as the Riemannian distance, geodesics, the exponential map and its\ninverse, parallel transports and transporters, notions of Lipschitz continuity, fi-\nnite differences, and covariant differentiation of tensor fields. Then, Chapter 11\ncovers elementary notions of convexity on Riemannian manifolds with simple\nimplications for optimization. This topic has been around since the 90s, and has\nbeen gaining traction in research lately.\nMore than 150 years ago, Riemann invented a new kind of geometry for the\nabstract purpose of understanding curvature in high-dimensional spaces. Today,\nthis geometry plays a central role in the development of efficient algorithms to\ntackle technological applications Riemann himself\u2014arguably\u2014could have never\nenvisioned. Through this book, I invite you to enjoy this singularly satisfying\nsuccess of mathematics, with an eye to turning geometry into algorithms.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2088, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5b25d82f-1b58-4db6-8e58-9e69983c7eee": {"__data__": {"id_": "5b25d82f-1b58-4db6-8e58-9e69983c7eee", "embedding": null, "metadata": {"page_label": "4", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "09aa6e63-55a4-4d58-80bb-5f31c5da631f", "node_type": "4", "metadata": {"page_label": "4", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "a55f5d484f12efd65a80423e7d5fae57f74b1f26d9939eeba5b015530b69b095", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n2 Simple examples\nBefore formally defining what manifolds are, and before introducing any particu-\nlar algorithms, this chapter surveys simple problems that are naturally modeled\nas optimization on manifolds. These problems are motivated by applications in\nvarious scientific and technological domains. We introduce them chiefly to il-\nlustrate how manifolds arise and to motivate the mathematical abstractions in\nsubsequent chapters.\nThe first example leads to optimization on an affine subspace: it falls within\nthe scope of optimization on manifolds, but one can also handle it with classical\ntools. Subsequently, we encounter optimization on spheres, products of spheres,\northonormal matrices, the set of all linear subspaces, rotation matrices, fixed-\nrank matrices, positive definite matrices and certain quadratic surfaces. Through\nthose, we get a glimpse of the wide reach of optimization on manifolds.\nBelow, we use a few standard concepts from linear algebra and calculus that\nare revisited in Section 3.1.\n2.1 Sensor network localization from directions: an affine subspace\nConsider n sensors located at unknown positions t1, . . . , tn in Rd. We aim to\nlocate the sensors, that is, estimate the positions ti, based on some directional\nmeasurements. Specifically, for each pair of sensors ( i, j) corresponding to an\nedge of a graph G, we receive a noisy measurement of the direction from tj to ti:\nvij \u2248 ti \u2212 tj\n\u2225ti \u2212 tj\u2225,\nwhere \u2225x\u2225 =\np\nx2\n1 + \u00b7 \u00b7\u00b7+ x2\nd is the Euclidean norm on Rd induced by the inner\nproduct \u27e8u, v\u27e9 = u\u22a4v = u1v1 + \u00b7 \u00b7\u00b7+ udvd.\nThere are two fundamental ambiguities in this task. First, directional mea-\nsurements reveal nothing about the global location of the sensors: translating\nthe sensors as a whole does not affect pairwise directions. Thus, we may assume\nwithout loss of generality that the sensors are centered:\nt1 + \u00b7 \u00b7\u00b7+ tn = 0.\nSecond, the measurements reveal nothing about the global scale of the sensor", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2204, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "02a86635-429e-4fc4-b356-e72553995a03": {"__data__": {"id_": "02a86635-429e-4fc4-b356-e72553995a03", "embedding": null, "metadata": {"page_label": "5", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3bfccd3d-5e50-44b8-b589-68d25a9fcd7d", "node_type": "4", "metadata": {"page_label": "5", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "c4b99b222909241a92e1a2b87ae4aaff9e1237429e4a28ffb77dda085d92bd2c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n2.2 Single extreme eigenvalue or singular value: spheres 5\narrangement. Specifically, scaling all positions ti by a scalar \u03b1 >0 as \u03b1ti has no\neffect on the directions separating the sensors, so that the true scale cannot be\nrecovered from the measurements. It is thus legitimate to fix the scale arbitrarily,\nto break symmetry. One fruitful way is to assume the following [HLV18]:\nX\n(i,j)\u2208G\n\u27e8ti \u2212 tj, vij\u27e9 = 1.\nIndeed, if this constraint holds for some set of locations t1, . . . , tn, then it does\nnot hold for locations \u03b1t1, . . . , \u03b1tn unless \u03b1 = 1.\nGiven a tentative estimator \u02c6t1, . . . ,\u02c6tn \u2208 Rd for the locations, we may assess\nits compatibility with the measurement vij by computing\n\u2225(\u02c6ti \u2212 \u02c6tj) \u2212\n\n\u02c6ti \u2212 \u02c6tj, vij\n\u000b\nvij\u2225.\nIndeed, if \u02c6ti \u2212\u02c6tj and vij are aligned in the same direction, this evaluates to zero.\nOtherwise, it evaluates to a positive number, growing as alignment degrades.\nCombined with the symmetry-breaking conditions, this suggests the following\nformulation for sensor network localization from direction measurements:\nmin\n\u02c6t1,...,\u02c6tn\u2208Rd\nX\n(i,j)\u2208G\n\u2225(\u02c6ti \u2212 \u02c6tj) \u2212\n\n\u02c6ti \u2212 \u02c6tj, vij\n\u000b\nvij\u22252\nsubject to \u02c6t1 + \u00b7 \u00b7\u00b7+ \u02c6tn = 0 and\nX\n(i,j)\u2208G\n\n\u02c6ti \u2212 \u02c6tj, vij\n\u000b\n= 1.\nThe role of the second constraint is clear: it excludes \u02c6t1 = \u00b7 \u00b7\u00b7= \u02c6tn = 0, which\nwould otherwise be optimal.\nGrouping the variables as the columns of a matrix, we find that the search\nspace for this problem is an affine subspace of Rd\u00d7n: this is a linear manifold. It\nis also an embedded submanifold of Rd\u00d7n. Hence, it falls within our framework.\nWith the simple cost function as above, this problem is in fact a convex\nquadratic minimization problem on an affine subspace. As such, it admits an\nexplicit solution which merely requires solving a linear system. Optimization al-\ngorithms can be used to solve this system implicitly. More importantly, the power\nof optimization algorithms lies in the flexibility that they offer: alternative cost\nfunctions may be used to improve robustness against specific noise models for\nexample, and those require more general algorithms [HLV18].\n2.2 Single extreme eigenvalue or singular value: spheres\nLet A \u2208 Rn\u00d7n be a symmetric matrix: A = A\u22a4. By the spectral theorem, A\nadmits n real eigenvalues \u03bb1 \u2264 \u00b7 \u00b7\u00b7 \u2264\u03bbn and corresponding real, orthonormal\neigenvectors v1, . . . , vn \u2208 Rn, where orthonormality is assessed with respect to\nthe standard inner product over Rn: \u27e8u, v\u27e9 = u\u22a4v.\nFor now, we focus on computing one extreme eigenpair ofA: (\u03bb1, v1) or (\u03bbn, vn)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2737, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eefec921-39ed-4192-9e5d-8c4777188967": {"__data__": {"id_": "eefec921-39ed-4192-9e5d-8c4777188967", "embedding": null, "metadata": {"page_label": "6", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7c1e82c4-7a78-41ad-a36a-6984f463e942", "node_type": "4", "metadata": {"page_label": "6", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "f727d979de6b4527664501bd7e29ee4be0093a9e09a2d18757c47ba69c780b48", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n6 Simple examples\nwill do. Let Rn\n\u2217 denote the set of nonzero vectors in Rn. It is well known that\nthe Rayleigh quotient,\nr: Rn\n\u2217 \u2192 R: x 7\u2192 r(x) = \u27e8x, Ax\u27e9\n\u27e8x, x\u27e9 ,\nattains its extreme values when x is aligned with \u00b1v1 or \u00b1vn, and that the\ncorresponding value of the quotient is\u03bb1 or \u03bbn. We will rediscover such properties\nthrough the prism of optimization on manifolds as a running example in this\nbook. One can gain some insight by checking that r(vi) = \u03bbi.\nSay we are interested in the smallest eigenvalue, \u03bb1. Then, we must solve the\nfollowing optimization problem:\nmin\nx\u2208Rn\u2217\n\u27e8x, Ax\u27e9\n\u27e8x, x\u27e9 .\nThe set Rn\n\u2217 is open in Rn: it is an open submanifold of Rn. Optimization over\nan open set has its challenges (more on this later). Fortunately, we can easily\ncircumvent these issues in this instance.\nSince the Rayleigh quotient is invariant to scaling, that is, since r(\u03b1x) = r(x)\nfor all nonzero real \u03b1, we may fix the scale arbitrarily. Given the denominator\nof r, one particularly convenient way is to restrict our attention to unit-norm\nvectors: \u2225x\u22252 = \u27e8x, x\u27e9 = 1. The set of such vectors is the unit sphere in Rn:\nSn\u22121 = {x \u2208 Rn : \u2225x\u2225 = 1}.\nThis is an embedded submanifold of Rn. Our problem becomes:\nmin\nx\u2208Sn\u22121\n\u27e8x, Ax\u27e9. (2.1)\nThis is perhaps the simplest non-trivial instance of an optimization problem on\na manifold: we use it recurringly to illustrate concepts as they occur.\nSimilarly to the above, we may compute the largest singular value of a matrix\nM \u2208 Rm\u00d7n together with associated left- and right-singular vectors by solving\nmax\nx\u2208Sm\u22121,y\u2208Sn\u22121\n\u27e8x, My\u27e9. (2.2)\nThis is the basis of principal component analysis: see also Section 2.4. The search\nspace is a Cartesian product of two spheres. This too is a manifold; specifically,\nan embedded submanifold of Rm \u00d7 Rn. In general:\nProducts of manifolds are manifolds.\nThis is an immensely useful property.\n2.3 Dictionary learning: products of spheres\nJPEG and its more recent version JPEG 2000 are some of the most commonly\nused compression standards for photographs. At their core, these algorithms rely", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2316, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7775c25e-9f92-4ed9-8377-b2f4128b5f5e": {"__data__": {"id_": "7775c25e-9f92-4ed9-8377-b2f4128b5f5e", "embedding": null, "metadata": {"page_label": "7", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "344d98cb-3668-4b68-9244-5302150bf8a4", "node_type": "4", "metadata": {"page_label": "7", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "f6110c66181c445861c84f606ffdc8e6f6c21be2a146522134419e7d8822d655", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n2.3 Dictionary learning: products of spheres 7\non basis expansions: discrete cosine transforms for JPEG, and wavelet transforms\nfor JPEG 2000. That is, an image (or rather, each patch of the image) is written\nas a linear combination of a fixed collection of basis images. To fix notation, say\nan image is represented as a vector y \u2208 Rd (its pixels rearranged into a single\ncolumn vector) and the basis images are b1, . . . , bd \u2208 Rd (each of unit norm).\nThere exists a unique set of coordinates c \u2208 Rd such that:\ny = c1b1 + \u00b7 \u00b7\u00b7+ cdbd.\nSince the basis images are fixed (and known to anyone creating or reading image\nfiles in this format), it is equivalent to store y or c.\nThe basis is designed carefully with two goals in mind. First, the transform\nbetween y and c should be fast to compute (one good starting point to that\neffect is orthogonality). Second, images encountered in practice should lead to\nmany of the coefficients ci being zero, or close to zero. Indeed, to recover y, it is\nonly necessary to record the nonzero coefficients. To compress further, we may\nalso decide not to store the small coefficients: if so, y can still be reconstructed\napproximately. Beyond compression, another benefit of sparse expansions is that\nthey can reveal structural information about the contents of the image.\nIn dictionary learning, we focus on the second goal. As a key departure from the\nabove, the idea here is not to design a basis by hand, but rather to learn a good\nbasis from data automatically. This way, we may exploit structural properties\nof images that come up in a particular application. For example, it may be the\ncase that photographs of faces can be expressed more sparsely in a dedicated\nbasis as compared to a standard wavelet basis. Pushing this idea further, we\nrelax the requirement of identifying a basis, instead allowing ourselves to pick\nmore than d images for our expansions. The collection of images b1, . . . , bn \u2208 Rd\nforms a dictionary. Its elements are called atoms, and they normally span Rd\nin an overcomplete way, meaning any image y can be expanded into a linear\ncombination of atoms in more than one way. The aim is that at least one of these\nexpansions should be sparse, or have many small coefficients. For the magnitudes\nof coefficients to be meaningful, we further require all atoms to have the same\nnorm: \u2225bi\u2225 = 1 for all i.\nThus, given a collection of k images y1, . . . , yk \u2208 Rd, the task in dictionary\nlearning is to find atoms b1, . . . , bn \u2208 Rd such that (as much as possible) each\nimage yi is a sparse linear combination of the atoms. Collect the input images as\nthe columns of a data matrix Y \u2208 Rd\u00d7k, and the atoms into a matrix D \u2208 Rd\u00d7n\n(to be determined). Expansion coefficients for the images in this dictionary form\nthe columns of a matrix C \u2208 Rn\u00d7k so that\nY = DC.\nTypically, many choices of C are possible. We aim to pick D such that there\nexists a valid (or approximately valid) choice of C with numerous zeros. Let\n\u2225C\u22250 denote the number of entries of C different from zero. Then, one possible\nformulation of dictionary learning balances both aims with a parameter \u03bb >0", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3383, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b651f8d9-d310-4170-9c22-2d2c83cf2015": {"__data__": {"id_": "b651f8d9-d310-4170-9c22-2d2c83cf2015", "embedding": null, "metadata": {"page_label": "8", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c8875e3-2734-4c9d-9ccb-d98e56ab3b88", "node_type": "4", "metadata": {"page_label": "8", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "8a91a36524feb00fe10642b7a603d4f28f8b13598291c44d2553fbdc2974526a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n8 Simple examples\nas (with b1, . . . , bn the columns of the dictionary matrix D):\nmin\nD\u2208Rd\u00d7n,C\u2208Rn\u00d7k\n\u2225Y \u2212 DC\u22252 + \u03bb\u2225C\u22250 (2.3)\nsubject to \u2225b1\u2225 = \u00b7 \u00b7\u00b7= \u2225bn\u2225 = 1.\nThe matrix norm \u2225 \u00b7 \u2225is the Frobenius norm, induced by the standard inner\nproduct \u27e8U, V\u27e9 = Tr(U\u22a4V ).\nEvidently, allowing the dictionary to be overcomplete ( n > d) helps sparsity.\nAn extreme case is to set n = k, in which case an optimal solution consists in\nletting D be Y with normalized columns. Then, each image can be expressed\nwith a single nonzero coefficient (C is diagonal). This is useless of course, if only\nbecause both parties of the communication must have access to the (possibly\nhuge) dictionary, and because this choice may generalize poorly when presented\nwith new images. Interesting scenarios involve n much smaller than k.\nThe search space for D is a product of several spheres, which is an embedded\nsubmanifold of Rd\u00d7n called the oblique manifold :\nOB(d, n) = (Sd\u22121)n =\n\b\nX \u2208 Rd\u00d7n : diag(X\u22a4X) = 1\n\t\n,\nwhere 1 \u2208 Rn is the all-ones vector and diag: Rn\u00d7n \u2192 Rn extracts the diagonal\nentries of a matrix. The search space in C is the linear manifold Rn\u00d7k. Overall,\nthe search space of the dictionary learning optimization problem is\nOB(d, n) \u00d7 Rn\u00d7k,\nwhich is an embedded submanifold of Rd\u00d7n \u00d7 Rn\u00d7k.\nWe note in closing that the cost function in (2.3) is discontinuous because of\nthe term \u2225C\u22250, making it hard to optimize. A standard reformulation replaces\nthe culprit with \u2225C\u22251: the sum of absolute values of the entries of C. This is\ncontinuous but nonsmooth. A possible further step then is to smooth the cost\nfunction, for example exploiting that |x| \u2248\n\u221a\nx2 + \u03b52 or |x| \u2248\u03b5 log(ex/\u03b5 + e\u2212x/\u03b5)\nfor small \u03b5 >0: these are standard tricks.\nRegardless of changes to the cost function, the manifold OB( d, n) is non-\nconvex, so that finding a global optimum for dictionary learning as stated above\nis challenging: see work by Sun et al. [SQW17] for some guarantees.\n2.4 Principal component analysis: Stiefel and Grassmann\nLet x1, . . . , xn \u2208 Rd represent a large collection of centered data points in a\nd-dimensional linear space. We may think of it as a cloud of points. It may be\nthe case that this cloud lies on or near a low-dimensional subspace of Rd, and\nit may be distributed anisotropically in that subspace, meaning it shows more\nvariation along some directions than others. One of the pillars of data analysis is\nto determine the main directions of variation of the data. This goes by the name\nof principal component analysis (PCA), which we encountered in Section 2.2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2808, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0f73c86c-1162-4372-94fc-883d11c80fa4": {"__data__": {"id_": "0f73c86c-1162-4372-94fc-883d11c80fa4", "embedding": null, "metadata": {"page_label": "9", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "956b72c4-b94d-4bdb-a1f1-d751a76f4f27", "node_type": "4", "metadata": {"page_label": "9", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "265af4a1efd3ffac5e6149fd02a89bd66ab6517a896638a0d386047610be33da", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n2.4 Principal component analysis: Stiefel and Grassmann 9\nOne way to think of a main direction of variation, called a principal com-\nponent, is as a vector u \u2208 Sd\u22121 such that projecting the data points to the\none-dimensional subspace spanned by u \u2018preserves most of the variance.\u2019 Specif-\nically, let X \u2208 Rd\u00d7n be the matrix whose columns are the data points and let\nuu\u22a4 be the orthogonal projector from Rd to the span of u. We wish to maximize\nthe following for u \u2208 Sd\u22121:\nnX\ni=1\n\u2225uu\u22a4xi\u22252 = \u2225uu\u22a4X\u22252 = \u27e8X\u22a4uu\u22a4, X\u22a4uu\u22a4\u27e9 = \u27e8XX \u22a4u, u\u27e9.\nWe recognize the Rayleigh quotient of the matrix XX \u22a4 to be maximized for u\nover Sd\u22121 (Section 2.2). An optimal solution is given by a dominant eigenvector\nof XX \u22a4, or equivalently by a dominant left singular vector of X.\nLet u1 \u2208 Sd\u22121 be a principal component. We would like to find a second one.\nThat is, we aim to find u2 \u2208 Sd\u22121, orthogonal to u1, such that projecting the\ndata to the subspace spanned by u1 and u2 preserves the most variance. The\northogonal projector to that subspace is u1u\u22a4\n1 + u2u\u22a4\n2. We maximize\n\u2225(u1u\u22a4\n1 + u2u\u22a4\n2)X\u22252 = \u27e8XX \u22a4u1, u1\u27e9 + \u27e8XX \u22a4u2, u2\u27e9\nover u2 \u2208 Sd\u22121 with u\u22a4\n2u1 = 0. The search space for u2 is an embedded subman-\nifold of Rd: it is a unit sphere in the subspace orthogonal to u1.\nIt is often more convenient to optimize for u1 and u2 simultaneously rather\nthan sequentially. Then, since the above cost function is symmetric in u1 and\nu2, as is the constraint u\u22a4\n2u1 = 0, we add weights to the two terms to ensure u1\ncaptures a principal component and u2 captures a second principal component:\nmax\nu1,u2\u2208Sd\u22121,u\u22a4\n2 u1=0\n\u03b11\u27e8XX \u22a4u1, u1\u27e9 + \u03b12\u27e8XX \u22a4u2, u2\u27e9,\nwith \u03b11 > \u03b12 > 0 arbitrary.\nMore generally, aiming for k principal components, we look for a matrix U \u2208\nRd\u00d7k with k orthonormal columns u1, . . . , uk \u2208 Rd. The set of such matrices is\ncalled the Stiefel manifold :\nSt(d, k) = {U \u2208 Rd\u00d7k : U\u22a4U = Ik},\nwhere Ik is the identity matrix of size k. It is an embedded submanifold of Rd\u00d7k.\nThe orthogonal projector to the subspace spanned by the columns of U is UU \u22a4.\nHence, PCA amounts to solving the problem:\nmax\nU\u2208St(d,k)\nkX\ni=1\n\u03b1i\u27e8XX \u22a4ui, ui\u27e9 = max\nU\u2208St(d,k)\n\u27e8XX \u22a4U, UD\u27e9, (2.4)\nwhere D \u2208 Rk\u00d7k is diagonal with diagonal entries \u03b11 > \u00b7 \u00b7\u00b7> \u03b1k > 0.\nIt is well known that collecting k top eigenvectors of XX \u22a4 (or, equivalently,\nk top left singular vectors of X) yields a global optimum of (2.4), meaning this\noptimization problem can be solved efficiently using tools from numerical linear", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2708, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "74a77453-e29f-4796-b16b-64cd9c07b2b9": {"__data__": {"id_": "74a77453-e29f-4796-b16b-64cd9c07b2b9", "embedding": null, "metadata": {"page_label": "10", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e86599d-7b92-4369-afb2-697c04aace39", "node_type": "4", "metadata": {"page_label": "10", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "80a32a8d580144659c44249bcc94998b672a54afb8c7b8bb1fab35d5b8711faf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n10 Simple examples\nalgebra. Still, the optimization perspective offers significant flexibility that stan-\ndard linear algebra algorithms cannot match. Specifically, within an optimization\nframework, it is possible to revisit the variance criterion by changing the cost\nfunction. This allows one to promote sparsity or robustness against outliers, for\nexample to develop variants such as sparse PCA [dBEG08, JNRS10] and robust\nPCA [MT11, GZAL14, MZL19, NNSS20]. There may also be computational ad-\nvantages, for example in tracking and online models where the dataset changes\nor grows with time: it may be cheaper to update a previously computed good\nestimator using few optimization steps than to run a complete eigenvalue or\nsingular value decomposition anew.\nIf the top k principal components are of interest but their ordering is not,\nthen we do not need the weight matrix D. In this scenario, we are seeking an\northonormal basis U for a k dimensional subspace of Rd such that projecting\nthe data to that subspace preserves as much of the variance as possible. This\ndescription makes it clear that the particular basis is irrelevant: only the selected\nsubspace matters. This is apparent in the cost function,\nf(U) = \u27e8XX \u22a4U, U\u27e9,\nwhich is invariant under orthogonal transformations. Specifically, for allQ in the\northogonal group\nO(k) = {Q \u2208 Rk\u00d7k : Q\u22a4Q = Ik},\nwe have f(UQ) = f(U). This induces an equivalence relation1 \u223c on the Stiefel\nmanifold:\nU \u223c V \u21d0 \u21d2 V = UQ for some Q \u2208 O(k).\nThis equivalence relation partitions St( d, k) into equivalence classes:\n[U] = {V \u2208 St(d, k) : U \u223c V } = {UQ : Q \u2208 O(k)}.\nThe set of equivalence classes is called the quotient set :\nSt(d, k)/\u223c = St(d, k)/O(k) = {[U] : U \u2208 St(d, k)}.\nImportantly, U, V\u2208 St(d, k) are equivalent if and only if their columns span the\nsame subspace of Rd. In other words: the quotient set is in one-to-one correspon-\ndence with the set of subspaces of dimension k in Rd. With the right geometry,\nthe latter is called the Grassmann manifold :\nGr(d, k) = { subspaces of dimension k in Rd } \u2261St(d, k)/O(k),\nwhere the symbol \u2261 reads \u201cis equivalent to\u201d (context indicates in what sense).\nAs defined here, the Grassmann manifold is a quotient manifold . This type of\n1 Recall that an equivalence relation \u223c on a set M is a reflexive ( a \u223c a), symmetric ( a \u223c\nb \u21d0\u21d2 b \u223c a) and transitive (a \u223c b and b \u223c c =\u21d2 a \u223c c) binary relation. The equivalence\nclass [a] is the set of elements of M that are equivalent to a. Each element of M belongs to\nexactly one equivalence class.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2784, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c9a419a7-dcd6-46a5-9eb7-51802c7488b6": {"__data__": {"id_": "c9a419a7-dcd6-46a5-9eb7-51802c7488b6", "embedding": null, "metadata": {"page_label": "11", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1e65bf96-4437-4a91-8a4c-d595e883e6ec", "node_type": "4", "metadata": {"page_label": "11", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "47fb7e5d94a3db20111b572443d31e1dfa7ff406af0ddff014b9c1bf38b2427f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n2.5 Synchronization of rotations: special orthogonal group 11\nmanifold is more abstract than embedded submanifolds, but we can still develop\nnumerically efficient tools to work with them.\nWithin our framework, computing the dominant eigenspace of dimension k of\nthe matrix XX \u22a4 can be written as:\nmax\n[U]\u2208Gr(d,k)\n\u27e8XX \u22a4U, U\u27e9.\nThe cost function is well defined over Gr( d, k) since it depends only on the\nequivalence class of U, not on U itself.\nGoing back to (2.4), we note in passing thatk top left and right singular vectors\nof a matrix M \u2208 Rm\u00d7n can be computed by solving the following problem on\na product of Stiefel manifolds (this and (2.4) are sometimes called Brockett cost\nfunctions):\nmax\nU\u2208St(m,k),V \u2208St(n,k)\n\u27e8MV , UD\u27e9,\nwhere D = diag(\u03b11, . . . , \u03b1k) with arbitrary \u03b11 > \u00b7 \u00b7\u00b7> \u03b1k > 0 as above.\nA book by Trendafilov and Gallo provides more in-depth discussion of appli-\ncations of optimization on manifolds to data analysis [TG21].\n2.5 Synchronization of rotations: special orthogonal group\nIn structure from motion (SfM), the 3D structure of an object is to be recon-\nstructed from several 2D images of it. For example, in the paper Building Rome\nin a Day [ASS+09], the authors automatically construct a model of the Colos-\nseum from over 2000 photographs freely available on the Internet. Because the\npictures are acquired from an unstructured source, one of the steps in the re-\nconstruction pipeline is to estimate camera locations and pose. The pose of a\ncamera is its orientation in space.\nIn single particle reconstruction through cryo electron microscopy, an electron\nmicroscope is used to produce 2D tomographic projections of biological objects\nsuch as proteins and viruses. Because shape is a determining factor of function,\nthe goal is to estimate the 3D structure of the object from these projections.\nContrary to X-ray crystallography (another fundamental tool of structural biol-\nogy), the orientations of the objects in the individual projections are unknown. In\norder to estimate the structure, a useful step is to estimate the individual orien-\ntations (though high noise levels do not always allow it, in which case alternative\nstatistical techniques must be used.)\nMathematically, orientations correspond to rotations of R3. Rotations in Rd\ncan be represented with orthogonal matrices:\nSO(d) = {R \u2208 Rd\u00d7d : R\u22a4R = Id and det( R) = +1}.\nThe determinant condition excludes reflections ofRd. The set SO(d) is the special", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2702, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "771b2a88-48a5-4711-8fdd-6eded2b2ab3d": {"__data__": {"id_": "771b2a88-48a5-4711-8fdd-6eded2b2ab3d", "embedding": null, "metadata": {"page_label": "12", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "973a168c-2c19-492d-b3d6-780f1ed423fc", "node_type": "4", "metadata": {"page_label": "12", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "574bd9dcd5b03b5cc6155403d44549ec1d2a8e9d7c9be3fd06c2942f385e3d3e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n12 Simple examples\northogonal group: it is both a group (in the mathematical sense of the term) and\na manifold (an embedded submanifold of Rd\u00d7d)\u2014it is a Lie group.\nIn both applications described above, similar images or projections can be\ncompared to estimate relative orientations. Synchronization of rotations is a\nmathematical abstraction of the ensuing task. It consists in estimating n indi-\nvidual rotation matrices,\nR1, . . . , Rn \u2208 SO(d),\nfrom pairwise relative rotation measurements: for some pairs (i, j) corresponding\nto the edges of a graph G, we observe a noisy version of RiR\u22121\nj . Let Hij \u2208 SO(d)\ndenote such a measurement. Then, one possible formulation of synchronization\nof rotations is:\nmin\n\u02c6R1,..., \u02c6Rn\u2208SO(d)\nX\n(i,j)\u2208G\n\u2225 \u02c6Ri \u2212 Hij \u02c6Rj\u22252.\nThis is an optimization problem over SO( d)n, which is a manifold.\nThis also comes up insimultaneous localization and mapping (SLAM), whereby\na robot must simultaneously map its environment and locate itself in it as it\nmoves around [RDTEL21]. An important aspect of SLAM is to keep track of the\nrobot\u2019s orientation accurately, by integrating all previously acquired information\nto correct estimator drift.\n2.6 Low-rank matrix completion: fixed-rank manifold\nLet M \u2208 Rm\u00d7n be a large matrix of interest. Given some of its entries, our task\nis to estimate the whole matrix. A commonly cited application for this setup\nis that of recommender systems, where row i corresponds to a user, column j\ncorresponds to an item (a movie, a book. . . ) and entry Mij indicates how much\nuser i appreciates item j: positive values indicate appreciation, zero is neutral,\nand negative values indicate dislike. The known entries may be collected from\nuser interactions. Typically, most entries are unobserved. Predicting the missing\nvalues may be helpful to automate personalized recommendations.\nOf course, without further knowledge about how the entries of the matrix\nare related, the completion task is ill-posed. Hope comes from the fact that\ncertain users share similar traits, so that what one user likes may be informative\nabout what another, similar user may like. In the same spirit, certain items may\nbe similar enough that whole groups of users may feel similarly about them.\nOne mathematically convenient way to capture this idea is to assume M has\n(approximately) low rank. The rationale is as follows: if M has rank r, then it\ncan be factored as\nM = LR\u22a4,\nwhere L \u2208 Rm\u00d7r and R \u2208 Rn\u00d7r are full-rank factor matrices. Row i of L, \u2113i,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2741, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6935c2da-a36d-4996-8f6a-60d1f4f0a83e": {"__data__": {"id_": "6935c2da-a36d-4996-8f6a-60d1f4f0a83e", "embedding": null, "metadata": {"page_label": "13", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4befef3e-ceed-43e4-8556-0b23b59c302d", "node_type": "4", "metadata": {"page_label": "13", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "896a8f34736d8d4a9bb8622371fd404f2ab29cfa592bec9fd166ba92fa6a38ad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n2.6 Low-rank matrix completion: fixed-rank manifold 13\nattributes r numbers to user i, while the jth row of R, rj, attributes r numbers to\nitem j. Under the low-rank model, the rating of user i for item j is Mij = \u27e8\u2113i, rj\u27e9.\nOne interpretation is that there arer latent features (these could be movie genres\nfor example): a user has some positive or negative appreciation for each feature,\nand an item has traits aligned with or in opposition to these features; the rating\nis obtained as the inner product of the two feature vectors.\nUnder this model, predicting the user ratings for all items amounts tolow-rank\nmatrix completion. Let \u2126 denote the set of pairs ( i, j) such that Mij is observed.\nAllowing for noise in the observations and inaccuracies in the model, we aim to\nsolve\nmin\nX\u2208Rm\u00d7n\nX\n(i,j)\u2208\u2126\n(Xij \u2212 Mij)2\nsubject to rank( X) = r.\nThe search space for this optimization problem is the set of matrices of a given\nsize and rank:\nRm\u00d7n\nr = {X \u2208 Rm\u00d7n : rank(X) = r}.\nThis set is an embedded submanifold of Rm\u00d7n which is frequently useful in\nmachine learning applications.\nAnother use for this manifold is solving high-dimensional matrix equations\nthat may come up in systems and control applications: aiming for a low-rank\nsolution may be warranted in certain settings, and exploiting this can lower\nthe computational burden substantially. Yet another context where optimiza-\ntion over low-rank matrices occurs is in completing and denoising approximately\nseparable bivariate functions based on sampled values [Van10, Van13, MV13].\nThe same set can also be endowed with other geometries, that is, it can be\nmade into a manifold in other ways. For example, exploiting the factored form\nmore directly, note that any matrix in Rm\u00d7n\nr admits a factorization as LR\u22a4 with\nboth L and R of full rank r. This correspondence is not one-to-one however,\nsince the pairs (L, R) and (LJ\u22121, RJ\u22a4) map to the same matrix in Rm\u00d7n\nr for all\ninvertible matrices J: they are equivalent. Similarly to the Grassmann manifold,\nthis leads to a definition of Rm\u00d7n\nr as a quotient manifold instead of an embedded\nsubmanifold. Many variations on this theme are possible, some of them more\nuseful than others depending on the application [Mey11, Mis14].\nThe set Rm\u00d7n\nr is not closed in Rm\u00d7n, which may create difficulties for op-\ntimization. The closure of the set corresponds to all matrices of rank at most\nr (rather than exactly equal to r). That set is not a manifold, but it can be\nsmoothly parameterized by a manifold in several ways [LKB22b]. One particu-\nlarly simple way is through the map ( L, R) 7\u2192 LR\u22a4 where L and R are allowed\nto be rank deficient.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2895, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1bb718f6-ed35-4b60-9414-7f5a64bdf230": {"__data__": {"id_": "1bb718f6-ed35-4b60-9414-7f5a64bdf230", "embedding": null, "metadata": {"page_label": "14", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5bf16086-ab9d-45f9-98d0-b62d20c20e1d", "node_type": "4", "metadata": {"page_label": "14", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "cf7d9286accbf6b0265b1d90c594d5e8f9f9178aa889719dce4a881ca4a8e05f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n14 Simple examples\n2.7 Gaussian mixture models: positive definite matrices\nA common model in machine learning assumes data x1, . . . , xn \u2208 Rd are sampled\nindependently from a mixture of K Gaussians, that is, each data point is sampled\nfrom a probability distribution with density of the form\nf(x) =\nKX\nk=1\nwk\n1p\n2\u03c0 det(\u03a3k)\ne\u2212\n(x\u2212\u00b5k)\u22a4\u03a3\u22121\nk (x\u2212\u00b5k)\n2 ,\nwhere the centers \u00b51, . . . , \u00b5K \u2208 Rd, covariances \u03a3 1, . . . ,\u03a3K \u2208 Sym(d)+ and\nmixing probabilities ( w1, . . . , wK) \u2208 \u2206K\u22121\n+ are to be determined. We use the\nfollowing notation:\nSym(d)+ = {X \u2208 Rd\u00d7d : X = X\u22a4 and X \u227b 0}\nfor symmetric, positive definite matrices of size d, and\n\u2206K\u22121\n+ = {w \u2208 RK : w1, . . . , wK > 0 and w1 + \u00b7 \u00b7\u00b7+ wK = 1}\nfor the positive part of the simplex, that is, the set of non-vanishing discrete\nprobability distributions over K objects. In this model, with probability wk, a\npoint x is sampled from the kth Gaussian, with mean \u00b5k and covariance \u03a3 k.\nThe aim is only to estimate the parameters, not to estimate which Gaussian\neach point xi was sampled from.\nFor a given set of observations x1, . . . , xn, a maximum likelihood estimator\nsolves:\nmax\n\u02c6\u00b51,...,\u02c6\u00b5K\u2208Rd,\n\u02c6\u03a31,...,\u02c6\u03a3K\u2208Sym(d)+,\nw\u2208\u2206K\u22121\n+\nnX\ni=1\nlog\n KX\nk=1\nwk\n1p\n2\u03c0 det(\u03a3k)\ne\u2212\n(xi\u2212\u00b5k)\u22a4\u03a3\u22121\nk (xi\u2212\u00b5k)\n2\n!\n. (2.5)\nThis is an optimization problem over Rd\u00d7K \u00d7 (Sym(d)+)K \u00d7 \u2206K\u22121\n+ , which can\nbe made into a manifold because Sym( d)+ and \u2206K\u22121\n+ can be given a manifold\nstructure.\nThe direct formulation of maximum likelihood estimation for Gaussian mixture\nmodels in (2.5) is, however, not computationally favorable. See [HS15] for a\nbeneficial reformulation, still on a manifold.\n2.8 Smooth semidefinite programs\nSemidefinite programs (SDPs) are optimization problems of the form\nmin\nX\u2208Sym(n)\n\u27e8C, X\u27e9 subject to A(X) = b and X \u2ab0 0, (2.6)\nwhere Sym(n) is the space of real, symmetric matrices of size n \u00d7 n, \u27e8A, B\u27e9 =\nTr(A\u22a4B), A: Sym(n) \u2192 Rm is a linear map defined by m symmetric matrices\nA1, . . . , Am as A(X)i = \u27e8Ai, X\u27e9, and X \u2ab0 0 means X is positive semidefinite.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2250, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7f033283-2b30-4d83-ab86-659b0b67f114": {"__data__": {"id_": "7f033283-2b30-4d83-ab86-659b0b67f114", "embedding": null, "metadata": {"page_label": "15", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "152dc485-c35f-4b67-8054-9485b33fbc31", "node_type": "4", "metadata": {"page_label": "15", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "8ce7a055c0bee1e84cdc55eb0f4bdfeb0ab1b06e1f25ab8df5a557a9e5aaccda", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n2.8 Smooth semidefinite programs 15\nSDPs are convex and they can be solved to global optimality in polyno-\nmial time using interior point methods [Nes18, \u00a7 5.4.4]. Still, handling the pos-\nitive semidefiniteness constraint X \u2ab0 0 and the dimensionality of the problem\n(namely, the n(n+1)\n2 variables required to define X) both pose significant com-\nputational challenges for large n.\nA popular way to address both issues is the Burer\u2013Monteiro approach [BM05],\nwhich consists in factorizing X as X = Y Y\u22a4 with Y \u2208 Rn\u00d7p: the number p\nof columns of Y is a parameter. Notice that X is now automatically positive\nsemidefinite. If p \u2265 n, the SDP can be rewritten equivalently as\nmin\nY \u2208Rn\u00d7p\n\u27e8CY , Y\u27e9 subject to A(Y Y\u22a4) = b. (2.7)\nIf p < n, this corresponds to the SDP with the additional constraint rank(X) \u2264 p.\nThere is a computational advantage to takingp as small as possible. Interestingly,\nif the set of matrices X that are feasible for the SDP is compact, then the\nPataki\u2013Barvinok bound [Pat98, Bar95] provides that at least one of the global\noptimizers of the SDP has rankr such that r(r+1)\n2 \u2264 m. In other words: assuming\ncompactness, the Burer\u2013Monteiro formulation (2.7) is equivalent to the original\nSDP so long as p satisfies p(p+1)\n2 \u2265 m. This is already the case for p = O(\u221am),\nwhich may be significantly smaller than n.\nThe positive semidefiniteness constraint disappeared, and the dimensionality\nof the problem went from O(n2) to np\u2014a potentially appreciable gain. Yet, we\nlost something important along the way: the Burer\u2013Monteiro problem is not\nconvex. It is not immediately clear how to solve it.\nThe search space of the Burer\u2013Monteiro problem is the set of feasible Y :\nM = {Y \u2208 Rn\u00d7p : A(Y Y\u22a4) = b}. (2.8)\nAssume the map Y 7\u2192 A(Y Y\u22a4) has the property that its differential at all Y in\nM has rank m. Then, M is an embedded submanifold of Rn\u00d7p. In this special\ncase, we may try to solve the Burer\u2013Monteiro problem through optimization over\nthat manifold. It turns out that non-convexity is mostly benign in that scenario,\nin a precise sense [BVB19]:\nIf M is compact and p(p+1)\n2 > m, then, for a generic cost matrix C, the smooth\noptimization problem minY \u2208M \u27e8CY , Y\u27e9 has no spurious local minima, in the\nsense that any point Y which satisfies first- and second-order necessary\noptimality conditions is a global optimum.\n(Necessary optimality conditions are detailed in Sections 4.2 and 6.1.) Addition-\nally, these global optima map to global optima of the SDP through X = Y Y\u22a4.\nThis suggests that smooth-and-compact SDPs may be solved to global optimality\nvia optimization on manifolds. The requirement that M be a regularly defined\nsmooth manifold is not innocuous, but it is satisfied in a number of interesting\napplications.\nThere has been a lot of work on this front in recent years, including the early", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3076, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ec107ed3-2444-41b8-bd4d-cb3b552f92ba": {"__data__": {"id_": "ec107ed3-2444-41b8-bd4d-cb3b552f92ba", "embedding": null, "metadata": {"page_label": "16", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bc5dcddf-91a6-47b6-ace9-3bde0377f39a", "node_type": "4", "metadata": {"page_label": "16", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "c92f0c6c7a63473751cc39534f85e68ee05fb79b3ea34aff99f03a7c7940d8da", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n16 Simple examples\nwork by Burer and Monteiro [BM03, BM05], the first manifold-inspired perspec-\ntive by Journ\u00b4 ee et al. [JBAS10], qualifications of the benign non-convexity at the\nPataki\u2013Barvinok threshold [BVB16, BVB19] and below in special cases [BBV16],\na proof that p cannot be set much lower than that threshold in general [WW20],\nsmoothed analyses to assess whether points which satisfy necessary optimal-\nity conditions approximately are also approximately optimal [BBJN18, PJB18,\nCM19] and extensions to accommodate scenarios where M is not a smooth\nmanifold but, more generally, a real algebraic variety [BBJN18, Cif21]. See all\nthese references for applications, including Max-Cut, community detection, the\ntrust-region subproblem, synchronization of rotations and more.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1040, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "15d32515-ba1d-4e6f-b1c4-5921b610b0b4": {"__data__": {"id_": "15d32515-ba1d-4e6f-b1c4-5921b610b0b4", "embedding": null, "metadata": {"page_label": "17", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1866a56e-4726-4022-8cf1-dd68d0b5efe5", "node_type": "4", "metadata": {"page_label": "17", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "5535d4d31c9e01e65a3134ec82198fc7e795399f287621d6f63826a5ec630bda", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n3 Embedded geometry: first order\nOur goal is to develop optimization algorithms to solve problems of the form\nmin\nx\u2208M\nf(x), (3.1)\nwhere M is a smooth and possibly nonlinear space, and f : M \u2192R is a smooth\ncost function. In order to do so, our first task is to clarify what we mean by a\n\u201csmooth space,\u201d and a \u201csmooth function\u201d on such a space. Then, we need to\ndevelop any tools required to construct optimization algorithms in this setting.\nLet us start with a bird\u2019s-eye view of what this entails, and formalize later on.\nFor smoothness of M, our model space is the unit sphere in Rd:\nSd\u22121 = {x \u2208 Rd : x\u22a4x = 1}. (3.2)\nIntuitively, we think of Sd\u22121 as a smooth nonlinear space in Rd. Our definitions\nbelow are compatible with this intuition: we call S d\u22121 an embedded submanifold\nof Rd.\nAn important element in these definitions is to capture the idea that S d\u22121\ncan be locally approximated by a linear space around any point x: we call these\ntangent spaces, denoted by T xSd\u22121. This is as opposed to a cube for which no\ngood linearization exists at the edges. More specifically for our example, S d\u22121\nis defined by the constraint x\u22a4x = 1. We may expect that differentiating this\nconstraint should yield a suitable linearization, and indeed it does:\nTxSd\u22121 = {v \u2208 Rd : v\u22a4x + x\u22a4v = 0} = {v \u2208 Rd : x\u22a4v = 0}. (3.3)\nIn the same spirit, it stands to reason that linear spaces and open subsets of\nlinear spaces should also be considered smooth, as they are locally linear too.\nWe say a function from Rd to R is smooth if it is infinitely differentiable. We\nmay expect that a function f : Sd\u22121 \u2192 R obtained by restriction to S d\u22121 of a\nsmooth function on Rd ought to be considered smooth. We adopt (essentially)\nthis as our definition of smooth functions on S d\u22121.\nIn this early chapter, we give a restricted definition of smoothness, focusing on\nembedded submanifolds. This allows us to build our initial toolbox more rapidly,\nand is sufficient to handle many applications. We extend our perspective to the\ngeneral framework later on, in Chapter 8.\nTo get started with a list of required tools, it is useful to review briefly the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2381, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "22cb6cc1-fb75-484e-b89b-dba4da317497": {"__data__": {"id_": "22cb6cc1-fb75-484e-b89b-dba4da317497", "embedding": null, "metadata": {"page_label": "18", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "81419660-eaa5-4cde-a282-a31e3e70cc4b", "node_type": "4", "metadata": {"page_label": "18", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "6308583072760becc559b34cdf39617f2cd019d7f51531ba9dbb4f2d6aa9f2d6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n18 Embedded geometry: first order\nmain ingredients of optimization on a linear space E:\nmin\nx\u2208E\nf(x). (3.4)\nFor example, E = Rd or E = Rn\u00d7p. Perhaps the most fundamental algorithm to\naddress this class of problems is gradient descent, also known as steepest descent.\nGiven an initial guess x0 \u2208 E, this algorithm produces iterates on E (a sequence\nof points on E) as follows:1\nxk+1 = xk \u2212 \u03b1kgradf(xk), k = 0, 1, 2, . . . (3.5)\nwhere the \u03b1k > 0 are aptly chosen step-sizes and gradf : E \u2192 Eis the gradient of\nf. Under mild assumptions, the accumulation points of the sequencex0, x1, x2, . . .\nhave relevant properties for the optimization problem (3.4). We study these later,\nin Chapter 4.\nFrom this discussion, we can identify a list of desiderata for a geometric tool-\nbox, meant to solve\nmin\nx\u2208Sd\u22121\nf(x) (3.6)\nwith some smooth function f on the sphere. The most pressing point is to find\nan alternative for the implicit use of linearity in (3.5). Indeed, above, both xk\nand gradf(xk) are elements of E. Since E is a linear space, they can be combined\nwith linear operations. Putting aside for now the issue of defining a proper notion\nof gradient for a function f on Sd\u22121, we must still contend with the issue that\nSd\u22121 is not a linear space: we have no notion of linear combination here.\nAlternatively, we can reinterpret iteration (3.5) and say:\nTo produce xk+1 \u2208 Sd\u22121, move away from xk along the direction \u2212gradf(xk)\nover some distance, while remaining on Sd\u22121.\nSurely, if the purpose is to remain on S d\u22121, it would make little sense to move\nradially away from the sphere. Rather, using the notion that smooth spaces can\nbe linearized around x by a tangent space TxSd\u22121, we only consider moving along\ndirections in TxSd\u22121. To this end, we introduce the concept of retraction at x: a\nmap Rx : TxSd\u22121 \u2192 Sd\u22121 which allows us to move away from x smoothly along\na prescribed tangent direction while remaining on the sphere. One suggestion\nmight be as follows, with \u2225u\u2225 =\n\u221a\nu\u22a4u (see Figure 3.1):\nRx(v) = x + v\n\u2225x + v\u2225. (3.7)\nIn this chapter, we give definitions that allow for this natural proposal.\nIt remains to make sense of the notion of gradient for a function on a smooth,\nnonlinear space. Once more, we take inspiration from the linear case. For a\nsmooth function f : E \u2192R, the gradient is defined with respect to an inner\n1 Here, xk designates an element in a sequence x0, x1, . . .Sometimes, we also use subscript\nnotation such as xi to select the ith entry of a column vectorx. Context tells which is meant.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2776, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f15d7924-e0e2-42c6-a4a5-38e0f9fc31ef": {"__data__": {"id_": "f15d7924-e0e2-42c6-a4a5-38e0f9fc31ef", "embedding": null, "metadata": {"page_label": "19", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d0357042-73cb-4bd8-8114-fedd5c80d885", "node_type": "4", "metadata": {"page_label": "19", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "a735bb0549c06832456fdba6092e6c433842ed43126b8bb95bb170421e3210af", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n3 Embedded geometry: first order 19\nTxSd\u22121\nx\nv\nRx(v)\nSd\u22121\nFigure 3.1 Retraction Rx(v) = x+v\n\u2225x+v\u2225 on the sphere.\nproduct \u27e8\u00b7, \u00b7\u27e9 : E \u00d7 E \u2192R (see Definition 3.1 below for a reminder): grad f(x) is\nthe unique element of E such that, for all v \u2208 E,\nDf(x)[v] = \u27e8v, gradf(x)\u27e9, (3.8)\nwhere Df(x): E \u2192R is the differential of f at x, that is, it is the linear map\ndefined by:\nDf(x)[v] = lim\nt\u21920\nf(x + tv) \u2212 f(x)\nt . (3.9)\nCrucially, the gradient of f depends on a choice of inner product (while the\ndifferential of f does not).\nFor example, on E = Rd equipped with the standard inner product\n\u27e8u, v\u27e9 = u\u22a4v (3.10)\nand the canonical basis e1, . . . , ed \u2208 Rd (the columns of the identity matrix), the\nith coordinate of grad f(x) \u2208 Rd is given by\ngradf(x)i = \u27e8ei, gradf(x)\u27e9 = Df(x)[ei]\n= lim\nt\u21920\nf(x + tei) \u2212 f(x)\nt \u225c \u2202f\n\u2202xi\n(x), (3.11)\nthat is, the ith partial derivative of f as a function of x1, . . . , xd \u2208 R. This covers\na case so common that it is sometimes presented as the definition of the gradient:\ngradf =\nh\n\u2202f\n\u2202x1\n\u00b7 \u00b7\u00b7 \u2202f\n\u2202xd\ni\u22a4\n.\nTurning to our nonlinear example again, in order to define a proper notion\nof gradient for f : Sd\u22121 \u2192 R, we find that we need to (a) provide a meaningful", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1443, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8495a69b-bd69-4b34-a2e0-2a2068d8509e": {"__data__": {"id_": "8495a69b-bd69-4b34-a2e0-2a2068d8509e", "embedding": null, "metadata": {"page_label": "20", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a4f9cfa2-8f00-4beb-9aee-9a43a342409b", "node_type": "4", "metadata": {"page_label": "20", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "dd006272215604eca784fb5a2a800c965835dea202af77b02f93755f375400a5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n20 Embedded geometry: first order\nnotion of differential D f(x): T xSd\u22121 \u2192 R, and (b) introduce inner products on\nthe tangent spaces of S d\u22121. Let us focus on the latter for this outline.\nSince TxSd\u22121 is a different linear subspace for various x \u2208 Sd\u22121, we need a\ndifferent inner product for each point: \u27e8\u00b7, \u00b7\u27e9x denotes our choice of inner product\non TxSd\u22121. If this choice of inner products varies smoothly with x (in a sense we\nmake precise below), then we call it a Riemannian metric, and S d\u22121 equipped\nwith this metric is called a Riemannian manifold. This allows us to define the\nRiemannian gradient of f on Sd\u22121: gradf(x) is the unique tangent vector at x\nsuch that, for all v \u2208 TxSd\u22121,\nDf(x)[v] = \u27e8v, gradf(x)\u27e9x .\nThus, first we choose a Riemannian metric, then a notion of gradient ensues.\nOne arguably natural way of endowing S d\u22121 with a metric is to exploit the\nfact that each tangent space T xSd\u22121 is a linear subspace of Rd, so that we may\ndefine \u27e8\u00b7, \u00b7\u27e9x by restricting the inner product of Rd (3.10) to T xSd\u22121: for all\nu, v\u2208 TxSd\u22121, \u27e8u, v\u27e9x = \u27e8u, v\u27e9. This is indeed a Riemannian metric, and S d\u22121\nendowed with this metric is called a Riemannian submanifold of Rd.\nFor Riemannian submanifolds, the Riemannian gradient is particularly simple\nto compute. As per our definitions, f : Sd\u22121 \u2192 R is smooth if and only if there\nexists a function \u00aff : Rd \u2192 R, smooth in the usual sense, such that f and \u00aff\ncoincide on Sd\u22121. We will argue that\ngradf(x) = Projx(grad \u00aff(x)), with Proj x(v) = (Id \u2212 xx\u22a4)v,\nwhere Proj x : Rd \u2192 TxSd\u22121 is the orthogonal projector from Rd to T xSd\u22121\n(orthogonal with respect to the inner product on Rd.) The functions f and \u00aff\noften have the same analytical expression. For example, f(x) = x\u22a4Ax (for some\nmatrix A \u2208 Rd\u00d7d) is smooth on Sd\u22121 because \u00aff(x) = x\u22a4Ax is smooth on Rd and\nthey coincide on S d\u22121. To summarize:\nFor Riemannian submanifolds, the Riemannian gradient is the orthogonal\nprojection of the \u201cclassical\u201d gradient to the tangent spaces.\nWith these tools in place, we can justify the following algorithm, an instance\nof Riemannian gradient descent on Sd\u22121. Given x0 \u2208 Sd\u22121, let\nxk+1 = Rxk (\u2212\u03b1kgradf(xk)), with grad f(x) = (Id \u2212 xx\u22a4)grad \u00aff(x),\nand R x(v) = x + v\n\u2225x + v\u2225,\nwhere \u00aff is a smooth extension of f to Rd. More importantly, these tools give\nus a formal framework to design and analyze such algorithms on a large class of\nsmooth, nonlinear spaces.\nWe now proceed to construct precise definitions, starting with a few reminders\nof linear algebra and multivariate calculus in linear spaces.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2802, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fc4ea02b-19a8-4f04-9ef4-fb3e6f2ea994": {"__data__": {"id_": "fc4ea02b-19a8-4f04-9ef4-fb3e6f2ea994", "embedding": null, "metadata": {"page_label": "21", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c855bfe5-9c80-49fc-9c96-8363897da3f8", "node_type": "4", "metadata": {"page_label": "21", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "5153d01275b408ce1ae01b5715858a816a7b93785856b68378c8d139a7415cb4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n3.1 Reminders of Euclidean space 21\n3.1 Reminders of Euclidean space\nThe letter E always denotes alinear space (or vector space) over the reals, that is,\na set equipped with (and closed under) vector addition and scalar multiplication\nby real numbers. Frequent examples include Rd (column vectors of length d),\nRn\u00d7p (matrices of size n \u00d7 p), Sym( n) (real, symmetric matrices of size n),\nSkew(n) (real, skew-symmetric matrices of size n), and their (linear) subspaces.\nWe write span( u1, . . . , um) to denote the subspace of E spanned by vectors\nu1, . . . , um \u2208 E. By extension, span( X) for a matrix X \u2208 Rn\u00d7m denotes the\nsubspace of Rn spanned by the columns of X.\nGiven two linear spaces E and F, a linear map or linear operator is a map\nL: E \u2192 Fsuch that L(au + bv) = aL(u) + bL(v) for all u, v\u2208 Eand a, b\u2208 R.\nWe let im L denote the image (or the range) of L, and we let ker L denote the\nkernel (or null space) of L.\nA basis for E is a set of vectors (elements of E) e1, . . . , ed such that each vector\nx \u2208 Ecan be expressed uniquely as a linear combination x = a1e1 + \u00b7 \u00b7\u00b7+ aded\nwith real coefficients a1, . . . , ad. All bases have the same number of elements,\ncalled the dimension of E (dim E = d): it is always finite in our treatment. Each\nbasis induces a one-to-one linear map identifying E and Rd to each other: we\nwrite E \u2261Rd.\nTopology.\nRecall that a topology on a set is a collection of subsets called open such that\n(a) the whole set and the empty set are open, (b) any union of opens is open,\nand (c) the intersection of a finite number of opens is open\u2014more on this in\nSection 8.2. A subset is closed if its complement is open. A subset may be open,\nclosed, both, or neither. We always equip Rd with its usual topology [Lee12,\nEx. A.6]. Each linear space E of dimension d inherits the topology of Rd through\ntheir identification as above. A neighborhood of x in E is an open subset of\nE which contains x. Some authors call such sets open neighborhoods. All our\nneighborhoods are open, hence we omit the qualifier.\nEuclidean structure.\nIt is useful to endow E with more structure.\nDefinition 3.1. An inner product on E is a function \u27e8\u00b7, \u00b7\u27e9 : E \u00d7E \u2192R with the\nfollowing properties. For all u, v, w\u2208 Eand a, b\u2208 R, we have:\n1. Symmetry: \u27e8u, v\u27e9 = \u27e8v, u\u27e9;\n2. Linearity: \u27e8au + bv, w\u27e9 = a \u27e8u, w\u27e9 + b \u27e8v, w\u27e9; and\n3. Positive definiteness: \u27e8u, u\u27e9 \u22650 and \u27e8u, u\u27e9 = 0 \u21d0 \u21d2u = 0.\nTwo vectors u, vare orthogonal if \u27e8u, v\u27e9 = 0.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2686, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "93468089-4cef-468e-8b3b-dd841ed7dd89": {"__data__": {"id_": "93468089-4cef-468e-8b3b-dd841ed7dd89", "embedding": null, "metadata": {"page_label": "22", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "68550d54-4c97-4dfc-8216-52f790a932b7", "node_type": "4", "metadata": {"page_label": "22", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "dc1e6b44a51934732e7bb3e632a498969beb8929966eaac707c2970213bfc49f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n22 Embedded geometry: first order\nDefinition 3.2. A linear space E with an inner product \u27e8\u00b7, \u00b7\u27e9 is a Euclidean\nspace. An inner product induces a norm on E called the Euclidean norm:\n\u2225u\u2225 =\np\n\u27e8u, u\u27e9. (3.12)\nDefinition 3.3. A basis u1, . . . , ud of a Euclidean space E is orthonormal if\n\u22001 \u2264 i, j\u2264 d, \u27e8ui, uj\u27e9 =\n(\n1 if i = j,\n0 otherwise.\nThe standard inner product on Rd and the associated norm are:\n\u27e8u, v\u27e9 = u\u22a4v =\nX\ni\nuivi, \u2225u\u2225 =\nsX\ni\nu2\ni . (3.13)\nSimilarly, the standard inner product on linear spaces of matrices such as Rn\u00d7p\nand Sym(n) is the so-calledFrobenius inner product, with its associatedFrobenius\nnorm:\n\u27e8U, V\u27e9 = Tr(U\u22a4V ) =\nX\nij\nUijVij, \u2225U\u2225 =\nsX\nij\nU2\nij, (3.14)\nwhere Tr(M) = P\ni Mii is the trace of a matrix. Summations are over all entries.\nWhen we do not specify it, we mean to use the standard inner product and norm.\nWe often use the following properties of the above inner product, with matrices\nU, V, W, A, Bof compatible sizes:\n\u27e8U, V\u27e9 = \u27e8U\u22a4, V\u22a4\u27e9, \u27e8UA, V\u27e9 = \u27e8U, V A\u22a4\u27e9,\n\u27e8BU, V\u27e9 = \u27e8U, B\u22a4V \u27e9, \u27e8U \u2299 W , V\u27e9 = \u27e8U, V\u2299 W\u27e9, (3.15)\nwhere \u2299 denotes entrywise multiplication (Hadamard product).\nAlthough we only consider linear spaces over the reals, we can still handle\ncomplex matrices. For example, Cn is a real linear space of dimension 2 n. The\nstandard basis for it is e1, . . . , en, ie1, . . . , ien, where e1, . . . , en form the standard\nbasis of Rn (the columns of the identity matrix of size n), and i is the imaginary\nunit. Indeed, any vector in Cn can be written uniquely as a linear combination of\nthose basis vectors using real coefficients. The standard inner product and norm\non Cn as a real linear space are:\n\u27e8u, v\u27e9 = \u211c{u\u2217v} = \u211c\n(X\nk\n\u00afukvk\n)\n, \u2225u\u2225 =\nsX\nk\n|uk|2, (3.16)\nwhere u\u2217 is the Hermitian conjugate-transpose of u, \u00afuk is the complex conjugate\nof uk, |uk| is its magnitude and \u211c{a} is the real part of a. This perspective\nis equivalent to identifying Cn with R2n, where real and imaginary parts are\nconsidered as two vectors in Rn. Likewise, the set of complex matrices Cn\u00d7p is\na real linear space of dimension 2 np, with the following standard inner product", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2349, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "496c27c2-50c3-42f0-8741-a083e23ff5f9": {"__data__": {"id_": "496c27c2-50c3-42f0-8741-a083e23ff5f9", "embedding": null, "metadata": {"page_label": "23", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8076c518-d4c8-459c-8000-a401e4dda760", "node_type": "4", "metadata": {"page_label": "23", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "983da0f907b0417e89be30160b5aed91166007452017d6cf8e097ad7bfe51853", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n3.1 Reminders of Euclidean space 23\nand norm:\n\u27e8U, V\u27e9 = \u211c{Tr(U\u2217V )} = \u211c\n(X\nk\u2113\n\u00afUk\u2113Vk\u2113\n)\n, \u2225U\u2225 =\nsX\nk\u2113\n|Uk\u2113|2. (3.17)\nThe analog of (3.15) in the complex case is:\n\u27e8U, V\u27e9 = \u27e8U\u2217, V\u2217\u27e9, \u27e8UA, V\u27e9 = \u27e8U, V A\u2217\u27e9,\n\u27e8BU, V\u27e9 = \u27e8U, B\u2217V \u27e9, \u27e8U \u2299 W , V\u27e9 = \u27e8U, V\u2299 \u00afW\u27e9. (3.18)\nA linear map between two Euclidean spaces has a unique adjoint, which we\nnow define. These are often useful in deriving gradients of functions\u2014more on\nthis in Section 4.7.\nDefinition 3.4. Let E and F be two Euclidean spaces, with inner products \u27e8\u00b7, \u00b7\u27e9E\nand \u27e8\u00b7, \u00b7\u27e9F, respectively. Let L: E \u2192 Fbe a linear map. The adjoint of L is the\nlinear map L\u2217 : F \u2192 Edefined by the following property:\n\u2200u \u2208 E, v\u2208 F, \u27e8L(u), v\u27e9F = \u27e8u, L\u2217(v)\u27e9E .\nDefinition 3.5. Let E be a Euclidean space with inner product \u27e8\u00b7, \u00b7\u27e9. If the linear\nmap A: E \u2192 Esatisfies\n\u2200u, v\u2208 E, \u27e8A(u), v\u27e9 = \u27e8u, A(v)\u27e9,\nthat is, if A = A\u2217, we say A is self-adjoint or symmetric.\nAs we can see from (3.15) and (3.18), adjoints and matrix transposes are\nintimately related: it is an exercise to make this precise.\nSelf-adjoint linear maps have important spectral properties.\nTheorem 3.6 (Spectral theorem). A self-adjoint map A on a Euclidean space\nE admits an orthonormal basis of eigenvectors v1, . . . , vd \u2208 Eassociated to real\neigenvalues \u03bb1, . . . , \u03bbd so that A(vi) = \u03bbivi for i = 1, . . . , dwith d = dim E.\nDefinition 3.7. A self-adjoint map A on E is positive semidefinite if, for all\nu \u2208 E, we have \u27e8u, A(u)\u27e9 \u22650; we write A \u2ab00. Owing to the spectral theorem, this\nis equivalent to all eigenvalues of A being nonnegative. Similarly, A is positive\ndefinite if \u27e8u, A(u)\u27e9 > 0 for all nonzero u \u2208 E; we write A \u227b0. This is equivalent\nto all eigenvalues of A being positive.\nNorms on vector spaces induce norms for linear maps.\nDefinition 3.8. The operator norm of L: E \u2192 Fis defined as\n\u2225L\u2225 = max\nu\u2208E,u\u0338=0\n\u2225L(u)\u2225F\n\u2225u\u2225E\n,\nwhere \u2225 \u00b7 \u2225E and \u2225 \u00b7 \u2225F denote the norms on the Euclidean spaces E and F,\nrespectively.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2173, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "040f73eb-1bd0-48d3-9fc5-da6ff10960a1": {"__data__": {"id_": "040f73eb-1bd0-48d3-9fc5-da6ff10960a1", "embedding": null, "metadata": {"page_label": "24", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9a4db0e2-2c1a-4748-a937-e53ad119f5f9", "node_type": "4", "metadata": {"page_label": "24", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "1f66f6afbe91daba4123e7ee7b3c541f08bbec129abd292df5657168ffa2143c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n24 Embedded geometry: first order\nEquivalently, \u2225L\u2225 is the smallest real such that \u2225L(u)\u2225F \u2264 \u2225L\u2225\u2225u\u2225E for all u\nin E. The singular values of L are the nonnegative square roots of the eigenvalues\nof L\u2217 \u25e6 L, and \u2225L\u2225 is the largest singular value of L. For a self-adjoint map A\nwith eigenvalues \u03bb1, . . . , \u03bbd, it follows that \u2225A\u2225 = max1\u2264i\u2264d |\u03bbi|.\nCalculus.\nWe write F : A \u2192 B to designate a map F whose domain is all of A. If C is\na subset of A, we write F|C : C \u2192 B to designate the restriction of F to the\ndomain C, so that F|C(x) = F(x) for all x \u2208 C.\nLet U, Vbe open sets in two linear spaces E, F. A map F : U \u2192 V is smooth\nif it is infinitely differentiable (class C\u221e) on its domain. We also say that F is\nsmooth at a point x \u2208 U if there exists a neighborhood U\u2032 of x such that F|U\u2032 is\nsmooth. Accordingly, F is smooth if it is smooth at all points in its domain.\nIf F : U \u2192 V is smooth at x, the differential of F at x is the linear map\nDF(x): E \u2192 Fdefined by\nDF(x)[u] = d\ndtF(x + tu)\n\f\f\f\f\nt=0\n= lim\nt\u21920\nF(x + tu) \u2212 F(x)\nt . (3.19)\nFor a curve c: R \u2192 E, we write c\u2032 to denote its velocity: c\u2032(t) = d\ndt c(t).\nFor a smooth function f : E \u2192R defined on a Euclidean space E, the (Eu-\nclidean) gradient of f is the map gradf : E \u2192 Edefined by the following property:\n\u2200x, v\u2208 E, \u27e8gradf(x), v\u27e9 = Df(x)[v].\nThe (Euclidean) Hessian of f at x is the linear map Hess f(x): E \u2192 Edefined\nby\nHessf(x)[v] = D(gradf)(x)[v] = lim\nt\u21920\ngradf(x + tv) \u2212 gradf(x)\nt .\nThe Clairaut\u2013Schwarz theorem implies that Hessf(x) is self-adjoint with respect\nto the inner product of E.\nExercise 3.9 (Adjoint and transpose) . Let u1, . . . , un form an orthonormal\nbasis of E. Likewise, let v1, . . . , vm form an orthonormal basis of F. Consider a\nlinear map L: E \u2192 F. For each 1 \u2264 i \u2264 n, the vector L(ui) is an element of F;\ntherefore, it expands uniquely in the basis v1, . . . , vm as follows:\nL(ui) =\nmX\nj=1\nMjivj,\nwhere we collect the coefficients into a matrixM \u2208 Rm\u00d7n. This matrix represents\nL with respect to the chosen bases. Show that the matrix which represents L\u2217 with\nrespect to those same bases is M\u22a4: the transpose of M. In particular, a linear\nmap A: E \u2192 Eis self-adjoint if and only if the matrix associated to it with\nrespect to the basis u1, . . . , un is symmetric.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2515, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cfb8b5e8-8291-4e8a-ae27-7981082ac8b3": {"__data__": {"id_": "cfb8b5e8-8291-4e8a-ae27-7981082ac8b3", "embedding": null, "metadata": {"page_label": "25", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b80ea78d-1c10-4080-8683-a6c705f6c303", "node_type": "4", "metadata": {"page_label": "25", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "2e8087bb502b7c102527e8b4c73c587210b8e580ce560891c5ee8d5c6d1ab349", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n3.2 Embedded submanifolds of a linear space 25\n3.2 Embedded submanifolds of a linear space\nWe set out to define what it means for a subset M of a linear space E to be\nsmooth. Our main angle is to capture the idea that a smooth set can be linearized\nin some meaningful way around each point. To make sense of what that might\nmean, consider the sphere S d\u22121. This is the set of vectors x \u2208 Rd satisfying\nh(x) = x\u22a4x \u2212 1 = 0.\nAs we discussed in the introduction, it can be adequately linearized around each\npoint by the set (3.3). The perspective we used to obtain this linearization is\nthat of differentiating the defining equation. More precisely, consider a truncated\nTaylor expansion of h:\nh(x + tv) = h(x) + t Dh(x)[v] + O(t2).\nIf x is in S d\u22121 and v is in ker Dh(x) (so that h(x) = 0 and D h(x)[v] = 0), then\nh(x + tv) = O(t2), indicating that x + tv nearly satisfies the defining equation\nof Sd\u22121 for small t. This motivates us to consider the subspace ker D h(x) as a\nlinearization of Sd\u22121 around x. Since\nDh(x)[v] = lim\nt\u21920\nh(x + tv) \u2212 h(x)\nt = x\u22a4v + v\u22a4x = 2x\u22a4v,\nthe kernel of D h(x) is the subspace orthogonal to x in Rd (with respect to the\nusual inner product). This coincides with (3.3), arguably in line with intuition.\nAt first, one might think that if a set is defined by an equation of the form\nh(x) = 0 with some smooth function h, then that set is smooth and can be\nlinearized by the kernels of D h. However, this is not the case. Indeed, consider\nthe following example in R2 (see Figure 3.2):\nX =\n\b\nx \u2208 R2 : h(x) = x2\n1 \u2212 x2\n2 = 0\n\t\n.\nThe defining function h is smooth, yet the set X is a cross in the plane formed\nby the union of the lines x1 = x2 and x1 = \u2212x2. We want to exclude such sets\nbecause of the kink at the origin. What went wrong with it? If we blindly use\nthe kernel of the differential to linearize X, we first determine\nDh(x) =\n\u0014 \u2202h\n\u2202x1\n(x), \u2202h\n\u2202x2\n(x)\n\u0015\n= [2x1, \u22122x2] .\nAt x = 0, D h(0) = [0 , 0], whose kernel is all of R2: that does not constitute a\nreasonable linearization of X around the origin.\nWe can gain further insight into the issue at hand by considering additional\nexamples. The zero-sets of the functions h(x) = x2\n1 \u2212x3\n2 and h(x) = x2\n1 \u2212x4\n2 from\nR2 to R, respectively, define a cusp and a double parabola, both of which fail\nour intuitive test of smoothness at the origin. What the cross, cusp and double\nparabola have in common is that the rank of D h(x) suddenly drops from one to\nzero at the origin, whereas for the sphere that rank is constant (and maximal)\non the whole set.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2788, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cf017f8b-80a8-416f-9144-149249dbbc4f": {"__data__": {"id_": "cf017f8b-80a8-416f-9144-149249dbbc4f", "embedding": null, "metadata": {"page_label": "26", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d2ae7594-0539-46a9-9879-5197d24318f6", "node_type": "4", "metadata": {"page_label": "26", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "307682ce44ee493be8ac150b4b1d1c88223dea59ea2394692b3181fecebfdcfc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n26 Embedded geometry: first order\nCircle: x2\n1 + x2\n2 \u2212 1 = 0\n Cross: x2\n1 \u2212 x2\n2 = 0\nCusp: x2\n1 \u2212 x3\n2 = 0\n Double parabola: x2\n1 \u2212 x4\n2 = 0\nFigure 3.2 Four different sets S defined as the zero-sets of a smooth function from R2\nto R. For each, the sets TxS (Definition 3.14) are drawn at a few different points. Only\nthe circle (top left) is an embedded submanifold of R2.\nThese observations motivate the definition below. Since smoothness is a local\nnotion, the definition is phrased in terms of what the setM looks like around each\npoint. Since a set M may be equivalently defined by many different functions h,\nand since it may not be practical (or even possible, see Section 3.10) to define\nall of M with a single function h, the definition allows for a different one to be\nused around each point.\nDefinition 3.10. Let E be a linear space of dimension d. A non-empty subset\nM of E is a (smooth) embedded submanifold of E of dimension n if either\n1. n = d and M is open in E\u2014we also call this an open submanifold; or\n2. n = d \u2212 k for some k \u2265 1 and, for each x \u2208 M, there exists a neighborhood\nU of x in E and a smooth function h: U \u2192 Rk such that\n(a) If y is in U, then h(y) = 0 if and only if y \u2208 M; and\n(b) rank Dh(x) = k.\nSuch a function h is called a local defining function for M at x.\nIf M is a linear (sub)space, we also call it a linear manifold.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1617, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a0b544c1-5ed7-4e48-8552-af907b38a28d": {"__data__": {"id_": "a0b544c1-5ed7-4e48-8552-af907b38a28d", "embedding": null, "metadata": {"page_label": "27", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "683e1873-ea0b-4047-99c4-e736cbfb4df7", "node_type": "4", "metadata": {"page_label": "27", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "6939d040ebfb1d232e98748c82b7cc75e58e110618800c4f70548debe9f2a766", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n3.2 Embedded submanifolds of a linear space 27\nCondition 2(a) in the previous definition can be stated equivalently as:\nM \u2229U = h\u22121(0) \u225c {y \u2208 U : h(y) = 0}.\nIt is an exercise to verify that Definition 3.10 excludes various pathological sets\nsuch as the cross ( x2\n1 = x2\n2), cusp (x2\n1 = x3\n2) and double parabola ( x2\n1 = x4\n2).\nDifferential geometry defines a broader class of smooth sets called (smooth)\nmanifolds. We typically omit the word \u2018smooth\u2019 as all of our manifolds are\nsmooth, though bear in mind that in the literature there exist different kinds of\nmanifolds, not all of which are smooth. Embedded submanifolds are manifolds.\nWhen the statements we make hold true \u22c6for smooth manifolds in general, we\nuse the word manifold (rather than embedded submanifold) to signal it. This is\ncommon throughout Chapters 3 and 5.\nThe hope is that limiting our initial treatment of manifolds to embedded sub-\nmanifolds provides a more intuitive entry point to build all the tools we need for\noptimization. This is all the more relevant considering that many of the manifolds\nwe encounter in applications are in fact embedded submanifolds, presented to us\nas zero-sets of their local defining functions. All of our optimization algorithms\nwork on general manifolds. The general theory is in Chapter 8.\nTo build additional support for our definition of embedded submanifolds, we\nfurther argue that small patches of M can be deformed into linear subspaces\nin a smooth and smoothly invertible way. This captures an important feature\nof smoothness, namely: upon zooming close to a point of M, what we see can\nhardly be distinguished from what we would have seen had M been a linear\nsubspace of E.\nDefinition 3.11. A diffeomorphism is a bijective map F : U \u2192 V , where U, V\nare open sets and such that both F and F\u22121 are smooth.\nTheorem 3.12. Let E be a linear space of dimension d. A subset M of E is\nan embedded submanifold of E of dimension n = d \u2212 k if and only if for each\nx \u2208 Mthere exists a neighborhood U of x in E, an open set V in Rd and a\ndiffeomorphism F : U \u2192 V such that F(M \u2229U) = E \u2229 V , where E = {y \u2208 Rd :\nyn+1 = \u00b7 \u00b7\u00b7= yd = 0} is a linear subspace of Rd.\nThe main tool we need to prove Theorem 3.12 is the standard inverse function\ntheorem, stated here without proof [Lee12, Thm. C.34].\nTheorem 3.13 (Inverse function theorem) . Suppose U \u2286 Eand V \u2286 Fare\nopen subsets of linear spaces of the same dimension, and F : U \u2192 V is smooth.\nIf DF(x) is invertible at some point x \u2208 U, then there exist neighborhoodsU\u2032 \u2286 U\nof x and V \u2032 \u2286 V of F(x) such that F|U\u2032 : U\u2032 \u2192 V \u2032 (the restriction of F to U\u2032\nand V \u2032) is a diffeomorphism.\nEquipped with this tool, we proceed to prove Theorem 3.12.\nProof of Theorem 3.12. We prove one direction of the theorem, namely: we as-\nsume M is an embedded submanifold and construct diffeomorphisms F. The", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3097, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ee55e73e-1a3a-4845-94ca-dc20a8496c6a": {"__data__": {"id_": "ee55e73e-1a3a-4845-94ca-dc20a8496c6a", "embedding": null, "metadata": {"page_label": "28", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c40cfe6c-1291-4d7f-8568-1c924878b41b", "node_type": "4", "metadata": {"page_label": "28", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "958cc3c5c9089453b1623c3f8686c6a1460a2897956d67b5a118f64f4d8db575", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n28 Embedded geometry: first order\nother direction is left as an exercise. For the latter, it is helpful to note that if\nF is a diffeomorphism with inverse F\u22121, then DF(x) is invertible and\n(DF(x))\u22121 = DF\u22121(F(x)). (3.20)\nTo see this, apply the chain rule to differentiate F\u22121 \u25e6 F, noting that this is\nnothing but the identity map.\nIf n = d (that is, M is open in E), the claim is clear: simply let F be any\ninvertible linear map from E to Rd (for example, using a basis of E), and restrict\nits domain and codomain to U = M and V = F(U).\nWe now consider the more interesting case where n = d \u2212 k with k \u2265 1. Let\nh: U \u2192 Rk be any local defining function for M at x. We work in coordinates\non E, which is thus identified with Rd. Then, we can think of D h(x) as a matrix\nof size k \u00d7d. By assumption, Dh(x) has rank k. This means that it is possible to\npick k columns of that matrix which form a k \u00d7 k invertible matrix. If needed,\npermute the chosen coordinates so that the last k columns have that property\n(this is without loss of generality). Then, we can write D h(x) in block form so\nthat\nDh(x) =\n\u0002\nA B\n\u0003\n,\nwhere B \u2208 Rk\u00d7k is invertible and A is in Rk\u00d7(d\u2212k). Now consider the function\nF : U \u2192 Rd (recall U \u2286 Eis the domain of h) defined by\nF(y) = (y1, . . . , yd\u2212k, h1(y), . . . , hk(y))\u22a4, (3.21)\nwhere y1, . . . , yd denote the coordinates of y \u2208 E. In order to apply the inverse\nfunction theorem to F at x, we must verify that F is smooth\u2014this is clear\u2014and\nthat the differential of F at x is invertible. Working this out one row at a time,\nwe get the following expression for that differential:\nDF(x) =\n\u0014Id\u2212k 0\nA B\n\u0015\n,\nwhere Id\u2212k is the identity matrix of sized\u2212k, and 0 here denotes a zero matrix of\nsize (d\u2212k)\u00d7k. The matrix DF(x) is invertible, as demonstrated by the following\nexpression for its inverse:\n(DF(x))\u22121 =\n\u0014 Id\u2212k 0\n\u2212B\u22121A B \u22121\n\u0015\n. (3.22)\n(Indeed, their product is Id.) Hence, the inverse function theorem asserts that we\nmay reduce U to a possibly smaller neighborhood of x so that F (now restricted\nto that new neighborhood) is a diffeomorphism from U to V = F(U). The\nproperty F(M \u2229U) = E \u2229 V follows by construction of F from the property\nM \u2229U = h\u22121(0).\nIn order to understand the local geometry of a set around a point, we aim to\ndescribe acceptable directions of movement through that point. This is close in\nspirit to the tools we look to develop for optimization, as they involve moving", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2666, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "abc0cec7-7e75-411f-a64f-80ab8ee8fca2": {"__data__": {"id_": "abc0cec7-7e75-411f-a64f-80ab8ee8fca2", "embedding": null, "metadata": {"page_label": "29", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9a5c1e87-f0c1-4b5a-92d2-0a9044c708f5", "node_type": "4", "metadata": {"page_label": "29", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "6419e8339c35596b4ab430f60f2d6737593d84a6f8cbc1af37c8907b4ec334c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n3.2 Embedded submanifolds of a linear space 29\naway from a point while remaining on the set. Specifically, for a subset M of a\nlinear space E, consider all the smooth curves of E which lie entirely on M and\npass through a given point x. Collect their velocities as they do so in a set T xM\ndefined below. In that definition, c is smooth in the usual sense as a map from\n(an open subset of) R to E\u2014two linear spaces.\nDefinition 3.14. Let M be a subset of E. For all x \u2208 M, define:\nTxM = {c\u2032(0) | c: I \u2192 Mis smooth and c(0) = x}, (3.23)\nwhere I is any open interval containing t = 0. That is, v is in TxM if and only\nif there exists a smooth curve on M passing through x with velocity v.\nNote that TxM is a subset of E. For the sphere, it is easy to convince oneself\nthat TxM coincides with the subspace in (3.3). We show in the next theorem\nthat this is always the case for embedded submanifolds.\nTheorem 3.15. Let M be an embedded submanifold of E. Consider x \u2208 Mand\nthe set TxM (3.23). If M is an open submanifold, then TxM = E. Otherwise,\nTxM = ker Dh(x) with h any local defining function at x.\nProof. For open submanifolds, the claim is clear. By definition, TxM is included\nin E. The other way around, for anyv \u2208 E, consider c(t) = x+tv: this is a smooth\ncurve from some non-empty interval around 0 to M such that c(0) = x, hence\nv = c\u2032(0) is in TxM. This shows E is included in TxM, so that the two coincide.\nNow consider the case of M an embedded submanifold of dimension n = d\u2212k\nwith k \u2265 1. Let h: U \u2192 Rk be a local defining function of M around x. The\nproof is in two steps. First, we show that TxM is included in ker Dh(x). Then, we\nshow that TxM contains a linear subspace of the same dimension as ker D h(x).\nThese two facts combined indeed confirm that T xM = ker Dh(x).\nStep 1. If v is in T xM, there exists c: I \u2192 M, smooth, such that c(0) = x\nand c\u2032(0) = v. Since c(t) is in M, we have h(c(t)) = 0 for all t \u2208 I (if need\nbe, restrict the interval I to ensure c(t) remains in the domain of h). Thus, the\nderivative of h \u25e6 c vanishes at all times:\n0 = d\ndth(c(t)) = Dh(c(t))[c\u2032(t)].\nIn particular, at t = 0 this implies D h(x)[v] = 0, that is, v \u2208 ker Dh(x). This\nconfirms TxM \u2286ker Dh(x).\nStep 2 . To show that T xM contains a subspace of the same dimension as\nker Dh(x) (namely, of dimension n = d \u2212 k), we must construct smooth curves\non M that pass through x with various velocities. To do so, we call upon Theo-\nrem 3.12. The latter provides us with a diffeomorphism F : U \u2192 V (where U is\nnow a possibly smaller neighborhood of x than the original domain of h.) We use\nF\u22121 to construct smooth curves on M that pass through x. Specifically, pick an", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2923, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "35c12bd2-823e-4abe-a41a-89e64b429294": {"__data__": {"id_": "35c12bd2-823e-4abe-a41a-89e64b429294", "embedding": null, "metadata": {"page_label": "30", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2185e75d-63fe-450f-b510-55fb11f1b243", "node_type": "4", "metadata": {"page_label": "30", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "21757b177acb9fcf45bab87e0be5088f8d95fab91836146cae977a5668c98fa1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n30 Embedded geometry: first order\narbitrary u \u2208 Rd\u2212k and let\n\u03b3(t) = F(x) + t\n\u0014u\n0\n\u0015\n.\n(Here, 0 denotes a zero vector of size k.) Notice that \u03b3 remains in E \u2229 V for t\nclose to zero, where E is the subspace of Rd consisting of all vectors whose last\nk entries are zero. Since F\u22121(E \u2229 V ) = M \u2229U, it follows that\nc(t) = F\u22121(\u03b3(t)) (3.24)\nresides in M for t close to zero. Moreover, c(0) = x and c is smooth since F\u22121\nis smooth. It follows that c is indeed a smooth curve on M passing through x.\nWhat is the velocity of this curve at x? Applying the chain rule to (3.24), we get\nc\u2032(t) = DF\u22121(\u03b3(t)) [\u03b3\u2032(t)].\nIn particular, at t = 0 we have\nc\u2032(0) = DF\u22121(F(x))\n\u0014u\n0\n\u0015\n.\nSince F is a diffeomorphism, we know from (3.20) that D F\u22121(F(x)) is an in-\nvertible linear map, equal to (D F(x))\u22121. The specific form of c\u2032(0) is unimpor-\ntant. What matters is that each c\u2032(0) of the form above certainly belongs to\nTxM (3.23). Since D F\u22121(F(x)) is invertible and u \u2208 Rd\u2212k is arbitrary, this\nmeans that TxM contains a subspace of dimension d\u2212k. But we know from the\nprevious step that T xM is included in a subspace of dimension d \u2212 k, namely,\nker Dh(x). It follows that TxM = ker Dh(x). Since this holds for all x \u2208 M, the\nproof is complete.\nThus, for an embedded submanifold M of dimension n = d \u2212 k, for each x in\nM, the set T xM is a linear subspace of E of dimension n. These subspaces are\nthe linearizations of the smooth set M.\nDefinition 3.16. We call TxM the tangent space to M at x. Vectors in TxM\nare called tangent vectors to M at x. The dimension of TxM (which is inde-\npendent of x) coincides with the dimension of M, denoted by dim M.\nWe consider three brief examples of embedded submanifolds: two obvious by\nnow, and one arguably less obvious. It is good to keep all three in mind when\nassessing whether a certain proposition concerning embedded submanifolds is\nlikely to be true. Chapter 7 details further examples.\nExample 3.17. The set Rd is a linear manifold of dimension d with tangent\nspaces TxM = Rd for all x \u2208 Rd. The affine space {x \u2208 Rd : Ax = b} defined\nby a matrix A \u2208 Rk\u00d7d of rank k and arbitrary vector b \u2208 Rk is a manifold of\ndimension n = d \u2212 k embedded in Rd.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2431, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "72f23256-f74d-4625-946d-f7d38de94585": {"__data__": {"id_": "72f23256-f74d-4625-946d-f7d38de94585", "embedding": null, "metadata": {"page_label": "31", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "663b50ef-319d-461a-92e1-ea6fa26adb12", "node_type": "4", "metadata": {"page_label": "31", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "9570a3ec9efc5ea9c167d27d74f68f51df1f7a52586da6bbb1c27f6dd4c6fa14", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n3.2 Embedded submanifolds of a linear space 31\nExample 3.18. The sphere Sd\u22121 = {x \u2208 Rd : x\u22a4x = 1} is the zero level set of\nh(x) = x\u22a4x \u2212 1, smooth from Rd to R. Since Dh(x)[v] = 2 x\u22a4v, it is clear that\nrank Dh(x) = 1 for all x \u2208 Sd\u22121. As a result, Sd\u22121 is an embedded submanifold\nof Rd of dimension n = d \u2212 1. Furthermore, its tangent spaces are given by\nTxSd\u22121 = ker Dh(x) = {v \u2208 Rd : x\u22a4v = 0}.\nExample 3.19. Let Sym(2)1 denote the set of symmetric matrices of size two\nand rank one, that is,\nSym(2)1 =\n\u001a\nX =\n\u0014x1 x2\nx2 x3\n\u0015\n: rank X = 1\n\u001b\n.\nThis is a subset of Sym(2), the linear space of symmetric matrices of size two.\nThe rank function is not a smooth map (it is not even continuous), hence we\ncannot use it as a local defining function. Nevertheless, we can construct local\ndefining functions for Sym(2)1. Indeed, a matrix of size 2 \u00d7 2 has rank one if\nand only if it is nonzero and its determinant is zero, hence:\nSym(2)1 =\n\u001a\nX =\n\u0014x1 x2\nx2 x3\n\u0015\n: x1x3 \u2212 x2\n2 = 0 and X \u0338= 0\n\u001b\n.\nLet U = Sym(2)\\{0} be the open subset of Sym(2) obtained by removing the zero\nmatrix. Consider h: U \u2192 R defined by h(X) = x1x3 \u2212 x2\n2. Clearly, h is smooth\nand h\u22121(0) = Sym(2)1 \u2229 U = Sym(2)1. Furthermore,\nDh(X)[ \u02d9X] = \u02d9x1x3 + x1 \u02d9x3 \u2212 2x2 \u02d9x2 =\n\u0002\nx3 \u22122x2 x1\n\u0003\n\uf8ee\n\uf8f0\n\u02d9x1\n\u02d9x2\n\u02d9x3\n\uf8f9\n\uf8fb,\nwhere \u02d9X is a matrix in Sym(2): the dot is a visual indication that we should\nthink of \u02d9X as a perturbation of X. This linear map has rank one provided X \u0338= 0,\nwhich is always the case in the domain of h. Hence, h is a defining function for\nSym(2)1 around any X in Sym(2)1. This confirms that the latter is an embedded\nsubmanifold of Sym(2) of dimension dim Sym(2) \u2212 1 = 3 \u2212 1 = 2 . Its tangent\nspace at X is given by ker Dh(X):\nTXSym(2)1 =\n\u001a\n\u02d9X =\n\u0014\u02d9x1 \u02d9x2\n\u02d9x2 \u02d9x3\n\u0015\n: \u02d9x1x3 + x1 \u02d9x3 \u2212 2x2 \u02d9x2 = 0\n\u001b\n.\nContrary to the two previous examples, this manifold is neither open nor closed\nin its embedding space. It is also not connected. Visualized in R3, it corresponds\nto a double, infinite elliptic cone. Indeed, X \u0338= 0 is in Sym(2)1 if and only if\n(x1 + x3)2 = (2x2)2 + (x1 \u2212 x3)2.\nAfter the linear change of variables z1 = x1 \u2212 x3, z2 = 2x2 and z3 = x1 + x3,\nthe defining equation becomes z2\n1 + z2\n2 = z2\n3, omitting the origin.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2450, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e7b4c20c-cb8f-4c4c-84df-7d9f395b4179": {"__data__": {"id_": "e7b4c20c-cb8f-4c4c-84df-7d9f395b4179", "embedding": null, "metadata": {"page_label": "32", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b66b086-3b1b-4414-8ec7-c09b3506cecd", "node_type": "4", "metadata": {"page_label": "32", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "b73747f18d173b1840ed0412609b18fd4eae5c67700f9a88df9fe3b1f6582af5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n32 Embedded geometry: first order\nWe can combine manifolds to form new ones. For instance, it is an exercise to\nshow that Cartesian products of manifolds are manifolds. 2\nProposition 3.20. Let M, M\u2032 be embedded submanifolds of E, E\u2032 (respectively).\nThen, M \u00d7 M\u2032 is an embedded submanifold of E \u00d7 E\u2032 of dimension dim M +\ndim M\u2032 with tangent spaces given by:\nT(x,x\u2032)(M \u00d7 M\u2032) = TxM \u00d7Tx\u2032M\u2032.\nFor example, after showing that the sphere S d\u22121 is an embedded submanifold\nof Rd, it follows that S d\u22121 \u00d7 \u00b7\u00b7 \u00b7 \u00d7Sd\u22121 = (Sd\u22121)k is an embedded submanifold\nof (Rd)k \u2261 Rd\u00d7k: it is called the oblique manifold OB(d, k).\nIn closing this section, we equip embedded submanifolds ofE with the topology\ninduced by E.3 Having a topology notably allows us to define notions such as\nlocal optima and convergence of sequences on M. Both are useful when studying\niterative optimization algorithms.\nDefinition 3.21. A subset U of M is open (resp., closed) in M if U is the\nintersection of M with an open (resp., closed) subset of E. This is called the\nsubspace topology.\nEchoing the conventions laid out in Section 3.1, our neighborhoods are open.\nDefinition 3.22. A neighborhood of x in M is an open subset of M which\ncontains x. By extension, a neighborhood of a subset of M is an open set of M\nwhich contains that subset.\nIt is an exercise to show that open subsets of a manifold M are manifolds; we\ncall them open submanifolds of M.\nProposition 3.23. Let M be an embedded submanifold of E. Any open subset\nof M is also an embedded (but not necessarily open) submanifold of E, with same\ndimension and tangent spaces as M.\nExercise 3.24. Complete the proof of Theorem 3.12.\nExercise 3.25. Give a proof of Proposition 3.20.\nExercise 3.26. Give a proof of Proposition 3.23. In particular, deduce that the\nrelative interior of the simplex, that is,\n\u2206d\u22121\n+ = {x \u2208 Rd : x1 + \u00b7 \u00b7\u00b7+ xd = 1 and x1, . . . , xd > 0}, (3.25)\nis an embedded submanifold of Rd. This is useful to represent non-vanishing\ndiscrete probability distributions. See also Exercise 3.41.\n2 We collect facts about product manifolds along the way: see Table 7.2 for a summary.\n3 About terminology: the general definition of submanifolds allows for other topologies. The\nqualifier \u2018embedded\u2019 (some say \u2018regular\u2019) indicates we use the induced topology. More on\nthis in Section 8.14.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2573, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9a250d43-f9a0-4819-b66c-4fd0549a3e87": {"__data__": {"id_": "9a250d43-f9a0-4819-b66c-4fd0549a3e87", "embedding": null, "metadata": {"page_label": "33", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b9a31115-dc09-4e1a-8519-c6a13be8e4ec", "node_type": "4", "metadata": {"page_label": "33", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "f7d4b1a5594e2d50cb13befa4c6421faa7e0b2fc75bae92d1d4212e7bb2c6753", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n3.3 Smooth maps on embedded submanifolds 33\nExercise 3.27. Show that the cross X = {x \u2208 R2 : x2\n1 = x2\n2} is not an embedded\nsubmanifold. It is not sufficient to show that x 7\u2192 x2\n1 \u2212 x2\n2 is not a local defining\nfunction at the origin: it is necessary to show that no local defining function\nexists at that point. Hint: proceeding by contradiction, assume there exists a local\ndefining function around the origin and show that its kernel is too large.\nExercise 3.28. Show that the cusp C = {x \u2208 R2 : x2\n1 = x3\n2} is not an embedded\nsubmanifold. Hint: argue that T0C (as defined by (3.23)) is too low-dimensional.\nExercise 3.29. Show that the double parabola P = {x \u2208 R2 : x2\n1 = x4\n2} is not\nan embedded submanifold, yet TxP as defined by (3.23) is a linear subspace of\ndimension one in R2 for all x \u2208 P. This example shows that Definition 3.10\nis more restrictive than just requiring all sets TxP to be subspaces of the same\ndimension. Hint: proceeding by contradiction, assume P is an embedded subman-\nifold and call upon Theorem 3.12 to construct a special diffeomorphism F; then,\nderive a contradiction from the fact that P around the origin does not look like\na one-dimensional curve. Specifically, you may want to use the fact that it is\nimpossible to have three or more disjoint open intervals of the real line sharing\na common accumulation point.\n3.3 Smooth maps on embedded submanifolds\nNow that we have a notion of smooth sets, we can introduce the all important\nnotion of smooth maps between smooth sets. It relies heavily on the classical\nnotion of smooth maps between (open subsets of) linear spaces. In optimization,\ntwo examples of maps between manifolds are cost functions ( M \u2192R) and\niteration maps (M \u2192 M); more will come up.\nDefinition 3.30. Let M and M\u2032 be embedded submanifolds of E and E\u2032 (re-\nspectively). A map F : M \u2192 M\u2032 is smooth at x \u2208 Mif there exists a function\n\u00afF : U \u2192 E\u2032 which is smooth on a neighborhood U of x in E and such that F and\n\u00afF coincide on M \u2229U, that is, F(y) = \u00afF(y) for all y \u2208 M \u2229U. We call \u00afF a\n(local) smooth extension of F around x. The map F is smooth if it is smooth\nat all x \u2208 M.\nIn the above definition, \u00afF is a map between open subsets of linear spaces:\nfor it to be smooth means it is infinitely differentiable on its domain, in the\nusual sense. By definition, if \u00afF is any smooth map on E, and M is an embedded\nsubmanifold of E, then the restriction F = \u00afF|M is smooth on M. This still holds\nif \u00afF is only defined on a neighborhood of M in E, that is, on an open subset of\nE which contains M.\nConversely, the following proposition states that a smooth map on M admits\na smooth extension to a neighborhood of M: there is no need to pick a different\nlocal smooth extension around each point. While this fact is not needed to es-\ntablish results hereafter, it is convenient to shorten exposition; so much so that", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3125, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bea2dca8-5e0b-4718-87e8-3eeaffd7e465": {"__data__": {"id_": "bea2dca8-5e0b-4718-87e8-3eeaffd7e465", "embedding": null, "metadata": {"page_label": "34", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "efffe59f-d078-4e64-b174-dac797b53c5e", "node_type": "4", "metadata": {"page_label": "34", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "f6f1ccca73fe95d71d31b80d5142d2980f8bb354b97f74e91b46d377a7d53ac5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n34 Embedded geometry: first order\nwe typically think of it as the definition of a smooth map. See Section 3.10 for a\ndiscussion.\nProposition 3.31. Let M and M\u2032 be embedded submanifolds of E and E\u2032. A\nmap F : M \u2192 M\u2032 is smooth if and only if F = \u00afF|M where \u00afF is some smooth\nmap from a neighborhood of M in E to E\u2032.\nIn particular, a real-valued function f : M \u2192R is smooth if and only if there\nexists a smooth extension \u00aff : U \u2192 R defined on a neighborhood U of M in E\nand which coincides with f on M.\nDefinition 3.32. A scalar field on a manifold M is a function f : M \u2192R. If f\nis a smooth function, we say it is a smooth scalar field. The set of smooth scalar\nfields on M is denoted by F(M).\nSmoothness is preserved under composition, and also under linear combina-\ntions and products of maps when those are defined: see the exercises in the next\nsection.\nExercise 3.33. Give an example of an embedded submanifold M in a linear\nspace E and a smooth function f : M \u2192R for which there does not exist a\nsmooth extension \u00aff : E \u2192R smooth on all of E. Hint: use Example 3.19, or use\nProposition 3.23 and consider removing a point from a simple manifold.\n3.4 The differential of a smooth map\nLet \u00afF : U \u2286 E \u2192 E\u2032 be a smooth function between two linear spaces, possibly\nrestricted to an open set U. The differential of \u00afF at x \u2208 U is a linear map\nD \u00afF(x): E \u2192 E\u2032 defined by:\nD \u00afF(x)[v] = lim\nt\u21920\n\u00afF(x + tv) \u2212 \u00afF(x)\nt . (3.26)\nThis tells us how \u00afF(x) changes when we push x along v. Applying this definition\nto a smooth map F : M \u2192 M\u2032 between two embedded submanifolds is problem-\natic because x + tv generally does not belong to M, even for tiny nonzero values\nof t: F may not be defined there.\nWe can propose to resolve this issue in at least two ways:\n1. Rely on Definition 3.14: t 7\u2192 x + tv is nothing but a curve in E which passes\nthrough x with velocity v; we can use curves on M instead.\n2. Rely on Definition 3.30: we can smoothly extend F and differentiate the ex-\ntension instead.\nAs it turns out, these two approaches are equivalent. We start with the first one\nbecause it is more geometric: it gives the right picture of how things work on\ngeneral manifolds. The second one is convenient for computation.\nFor any tangent vector v \u2208 TxM, there exists a smooth curve c on M passing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2541, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d9b48161-3c41-4418-b68d-2fe3646d7676": {"__data__": {"id_": "d9b48161-3c41-4418-b68d-2fe3646d7676", "embedding": null, "metadata": {"page_label": "35", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "88ad0fd3-b84a-41aa-8528-3a85c17356fd", "node_type": "4", "metadata": {"page_label": "35", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "ce5dbe96f60c248238843b75db18a70043fc1d448f10e2001ee00b5d4ea6ae45", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n3.4 The differential of a smooth map 35\nM\nM\u2032\nF\nc(t) F(c(t))\nx\nv\nF(x)\nDF(x)[v]\nFigure 3.3 A smooth map F : M \u2192 M\u2032 pushes smooth curves c on M to smooth curves\nF \u25e6 c on M\u2032.\nthrough x with velocity v. Then, t 7\u2192 F(c(t)) itself defines a curve on M\u2032 passing\nthrough F(x). That curve is smooth by composition. Thus, it passes through\nF(x) with a certain velocity. By definition, that velocity is a tangent vector of\nM\u2032 at F(x). We call this tangent vector the differential ofF at x along v, denoted\nby DF(x)[v]. See Figure 3.3.\nDefinition 3.34. The differential of F : M \u2192 M\u2032 at the point x \u2208 Mis the\nlinear map DF(x): T xM \u2192TF(x)M\u2032 defined by:\nDF(x)[v] = d\ndtF(c(t))\n\f\f\f\f\nt=0\n= (F \u25e6 c)\u2032(0), (3.27)\nwhere c is a smooth curve on M passing through x at t = 0 with velocity v.\nWe must clarify two things: (a) that this definition does not depend on the\nchoice of curve c (as many may satisfy the requirements), and (b) that D F(x) is\nindeed linear. To do so, we connect with the second approach.\nLet M and M\u2032 be embedded submanifolds of E and E\u2032. Then, the smooth map\nF : M \u2192 M\u2032 admits a smooth extension \u00afF : U \u2192 E\u2032, where U is a neighborhood\nof M in E and F = \u00afF|M. Observe that F \u25e6c = \u00afF \u25e6c. The latter is a composition\nof functions between open subsets of linear spaces, hence the usual chain rule\napplies:\nDF(x)[v] = d\ndtF(c(t))\n\f\f\f\f\nt=0\n= d\ndt\n\u00afF(c(t))\n\f\f\f\f\nt=0\n= D \u00afF(c(0))[c\u2032(0)] = D \u00afF(x)[v]. (3.28)\nThis holds for all v \u2208 TxM. We summarize as follows.\nProposition 3.35. With notation as above, DF(x) = D \u00afF(x)|TxM.\nThis proposition confirms that D F(x) is linear since D \u00afF(x) is linear. It also\nshows that the definition by eq. (3.27) depends on c only through c(0) and c\u2032(0),\nas required.\nOne may now wonder whether eq. (3.28) depends on the choice of smooth\nextension \u00afF. It does not: that is clear from eq. (3.27). We can also verify it", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2100, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c20c91b8-c61f-4cc5-ba7b-0b9ae371f7ef": {"__data__": {"id_": "c20c91b8-c61f-4cc5-ba7b-0b9ae371f7ef", "embedding": null, "metadata": {"page_label": "36", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1d1ce352-2bf2-4df7-a62f-7cd32edac32a", "node_type": "4", "metadata": {"page_label": "36", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "45101ab750531c438f119f8836752b7a13eebd6888dc30f8c522f45bdfe9980d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n36 Embedded geometry: first order\nexplicitly. Let \u02c6F be another smooth extension of F. Then, for all smooth curves\nc with c(0) = x and c\u2032(0) = v we have\nD \u00afF(x)[v] = ( \u00afF \u25e6 c)\u2032(0) = ( \u02c6F \u25e6 c)\u2032(0) = D \u02c6F(x)[v].\nThus, the choice of smooth extension is inconsequential.\nExample 3.36. Given a real, symmetric matrix A \u2208 Sym(d), the Rayleigh\nquotient at a nonzero vector x \u2208 Rd is given by x\u22a4Ax\nx\u22a4x . Since this quotient is\ninvariant under scaling of x, we may restrict our attention to unit-norm vectors.\nThis yields a function on the sphere:\nf : Sd\u22121 \u2192 R: x 7\u2192 x\u22a4Ax.\nAs we will gradually rediscover, the extreme points (maxima and minima) of f\nare tightly related to extremal eigenvectors of A. The function f can be smoothly\nextended to Rd by \u00aff(x) = x\u22a4Ax, hence f is smooth according to Definition 3.30.\nUsing this smooth extension, we can also obtain an expression for its differential.\nIndeed, for all v \u2208 Rd,\nD \u00aff(x)[v] = lim\nt\u21920\n\u00aff(x + tv) \u2212 \u00aff(x)\nt\n= lim\nt\u21920\n(x + tv)\u22a4A(x + tv) \u2212 x\u22a4Ax\nt\n= x\u22a4Av + v\u22a4Ax\n= x\u22a4(A + A\u22a4)v = 2x\u22a4Av.\nHence, Proposition 3.35 yields:\nDf(x)[v] = D \u00aff(x)[v] = 2x\u22a4Av\nfor all v \u2208 TxSd\u22121 = {v \u2208 Rd : x\u22a4v = 0}. Formally, D \u00aff(x) is defined on all of\nRd while Df(x) is only defined on TxSd\u22121.\nExercise 3.37. For smooth maps F1, F2 : M \u2192 E\u2032 and real numbers a1, a2, show\nthat F : x 7\u2192 a1F1(x) + a2F2(x) is smooth and we have linearity:\nDF(x) = a1DF1(x) + a2DF2(x).\nExercise 3.38. For smooth maps f : M \u2192R and G: M \u2192 E\u2032, show that\nfG : x 7\u2192 f(x)G(x) is smooth from M to E\u2032 and we have a product rule:\nD(fG)(x)[v] = Df(x)[v] \u00b7 G(x) + f(x) \u00b7 DG(x)[v].\n(The dots \u00b7 are only used to clarify the factors of the product visually.)\nExercise 3.39. Let F : M \u2192 M\u2032 and G: M\u2032 \u2192 M\u2032\u2032 be smooth, where M, M\u2032\nand M\u2032\u2032 are embedded submanifolds of E, E\u2032 and E\u2032\u2032, respectively. Show that\ncomposition preserves smoothness, that is, G \u25e6 F : x 7\u2192 G(F(x)) is smooth. Also\nshow that we have a chain rule:\nD(G \u25e6 F)(x)[v] = DG(F(x))[DF(x)[v]]. (3.29)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2200, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "38b56389-b8b7-4e49-94b0-f86ef1a6c077": {"__data__": {"id_": "38b56389-b8b7-4e49-94b0-f86ef1a6c077", "embedding": null, "metadata": {"page_label": "37", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d77a8f98-01c6-4092-9d4e-689fe0317269", "node_type": "4", "metadata": {"page_label": "37", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "4d10b28859e4aea7e24136326b80e08bd02001b251bc044284a558736df9d48d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n3.5 Vector fields and the tangent bundle 37\nExercise 3.40. Let M, M\u2032, N be three manifolds, and consider a smooth map\nF : M \u00d7 M\u2032 \u2192 N(see Proposition 3.20 for the product manifold.) Show that\nDF(x, y)[(u, v)] = D\n\u0000\nx 7\u2192 F(x, y)\n\u0001\n(x)[u] + D\n\u0000\ny 7\u2192 F(x, y)\n\u0001\n(y)[v],\nwhere (x, y) \u2208 M \u00d7 M\u2032 and (u, v) \u2208 T(x,y)(M \u00d7 M\u2032) = T xM \u00d7TyM\u2032 are\narbitrary. The notation x 7\u2192 F(x, y) denotes the function from M to N obtained\nby fixing the second input of F to y.\nExercise 3.41. Let M be an embedded submanifold of a linear space E, and let\nN be a subset of M defined by N = g\u22121(0), where g : M \u2192R\u2113 is smooth and\nrank Dg(x) = \u2113 for all x \u2208 N. Show that N is itself an embedded submanifold of\nE, of dimension dim M\u2212\u2113, with tangent spaces TxN = ker Dg(x) \u2282 TxM. Here,\nwe assume \u2113 \u2265 1; see also Exercise 3.26. We call N an embedded submanifold\nof M; see also Section 8.14.\n3.5 Vector fields and the tangent bundle\nA map V which associates to each point x \u2208 Ma tangent vector at x is called\na vector field on M. For example, the gradient of a function f : M \u2192R (still\nto be defined) is a vector field. In order to define a notion of smooth vector\nfield, we need to present V as a map between manifolds. Since the range of V\nincludes tangent vectors from all possible tangent spaces of M, the first step\nis to introduce the tangent bundle : this is the disjoint union of all the tangent\nspaces of M. By \u201cdisjoint\u201d we mean that, for each tangent vector v \u2208 TxM, we\nretain the pair (x, v) rather than simply v. This is important to avoid ambiguity\nbecause some tangent vectors, seen as vectors in E, may belong to more than\none tangent space. For example, the zero vector belongs to all of them.\nDefinition 3.42. The tangent bundle of a manifold M is the disjoint union of\nthe tangent spaces of M:\nTM = {(x, v) : x \u2208 Mand v \u2208 TxM}. (3.30)\nWith some abuse of notation, for a tangent vector v \u2208 TxM, we sometimes\nconflate the notions of v and (x, v). We may write (x, v) \u2208 TxM, or even v \u2208\nTM if it is clear from context that the base of v is x.\nThe tangent bundle is a manifold.\nTheorem 3.43. If M is an embedded submanifold of E, the tangent bundle TM\nis an embedded submanifold of E \u00d7 Eof dimension 2 dimM.\nProof. For open submanifolds, the claim is clear: T xM = E for each x \u2208 M,\nhence T M = M \u00d7 E. This is an open subset of E \u00d7 E, hence it is an open\nsubmanifold of that space.\nConsidering the other case, pick an arbitrary point \u00afx \u2208 Mand let h: U \u2192 Rk", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2693, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "70acec92-85cf-4853-b0b8-c7b3378d6cd9": {"__data__": {"id_": "70acec92-85cf-4853-b0b8-c7b3378d6cd9", "embedding": null, "metadata": {"page_label": "38", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4634ecb0-a0c3-43dc-8858-e31e6b1bd7d3", "node_type": "4", "metadata": {"page_label": "38", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "de62cb2626e4778c306c67bd2f3c686c4e9e61d999df9c9bdd85062f49d62030", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n38 Embedded geometry: first order\nbe a local defining function for M at \u00afx, that is, U is a neighborhood of \u00afx in E,\nh is smooth, M \u2229U = {x \u2208 U : h(x) = 0}, and Dh(\u00afx): E \u2192Rk has rank k.\nIf needed, restrict the domain U to secure the property rank D h(x) = k for\nall x \u2208 U: this is always possible, see Lemma 3.74. Then, we can claim that\nTxM = ker Dh(x) for all x \u2208 M \u2229U. Consequently, a pair ( x, v) \u2208 U \u00d7 Eis in\nTM if and only if it satisfies the following equations:\nh(x) = 0 and D h(x)[v] = 0.\nAccordingly, define the smooth function H : U \u00d7 E \u2192R2k as:\nH(x, v) =\n\u0014 h(x)\nDh(x)[v]\n\u0015\n.\nThe aim is to show that H is a local defining function for T M. We already\nhave that TM \u2229(U \u00d7 E) = H\u22121(0). If we establish that D H(x, v) has rank 2 k\nfor all ( x, v) \u2208 TM \u2229(U \u00d7 E), we will have shown that T M is an embedded\nsubmanifold of E \u00d7 E. Let us compute the differential of H (we know it exists\nsince h is smooth):\nDH(x, v)[ \u02d9x, \u02d9v] =\n\u0014 Dh(x)[ \u02d9x]\nL(x, v)[ \u02d9x] + Dh(x)[ \u02d9v]\n\u0015\n=\n\u0014Dh(x) 0\nL(x, v) D h(x)\n\u0015\u0014 \u02d9x\n\u02d9v\n\u0015\n,\nwhere L(x, v): E \u2192 Eis some linear map which depends on both x and v (it\ninvolves the second derivative of h, but its specific form is irrelevant to us.) The\nblock triangular form of D H(x, v) allows us to conclude that rank D H(x, v) =\nrank Dh(x) + rank Dh(x) = 2 k, as required. Indeed, The rank is at most 2 k\nbecause H maps into R2k, and the rank is at least 2 k because the two diagonal\nblocks each have rank k. Since we can build such H on a neighborhood of any\npoint in TM, we conclude that T M is an embedded submanifold of E \u00d7 E. For\nthe dimension, use T (x,v)TM = ker DH(x, v) and the rank-nullity theorem to\nconclude that dim TM = dim T(x,v)TM = 2 dimE \u22122k = 2 dimM.\nThe topology we choose for T M is the embedded submanifold topology, as\nin Definition 3.21. This is different from the so-called disjoint union topology,\nwhich we never use. In Lemma 4.21, we argue that T (x,0)TM = TxM \u00d7TxM.\nSince T M is a manifold, we can now use Definition 3.30 to define smooth\nvector fields as particular smooth maps from M to TM. Be aware that some\nauthors refer to smooth vector fields as vector fields.\nDefinition 3.44. A vector field on a manifold M is a map V : M \u2192TM such\nthat V (x) is in TxM for all x \u2208 M. If V is a smooth map, we say it is a smooth\nvector field. The set of smooth vector fields is denoted by X(M).\nA vector field on an embedded submanifold is smooth if and only if it is the\nrestriction of a smooth vector field on a neighborhood of M in the embedding\nspace.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2752, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "14171535-18e7-439e-8fc4-196432228d13": {"__data__": {"id_": "14171535-18e7-439e-8fc4-196432228d13", "embedding": null, "metadata": {"page_label": "39", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eca65b99-5757-4ccd-88bc-c88c827b6260", "node_type": "4", "metadata": {"page_label": "39", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "b7d141cba0e9a39e77a78372d6a4e2e2aeef36b35fcf6b77ec00618b3f6ee6b3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n3.6 Moving on a manifold: retractions 39\nProposition 3.45. For M an embedded submanifold of E, a vector field V on\nM is smooth if and only if there exists a smooth vector field \u00afV on a neighborhood\nof M such that V = \u00afV |M.\nProof. Assume V : M \u2192TM is a smooth vector field on M. Then, since TM is\nan embedded submanifold of E \u00d7E, by Proposition 3.31, there exists a neighbor-\nhood U of M in E and a smooth function \u00af\u00afV : U \u2192 E \u00d7Esuch that V = \u00af\u00afV |M. De-\nnote the two components of\u00af\u00afV as \u00af\u00afV (x) = ( \u00af\u00afV1(x), \u00af\u00afV2(x)). Of course, \u00af\u00afV1, \u00af\u00afV2 : U \u2192 E\nare smooth. Define \u00afV (x) = ( x, \u00af\u00afV2(x)): this is a smooth vector field on U such\nthat V = \u00afV |M. The other direction is clear.\nIn closing, we note a useful identification for the tangent bundle of a product\nmanifold M \u00d7 M\u2032 (Proposition 3.20) which amounts to reordering parameters:\nT(M \u00d7 M\u2032) = {((x, x\u2032), (v, v\u2032)) : x \u2208 M, v\u2208 TxM, x\u2032 \u2208 M\u2032, v\u2032 \u2208 Tx\u2032M\u2032}\n\u2261 {((x, v), (x\u2032, v\u2032)) : x \u2208 M, v\u2208 TxM, x\u2032 \u2208 M\u2032, v\u2032 \u2208 Tx\u2032M\u2032}\n= TM \u00d7TM\u2032. (3.31)\nExercise 3.46. For f \u2208 F(M) and V, W\u2208 X(M), verify that the vector fields\nfV and V + W are smooth, where we define (fV )(x) = f(x)V (x) and also\n(V + W)(x) = V (x) + W(x). For pointwise scaling, we purposefully write fV\nand not V f. Later, \u22c6we will give a different meaning to the notation V f(see p89).\n3.6 Moving on a manifold: retractions\nGiven a point x \u2208 Mand a tangent vector v \u2208 TxM, we often need to move\naway from x along the direction v while remaining on the manifold: this is the\nbasic operation of a gradient descent algorithm, and of essentially all optimization\nalgorithms on manifolds. We can achieve this by following any smooth curvec on\nM such that c(0) = x and c\u2032(0) = v, but of course there exist many such curves.\nA retraction picks a particular curve for each possible (x, v) \u2208 TM. Furthermore,\nthis choice of curve depends smoothly on (x, v), in a sense we make precise using\nthe fact that the tangent bundle T M is a manifold.\nDefinition 3.47. A retraction on a manifold M is a smooth map\nR: T M \u2192 M: (x, v) 7\u2192 Rx(v)\nsuch that each curve c(t) = Rx(tv) satisfies c(0) = x and c\u2032(0) = v.\nFor M embedded in E, smoothness of R is understood in the sense of Defini-\ntion 3.30 for a map from T M to M, that is, R is smooth if and only if there\nexists a smooth map R from a neighborhood of T M in E \u00d7 Einto E such that\nR = R|TM.\nLet us illustrate this concept through examples. Chapter 7 has more.\nExample 3.48. On a linear manifold, Rx(v) = x + v is a retraction.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2721, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4aceddb4-bef6-46d0-b780-b152ef7afb70": {"__data__": {"id_": "4aceddb4-bef6-46d0-b780-b152ef7afb70", "embedding": null, "metadata": {"page_label": "40", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3370b1ac-4fe6-40b2-a719-2fc3adf37b17", "node_type": "4", "metadata": {"page_label": "40", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "454c59db16d44a0430eb954660c94f77877f0c2d7452b252f0fe9dd4ed17dac7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n40 Embedded geometry: first order\nExample 3.49. Let x be a point on the sphere Sd\u22121 and let v be tangent at x,\nthat is, x\u22a4v = 0. To move away from x along v while remaining on the sphere,\none way is to take the step in Rd then to project back to the sphere:\nRx(v) \u225c x + v\n\u2225x + v\u2225 = x + vp\n1 + \u2225v\u22252 . (3.32)\nConsider the curve c: R \u2192 Sd\u22121 defined by:\nc(t) = Rx(tv) = x + tvp\n1 + t2\u2225v\u22252 .\nEvidently, c(0) = x. Moreover, one can compute c\u2032(0) = v, that is, locally around\nx, up to first order, the retraction curve moves along v. To verify that R (3.32)\nis smooth, check that Rx(v) \u225c (x + v)/\np\n1 + \u2225v\u22252 is a smooth extension to all\nof Rd \u00d7 Rd. This is an example of a retraction based on metric projection : we\nstudy them in Section 5.12.\nAnother reasonable choice is to move away from x along a great circle:\nRx(v) \u225c cos(\u2225v\u2225)x + sin(\u2225v\u2225)\n\u2225v\u2225 v, (3.33)\nwith the usual convention sin(0)/0 = 1. Indeed, the curve\nc(t) = Rx(tv) = cos(t\u2225v\u2225)x + sin(t\u2225v\u2225)\n\u2225v\u2225 v\ntraces out the great circle on Sd\u22121 passing through x at t = 0 with velocity c\u2032(0) =\nv. With the right Riemannian metric (Section 3.7), such curves are geodesics\n(Section 5.8) and the retraction (3.33) is the exponential map (Section 10.2).\nIt is also common to define retractions without referring to curves. To see how,\nlet Rx : TxM \u2192 Mdenote the restriction of a smooth map R: T M \u2192 Mto\nthe tangent space at x. By the chain rule, each curve c(t) = Rx(tv) satisfies\nc(0) = Rx(0) and c\u2032(0) = DRx(0)[v].\nThus, R is a retraction exactly if, for all ( x, v) \u2208 TM, we have\n1. R x(0) = x, and\n2. DR x(0): T xM \u2192TxM is the identity map: DR x(0)[v] = v.\nThis characterization of retractions is equivalent to Definition 3.47.\nSometimes, it is convenient to relax the definition of retraction to allow maps\nR that are defined only on an open subset of the tangent bundle, provided all\nzero vectors belong to its domain. For example, this is the case for the manifold\nof fixed-rank matrices (Section 7.5).\nExercise 3.50. Let M, M\u2032 be equipped with retractions R, R\u2032. Show that the\nmap R\u2032\u2032 : T(M \u00d7 M\u2032) \u2192 M \u00d7 M\u2032 defined by R\u2032\u2032\n(x,x\u2032)(v, v\u2032) = (Rx(v), R\u2032\nx\u2032(v\u2032)) is\na valid retraction for the product manifold M \u00d7 M\u2032.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2417, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cde1b907-4031-41fd-8390-f8937948d57c": {"__data__": {"id_": "cde1b907-4031-41fd-8390-f8937948d57c", "embedding": null, "metadata": {"page_label": "41", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cca66543-14d9-43cf-a912-787d24d8b983", "node_type": "4", "metadata": {"page_label": "41", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "d6f1220fee52555e45a50243d66ce1691c881689e80f9bb37b81c429392e2138", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n3.7 Riemannian manifolds and submanifolds 41\n3.7 Riemannian manifolds and submanifolds\nIt is convenient to equip each tangent space of the manifold M with an inner\nproduct (recall Definition 3.1). This is the key ingredient to define gradients in\nthe next section. Since there are now many inner products (one for each point\non the manifold), we distinguish them with a subscript. That said, it is common\nto omit the subscript when it is clear from context: we do so on occasion in later\nchapters.\nDefinition 3.51. An inner product on TxM is a bilinear, symmetric, positive\ndefinite function \u27e8\u00b7, \u00b7\u27e9x : TxM \u00d7TxM \u2192R. It induces a norm for tangent vec-\ntors: \u2225u\u2225x =\np\n\u27e8u, u\u27e9x. A metric on M is a choice of inner product \u27e8\u00b7, \u00b7\u27e9x for\neach x \u2208 M.\nOf particular interest are metrics which, in some sense, vary smoothly with\nx. To give a precise meaning to this requirement, the following definition builds\nupon the notions of smooth scalar and vector fields.\nDefinition 3.52. A metric \u27e8\u00b7, \u00b7\u27e9x on M is a Riemannian metric if it varies\nsmoothly with x, in the sense that for all smooth vector fields V, Won M the\nfunction x 7\u2192 \u27e8V (x), W(x)\u27e9x is smooth from M to R.\nDefinition 3.53. A Riemannian manifold is a manifold with a Riemannian\nmetric.\nA Euclidean space is a linear space E with an inner product \u27e8\u00b7, \u00b7\u27e9 (the same at\nall points)\u2014we call it the Euclidean metric. When M is an embedded subman-\nifold of a Euclidean space E, the tangent spaces of M are linear subspaces of E.\nThis suggests a particularly convenient way of defining an inner product on each\ntangent space: simply restrict the inner product of E to each one. The resulting\nmetric on M is called the induced metric. As we now show, the induced metric\nis a Riemannian metric, leading to the notion of Riemannian submanifold.\nProposition 3.54. Let M be an embedded submanifold of E, and let \u27e8\u00b7, \u00b7\u27e9 be the\nEuclidean metric on E. Then, the metric on M defined at each x by restriction,\n\u27e8u, v\u27e9x = \u27e8u, v\u27e9 for u, v\u2208 TxM, is a Riemannian metric.\nProof. For any two smooth vector fields V, W\u2208 X(M), let \u00afV ,\u00afW be two smooth\nextensions of V, W to a neighborhood U of M in E. Then, consider g(x) =\n\u27e8V (x), W(x)\u27e9x (a function on M) and let \u00afg(x) = \u27e8\u00afV (x), \u00afW(x)\u27e9 (a function on\nU). Clearly, \u00afg is smooth and g = \u00afg|M. Hence, g is smooth.\nDefinition 3.55. Let M be an embedded submanifold of a Euclidean space E.\nEquipped with the Riemannian metric obtained by restriction of the metric of E,\nwe call M a Riemannian submanifold of E.\nThis is arguably the most common type of Riemannian manifold in applica-\ntions. Be mindful \u22c6that a Riemannian submanifold is not merely a submanifold\nwith some Riemannian structure: the words single out a precise choice of metric.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2969, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d9721c4d-68e0-4d40-b20e-cb2d16c3649b": {"__data__": {"id_": "d9721c4d-68e0-4d40-b20e-cb2d16c3649b", "embedding": null, "metadata": {"page_label": "42", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37c93f88-d09b-48a5-82cd-2ee15bbe4919", "node_type": "4", "metadata": {"page_label": "42", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "2fc7d83f1bb2bd45d1eb297b97cd27ec1afaf3d9523f20a5ca481a7b65f71824", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n42 Embedded geometry: first order\nExample 3.56. Endow Rd with the standard metric \u27e8u, v\u27e9 = u\u22a4v and consider\nthe sphere Sd\u22121 = {x \u2208 Rd : \u2225x\u2225 = 1 }, embedded in Rd. With the inherited\nmetric \u27e8u, v\u27e9x = \u27e8u, v\u27e9 = u\u22a4v on each tangent space TxSd\u22121, the sphere becomes\na Riemannian submanifold of Rd.\nExample 3.57. Let M, M\u2032 be Riemannian manifolds with metrics \u27e8\u00b7, \u00b7\u27e9M and\n\u27e8\u00b7, \u00b7\u27e9M\u2032\n, respectively. Recall from Proposition 3.20 that the product M \u00d7 M\u2032 is\nitself a manifold. The product metric \u27e8\u00b7, \u00b7\u27e9 on M\u00d7M \u2032 is defined as follows. For\n(u, u\u2032), (v, v\u2032) in the tangent space T(x,x\u2032)(M \u00d7 M\u2032),\n\u27e8(u, u\u2032), (v, v\u2032)\u27e9(x,x\u2032) = \u27e8u, v\u27e9M\nx + \u27e8u\u2032, v\u2032\u27e9M\u2032\nx\u2032 .\nIt is an exercise to show that this is a Riemannian metric (see Exercise 3.73).\nWith this metric, we call M \u00d7 M\u2032 a Riemannian product manifold.\n3.8 Riemannian gradients\nLet M be a Riemannian manifold, that is, a manifold with a Riemannian metric.\nGiven a smooth function f : M \u2192R, we are finally in a position to define its\ngradient.\nDefinition 3.58. Let f : M \u2192R be smooth on a Riemannian manifold M. The\nRiemannian gradient of f is the vector field gradf on M uniquely defined by the\nfollowing identities:\n\u2200(x, v) \u2208 TM, Df(x)[v] = \u27e8v, gradf(x)\u27e9x , (3.34)\nwhere Df(x) is as in Definition 3.34 and \u27e8\u00b7, \u00b7\u27e9x is the Riemannian metric.\nIt is an exercise to show that (3.34) indeed uniquely determines grad f(x) for\neach x in M, confirming gradf is well defined.\nTo work out the gradient of f, the preferred way is to obtain an expression for\nDf(x)[v] and to manipulate it until it takes the form \u27e8v, \u00b7\u27e9x, where \u00b7 is tangent at\nx. That yields the gradient by uniqueness. We discuss this more in Section 4.7.\nAlternatively, an indirect approach is through retractions as follows.\nProposition 3.59. Let f : M \u2192R be a smooth function on a Riemannian\nmanifold M equipped with a retraction R. Then, for all x \u2208 M,\ngradf(x) = grad(f \u25e6 Rx)(0), (3.35)\nwhere f \u25e6Rx : TxM \u2192R is defined on a Euclidean space (the linear space TxM\nwith inner product \u27e8\u00b7, \u00b7\u27e9x), hence its gradient is a \u201cclassical\u201d gradient. See also\nExercise 10.73 for the gradient of f \u25e6 Rx away from the origin.\nProof. By the chain rule, for all tangent vectors v \u2208 TxM,\nD(f \u25e6 Rx)(0)[v] = Df(Rx(0))[DRx(0)[v]] = Df(x)[v],", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2467, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "15c68cdc-8c78-4217-b15b-5fa78fc8ddac": {"__data__": {"id_": "15c68cdc-8c78-4217-b15b-5fa78fc8ddac", "embedding": null, "metadata": {"page_label": "43", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "932e586a-ea57-4faa-9b32-e98017647917", "node_type": "4", "metadata": {"page_label": "43", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "59fd59a149ad23c2c58b05b9a7956c1e84d4b22829dbaf16199ef38a15803898", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n3.8 Riemannian gradients 43\nsince Rx(0) = x and DRx(0) is the identity map (these are the defining properties\nof retractions). Using the definition of gradient for both f \u25e6Rx and f we conclude\nthat, for all v \u2208 TxM,\n\u27e8grad(f \u25e6 Rx)(0), v\u27e9x = \u27e8gradf(x), v\u27e9x .\nThe claim follows by uniqueness of the gradient.\nSay M is embedded in the Euclidean space E with Euclidean metric \u27e8\u00b7, \u00b7\u27e9 (for\nnow, that metric may or may not be related to the Riemannian metric on M).\nSince f is smooth, it has a smooth extension \u00aff defined on a neighborhood of M\nin E. The latter has a Euclidean gradient grad \u00aff. Combining (3.28) with (3.34),\nwe find:\n\u2200(x, v) \u2208 TM, \u27e8v, gradf(x)\u27e9x = Df(x)[v]\n= D \u00aff(x)[v] = \u27e8v, grad \u00aff(x)\u27e9. (3.36)\nThe core observation is: TxM is a subspace of E, and grad \u00aff(x) is a vector in E;\nas such, the latter can be uniquely decomposed in E as\ngrad \u00aff(x) = grad \u00aff(x)\u2225 + grad \u00aff(x)\u22a5,\nwith one component in T xM and the other orthogonal to T xM, orthogonality\nbeing judged by the inner product of E. Explicitly, grad \u00aff(x)\u2225 is in TxM and\n\u2200v \u2208 TxM,\n\nv, grad \u00aff(x)\u22a5\n\u000b\n= 0.\nAs a result, we get from (3.36) that, for all ( x, v) in TM,\n\u27e8v, gradf(x)\u27e9x =\n\nv, grad \u00aff(x)\n\u000b\n=\n\nv, grad \u00aff(x)\u2225 + grad \u00aff(x)\u22a5\n\u000b\n=\n\nv, grad \u00aff(x)\u2225\n\u000b\n. (3.37)\nThis relates the Riemannian gradient of f and the Euclidean gradient of \u00aff.\nNow further assume that M is a Riemannian submanifold of E. Then, since\ngrad \u00aff(x)\u2225 is tangent at x and since the Riemannian metric is merely a restriction\nof the Euclidean metric to the tangent spaces, we find:\n\u2200(x, v) \u2208 TM, \u27e8v, gradf(x)\u27e9x = \u27e8v, grad \u00aff(x)\u2225\u27e9x.\nBy identification, it follows that, for Riemannian submanifolds,\ngradf(x) = grad \u00aff(x)\u2225. (3.38)\nIn other words: to determine grad f, first obtain an expression for the (classical)\ngradient of any smooth extension of f, then orthogonally project to the tangent\nspaces. This is a practical recipe because we often have access to a smooth\nextension. It motivates us to introduce orthogonal projectors.\nDefinition 3.60. Let M be an embedded submanifold of a Euclidean space E\nequipped with a Euclidean metric \u27e8\u00b7, \u00b7\u27e9. The orthogonal projector to TxM is the\nlinear map Projx : E \u2192 Echaracterized by the following properties:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2441, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4cab2d74-a52f-45ba-8e2f-18de11d07d1c": {"__data__": {"id_": "4cab2d74-a52f-45ba-8e2f-18de11d07d1c", "embedding": null, "metadata": {"page_label": "44", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9196b97d-32dd-4675-b2cb-5bd7ee365e84", "node_type": "4", "metadata": {"page_label": "44", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "aa3bcd9b208032f1d9271420305bdbda14f9e9ff96ab6c87ea68f41b3e10264b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n44 Embedded geometry: first order\n1. Range: im(Projx) = TxM;\n2. Projector: Projx \u25e6 Projx = Projx;\n3. Orthogonal: \u27e8u \u2212 Projx(u), v\u27e9 = 0 for all v \u2208 TxM and u \u2208 E.\nFor an open submanifold, Proj x is the identity because TxM = E. The useful\nproposition below summarizes the above discussion.\nProposition 3.61. Let M be a Riemannian submanifold of E endowed with the\nmetric \u27e8\u00b7, \u00b7\u27e9 and let f : M \u2192R be a smooth function. The Riemannian gradient\nof f is given by\ngradf(x) = Projx(grad \u00aff(x)), (3.39)\nwhere \u00aff is any smooth extension of f to a neighborhood of M in E.\nExample 3.62. We continue with the Rayleigh quotient from Example 3.36:\nf(x) = x\u22a4Ax. Equip Rd with the standard Euclidean metric \u27e8u, v\u27e9 = u\u22a4v. Then,\nusing A = A\u22a4, for all v \u2208 Rd,\nD \u00aff(x)[v] = 2x\u22a4Av = \u27e82Ax, v\u27e9.\nHence, by identification with Definition 3.58,\ngrad \u00aff(x) = 2Ax.\nTo get a notion of gradient for f on Sd\u22121, we need to choose a Riemannian metric\nfor Sd\u22121. One convenient choice is to turn Sd\u22121 into a Riemannian submanifold\nof Rd by endowing it with the induced Riemannian metric. In that scenario,\nProposition 3.61 suggests we should determine the orthogonal projectors of Sd\u22121.\nFor the chosen Euclidean metric,\nTxSd\u22121 = {v \u2208 Rd : x\u22a4v = 0} = {v \u2208 Rd : \u27e8x, v\u27e9 = 0}\nis the orthogonal complement of x in Rd. Thus, orthogonal projection from Rd\nto that tangent space simply removes any component aligned with x:\nProjx(u) = u \u2212 (x\u22a4u)x = (Id \u2212 xx\u22a4)u. (3.40)\nIt follows that the Riemannian gradient of f on Sd\u22121 is:\ngradf(x) = Projx(grad \u00aff(x)) = 2\n\u0000\nAx \u2212 (x\u22a4Ax)x\n\u0001\n.\nNotice something quite revealing: for x \u2208 Sd\u22121,\ngradf(x) = 0 \u21d0 \u21d2 Ax = ( x\u22a4Ax)| {z }\nsome scalar\nx.\nIn words: all points where the Riemannian gradient vanishes are eigenvectors\nof A. Conversely: the gradient vanishes at all unit-norm eigenvectors of A. This\nbasic observation is crucial to understand the behavior of optimization algorithms\nfor f on Sd\u22121.\nOrthogonal projectors are self-adjoint (Definition 3.5).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2202, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "57b34d24-928b-4af2-8af8-e06d6f404175": {"__data__": {"id_": "57b34d24-928b-4af2-8af8-e06d6f404175", "embedding": null, "metadata": {"page_label": "45", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "697667af-5a60-4eb9-9ec5-833d915af0ca", "node_type": "4", "metadata": {"page_label": "45", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "88f3eef98b97a67a454f7bbac664ebb2aed8ec591cfe90740595599178736386", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n3.8 Riemannian gradients 45\nProposition 3.63. Let Projx be the orthogonal projector from E to a linear\nsubspace of E. Then, Projx is self-adjoint. Explicitly:\n\u2200u, v\u2208 E, \u27e8u, Projx(v)\u27e9 = \u27e8Projx(u), v\u27e9,\nwhere \u27e8\u00b7, \u00b7\u27e9 is the Euclidean metric on E.\nProof. From the properties of orthogonal projectors, for all u, v\u2208 E:\n0 = \u27e8u \u2212 Projx(u), Projx(v)\u27e9\n= \u27e8u, Projx(v)\u27e9 \u2212 \u27e8Projx(u), Projx(v)\u27e9\n= \u27e8u, Projx(v)\u27e9 \u2212 \u27e8Projx(u), v\u2212 (v \u2212 Projx(v))\u27e9\n= \u27e8u, Projx(v)\u27e9 \u2212 \u27e8Projx(u), v\u27e9 + \u27e8Projx(u), v\u2212 Projx(v)\u27e9| {z }\n=0\n.\nThis concludes the proof.\nExercise 3.64. Show that gradf(x) is uniquely defined by (3.34).\nExercise 3.65. We noted that the relative interior of the simplex, namely,\nM = \u2206d\u22121\n+ =\n\b\nx \u2208 Rd : x1, . . . , xd > 0 and x1 + \u00b7 \u00b7\u00b7+ xd = 1\n\t\n,\nis an embedded submanifold of Rd (Exercise 3.26). Its tangent spaces are:\nTxM =\n\b\nv \u2208 Rd : v1 + \u00b7 \u00b7\u00b7+ vd = 0\n\t\n.\nShow that \u27e8u, v\u27e9x = Pd\ni=1\nuivi\nxi\ndefines a Riemannian metric on M. This is called\nthe Fisher\u2013Rao metric. Consider a smooth function f : M \u2192R and a smooth\nextension \u00aff on a neighborhood of M in Rd (equipped with the canonical Euclidean\nmetric). Give an expression for gradf(x) in terms of grad \u00aff(x).\nNote: This exercise provides an example where gradf(x) is not simply the pro-\njection of grad \u00aff(x) to TxM. This is because, while M is an embedded submani-\nfold of Rd and it is a Riemannian manifold, it is not a Riemannian submanifold\nof Rd.\nExercise 3.66. Let E be a Euclidean space, and let L(E, E) denote the set of\nlinear maps from E into itself: this is a linear space. If E is identified with Rd,\nthen L(E, E) is identified with Rd\u00d7d. For M an embedded submanifold of E, show\nthat the map (recall Definition 3.60)\nProj: M \u2192 L(E, E): x 7\u2192 Projx\nis smooth. Deduce that, if f : M \u2192R is a smooth function on a Riemannian\nsubmanifold of E, then the Riemannian gradient of f is a smooth vector field.\nNote: It is true in general that the Riemannian gradient of a smooth function\nis smooth, but with the tools we have developed so far the proof is substantially\nsimpler for Riemannian submanifolds. See Section 3.9 for the more general case.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2347, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fba38032-63d4-4215-8917-bce375464cd6": {"__data__": {"id_": "fba38032-63d4-4215-8917-bce375464cd6", "embedding": null, "metadata": {"page_label": "46", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "05033585-b194-4dd8-a521-25ad5f082f25", "node_type": "4", "metadata": {"page_label": "46", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "a484e16c04ff2c849d58a2b6ff56790a9897715bf9716d05f1088b29ed5fa9b4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n46 Embedded geometry: first order\nExercise 3.67. For a smooth function f : M \u00d7 M\u2032 \u2192 R on a Riemannian\nproduct manifold (see Example 3.57), show that\ngradf(x, y) =\n\u0010\ngrad\n\u0000\nx 7\u2192 f(x, y)\n\u0001\n(x), grad\n\u0000\ny 7\u2192 f(x, y)\n\u0001\n(y)\n\u0011\n,\nwhere x 7\u2192 f(x, y) is the function from M to R obtained from f by fixing the\nsecond input to y, and similarly for y 7\u2192 f(x, y).\n3.9 Local frames*\nThis section introduces a technical tool which proves useful in certain proofs.\nA reader focused on Riemannian submanifolds can safely skip it. We show that\nembedded submanifolds admit local frames (as defined below) around all points.\nAs a first application, we use local frames to show that the gradient of a smooth\nfunction is a smooth vector field. Contrast this with Exercise 3.66 which is re-\nstricted to Riemannian submanifolds. The section goes on to sketch a proof of a\nmore general result known as the musical isomorphism.\nDefinition 3.68. Given a point x on a manifold M of dimension n, a lo-\ncal frame around x is a set of smooth vector fields W1, . . . , Wn defined on a\nneighborhood of x in M such that, for all y in that neighborhood, the vectors\nW1(y), . . . , Wn(y) form a basis for the tangent space TyM.\nProposition 3.69. Let M be an embedded submanifold of a linear space E.\nThere exists a local frame around any x \u2208 M.\nProof. Let E have dimension d and let M have dimension n = d \u2212 k. Theo-\nrem 3.12 provides us with a neighborhood U of x in E, an open set V in Rd\nand a diffeomorphism F : U \u2192 V such that M \u2229U = F\u22121(E \u2229 V ), where\nE = {y \u2208 Rd : yn+1 = \u00b7 \u00b7\u00b7= yd = 0}. The set U = M \u2229U is a neighborhood of\nx on M. We aim to build a local frame on U using F. The proof echoes that of\nTheorem 3.15.\nLet e1, . . . , ed denote the columns of the identity matrix of size d. Fix an\narbitrary y \u2208 U. The point F(y) is in E \u2229 V . For each i in 1, . . . , n, consider the\ncurve\n\u03b3i(t) = F(y) + tei.\nNotice that \u03b3i(t) remains in E \u2229 V for t close to zero. Therefore,\nci(t) = F\u22121(\u03b3i(t))\nis a smooth curve which remains in U for t close to zero. In particular, ci(0) = y.\nTherefore, c\u2032\ni(0) is a tangent vector to M at y. As a result, we may define vector\nfields W1, . . . , Wn on U as\nWi(y) = c\u2032\ni(0) =\n\u0000\nt 7\u2192 F\u22121(F(y) + tei)\n\u0001\u2032\n(0).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2468, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "386f25b4-a00e-4d69-89f8-11bfe7537874": {"__data__": {"id_": "386f25b4-a00e-4d69-89f8-11bfe7537874", "embedding": null, "metadata": {"page_label": "47", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d23763b-35ba-49be-bc24-4131a38ad76f", "node_type": "4", "metadata": {"page_label": "47", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "98c4c35ae0a42d533306ef9e469c5349595c6cebe666881acf06c2d00fad0b9b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n3.9 Local frames* 47\nThese vector fields are smooth since F and F\u22121 are smooth. It remains to verify\nthat they form a local frame. To this end, use the chain rule to see that\nWi(y) = DF\u22121(\u03b3i(0))[\u03b3\u2032\ni(0)] = DF\u22121(F(y))[ei]. (3.41)\nSince F is a diffeomorphism, we know that D F\u22121(F(y)) = (DF(y))\u22121. In par-\nticular, D F\u22121(F(y)) is invertible. It then follows from linear independence of\ne1, . . . , en in Rd that W1(y), . . . , Wn(y) are linearly independent in T yM. This\nholds for all y in U, which is a neighborhood of x in M, concluding the proof.\n(We have constructed a rather special local frame called a coordinate frame:\ncompare with Sections 8.3 and 8.8.)\nOn some manifolds, it is possible to find a global frame, that is, a set of smooth\nvector fields defined on the whole manifold and which provide a basis for all\ntangent spaces. Such manifolds are called parallelizable. For example, Rn and\nthe circle S 1 are parallelizable. However, the two-dimensional sphere S 2 is not.\nThe obstruction comes from the famous Hairy Ball Theorem, which implies that\nif W is a smooth vector field on S 2 then W(x) = 0 for some x. Therefore, any\npair of smooth vector fields W1, W2 on S2 fails to provide a basis for at least one\nof the tangent spaces.\nProposition 3.69 allows us to prove the following statement for embedded\nsubmanifolds equipped with a Riemannian metric.\nProposition 3.70. Let f : M \u2192R be a smooth function on a Riemannian\nmanifold M. The gradient vector field gradf is a smooth vector field on M.\nProof. Pick any point x \u2208 Mand a local frame W1, . . . , Wn defined on a neigh-\nborhood U of x in M, where dim M = n. By the properties of local frames, there\nexist unique functions g1, . . . , gn : U \u2192R such that\ngradf(y) = g1(y)W1(y) + \u00b7 \u00b7\u00b7+ gn(y)Wn(y)\nfor all y \u2208 U. If g1, . . . , gn are smooth, then grad f is smooth on U. Since U is a\nneighborhood of an arbitrary point, showing so is sufficient to prove that grad f\nis smooth. To show that each gi is indeed smooth, consider the following linear\nsystem which defines them. Taking the inner product of the above identity with\neach of the local frame fields against the Riemannian metric yields:\n\uf8ee\n\uf8ef\uf8f0\n\u27e8W1(y), W1(y)\u27e9y \u00b7 \u00b7\u00b7 \u27e8Wn(y), W1(y)\u27e9y\n... ...\n\u27e8W1(y), Wn(y)\u27e9y \u00b7 \u00b7\u00b7 \u27e8Wn(y), Wn(y)\u27e9y\n\uf8f9\n\uf8fa\uf8fb\n\uf8ee\n\uf8ef\uf8f0\ng1(y)\n...\ngn(y)\n\uf8f9\n\uf8fa\uf8fb\n=\n\uf8ee\n\uf8ef\uf8f0\n\u27e8gradf(y), W1(y)\u27e9y\n...\n\u27e8gradf(y), Wn(y)\u27e9y\n\uf8f9\n\uf8fa\uf8fb =\n\uf8ee\n\uf8ef\uf8f0\nDf(y)[W1(y)]\n...\nDf(y)[Wn(y)]\n\uf8f9\n\uf8fa\uf8fb.\nThe matrix of this system is invertible for all y in U and depends smoothly on\ny. Likewise, the right-hand side depends smoothly on y (consider extensions).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2790, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c6250e35-e200-449b-a7f7-c5895e72d2d0": {"__data__": {"id_": "c6250e35-e200-449b-a7f7-c5895e72d2d0", "embedding": null, "metadata": {"page_label": "48", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b2fd142-77ba-4699-a5da-c500089fb5fd", "node_type": "4", "metadata": {"page_label": "48", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "5193e3d5361b042fb2b96f54a8702bf358d32ac084464f4715ab5267a875a4b1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n48 Embedded geometry: first order\nHence, so does the solution of the system: this can be seen from Cramer\u2019s rule\nfor linear systems.\nThis last proposition is a corollary of a far more general fact. A smooth one-\nform on a manifold M is a linear map X : X(M) \u2192 F(M) which transforms a\nsmooth vector field V into a smooth scalar field X(V ) and such that X(gV ) =\ngX(V ) for allg : M \u2192R (we sayX is F(M)-linear). For example, the differential\nDf of a smooth functionf : M \u2192R is a smooth one-form defined by Df(V )(x) =\nDf(x)[V (x)]. If M is a Riemannian manifold with metric \u27e8\u00b7, \u00b7\u27e9, we can create a\nsmooth one-form using any smooth vector field. Indeed, given U \u2208 X(M), let\nX(V ) = \u27e8U, V\u27e9, where \u27e8U, V\u27e9 denotes the function x 7\u2192 \u27e8U(x), V(x)\u27e9x on M.\nIn fact, each smooth one-form corresponds to a smooth vector field in this way,\nand vice versa. This correspondence through the Riemannian metric is called the\nmusical isomorphism. Proposition 3.70 is a corollary of that fact: it establishes\nthe correspondence between D f and gradf.\nProposition 3.71. Let M be a Riemannian manifold with metric \u27e8\u00b7, \u00b7\u27e9. If\nX : X(M) \u2192 F(M) is a smooth one-form, then there exists a unique smooth\nvector field U \u2208 X(M) such that X(V ) = \u27e8U, V\u27e9 for all V \u2208 X(M).\nProof sketch. Let X be a smooth one-form. It is somewhat technical to show the\nfollowing property: given x \u2208 M,\nV (x) = 0 = \u21d2 X(V )(x) = 0. (3.42)\nThat property is a consequence of F(M)-linearity, as can be shown using local\nframes and tools developed later in Section 5.6: see Proposition 5.21 in particular.\nIn the proof sketch here, we simply assume that (3.42) holds. As an example,\nthe property is clear for X = Df.\nAn important consequence of (3.42) is that X(V )(x) depends on V only\nthrough V (x), that is, the dependence of X(V ) on V is pointwise. Indeed, for\nsome point x \u2208 M, consider any two smooth vector fields V1, V2 \u2208 X(M) such\nthat V1(x) = V2(x). Then, (V1 \u2212 V2)(x) = 0 and it follows from (3.42) and from\nlinearity of X that 0 = X(V1 \u2212 V2)(x) = X(V1)(x) \u2212 X(V2)(x). Thus,\nV1(x) = V2(x) = \u21d2 X(V1)(x) = X(V2)(x),\nas claimed.\nTherefore, X(V )(x) depends on V only through V (x), linearly. It is then a\nsimple fact about linear functions on the Euclidean space TxM that there exists\na unique vector in T xM, which we denote by U(x), such that\nX(V )(x) = \u27e8U(x), V(x)\u27e9x .\nAs this holds for all x \u2208 M, we deduce that there exists a unique vector field U\non M such that\nX(V ) = \u27e8U, V\u27e9.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2694, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dcce9de0-f090-4cc7-9b55-dda442826a14": {"__data__": {"id_": "dcce9de0-f090-4cc7-9b55-dda442826a14", "embedding": null, "metadata": {"page_label": "49", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2ad50fd9-2ab9-4cfc-a072-841d182ec51a", "node_type": "4", "metadata": {"page_label": "49", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "caed1e003319408659aa0a3adc94f3d45d20044cb2c0a5dfa10df73616d3c637", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n3.9 Local frames* 49\nIt remains to show that U is smooth.\nTo show that U is smooth, we merely need to show that it is smooth around\neach point. Given x \u2208 M, let W1, . . . , Wn \u2208 X(U) be a local frame on a neigh-\nborhood U of x, as provided by Proposition 3.69. Again using technical results\ndescribed in Section 5.6 (see Lemma 5.26 in particular), one can show that, pos-\nsibly at the expense of replacing U with a smaller neighborhood of x, there exist\nsmooth vector fields \u02dcW1, . . . ,\u02dcWn \u2208 X(M) such that \u02dcWi|U = Wi for all i. We need\nthis because we do not know how to apply X to Wi but we can apply X to \u02dcWi.\nNotice that for X = Df it is clear how to operate on locally defined vector fields,\nhence this step was not needed to establish Proposition 3.70. By the properties\nof local frames, there exist unique functions g1, . . . , gn : U \u2192R such that\nU|U = g1W1 + \u00b7 \u00b7\u00b7+ gnWn.\nTo show that U is smooth around x, it is sufficient to show that the functions\ng1, . . . , gn are smooth around x. To this end, observe that\nX( \u02dcWj)|U = \u27e8U, \u02dcWj\u27e9 |U\n= \u27e8U|U, \u02dcWj|U\u27e9\n=\nnX\ni=1\ngi \u27e8Wi, Wj\u27e9.\nSince X is a smooth one-form, we know that X( \u02dcWj) is a smooth function on\nall of M. Therefore, it is certainly smooth when restricted to U. We can deduce\nthat the functions g1, . . . , gn are smooth with the same reasoning as in the proof\nof Proposition 3.70. This concludes the proof sketch.\nOne-forms are also called cotangent vector fields. The musical isomorphism is\nalso called the tangent\u2013cotangent isomorphism.\nExercise 3.72. Let M be a Riemannian manifold. Show that for all x \u2208 Mthere\nexists an orthonormal local frame , that is, a local frame W1, . . . , Wn defined on\na neighborhood U of x with the additional property that\n\u2200y \u2208 U, \u27e8Wi(y), Wj(y)\u27e9y =\n(\n1 if i = j,\n0 otherwise.\nHint: apply the Gram\u2013Schmidt procedure to a local frame and check that its\nsmoothness is preserved.\nExercise 3.73. Verify that the product metric defined in Example 3.57 is indeed\na Riemannian metric.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2230, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3a9ca515-8699-478d-8eff-8412cea9d39a": {"__data__": {"id_": "3a9ca515-8699-478d-8eff-8412cea9d39a", "embedding": null, "metadata": {"page_label": "50", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0e73c68b-c872-4f49-bea7-b611cf550d66", "node_type": "4", "metadata": {"page_label": "50", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "6739b3afa76443ebb6fffdf79172011f382f76b011066ce11621b87cdd5385a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n50 Embedded geometry: first order\n3.10 Notes and references\nThe main sources for this chapter are [AMS08, Lee12, Lee18, O\u2019N83]. More\ncomprehensive reminders of topology, linear algebra and calculus can be found\nin [Lee12, App. A, B, C].\nThe treatment given here is restrictive but fully compatible with the general\ntreatment of differential and Riemannian geometry found in those references and\nalso presented in Chapter 8. Explicitly:\n1. Every subset M of a linear space E which we call an embedded submanifold\nfollowing Definition 3.10 is a smooth manifold in the sense of Definition 8.21,\nthat is, M admits a maximal atlas whose associated topology fulfills the usual\nconditions.\n2. There is a unique such maximal atlas whose topology matches the topology\nin Definition 3.21: we always mean to use that atlas.\n3. A subset M of a linear space E is an embedded submanifold of E in the sense\nof Definition 3.10 if and only if it is an embedded submanifold of E in the\ngeneral sense of Definition 8.73.\n4. A map between two embedded submanifolds of linear spaces is smooth in the\nsense of Definition 3.30 if and only if it is smooth in the general sense of\nDefinition 8.5, that is, through charts.\n5. The tangent spaces in Definition 3.14 correspond to the general notion of tan-\ngent spaces in Definition 8.33 through the standard identification mentioned\naround eq. (8.26).\nSee Sections 8.3 and 8.14 for some proofs and references. All other tools con-\nstructed here (tangent bundles, vector fields, retractions, Riemannian metrics,\netc.) are compatible with their general counterparts constructed in the other\nsections of Chapter 8.\nIn Definition 3.10, we require the differential of the local defining function h\naround x to have rank k at x. The next lemma shows that it would be equivalent\nto require D h to have rank k at all points of the domain of h, possibly after\nreducing that domain [Lee12, Prop. 4.1].\nLemma 3.74. Let U be a neighborhood of x in E. If h: U \u2192 Rk is smooth\nand rank Dh(x) = k, one can always restrict the domain U to a possibly smaller\nneighborhood U\u2032 of x such that rank Dh(x\u2032) = k for all x\u2032 in U\u2032.\nProof. The set U\u2032 \u2286 U of points x\u2032 where Dh(x\u2032) has rank k is an open set in E.\nIndeed, let A(x\u2032) \u2208 Rk\u00d7d be the matrix representing D h(x\u2032) in some basis of E,\nwith d = dim E. Consider the following function on U: g(x\u2032) = det(A(x\u2032)A(x\u2032)\u22a4).\nNotice that U\u2032 = U\\g\u22121(0). We know that g\u22121(0) is closed because it is the\npreimage of the closed set {0} for the continuous function g. Hence, U\u2032 is open.\nBy assumption, x is in U\u2032. Thus, it suffices to restrict the domain of h to U\u2032.\nThe rank condition in Definition 3.10 is key. Indeed, contrast this with the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2939, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "46855935-001f-404e-a6ac-ff8719e81b9e": {"__data__": {"id_": "46855935-001f-404e-a6ac-ff8719e81b9e", "embedding": null, "metadata": {"page_label": "51", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "30dcfeb3-14cc-4762-aece-246f70900bc5", "node_type": "4", "metadata": {"page_label": "51", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "6ab5dfd336899a1311b9b46057ad281bc1b7793a70df2b25c9d9a4d571b012b1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n3.10 Notes and references 51\nfollowing fact: any closed subset of a linear space E is the zero-set of a smooth\nfunction from E to R [Lee12, Thm. 2.29] (and E can be replaced by a manifold\nin this statement). For example, there exists a smooth function h: R2 \u2192 R such\nthat h\u22121(0) is a square in R2. Of course, a square is not smoothly embedded\nin R2 due to its corners, so we deduce that D h must have non-maximal rank at\ncertain points on the square.\nMoreover, it is not sufficient for the rank to be constant on the set. Here is\nan example: consider h: R2 \u2192 R defined by h(x) = ( x1x2)2. Its zero-set is a\ncross in R2. Yet, D h(x) = [ 2x1x2\n2, 2x2\n1x2 ] has constant rank on the cross: it is\nzero everywhere. Thus, we see that in order to exclude pathological sets such as\nthis cross it is not sufficient to ask for the rank of D h(x) to be constant along\nthe zero-set of h. However, it is sufficient to require a constant (possibly not\nmaximal) rank on a neighborhood of the zero-set (Proposition 8.77). The above\nh fails that test.\nSome embedded submanifolds of E with dim M < dim E cannot be defined\nwith a single defining function. Indeed, if M is the zero-set of a local defining\nfunction h: U \u2286 E \u2192R, then M is orientable in E [Lee12, Prop. 15.23]. Yet, one\ncan construct an open M\u00a8 obius bandas a non-orientable embedded submanifold\nof dimension two in R3. Thus, that manifold cannot be defined using a single\ndefining function.\nProposition 3.31 states that any smooth map between embedded submanifolds\nof linear spaces can be smoothly extended to a neighborhood of its (co)domain,\nand vice versa. This follows from the tubular neighborhood theorem found, for\nexample, in [Lee12, Thm. 6.24] and [Lee18, Thm. 5.25], as shown in Proposi-\ntions 8.79 and 8.80 for the general case of embedded submanifolds of manifolds.\nThus, we could also use Proposition 3.31 as the definition of smooth maps.\nThis is indeed practical in many situations, and this is why we introduced that\nresult early on. However, adopting this as our definition would make it harder\nto prove, for example, Proposition 3.70. This is because it would require one to\nexhibit a smooth extension around the whole manifold, as opposed to merely\nexhibiting a smooth extension around each point of the manifold.\nFor the special case of a smooth function f : M \u2192R where M is embedded\nand closed in E, it is also possible to smoothly extend f to all of E. Indeed,\nby [Lee12, Prop. 5.5], M is properly embedded in E if and only if it is closed, and\nsmooth functions on properly embedded submanifolds can be globally smoothly\nextended [Lee12, Lem. 5.34, Exercise 5-18]. This result (and others referenced\nabove) relies on partitions of unity [Lee12, pp40\u201347]. Importantly, this is not\ngenerally true for manifolds that are merely embedded (Exercise 3.33).\nDefinition 3.55 restricts the notion of Riemannian submanifolds of E to embed-\nded submanifolds of E. This is compatible with O\u2019Neill, who reserves the word\n\u201csubmanifold\u201d for embedded submanifolds [O\u2019N83, pp19, 57]. Certain authors\nadopt a more general definition, also allowing an immersed submanifold (Defi-\nnition 8.72) to be called a Riemannian submanifold. This is the case of Lee for\nexample [Lee18, p15].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3488, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "79964cb6-b6e3-4379-a8db-5014305132fc": {"__data__": {"id_": "79964cb6-b6e3-4379-a8db-5014305132fc", "embedding": null, "metadata": {"page_label": "52", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3e05bfa8-c92a-4deb-bc29-41b02384c11c", "node_type": "4", "metadata": {"page_label": "52", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "5840755703e9910691553a5fa830533abdd8a7b9bf27d151007123cc12d36c8f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n52 Embedded geometry: first order\nLocal frames (Definition 3.68) are discussed in [Lee12, pp177\u2013179]. Likewise,\nfor more on the musical isomorphism (Proposition 3.71), see [Lee12, pp341\u2013343]\nand [O\u2019N83, Prop. 3.10].\nMuch of the theory in this book could be constructed with weaker smoothness\nrequirements. Specifically, it is possible to define embedded submanifolds of linear\nspaces with local defining functions which are differentiable only p times (class\nCp instead of C\u221e). Likewise, we could work with functions on manifolds which\nhave only limited differentiability properties (see Remark 8.6). One reference\namong many on this topic is the book by Borwein and Zhu [BZ05, \u00a7 7].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 941, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "075b6f66-a8d7-4010-bb69-93c1ae78f005": {"__data__": {"id_": "075b6f66-a8d7-4010-bb69-93c1ae78f005", "embedding": null, "metadata": {"page_label": "53", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "84a672db-38a0-492c-b1a5-311657511aa0", "node_type": "4", "metadata": {"page_label": "53", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "e9101c0a709086e73fbbe69a5be8c02eae49c31216cd456c3485d12190cb09db", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n4 First-order optimization algorithms\nIn this chapter, we consider a first algorithm to solve problems of the form\nmin\nx\u2208M\nf(x), (4.1)\nwhere M is a (smooth) manifold and f : M \u2192R is a smooth function called the\ncost function or objective function. Discussions in this chapter apply for general\nmanifolds: embedded submanifolds as defined in the previous chapter form one\nclass of examples, and we detail other kinds of manifolds in Chapters 8 and 9.\nA (global) minimizer or (global) optimizer for (4.1) is a point x \u2208 Msuch\nthat f(x) \u2264 f(y) for all y \u2208 M. Defining this notion merely requires M to be a\nset and f to be a function: their smoothness is irrelevant. Minimizers may not\nexist, in which case it would be more appropriate to write (4.1) as inf x\u2208M f(x).\nMinimizers may also not be unique.\nWhile it is typically our goal to compute a global minimizer, this goal is\ngenerally out of reach. A more realistic (though still non-trivial) goal is to aim for\na local minimizer or local optimizer, that is, a pointx \u2208 Msuch that f(x) \u2264 f(y)\nfor all y in a neighborhood of x in M. In other words: a local minimizer appears\nto be optimal when compared only to its immediate surroundings. Likewise, a\nstrict local minimizer satisfies f(x) < f(y) for all y \u0338= x in some neighborhood\naround x. Recall that a neighborhood of x in M is an open subset of M which\ncontains x. Hence, the notion of local minimizer relies on the topology of M. For\nembedded submanifolds, we defined the topology in Definition 3.21. Just as for\nglobal minimizers, it does not rely on smoothness of either M or f.\nThus, importantly,\nOur problem is defined independently of the smooth structures we impose.\nYet, smoothness plays a crucial role in helping us solve that problem. As we\ndiscuss below, the notions of retraction and gradient afford us efficient means of\nmoving on the manifold while making progress toward our goal. In this sense,\nthe Riemannian geometry we impose on the problem is entirely ours to choose,\nand an integral part of our responsibilities as algorithm designer.\nSince optimization algorithms generate sequences of points on M, it is impor-\ntant to define terms pertaining to convergence. These are phrased in terms of\nthe topology on M.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2491, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c8be58e9-5042-480d-bb7a-e08eb1878a56": {"__data__": {"id_": "c8be58e9-5042-480d-bb7a-e08eb1878a56", "embedding": null, "metadata": {"page_label": "54", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1ed1b7d0-1dbc-4a38-b0f9-cdb2a04038e5", "node_type": "4", "metadata": {"page_label": "54", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "fffb04d3536eb6a7979be6c36420fa08802f472a95f10e3829c7de417a1d3f6e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n54 First-order optimization algorithms\nDefinition 4.1. Consider a sequence S of points x0, x1, x2, . . .on a manifold\nM. Then,\n1. A point x \u2208 Mis a limit of S if, for every neighborhood U of x in M, there\nexists an integer K such that xK, xK+1, xK+2, . . .are in U. The topology of\na manifold is Hausdorff (see Section 8.2), hence a sequence has at most one\nlimit. If x is the limit, we write limk\u2192\u221e xk = x or xk \u2192 x and we say the\nsequence converges to x.\n2. A point x \u2208 Mis an accumulation point of S if it is the limit of a subsequence\nof S, that is, if every neighborhood U of x in M contains an infinite number\nof elements of S.\nThis chapter focuses on Riemannian gradient descent. With just one additional\ngeometric tool, namely, the notion of vector transport or transporter introduced\nin Section 10.5, a number of other first-order optimization algorithms can be\naddressed, including Riemannian versions of nonlinear conjugate gradients and\nBFGS: see Section 4.9 for pointers.\nExercise 4.2. Give an example of a sequence that has no limit. Give an example\nof a sequence that has a single accumulation point yet no limit. Give an example\nof a sequence that has two distinct accumulation points. Show that if a sequence\nconverges to x, then all of its accumulation points are equal to x. Now consider\nthe particular case of M an embedded submanifold of a linear space E. Show\nthat a sequence on M may have a limit in E yet no limit in M. Argue that this\ncannot happen if M is closed in E.\n4.1 A first-order Taylor expansion on curves\nOptimization algorithms move from point to point on a manifold by following\nsmooth curves. In order to analyze these algorithms, we need to understand\nhow the cost function varies along those curves. In Rn for example, we could\nbe interested in how f(x + tv) varies as a function of t close to t = 0. The\ntool of choice for this task is a Taylor expansion. We now apply this concept to\nRiemannian manifolds.\nLet c: I \u2192 Mbe a smooth curve on M with c(0) = x and c\u2032(0) = v, where\nI is an open interval of R around t = 0. Evaluating f along this curve yields a\nreal function:\ng : I \u2192 R: t 7\u2192 g(t) = f(c(t)).\nSince g = f \u25e6c is smooth by composition and maps real numbers to real numbers,\nit admits a Taylor expansion:\ng(t) = g(0) + tg\u2032(0) + O(t2).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2541, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "56882d47-827c-49b4-b1f4-633d482a5bb3": {"__data__": {"id_": "56882d47-827c-49b4-b1f4-633d482a5bb3", "embedding": null, "metadata": {"page_label": "55", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36f7aede-a271-408c-b941-5d244187cec6", "node_type": "4", "metadata": {"page_label": "55", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "239097d4af41f276ad149fec8b0813f829eb37966aeecfbd67978221427ffd1c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n4.2 First-order optimality conditions 55\nClearly, g(0) = f(x). Furthermore, by the chain rule,\ng\u2032(t) = Df(c(t))[c\u2032(t)] = \u27e8gradf(c(t)), c\u2032(t)\u27e9c(t)\nso that g\u2032(0) = \u27e8gradf(x), v\u27e9x. Overall, we get this Taylor expansion:\nf(c(t)) = f(x) + t \u27e8gradf(x), v\u27e9x + O(t2). (4.2)\nIn particular, if the curve is obtained by retraction as c(t) = Rx(tv), then\nf(Rx(tv)) = f(x) + t \u27e8gradf(x), v\u27e9x + O(t2). (4.3)\nEquivalently, we may eliminate t by introducing s = tv in TxM:\nf(Rx(s)) = f(x) + \u27e8gradf(x), s\u27e9x + O(\u2225s\u22252\nx). (4.4)\nThe latter is a statement about the composition f \u25e6 R: T M \u2192R, called the\npullback of f (by R) to the tangent spaces. It is called this way as it quite\nliterally pulls the cost function from the manifold back to the tangent spaces.\nIn particular, f \u25e6 Rx : TxM \u2192R is the pullback of f to the tangent space at x.\nImportantly, this is a smooth function on a linear space: it has many uses, as we\nshall soon see.\nLater, in Section 5.9, we extend the above reasoning to work out second-order\nTaylor expansions.\nExercise 4.3. Given a smooth curve c: [0, 1] \u2192 Mwith c(0) = x and c(1) = y,\ncheck that there exists t \u2208 (0, 1) such that\nf(y) = f(x) + \u27e8gradf(c(t)), c\u2032(t)\u27e9c(t) . (4.5)\n(See Exercise 5.40 for the next order.)\n4.2 First-order optimality conditions\nIn general, checking whether a point x on M is a local minimizer for f : M \u2192R\nis difficult. We can however identify certain simple necessary conditions for a\npoint x to be a local minimizer. The following definition states such a condition.\nIt is called the first-order necessary optimality condition , because it involves\nfirst-order derivatives.\nDefinition 4.4. A point x \u2208 Mis critical (or stationary) for a smooth function\nf : M \u2192R if\n(f \u25e6 c)\u2032(0) \u2265 0\nfor all smooth curves c on M such that c(0) = x.\nIn words: it is not possible to move away from a critical point x and obtain\nan initial decrease in the value of f with a linear rate. Notice that it would be\nequivalent to require (f \u25e6 c)\u2032(0) = 0 in the definition: simply consider the curves", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2265, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6f5d1d11-31a1-4064-bd3b-e3650ee66979": {"__data__": {"id_": "6f5d1d11-31a1-4064-bd3b-e3650ee66979", "embedding": null, "metadata": {"page_label": "56", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fba8b88e-f446-4304-8d71-330148a57061", "node_type": "4", "metadata": {"page_label": "56", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "6b529a9e62c6e18b3bd9ebcf18d0999162fabfa96dcdbb0bf5651b16705f6982", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n56 First-order optimization algorithms\nt 7\u2192 c(t) and t 7\u2192 c(\u2212t) simultaneously. Still equivalently, x is critical for f\nexactly if Df(x) = 0 (by the chain rule on f \u25e6 c).\nProposition 4.5. Any local minimizer of a smooth function f : M \u2192R is a\ncritical point of f.\nProof. Let x be a local minimizer of f: there exists a neighborhood U of x in\nM such that f(y) \u2265 f(x) for all y \u2208 U. For contradiction, assume there exists\na smooth curve c: I \u2192 Mwith c(0) = x and (f \u25e6 c)\u2032(0) < 0. By continuity of\n(f \u25e6 c)\u2032, there exists \u03b4 >0 such that (f \u25e6 c)\u2032(\u03c4) < 0 for all \u03c4 \u2208 [0, \u03b4]. This further\nimplies that, for all t \u2208 (0, \u03b4],\nf(c(t)) = f(c(0)) +\nZ t\n0\n(f \u25e6 c)\u2032(\u03c4) d\u03c4 < f(x).\nYet, since c is continuous, c\u22121(U) = {t \u2208 I : c(t) \u2208 U}is open, and it contains 0\nbecause c(0) = x \u2208 U. Hence, c\u22121(U) \u2229 (0, \u03b4] is non-empty: there exists t \u2208 (0, \u03b4]\n(implying f(c(t)) < f(x)) such that c(t) is in U (implying f(c(t)) \u2265 f(x)): a\ncontradiction.\nIt is easy to check that local maximizers (defined in analogy to local mini-\nmizers) are also critical points. The converse of Proposition 4.5 does not hold in\ngeneral: see Chapter 11 for a special case.\nOn a Riemannian manifold, the critical points of a function are exactly those\npoints where the Riemannian gradient vanishes.\nProposition 4.6. Let f : M \u2192R be smooth on a Riemannian manifold M.\nThen, x is a critical point of f if and only if gradf(x) = 0.\nProof. Let c: I \u2192 Mbe any smooth curve on M with c(0) = x and c\u2032(0) = v.\nWe know that\n(f \u25e6 c)\u2032(0) = Df(x)[v] = \u27e8gradf(x), v\u27e9x .\nIf gradf(x) = 0, then x is clearly critical. The other way around, if x is critical,\nthen \u27e8gradf(x), v\u27e9x \u2265 0 for all v \u2208 TxM. Considering both v and \u2212v, it follows\nthat \u27e8gradf(x), v\u27e9x = 0 for all v \u2208 TxM. Thus, gradf(x) = 0.\nIn developing optimization algorithms, one of our more modest goals is to\nensure that accumulation points of sequences generated by those algorithms are\ncritical points: we aim for small gradients.\n4.3 Riemannian gradient descent\nThe standard gradient descent algorithm in Euclidean space E iterates\nxk+1 = xk \u2212 \u03b1kgradf(xk), k = 0, 1, 2, . . . ,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2338, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2f152088-2abc-443a-a0e2-847f88a5ad14": {"__data__": {"id_": "2f152088-2abc-443a-a0e2-847f88a5ad14", "embedding": null, "metadata": {"page_label": "57", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e5e9cf33-f963-4e5b-be81-902b7d729e3e", "node_type": "4", "metadata": {"page_label": "57", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "2ad983ebe595a2de3c945099b1d7ee75254dfb269280abf585a4ffd5d784c831", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n4.3 Riemannian gradient descent 57\nstarting with some x0 \u2208 Eand using some step-sizes \u03b1k > 0. Inspired by this, the\nfirst algorithm we consider for optimization on manifolds is Riemannian gradient\ndescent (RGD): given x0 \u2208 Mand a retraction R on M, iterate\nxk+1 = Rxk (\u2212\u03b1kgradf(xk)), k = 0, 1, 2, . . . .\nSee Algorithm 4.1. Importantly, the choice of retraction is part of the algorithm\nspecification.\nAlgorithm 4.1 RGD: the Riemannian gradient descent method\nInput: x0 \u2208 M\nFor k = 0, 1, 2, . . .\nPick a step-size \u03b1k > 0\nxk+1 = Rxk (sk), with step sk = \u2212\u03b1kgradf(xk)\nTo complete the specification of RGD, we need an explicit procedure to pick\nthe step-size \u03b1k at each iteration. This is called the line-search phase, and it can\nbe done in various ways. Define\ng(t) = f(Rxk (\u2212tgradf(xk))). (4.6)\nLine-search is about minimizing g approximately: well enough to make progress,\nyet bearing in mind that this is only a means to an end; we should not invest\ntoo much resources into it. Three common strategies include:\n1. Fixed step-size: \u03b1k = \u03b1 for all k.\n2. Optimal step-size: \u03b1k minimizes g(t) exactly; in rare cases, this can be done\ncheaply.\n3. Backtracking: starting with a guess t0 > 0, iteratively reduce it by a factor\nas ti = \u03c4ti\u22121 with \u03c4 \u2208 (0, 1) until ti is deemed acceptable, and set \u03b1k = ti.\nThere are various techniques to pick t0.\nWe discuss this more in Section 4.5. For now, we focus on identifying assumptions\nthat lead to favorable behavior.\nOur first assumption about problem (4.1) simply requires that the cost func-\ntion f be globally lower-bounded. This is normally the case for a well-posed\noptimization problem.\nA 4.1. There exists flow \u2208 R such that f(x) \u2265 flow for all x \u2208 M.\nWe expect that the algorithm may converge (or at least produce interesting\npoints) provided it makes some progress at every iteration. This is the object\nof the second assumption below. We first confirm that it is sufficient for our\npurposes, and later show how to fulfill it.\nGoing \u22c6forward in this chapter, we most often write \u27e8\u00b7, \u00b7\u27e9 and \u2225 \u00b7 \u2225instead of\n\u27e8\u00b7, \u00b7\u27e9x and \u2225 \u00b7 \u2225x when the base point x is clear from context.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2374, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "598dd297-f754-4755-b85a-fca15bf4b98c": {"__data__": {"id_": "598dd297-f754-4755-b85a-fca15bf4b98c", "embedding": null, "metadata": {"page_label": "58", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7e9e1df1-be84-4958-9b3f-dfda49f6687f", "node_type": "4", "metadata": {"page_label": "58", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "aa4d904b118b9537537385e65ef0e8252376b4df9733c1350958570a09737116", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n58 First-order optimization algorithms\nA 4.2. At each iteration, the algorithm achieves sufficient decrease for f, in that\nthere exists a constant c >0 such that, for all k,\nf(xk) \u2212 f(xk+1) \u2265 c\u2225gradf(xk)\u22252. (4.7)\nIt is the responsibility of the line-search procedure to ensure this assumption\nholds. This can be done under some conditions on f and the retraction, as we\ndiscuss later. When both assumptions hold, it is straightforward to guarantee\nthat RGD produces points with small gradient. There are no conditions on the\ninitialization x0 \u2208 M.\nProposition 4.7. Let f be a smooth function satisfying A4.1 on a Riemannian\nmanifold M. Let x0, x1, x2, . . .be iterates satisfying A4.2 with constant c. Then,\nlim\nk\u2192\u221e\n\u2225gradf(xk)\u2225 = 0.\nIn particular, all accumulation points (if any) are critical points. Furthermore,\nfor all K \u2265 1, there exists k in 0, . . . , K\u2212 1 such that\n\u2225gradf(xk)\u2225 \u2264\nr\nf(x0) \u2212 flow\nc\n1\u221a\nK\n.\nProof. The proof is based on a standard telescoping sum argument. We get the\nclaimed inequality for all K \u2265 1 as follows:\nf(x0) \u2212 flow\nA4.1\n\u2265 f(x0) \u2212 f(xK) =\nK\u22121X\nk=0\nf(xk) \u2212 f(xk+1)\nA4.2\n\u2265 Kc min\nk=0,...,K\u22121\n\u2225gradf(xk)\u22252.\nTo get the limit statement, observe that f(xk+1) \u2264 f(xk) for all k by A4.2.\nThen, taking K to infinity we see that\nf(x0) \u2212 flow \u2265\n\u221eX\nk=0\nf(xk) \u2212 f(xk+1),\nwhere the right-hand side is a series of nonnegative numbers. The bound implies\nthat the summands converge to zero, thus:\n0 = lim\nk\u2192\u221e\nf(xk) \u2212 f(xk+1) \u2265 c lim\nk\u2192\u221e\n\u2225gradf(xk)\u22252,\nwhich confirms that \u2225gradf(xk)\u2225 \u21920. Now, let x be an accumulation point\nof the sequence of iterates. By definition, there exists a subsequence of iterates\nx(0), x(1), x(2), . . .which converges to x. Then, since the norm of the gradient of\nf is a continuous function, it commutes with the limit and we find:\n0 = lim\nk\u2192\u221e\n\u2225gradf(xk)\u2225 = lim\nk\u2192\u221e\n\u2225gradf(x(k))\u2225\n= \u2225gradf( lim\nk\u2192\u221e\nx(k))\u2225 = \u2225gradf(x)\u2225,\nshowing all accumulation points are critical points.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2170, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "69ee86f2-caa2-458c-a7be-1971f4246313": {"__data__": {"id_": "69ee86f2-caa2-458c-a7be-1971f4246313", "embedding": null, "metadata": {"page_label": "59", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d2b381fa-9355-4137-9a68-23eb1092ae4a", "node_type": "4", "metadata": {"page_label": "59", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "c6f7124827b6fb8ac014c4cbcf154c5d89fcb8bae27e822a415fdcebad5f7b81", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n4.4 Regularity conditions and iteration complexity 59\nImportantly, the limit statement does not say that the sequence of iterates\nconverges to a critical point. It only states that, under the prescribed conditions,\nthe accumulation points of the sequence of iterates (of which there may be one,\nmore than one, or none) are critical points. To preserve conciseness, assuming\nthere exists at least one accumulation point (which is often the case), this prop-\nerty may be summarized as: gradient descent converges to critical points (note\nthe plural). See also Section 4.9.\nIn the next section, we explore regularity conditions to help us guarantee suffi-\ncient decrease using simple line-search procedures. The condition we introduce is\ninspired by the Taylor expansion of f along curves generated by the retraction.\n4.4 Regularity conditions and iteration complexity\nIn order to guarantee sufficient decrease as per A4.2, we need to understand how\nf(xk+1) compares to f(xk). Recall that xk+1 = Rxk (sk) with a chosen tangent\nvector sk. The Taylor expansion (4.4) thus states:\nf(xk+1) = f(Rxk (sk)) = f(xk) + \u27e8gradf(xk), sk\u27e9 + O(\u2225sk\u22252).\nIf the quadratic remainder term stays under control during all iterations, we\nmay deduce a guarantee on the progress f(xk) \u2212 f(xk+1). This motivates the\nfollowing assumption on the pullback f \u25e6 R. We provide further context for this\nassumption at the end of the section, as well as much later in Corollary 10.54,\nLemma 10.57 and Exercise 10.58.\nA 4.3. For a given subset S of the tangent bundle TM, there exists a constant\nL >0 such that, for all (x, s) \u2208 S,\nf(Rx(s)) \u2264 f(x) + \u27e8gradf(x), s\u27e9 + L\n2 \u2225s\u22252. (4.8)\nUnder this assumption (on an appropriate set S to be specified), there exists\na range of step-sizes that lead to sufficient decrease.\nProposition 4.8. Let f be a smooth function on a Riemannian manifold M.\nFor a retraction R, let f \u25e6 R satisfy A4.3 on a set S \u2286 TM with constant L. If\nthe pairs (x0, s0), (x1, s1), (x2, s2), . . .generated by Algorithm 4.1 with step-sizes\n\u03b1k \u2208 [\u03b1min, \u03b1max] \u2282 (0, 2/L)\nall lie in S, then the algorithm produces sufficient decrease. Specifically, A4.2\nholds with\nc = min\n\u0012\n\u03b1min \u2212 L\n2 \u03b12\nmin, \u03b1max \u2212 L\n2 \u03b12\nmax\n\u0013\n> 0.\nIn particular, for \u03b1k = 1\nL (constant) we have c = 1\n2L .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2508, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "142e72cc-0e7d-4de0-84cc-6124c4ccc4ed": {"__data__": {"id_": "142e72cc-0e7d-4de0-84cc-6124c4ccc4ed", "embedding": null, "metadata": {"page_label": "60", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36fe691f-205b-4047-9ce0-6ccc5212d21a", "node_type": "4", "metadata": {"page_label": "60", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "bb4894175ead028a096a3ef2933dcb153cd92dbafeae2a314fe0e5ddf7d3b66c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n60 First-order optimization algorithms\nProof. By assumption A4.3 on the pullback, for all k,\nf(xk+1) = f(Rxk (sk)) \u2264 f(xk) + \u27e8gradf(xk), sk\u27e9 + L\n2 \u2225sk\u22252.\nReorganizing and using sk = \u2212\u03b1kgradf(xk) reveals\nf(xk) \u2212 f(xk+1) \u2265\n\u0012\n\u03b1k \u2212 L\n2 \u03b12\nk\n\u0013\n\u2225gradf(xk)\u22252.\nThe coefficient is quadratic in \u03b1k, positive between its roots at 0 and 2 /L. By\nassumption on \u03b1k,\n\u03b1k \u2212 L\n2 \u03b12\nk \u2265 min\n\u0012\n\u03b1min \u2212 L\n2 \u03b12\nmin, \u03b1max \u2212 L\n2 \u03b12\nmax\n\u0013\n> 0,\nwhich concludes the proof.\nAs a particular case, if a valid constant L is known beforehand, then we get\nan explicit algorithm and associated guarantee as a corollary of Propositions 4.7\nand 4.8.\nCorollary 4.9. Let f be a smooth function satisfying A4.1 on a Riemannian\nmanifold M. For a retraction R, let f\u25e6R satisfy A4.3 on a set S \u2286 TM with con-\nstant L. Let (x0, s0), (x1, s1), (x2, s2), . . .be the pairs generated by Algorithm 4.1\nwith constant step-size \u03b1k = 1/L. If all these pairs are in S, then\nlim\nk\u2192\u221e\n\u2225gradf(xk)\u2225 = 0.\nFurthermore, for all K \u2265 1, there exists k in 0, . . . , K\u2212 1 such that\n\u2225gradf(xk)\u2225 \u2264\np\n2L(f(x0) \u2212 flow) 1\u221a\nK\n.\nThe conclusion of the above corollary can also be stated as follows: for all\n\u03b5 >0 there exists k in 0, . . . , K\u2212 1 such that \u2225gradf(xk)\u2225 \u2264\u03b5 provided K \u2265\n2L(f(x0) \u2212 flow) 1\n\u03b52 . Notice that the rate is independent of the dimension of M.\nHow reasonable is A4.3? Let us contemplate it through the Euclidean lens.\nConsider f smooth on a Euclidean spaceE equipped with the canonical retraction\nRx(s) = x + s. If f \u25e6 R satisfies A4.3 on the whole tangent bundle T E = E \u00d7 E,\nthen\n\u2200x, s\u2208 E, f (x + s) \u2264 f(x) + \u27e8gradf(x), s\u27e9 + L\n2 \u2225s\u22252. (4.9)\nThis expresses that the difference between f and its first-order Taylor expansion\nis uniformly upper-bounded by a quadratic. This property holds if the gradient\nof f is Lipschitz continuous with constant L, that is, if\n\u2200x, y\u2208 E, \u2225gradf(y) \u2212 gradf(x)\u2225 \u2264L\u2225y \u2212 x\u2225. (4.10)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2121, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "03d37f0b-e01b-4632-a3f4-b7d65fe406d7": {"__data__": {"id_": "03d37f0b-e01b-4632-a3f4-b7d65fe406d7", "embedding": null, "metadata": {"page_label": "61", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b3938c7b-3acf-4dfd-8488-82cea76a1fb4", "node_type": "4", "metadata": {"page_label": "61", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "ce095c5259a172ca50d15badea6ed3f78fa370e9cf1d61587a6dc61866fa98b1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n4.5 Backtracking line-search 61\nIndeed, with c(t) = x + ts, elementary calculus provides:\nf(x + s) \u2212 f(x) = f(c(1)) \u2212 f(c(0))\n=\nZ 1\n0\n(f \u25e6 c)\u2032(t) dt\n=\nZ 1\n0\nDf(c(t))[c\u2032(t)] dt =\nZ 1\n0\n\u27e8gradf(x + ts), s\u27e9dt.\nThen, under condition (4.10), by Cauchy\u2013Schwarz we have:\n|f(x + s) \u2212 f(x) \u2212 \u27e8gradf(x), s\u27e9| =\n\f\f\f\f\nZ 1\n0\n\u27e8gradf(x + ts) \u2212 gradf(x), s\u27e9dt\n\f\f\f\f\n\u2264\nZ 1\n0\n\u2225gradf(x + ts) \u2212 gradf(x)\u2225\u2225s\u2225dt\n\u2264 \u2225s\u2225\nZ 1\n0\nL\u2225ts\u2225dt\n= L\n2 \u2225s\u22252. (4.11)\nLipschitz continuity of the gradient (4.10) is a common assumption in Euclidean\noptimization, valued for the upper-bounds it provides (4.11). When working on\nmanifolds, generalizing (4.10) requires substantial work due to the comparison of\ngradients at two distinct points (hence of vectors in two distinct tangent spaces)\nbut it can be led to fruition: Sections 10.3, 10.4 and 10.5 provide a detailed\ndiscussion involving a special retraction. On the other hand, generalizing (4.11)\nposes no particular difficulty once a retraction is chosen. This is the reasoning\nthat led to A4.3, which we henceforth call a Lipschitz-type assumption. General\nretractions are covered by Lemma 10.57 under compactness assumptions. In\nparticular, that lemma can be useful to verify regularity assumptions such as A4.3\nwhen the sublevel sets of the cost function are compact.\nDefinition 4.10. A sublevel set of f is a set {x \u2208 M: f(x) \u2264 \u03b1} for some \u03b1.\nExercise 4.11. For the cost function f(x) = 1\n2 x\u22a4Ax on the sphere Sn\u22121 as\na Riemannian submanifold of Rn equipped with the retraction Rx(s) = x+s\n\u2225x+s\u2225,\ndetermine some L such that A4.3 holds over the whole tangent bundle.\n4.5 Backtracking line-search\nThe simplest result in the previous section is Corollary 4.9, which assumes a\nconstant step-size of 1 /L. In practice however, an appropriate constant L is\nseldom known. Even when one is available, it may be large due to particular\nbehavior of f \u25e6R in a limited part of the domain. That seemingly forces us to take\nsmall steps for the whole sequence of iterates, which evidently is not necessary.\nIndeed, only the local behavior of the cost function around xk matters to ensure", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2348, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ae729902-9efd-4aa1-8d38-da6c5c850954": {"__data__": {"id_": "ae729902-9efd-4aa1-8d38-da6c5c850954", "embedding": null, "metadata": {"page_label": "62", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "498cf7b6-cd23-463d-bbee-b2c842c7a2a3", "node_type": "4", "metadata": {"page_label": "62", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "505e0c4142076c1bfe98c0a7fba70fa665eaa017341b628dc87c651963ded0bb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n62 First-order optimization algorithms\nsufficient decrease at iteration k. Thus, we favor inexact line-search algorithms\nthat are adaptive.\nA common adaptive strategy to pick the step-sizes \u03b1k for RGD is called the\nbacktracking line-search: see Algorithm 4.2. For a specified initial step-size \u00af \u03b1,\nthis procedure iteratively reduces the tentative step-size by a factor \u03c4 \u2208 (0, 1)\n(often set to 0.8 or 0.5) until the Armijo\u2013Goldstein condition is satisfied, namely,\nf(x) \u2212 f(Rx(\u2212\u03b1gradf(x))) \u2265 r\u03b1\u2225gradf(x)\u22252, (4.12)\nfor some constant r \u2208 (0, 1) (often set to 10 \u22124).\nAlgorithm 4.2 Backtracking line-search\nParameters: \u03c4, r\u2208 (0, 1); for example, \u03c4 = 1\n2 and r = 10\u22124\nInput: x \u2208 M, \u00af\u03b1 >0\nSet \u03b1 \u2190 \u00af\u03b1\nWhile f(x) \u2212 f(Rx(\u2212\u03b1gradf(x))) < r\u03b1\u2225gradf(x)\u22252\nSet \u03b1 \u2190 \u03c4\u03b1\nOutput: \u03b1\nThe lemma and corollary below show that, under the regularity condition A4.3,\nbacktracking line-search produces sufficient decrease A4.2, with a constant c\nwhich depends on various factors. Importantly, the regularity constant L affects\nthe guarantee but need not be known to run the algorithm.\nLemma 4.12. Let f be a smooth function on a Riemannian manifold M. For\na retraction R, a point x \u2208 Mand an initial step-size \u00af\u03b1 >0, let A4.3 hold for\nf \u25e6 R with constant L on {(x, \u2212\u03b1gradf(x)) : \u03b1 \u2208 [0, \u00af\u03b1]}. Then, Algorithm 4.2\nwith parameters \u03c4, r\u2208 (0, 1) outputs a step-size \u03b1 such that\nf(x) \u2212 f(Rx(\u2212\u03b1gradf(x))) \u2265 r min\n\u0012\n\u00af\u03b1, 2\u03c4(1 \u2212 r)\nL\n\u0013\n\u2225gradf(x)\u22252\nafter computing at most\nmax\n\u0012\n1, 2 + log\u03c4\u22121\n\u0012 \u00af\u03b1L\n2(1 \u2212 r)\n\u0013\u0013\nretractions and cost function evaluations (assuming f(x) and gradf(x) were al-\nready computed).\nProof. Consider gradf(x) \u0338= 0 (otherwise, the claim is clear). For all step-sizes\n\u03b1 considered by Algorithm 4.2, the regularity assumption guarantees\nf(x) \u2212 f(Rx(\u2212\u03b1gradf(x))) \u2265 \u03b1\u2225gradf(x)\u22252 \u2212 L\n2 \u03b12\u2225gradf(x)\u22252.\nOn the other hand, if the algorithm does not terminate for a certain value \u03b1,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2117, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c2424a6c-d09c-4283-a6f4-44c09a174521": {"__data__": {"id_": "c2424a6c-d09c-4283-a6f4-44c09a174521", "embedding": null, "metadata": {"page_label": "63", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7321359f-4efa-4f13-b8e3-b8e7c7312892", "node_type": "4", "metadata": {"page_label": "63", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "2c279ec6270ecbb0c78c593325d4980b32f043d0ee420ad0ca68d6017cc4892a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n4.5 Backtracking line-search 63\nthen\nf(x) \u2212 f(Rx(\u2212\u03b1gradf(x))) < r\u03b1\u2225gradf(x)\u22252.\nIf both are true simultaneously, then\n\u03b1 >2(1 \u2212 r)\nL .\nThus, if \u03b1 drops below this bound, the line-search algorithm terminates. (Of\ncourse, it might also terminate earlier with a longer step-size: we consider the\nworst case.) This happens either because \u00af \u03b1 itself is smaller than 2(1\u2212r)\nL , or as\nthe result of a reduction of \u03b1 by the factor \u03c4. We conclude that the returned \u03b1\nsatisfies:\n\u03b1 \u2265 min\n\u0012\n\u00af\u03b1, 2\u03c4(1 \u2212 r)\nL\n\u0013\n.\nMoreover, the returned \u03b1 is of the form \u03b1 = \u00af\u03b1\u03c4n\u22121 where n is the number of\nretractions and cost function evaluations issued by Algorithm 4.2. Hence,\nn = 1 + log\u03c4\n\u0010\u03b1\n\u00af\u03b1\n\u0011\n= 1 + log\u03c4\u22121\n\u0010 \u00af\u03b1\n\u03b1\n\u0011\n\u2264 1 + max\n\u0012\n0, log\u03c4\u22121\n\u0012 \u00af\u03b1L\n2\u03c4(1 \u2212 r)\n\u0013\u0013\n,\nwhich concludes the proof.\nWhen used in conjunction with RGD, one may want to pick the initial step-\nsize \u00af\u03b1 dynamically as \u00af\u03b1k at iteration k. As long as the initializations \u00af\u03b1k remain\nbounded away from zero, we retain our convergence result.\nCorollary 4.13. Let f be a smooth function satisfying A4.1 on a Riemannian\nmanifold M. For a retraction R, let f \u25e6 R satisfy A4.3 on a set S \u2286 TM with\nconstant L. Let x0, x1, x2, . . .be the iterates generated by RGD (Algorithm 4.1)\nwith backtracking line-search (Algorithm 4.2) using fixed parameters \u03c4, r\u2208 (0, 1)\nand initial step-sizes \u00af\u03b10, \u00af\u03b11, \u00af\u03b12, . . .If for every k the set {(xk, \u2212\u03b1gradf(xk)) :\n\u03b1 \u2208 [0, \u00af\u03b1k]} is in S and if lim infk\u2192\u221e \u00af\u03b1k > 0, then\nlim\nk\u2192\u221e\n\u2225gradf(xk)\u2225 = 0.\nFurthermore, for all K \u2265 1, there exists k in 0, . . . , K\u2212 1 such that\n\u2225gradf(xk)\u2225 \u2264\nvuut f(x0) \u2212 flow\nr min\n\u0010\n\u00af\u03b10, . . . ,\u00af\u03b1K\u22121, 2\u03c4(1\u2212r)\nL\n\u0011 1\u221a\nK\n.\nThe amount of work per iteration is controlled as in Lemma 4.12.\nProof. By Lemma 4.12, backtracking line-search guarantees decrease in the form\nf(xk) \u2212 f(xk+1) \u2265 ck\u2225gradf(xk)\u22252, with ck = r min\n\u0012\n\u00af\u03b1k, 2\u03c4(1 \u2212 r)\nL\n\u0013\n.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2082, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "34f7a3cd-3829-4978-bac5-5ff9d1947e62": {"__data__": {"id_": "34f7a3cd-3829-4978-bac5-5ff9d1947e62", "embedding": null, "metadata": {"page_label": "64", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "58cb4199-06c6-4b17-8e1b-3218710060e8", "node_type": "4", "metadata": {"page_label": "64", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "1ec2edcf6003577eace71e7c71deae866a993eede580ce27f66ee57edb1cccc1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n64 First-order optimization algorithms\nFollowing the same proof as for Proposition 4.7,\nf(x0) \u2212 flow \u2265\nK\u22121X\nk=0\nf(xk) \u2212 f(xk+1) \u2265\nK\u22121X\nk=0\nck\u2225gradf(xk)\u22252\n\u2265 K \u00b7 min\nk=0,...,K\u22121\nck \u00b7 min\nk=0,...,K\u22121\n\u2225gradf(xk)\u22252.\nThis establishes the first claim. For the limit statement, observe that taking\nK \u2192 \u221eon the first line above shows that\nlim\nk\u2192\u221e\nck\u2225gradf(xk)\u22252 = 0.\nSince lim infk\u2192\u221e ck > 0, we deduce that lim k\u2192\u221e \u2225gradf(xk)\u22252 = 0.\nAs a remark, consider replacing the cost function f(x) by a shifted and posi-\ntively scaled version of itself, say g(x) = 8f(x) + 3. Arguably, the optimization\nproblem did not change, and we might expect a reasonable optimization algo-\nrithm initialized at x0 to produce the same iterates to minimize f or to minimize\ng. It is easily checked that the combination of Algorithms 4.1 and 4.2 has this\ninvariance property, provided the initial step-sizes \u00af\u03b1k are chosen in such a way\nthat the first step considered, namely, \u2212\u00af\u03b1kgradf(xk) is invariant under positive\nscaling of f. For the first iteration, this can be done for example by setting\n\u00af\u03b10 = \u21130\n\u2225gradf(x0)\u2225\nwith some constant \u21130, which is then the length of the first retracted step: it\ncan be set relative to the scale of the search space or to the expected distance\nbetween x0 and a solution (this does not need to be precise). For subsequent\niterations, a useful heuristic is (see [NW06, \u00a7 3.5, eq. (3.60)] for more)\n\u00af\u03b1k = 2f(xk\u22121) \u2212 f(xk)\n\u2225gradf(xk)\u22252 , (4.13)\nwhich also yields the desired invariance. It is common to initialize with a slightly\nlarger value, say, by a factor of 1 /\u03c4. One may also set \u00af\u03b1k to be the maximum\nbetween the above value and a small reference value, to ensure the first step-size\nremains bounded away from zero (as required by our convergence theory).\n4.6 Local convergence*\nIn numerical analysis, we study the behavior of sequences generated by itera-\ntive algorithms. We distinguish between the local and global behavior of those\nsequences. We say that a method enjoys global convergence if it generates se-\nquences that converge regardless of their initialization x0. It is worth noting two\ncommon points of confusion here:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2395, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9b34d375-d9f8-4fa8-a311-d05bc2885d0b": {"__data__": {"id_": "9b34d375-d9f8-4fa8-a311-d05bc2885d0b", "embedding": null, "metadata": {"page_label": "65", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63d0467a-f106-43bf-b1b8-da5ead3d9d2b", "node_type": "4", "metadata": {"page_label": "65", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "f9a01ac9bb5838568f3f73e9685f35ffaebcb54cb62ddafc13f585790d393550", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n4.6 Local convergence* 65\n1. \u201cGlobal\u201d convergence is not convergence to a global optimizer. It is merely a\nstatement that sequences converge \u201csomewhere\u201d for all x0.\n2. It is common to say that RGD enjoys global convergence, and indeed the\nresults presented in previous sections assume little about the initial point\nx0. However, we have not established convergence of RGD. Rather, we have\nshown that, under some assumptions, all accumulation points of RGD (if any)\nare critical points.\nStill, RGD usually converges to a critical point in practice, hence the habit of\ncalling it globally convergent. Our results in previous sections also qualify how\nfast the gradient norm converges to zero in the worst case. That rate, however,\nis underwhelming: it only guarantees a decrease as fast as 1 /\n\u221a\nk where k is the\niteration counter. While this result is correct (there exist difficult cost functions\neven on Rn that lead to such poor performance), it is common to observe an\neventually exponential decrease of the gradient norm. This asymptotic behavior\nof a sequence is the realm of local convergence: the study of how convergent\nsequences behave once they are close enough to their limit.\nThe discussions in this section require tools that we have not introduced yet.\nSpecifically, we use:\n\u2022 The Riemannian distance dist, which turns a connected Riemannian manifold\ninto a metric space. See Section 10.1.\n\u2022 The exponential map Exp, which is a special retraction. Of relevance here, it\nhas the property that for v \u2208 TxM small enough and y = Expx(v) we have\ndist(x, y) = \u2225v\u2225x. See Section 10.2.\n\u2022 The Riemannian Hessian Hessf of a smooth function f : M \u2192R, which\nis a kind of derivative of the gradient vector field. Of relevance here, (a)\nHessf(x) is a self-adjoint linear map on T xM (hence it has real eigenval-\nues), and (b) if grad f(x) = 0 and Hess f(x) \u227b 0, then x is a strict local\nminimizer of f. See Sections 5.5 and 6.1.\nLocal convergence rates are defined in general for sequences in metric spaces.\nThis applies to sequences of real numbers (tracking f(xk) or \u2225gradf(xk)\u2225) using\nthe absolute value distance on R, and it also applies to sequences of points on\nM using the Riemannian distance.\nDefinition 4.14. In a metric space with a distance dist, a sequence a0, a1, a2, . . .\nconverges at least linearlyto a\u22c6 if there exist positive reals\u03f50, \u03f51, \u03f52, . . .converging\nto zero such that dist(ak, a\u22c6) \u2264 \u03f5k and limk\u2192\u221e\n\u03f5k+1\n\u03f5k\n= \u00b5 for some \u00b5 \u2208 (0, 1).\nThe infimum over such \u00b5 is the linear convergence factor. If the latter is zero,\nthe convergence is superlinear.\nThe above is also called R-linear convergence, as opposed to the more restric-\ntive notion of Q-linear convergence which forces \u03f5k = dist(ak, a\u22c6).\nSuperlinear convergence rates can be further qualified. Anticipating the needs\nof Chapter 6, we already define quadratic convergence. It is straightforward to", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3133, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "901b2e12-f44d-4619-8661-19ac01f05098": {"__data__": {"id_": "901b2e12-f44d-4619-8661-19ac01f05098", "embedding": null, "metadata": {"page_label": "66", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f4451f81-d9da-4015-8cfa-79ddcaf0c12a", "node_type": "4", "metadata": {"page_label": "66", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "f25f75bf8c2a22d85c065698979e7c408e965b23f76fb9bd458a6dd7352daae6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n66 First-order optimization algorithms\ncheck that quadratic convergence implies superlinear convergence which itself\nimplies linear convergence.\nDefinition 4.15. In a metric space equipped with a distance dist, a sequence\na0, a1, a2, . . .converges at least quadratically to a\u22c6 if there exists a sequence\nof positive reals \u03f50, \u03f51, \u03f52, . . .converging to zero such that dist(ak, a\u22c6) \u2264 \u03f5k and\nlimk\u2192\u221e\n\u03f5k+1\n\u03f52\nk\n= \u00b5 for some finite \u00b5 \u2265 0.\nIn the remainder of this section, we build up toward a local convergence result\nfor RGD with constant step-size. To this end, we first secure a broader statement\ncalled the local contraction mapping theorem: it will serve us again to study the\nRiemannian Newton method in Section 6.2.\nAn important tool below is the observation that retractions provide local pa-\nrameterizations of manifolds. More explicitly, Rx provides a diffeomorphism (re-\ncall Definition 3.11) between a neighborhood of the origin in T xM (a linear\nspace) and a neighborhood of x in M. This fact is a direct consequence of a gen-\neralization of the inverse function theorem, stated now with a couple of relevant\ncorollaries. Note: in the theorem below, M and N could also be open subsets of\nmanifolds, as such sets are manifolds too.\nTheorem 4.16 (Inverse function theorem on manifolds) . Let F : M \u2192 Nbe a\nsmooth map between two manifolds. If DF(x) is invertible at some point x \u2208 M,\nthen there exist neighborhoods U \u2286 Mof x and V \u2286 Nof F(x) such that\nF|U : U \u2192 V is a diffeomorphism.\nProof sketch. The idea is to reduce the claim to Theorem 3.13. This is best done\nthrough charts. For submanifolds, these charts can be built via Theorem 3.12,\nas in Section 8.3. Details in [Lee12, Thm. 4.5].\nCorollary 4.17. Let R be a retraction on a manifold M. For each x, there\nexists a neighborhood U of the origin of TxM such that Rx|U : U \u2192 Uis a\ndiffeomorphism, where U = Rx(U) is a neighborhood of x on M.\nProof. The map R x : TxM \u2192 Msatisfies the assumptions of Theorem 4.16\naround the origin of T xM since DRx(0) is invertible.\nCorollary 4.18. Continuing from Corollary 4.17, if M is Riemannian then we\ncan choose U as an open ball of some radius r >0 around the origin in TxM,\nthat is, U = B(x, r) = {v \u2208 TxM : \u2225v\u2225x < r}.\nSome simple algorithms come down to the repeated application of a smooth\niteration map F : M \u2192 M, so that x1 = F(x0), x2 = F(x1) = F(F(x0)),\netc. The following theorem provides insight into the local convergence of such\nalgorithms near special points.\nTheorem 4.19 (Local contraction mapping). Let F : M \u2192 Mbe a smooth map\nto and from a Riemannian manifold M. Given x0 \u2208 M, consider the sequence", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2879, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d0826cd7-cf62-45d1-93fe-68d78d5a7fba": {"__data__": {"id_": "d0826cd7-cf62-45d1-93fe-68d78d5a7fba", "embedding": null, "metadata": {"page_label": "67", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1009a73d-8a24-41b2-8483-dbdb402b1fd6", "node_type": "4", "metadata": {"page_label": "67", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "f4c756d4c19248b915ac7f47a5abd87022fb2942360105b5958cdb3edf9cd128", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n4.6 Local convergence* 67\ndefined by\nxk+1 = F(xk) for k = 0, 1, 2, . . .\nIf x\u22c6 \u2208 Mis a fixed point (i.e., F(x\u22c6) = x\u22c6) and \u2225DF(x\u22c6)\u2225 < 1 (i.e., all singular\nvalues of DF(x\u22c6) are strictly smaller than one), then there exists a neighborhood\nU of x\u22c6 such that, if the sequence enters U, it stays in U and converges to x\u22c6 at\nleast linearly with a linear convergence factor which is at most \u2225DF(x\u22c6)\u2225.\nExplicitly, with R an arbitrary retraction on M we have\nlim\nk\u2192\u221e\n\u2225\u03bek+1\u2225\n\u2225\u03bek\u2225 \u2264 \u2225DF(x\u22c6)\u2225,\nwhere \u03bek is well defined by Rx\u22c6(\u03bek) = xk for k large enough. The conclusion\nfollows by letting R = Exp, in which case \u2225\u03bek\u2225 = dist(xk, x\u22c6) for large k.\nAdditionally,\n1. If \u2225DF(x\u22c6)\u2225 = 0, the convergence is at least quadratic.\n2. All claims still hold if F is only smoothly defined in a neighborhood of x\u22c6.\nProof. Owing to Corollary 4.18, we can choose a radius r >0 such that Rx\u22c6 is a\ndiffeomorphism from B(x\u22c6, r) to U = Rx\u22c6(B(x\u22c6, r)): we now tacitly restrict Rx\u22c6\nto those domains. Consider the set F\u22121(U) (where it is understood that we first\nrestrict F to the neighborhood of x\u22c6 where it is smooth, if need be): it is open\n(because F is continuous and U is open) and it contains x\u22c6 (because F(x\u22c6) = x\u22c6\nand x\u22c6 \u2208 U). Thus, F\u22121(U) is a neighborhood of x\u22c6. It follows that we can select\nr\u2032 \u2208 (0, r] such that\nU\u2032 \u225c Rx\u22c6(B(x\u22c6, r\u2032)) \u2286 F\u22121(U) and U\u2032 \u2286 U.\nAssume xk is in U\u2032. Then, xk+1 = F(xk) is in F(U\u2032), which is included in\nF(F\u22121(U)), that is, xk+1 \u2208 U. Thus, the vectors \u03bek, \u03bek+1 \u2208 Tx\u22c6M are well\ndefined by\nxk = Rx\u22c6(\u03bek), x k+1 = Rx\u22c6(\u03bek+1).\nConsider the following map restricted to open sets in T x\u22c6M:\n\u02dcF : B(x\u22c6, r\u2032) \u2192 B(x\u22c6, r), \u02dcF = R\u22121\nx\u22c6 \u25e6 F \u25e6 Rx\u22c6.\nIt is defined such that \u03bek+1 = \u02dcF(\u03bek). Since \u02dcF is smooth, we can use a standard\nTaylor expansion on Tx\u22c6M (a Euclidean space) to claim that\n\u02dcF(v) = \u02dcF(0) + D\u02dcF(0)[v] + E(v)\nwhere \u2225E(v)\u2225 \u2264c\u2225v\u22252 for some constant c, valid for all v \u2208 B(x\u22c6, r\u2032). Notice\nthat \u02dcF(0) = 0. Moreover, D \u02dcF(0) = DF(x\u22c6): that is due to the chain rule and the\nfact that DRx\u22c6(0) is the identity on Tx\u22c6M so that DR\u22121\nx\u22c6 (x\u22c6) is also the identity.\nIt follows that\n\u03bek+1 = \u02dcF(\u03bek) = DF(x\u22c6)[\u03bek] + E(\u03bek).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2355, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3f95405b-f342-4847-99fb-cb393a46d758": {"__data__": {"id_": "3f95405b-f342-4847-99fb-cb393a46d758", "embedding": null, "metadata": {"page_label": "68", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7ee1ca01-f1d4-470c-8c43-bb37ec4ba637", "node_type": "4", "metadata": {"page_label": "68", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "a4484b70049fab1a8b9665d24704dc0bb9f3b7f065133fcf80accacdd70ee436", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n68 First-order optimization algorithms\nTaking norms on both sides, we find\n\u2225\u03bek+1\u2225 \u2264 \u2225DF(x\u22c6)[\u03bek]\u2225 + \u2225E(\u03bek)\u2225 \u2264(\u2225DF(x\u22c6)\u2225 + c\u2225\u03bek\u2225) \u2225\u03bek\u2225. (4.14)\nRecall that \u2225\u03bek\u2225 < r\u2032. If need be, replace r\u2032 by a smaller positive constant\nsuch that \u2225DF(x\u22c6)\u2225 + cr\u2032 < 1: this is possible owing to our assumption that\n\u2225DF(x\u22c6)\u2225 < 1. Doing so, we ensure \u2225\u03bek+1\u2225 \u2264 \u2225\u03bek\u2225. In particular, xk \u2208 U\u2032 =\u21d2\nxk+1 \u2208 U\u2032. By induction, we can now define \u03beK through xK = Rx\u22c6(\u03beK) \u2208 U\u2032 for\nall K \u2265 k. Moreover, we have that \u2225\u03beK\u2225 converges to zero at least linearly. The\nlinear convergence factor is controlled by:\nlim\nK\u2192\u221e\n\u2225\u03beK+1\u2225\n\u2225\u03beK\u2225 \u2264 lim\nK\u2192\u221e\n\u2225DF(x\u22c6)\u2225 + c\u2225\u03beK\u2225 = \u2225DF(x\u22c6)\u2225.\nNow that convergence is established, we can return to (4.14) and notice that, if\nDF(x\u22c6) = 0, then we also have\nlim\nK\u2192\u221e\n\u2225\u03beK+1\u2225\n\u2225\u03beK\u22252 \u2264 c.\nThus, in that case, the sequence converges at least quadratically.\nWe now apply the above theorem to RGD with constant step-size, as this in-\ndeed corresponds to the iterative application of a smooth map. More realistically,\nwe would use a backtracking line-search procedure. However, that leads to an\niteration map that may lack smoothness as the selected step-size may depend\non x discontinuously. With different tools, it is still possible to establish linear\nconvergence of RGD with a line-search, see [AMS08, Thm. 4.5.6]. It is important\nto note that the retraction used in RGD is unrelated to the retraction used in\nthe proof of the local contraction mapping theorem\u2014For the latter, it makes the\nmost sense to use the exponential retraction.\nTheorem 4.20. Let M be a Riemannian manifold with a retraction R. Let\nf : M \u2192R be a smooth function. Assume x\u22c6 \u2208 Msatisfies\ngradf(x\u22c6) = 0 and Hessf(x\u22c6) \u227b 0.\nLet 0 < \u03bbmin \u2264 \u03bbmax be the smallest and largest eigenvalues of Hessf(x\u22c6), and\nlet \u03ba = \u03bbmax\n\u03bbmin\ndenote the condition number of Hessf(x\u22c6). Set L >1\n2 \u03bbmax. Given\nx0 \u2208 M, constant step-size Riemannian gradient descent iterates\nxk+1 = F(xk), with F(x) = Rx\n\u0012\n\u2212 1\nLgradf(x)\n\u0013\n.\nThere exists a neighborhood of x\u22c6 such that, if the above sequence enters the\nneighborhood, then it stays in that neighborhood and it converges to x\u22c6 at least\nlinearly. If L = \u03bbmax, the linear convergence factor is at most 1 \u2212 1/\u03ba.\nProof. Let us check the assumptions of Theorem 4.19. First, it is clear that", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2509, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c2c0d875-a1d7-459a-b5b7-e71f5e71e5cc": {"__data__": {"id_": "c2c0d875-a1d7-459a-b5b7-e71f5e71e5cc", "embedding": null, "metadata": {"page_label": "69", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "93c62cb1-d685-4125-adf6-d5fc2dd88ba5", "node_type": "4", "metadata": {"page_label": "69", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "b813ab9052fe0168e19fedecafe6af5c780a81d0a24c68928190e657f7c9283f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n4.6 Local convergence* 69\nF(x\u22c6) = x\u22c6. Second, let us investigate D F(x\u22c6): T x\u22c6M \u2192Tx\u22c6M. In particular,\nwe need to differentiate through R: T M \u2192 M:\nDF(x)[v] = D(x 7\u2192 R(x, G(x)))(x)[v] = DR(x, G(x))[(v, DG(x)[v])] ,\nwith G(x) = \u2212 1\nL gradf(x). This simplifies at x = x\u22c6 since G(x\u22c6) = 0. Indeed,\nLemma 4.21 below justifies the following:\nDF(x\u22c6)[v] = DR(x\u22c6, 0)[(v, DG(x\u22c6)[v])] = v + DG(x\u22c6)[v].\nAnticipating concepts from Chapter 5, the property G(x\u22c6) = 0 also implies via\nProposition 5.3 that\nDG(x\u22c6)[v] = \u2207vG = \u2212 1\nLHessf(x\u22c6)[v] (4.15)\nfor all v \u2208 Tx\u22c6M, where \u2207 is the Riemannian connection on M. Thus,\nDF(x\u22c6) = Id \u2212 1\nLHessf(x\u22c6), (4.16)\nwhere Id is the identity map on T x\u22c6M. Since both Id and Hess f(x\u22c6) are self-\nadjoint (Proposition 5.15), we find that D F(x\u22c6) is self-adjoint. Its eigenvalues\n(all real) are given by\n1 \u2212 \u03bb1\nL \u2264 \u00b7\u00b7 \u00b7 \u22641 \u2212 \u03bbn\nL ,\nwhere \u03bb1 \u2265 \u00b7 \u00b7 \u00b7 \u2265\u03bbn > 0 are the eigenvalues of Hess f(x\u22c6). It follows that the\noperator norm of D F(x\u22c6) is\n\u2225DF(x\u22c6)\u2225 = max\n\u0012\f\f\f\f1 \u2212 \u03bb1\nL\n\f\f\f\f,\n\f\f\f\f1 \u2212 \u03bbn\nL\n\f\f\f\f\n\u0013\n. (4.17)\nUnder our assumption on L, it is easy to check that \u2225DF(x\u22c6)\u2225 < 1. All conclu-\nsions now follow from Theorem 4.19.\nLemma 4.21. For each point x on a manifold M, we have\nT(x,0)TM = TxM \u00d7TxM. (4.18)\nLet R: T M \u2192 Mbe a retraction on M. Given x \u2208 M, the differential of R\nat (x, 0) is a linear map DR(x, 0): T (x,0)TM \u2192TR(x,0)M. Equivalently, it is a\nlinear map DR(x, 0): T xM \u00d7TxM \u2192TxM. For all u, v\u2208 TxM, it holds\nDR(x, 0)[(u, v)] = u + v. (4.19)\nProof. To verify (4.18), note the following:\n1. For each u \u2208 TxM, we can pick a smooth curvec on M such that c(0) = x and\nc\u2032(0) = u; then, \u03b3(t) = (c(t), 0) is a smooth curve on T M and \u03b3\u2032(0) = (u, 0)\nis tangent to TM at \u03b3(0) = (x, 0).\n2. For each v \u2208 TxM, the curve \u03b3(t) = ( x, tv) is smooth on T M and \u03b3\u2032(0) =\n(0, v) is tangent to T M at \u03b3(0) = (x, 0).\n3. By linearity, T (x,0)TM contains all pairs (u, v) \u2208 TxM \u00d7TxM.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2138, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b54039b2-32d9-4b7f-9999-13d24acef723": {"__data__": {"id_": "b54039b2-32d9-4b7f-9999-13d24acef723", "embedding": null, "metadata": {"page_label": "70", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d11a3b4e-4006-4167-a098-ae288a1e5324", "node_type": "4", "metadata": {"page_label": "70", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "4ed372e602a5fbd54ddd65ffe730ef13f5cb1eed053d96b750258ba7bd032720", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n70 First-order optimization algorithms\n4. The two sets are in fact the same since they are linear spaces of the same\ndimension.\nLet us establish (4.19). By linearity of differentials, we have\nDR(x, 0)[(u, v)] = DR(x, 0)[(u, 0)] + DR(x, 0)[(0, v)].\nFor the first term, pick a smooth curve c on M such that c(0) = x and c\u2032(0) = u.\nThen,\nDR(x, 0)[(u, 0)] = d\ndtR(c(t), 0)\n\f\f\f\f\nt=0\n= d\ndtc(t)\n\f\f\f\f\nt=0\n= c\u2032(0) = u.\nFor the second term, we have\nDR(x, 0)[(0, v)] = d\ndtR(x, tv)\n\f\f\f\f\nt=0\n= d\ndtRx(tv)\n\f\f\f\f\nt=0\n= v.\nThis concludes the proof. We used both defining properties of R.\nThe following lemma connects the regularity assumption A4.3 from Section 4.4\nto the Hessian of f at x\u22c6 as needed in Theorem 4.20.\nLemma 4.22. Let R be a retraction on a Riemannian manifold M. If A4.3\nholds for f : M \u2192R with constant L on TM and x\u22c6 \u2208 Mis critical, then\n\u2200v \u2208 Tx\u22c6M, \u27e8v, Hessf(x\u22c6)[v]\u27e9x\u22c6 \u2264 L\u2225v\u22252\nx\u22c6.\nIn particular, L is valid for Theorem 4.20 since L \u2265 \u03bbmax(Hessf(x\u22c6)).\nProof. We only need A4.3 for all pairs ( x\u22c6, v) \u2208 TM; that provides:\nf(Rx\u22c6(tv)) \u2264 f(x\u22c6) + Lt2\n2 \u2225v\u22252\nx\u22c6.\nAnticipating results from Chapter 5, we deduce from (5.28) that\nf(Rx\u22c6(tv)) = f(x\u22c6) + t2\n2 \u27e8v, Hessf(x\u22c6)[v]\u27e9x\u22c6 + O(t3).\nThe two combine to yield \u27e8v, Hessf(x\u22c6)[v]\u27e9x\u22c6 \u2264 L\u2225v\u22252\nx\u22c6 + O(t). Take t \u2192 0 to\nconclude.\nTheorem 4.20 above provides circumstances for iteratesxk of RGD to converge\nto a local minimizer x\u22c6 at least linearly. We remark in closing that, using tools\nfrom Section 10.4, it is easily argued that the gradient norm \u2225gradf(xk)\u2225 also\nconverges to zero, and likewise the cost function value f(xk) converges to f(x\u22c6),\nboth at least linearly. Specifically, this is done with Corollaries 10.48 and 10.54\nby arguing that the gradient of f is Lipschitz continuous in a neighborhood of\nx\u22c6.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2010, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb5ecb37-aed6-48d9-afd2-918ceb6ee97f": {"__data__": {"id_": "bb5ecb37-aed6-48d9-afd2-918ceb6ee97f", "embedding": null, "metadata": {"page_label": "71", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f6091ce1-e72f-4bd1-a09d-4707988d049d", "node_type": "4", "metadata": {"page_label": "71", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "2798953ad4cf2396df001a2e4e355042e26ddbad10232338b80eba8fa43ffa05", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n4.7 Computing gradients* 71\n4.7 Computing gradients*\nThis section provides some guidance on how to obtain an expression for the\ngradient of a function. It can be skipped safely. The reader may find it helpful\nto return to this section when working on particular applications.\nThe gradient of a function f : M \u2192R on a Riemannian manifold (recall\nDefinition 3.58 or 8.57) is defined in full generality as the unique vector field\ngradf on M such that, for all points x \u2208 Mand all tangent vectors v \u2208 TxM,\nDf(x)[v] = \u27e8gradf(x), v\u27e9x , (4.20)\nwhere \u27e8\u00b7, \u00b7\u27e9x is the inner product on T xM (the Riemannian metric at x). This\nsuggests a general strategy to obtain a formula for grad f(x):\n1. Determine an expression for the directional derivative D f(x)[v], and\n2. Re-arrange it until it is of the form \u27e8g, v\u27e9x, with some g \u2208 TxM.\nAt this point, we get the gradient by identification: grad f(x) = g. This requires\nessentially two steps: first, to write out Df(x)[v] somewhat explicitly as an inner\nproduct between two quantities; second, to use the notion of adjoint of a linear\nmap (recall Definition 3.4) to isolate v.\nIn working out directional derivatives, three rules get most of the work done\n(it is an exercise to verify them):\n1. The chain rule: as for (3.29), let F : M \u2192 M\u2032 and G: M\u2032 \u2192 M\u2032\u2032 be smooth\nmaps between manifolds M, M\u2032, M\u2032\u2032. The composition H = G \u25e6 F defined\nby H(x) = G(F(x)) is smooth with differential:\nDH(x)[v] = DG(F(x))[DF(x)[v]]. (4.21)\n2. The product rule: let F, Gbe two smooth maps from a manifold M to matrix\nspaces such that F(x) and G(x) can be matrix-multiplied to form the product\nmap H = F Gdefined by H(x) = F(x)G(x). For example,F maps M to Rn\u00d7k\nand G maps M to Rk\u00d7d. Then, H is smooth with differential:\nDH(x)[v] = DF(x)[v]G(x) + F(x)DG(x)[v]. (4.22)\nThis rule holds for any type of product. For example, with the entrywise\nproduct H(x) = F(x) \u2299 G(x), we have\nDH(x)[v] = DF(x)[v] \u2299 G(x) + F(x) \u2299 DG(x)[v]. (4.23)\nLikewise, with the Kronecker product H(x) = F(x) \u2297 G(x),\nDH(x)[v] = DF(x)[v] \u2297 G(x) + F(x) \u2297 DG(x)[v]. (4.24)\n3. Inner product rule: let F, G: M \u2192 Ebe two smooth maps from a manifold\nM to a linear space E equipped with an inner product \u27e8\u00b7, \u00b7\u27e9. Then, the scalar\nfunction h(x) = \u27e8F(x), G(x)\u27e9 is smooth with differential:\nDh(x)[v] = \u27e8DF(x)[v], G(x)\u27e9 + \u27e8F(x), DG(x)[v]\u27e9. (4.25)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2572, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "42d9b396-1186-4859-b8bd-db60c25e78c6": {"__data__": {"id_": "42d9b396-1186-4859-b8bd-db60c25e78c6", "embedding": null, "metadata": {"page_label": "72", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2a50a332-8620-4e66-8502-287080f4d095", "node_type": "4", "metadata": {"page_label": "72", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "52a644799077fd1aaa3137355d3b7ddfcb46988f79c84b8d0f45908448c2e66e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n72 First-order optimization algorithms\n(To differentiate inner products of two smooth vector fields on a manifold, we\nneed more tools: see Sections 5.4 and 5.7, specifically Theorems 5.6 and 5.29.)\nThen, to see how the notion of adjoint comes up, consider the common example\nof a cost function f : M \u2192R with\nf(x) = \u2225F(x)\u22252\nE = \u27e8F(x), F(x)\u27e9E , (4.26)\nwhere F : M \u2192 Eis a map from a Riemannian manifold to a Euclidean space\n(e.g., a matrix space) endowed with an inner product \u27e8\u00b7, \u00b7\u27e9E and associated norm\n\u2225 \u00b7 \u2225E. From (4.25), we know that\nDf(x)[v] = \u27e8DF(x)[v], F(x)\u27e9E + \u27e8F(x), DF(x)[v]\u27e9E = 2 \u27e8F(x), DF(x)[v]\u27e9E .\nThe linear map D F(x): T xM \u2192 Ehas an adjoint with respect to the inner\nproducts on T xM and E; we denote it by D F(x)\u2217 : E \u2192TxM. It follows by\ndefinition that\nDf(x)[v] = 2 \u27e8DF(x)\u2217[F(x)], v\u27e9x .\nThis holds for all v \u2208 TxM, thus by identification with (4.20) we find:\ngradf(x) = 2 DF(x)\u2217[F(x)]. (4.27)\nThis highlights the importance of computing adjoints of linear maps in obtaining\ngradients. Formulas (3.15) and (3.18) are particularly helpful in this respect. We\nfurther illustrate the computation of adjoints in examples below.\nIn many cases, it is sufficient to work out the gradient of a function defined\non a Euclidean space, then to use a rule to convert it to a Riemannian gradient.\nFor example, Proposition 3.61 shows how to obtain the Riemannian gradient of\na function f defined on a Riemannian submanifold of a Euclidean space E by\northogonal projection to tangent spaces. Thus, below we focus on the Euclidean\ncase.\nExample 4.23. Consider F : Rn\u00d7n \u2192 Rn\u00d7n defined by F(X) = Xk for some\npositive integer k. Using the product rule repeatedly, it is easy to see that\nDF(X)[U] = UX k\u22121 + XUX k\u22122 + X2UX k\u22123 + \u00b7 \u00b7\u00b7+ Xk\u22122UX + Xk\u22121U\n=\nkX\n\u2113=1\nX\u2113\u22121UX k\u2212\u2113. (4.28)\nEquipping Rn\u00d7n with the usual trace inner product, we find that the adjoint is\nsimply DF(X)\u2217 = DF(X\u22a4). Indeed, for all U, V\u2208 Rn\u00d7n, using (3.15),\n\u27e8DF(X)[U], V\u27e9 =\nkX\n\u2113=1\n\nX\u2113\u22121UX k\u2212\u2113, V\n\u000b\n=\nkX\n\u2113=1\n\nU, (X\u22a4)\u2113\u22121V (X\u22a4)k\u2212\u2113\u000b\n=\n\nU, DF(X\u22a4)[V ]\n\u000b\n. (4.29)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2275, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "90cf82b2-ed30-41b6-acf2-eefa40cdf3fe": {"__data__": {"id_": "90cf82b2-ed30-41b6-acf2-eefa40cdf3fe", "embedding": null, "metadata": {"page_label": "73", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a8d3e2bb-4d57-46c9-95cd-5faa902912b4", "node_type": "4", "metadata": {"page_label": "73", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "42eccd0342390b7ea41705e9319d08ae73aa3bf90064024b703e6707fbb0ad60", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n4.7 Computing gradients* 73\nSimilarly, for F(X) = Xk defined on Cn\u00d7n equipped with the usual inner prod-\nuct (3.17), the expression for DF(X) is unchanged, and DF(X)\u2217 = DF(X\u2217).\nExample 4.24. Consider F(X) = X\u22121 defined on the (open) set of invertible\nmatrices (real or complex). One can show that F is smooth on that domain. By\ndefinition,\nF(X)X = I\nfor all X in the domain of F. Differentiating that identity at X along U on both\nsides, the product rule yields\nDF(X)[U]X + F(X)U = 0.\nHence, we get the following useful expression:\nDF(X)[U] = \u2212X\u22121UX \u22121. (4.30)\nEquipping Rn\u00d7n or Cn\u00d7n with its usual inner product, the adjoint DF(X)\u2217 is\nDF(X\u22a4) or DF(X\u2217), as in the previous example.\nExample 4.25. Consider a differentiable scalar function g : R \u2192 R, and let\n\u02dcg : Rn\u00d7m \u2192 Rn\u00d7m denote its entrywise application to matrices so that \u02dcg(X)ij =\ng(Xij). Then, with g\u2032 the derivative of g,\n\u2200i, j, (D\u02dcg(X)[U])ij = Dg(Xij)[Uij] = g\u2032(Xij)Uij.\nLetting \u02dcg\u2032 : Rn\u00d7m \u2192 Rn\u00d7m denote the entrywise application of g\u2032 to matrices,\nwe can summarize this as\nD\u02dcg(X)[U] = \u02dcg\u2032(X) \u2299 U. (4.31)\nThis differential is self-adjoint with respect to the usual inner product, that is,\nD\u02dcg(X)\u2217 = D\u02dcg(X), since for all U, V\u2208 Rn\u00d7m, using (3.15), we have\n\u27e8D\u02dcg(X)[U], V\u27e9 = \u27e8\u02dcg\u2032(X) \u2299 U, V\u27e9 = \u27e8U, \u02dcg\u2032(X) \u2299 V \u27e9 = \u27e8U, D\u02dcg(X)[V ]\u27e9.\nThere does not always exist a complex equivalent because for g : C \u2192 C, even\nif Dg(x)[u] is well defined, there may not exist a function g\u2032 : C \u2192 C such that\nDg(x)[u] = g\u2032(x)u, i.e., g is not necessarily complex differentiable. Fortunately,\nthis is not an obstacle to computing directional derivatives: it merely means there\nmay not exist as simple an expression as above.\nExample 4.26. Consider a function g from R to R or from C to C with a\nconvergent Taylor series, that is,\ng(x) =\n\u221eX\nk=0\nakxk\nfor some coefficients a0, a1, . . ., with x possibly restricted to a particular domain.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2127, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3462da16-cc57-4141-9129-9dbaff173a92": {"__data__": {"id_": "3462da16-cc57-4141-9129-9dbaff173a92", "embedding": null, "metadata": {"page_label": "74", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9894277f-383d-49ae-a636-5262a65b1d47", "node_type": "4", "metadata": {"page_label": "74", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "1a9a5ce5143e3970d095806f9b93c55511a6470f0bbc959be1a7635c1f8581c8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n74 First-order optimization algorithms\nSuch functions can be extended to matrix functions, that is, to functions from\nRn\u00d7n to Rn\u00d7n or from Cn\u00d7n to Cn\u00d7n, simply by defining\nG(X) =\n\u221eX\nk=0\nakXk. (4.32)\nWe can gain insight into this definition by considering the ubiquitous special\ncase where X is diagonalizable, that is, X = V DV\u22121 for some diagonal matrix\nD = diag(\u03bb1, . . . , \u03bbn) containing the eigenvalues of X and some invertible matrix\nV containing its eigenvectors. Indeed, in this case,\nG(X) =\n\u221eX\nk=0\nak(V DV\u22121)k\n= V\n \u221eX\nk=0\nakDk\n!\nV \u22121 = V diag(g(\u03bb1), . . . , g(\u03bbn))V \u22121.\nThus, G is well defined at X provided the eigenvalues of X belong to the domain\nof definition of g. In this case, the matrix function G transforms the eigenvalues\nthrough g.\nImportant examples include the matrix exponential, matrix logarithm and ma-\ntrix square root functions. In Matlab, these are available as expm, logm and\nsqrtm, respectively. In Manopt, their differentials are available as dexpm, dlogm\nand dsqrtm.\nThis view of matrix functions is sufficient for our discussion but it has its\nlimitations. In particular, the Taylor series expansion does not make it imme-\ndiately clear why the matrix logarithm and matrix square root can be defined\nfor all matrices whose real eigenvalues (if any) are positive. For a more formal\ndiscussion of matrix functions\u2014including definitions that allow us to go beyond\nTaylor series and diagonalizable matrices\u2014as well as details regarding domains\nof definition and numerical computation, see [Hig08]. Generalized matrix func-\ntions (which apply to non-square matrices) and their differentials are discussed\nin [Nof17].\nProvided one can compute the matrix function, a theorem by Mathias offers a\nconvenient way to compute its directional derivatives (also called G\u02c6 ateauxand,\nunder stronger conditions, Fr\u00b4 echet derivative) [Mat96], [Hig08, \u00a7 3.2, Thm. 3.6,\n3.8, eq. (3.16)]: if g is 2n \u2212 1 times continuously differentiable on some open\ndomain in R or C and the eigenvalues of X belong to this domain, then,\nG\n\u0012\u0014X U\n0 X\n\u0015\u0013\n=\n\u0014G(X) D G(X)[U]\n0 G(X)\n\u0015\n. (4.33)\nThus, for the cost of one matrix function computation on a matrix of size 2n\u00d72n,\nwe get G(X) and DG(X)[U]. This is useful, though we should bear in mind\nthat computing matrix functions is usually easier for symmetric or Hermitian\nmatrices: here, even if X is favorable in that regard, the structure is lost by\nforming the block matrix. If the matrices are large or poorly conditioned, it may", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2736, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5e978f30-bafa-4e10-9e2e-1317f23d9f85": {"__data__": {"id_": "5e978f30-bafa-4e10-9e2e-1317f23d9f85", "embedding": null, "metadata": {"page_label": "75", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "91b6425b-bf38-400e-a2a2-d299fa21384d", "node_type": "4", "metadata": {"page_label": "75", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "ee5b1818eb7b5f091dbc12f1eeb617c440d476df21a707396dcd04620ae15e07", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n4.7 Computing gradients* 75\nhelp to explore alternatives [AMH09], [Hig08, \u00a7 10.6, \u00a7 11.8]. If an eigenvalue\ndecomposition of X is available, there exists an explicit expression for DG(X)[U]\ninvolving Loewner matrices [Hig08, Cor. 3.12].\nWe can gain insight into the adjoint of the directional derivative of a matrix\nfunction through (4.32) and Example 4.23. Indeed,\nDG(X)[U] =\n\u221eX\nk=0\nakD\n\u0000\nX 7\u2192 Xk\u0001\n(X)[U]\n=\n\u221eX\nk=0\nak\nkX\n\u2113=1\nX\u2113\u22121UX k\u2212\u2113. (4.34)\nAssume the Taylor expansion coefficients ak are real: this holds for the matrix\nexponential, logarithm and square root. It is then straightforward to see that the\nadjoint with respect to the usual inner product obeys\nDG(X)\u2217 = DG(X\u2217). (4.35)\nIndeed,\n\u27e8DG(X)[U], V\u27e9 =\n* \u221eX\nk=0\nak\nkX\n\u2113=1\nX\u2113\u22121UX k\u2212\u2113, V\n+\n=\n\u221eX\nk=0\nak\nkX\n\u2113=1\n\nX\u2113\u22121UX k\u2212\u2113, V\n\u000b\n=\n\u221eX\nk=0\nak\nkX\n\u2113=1\n\nU, (X\u2217)\u2113\u22121V (X\u2217)k\u2212\u2113\u000b\n= \u27e8U, DG(X\u2217)[V ]\u27e9. (4.36)\nOf course, X\u2217 = X\u22a4 in the real case.\nExample 4.27. Formulas for the directional derivatives of factors of certain\nmatrix factorizations are known, including QR, LU, Cholesky, polar factoriza-\ntion, eigenvalue decomposition and SVD. See [Deh95, \u00a7 3.1], [DMV99, DE99,\nFep17, FL19, BZA20] and [AMS08, Ex. 8.1.5] among others.\nExample 4.28. The directional derivative of g(X) = log(det( X)) is given by\nDg(X)[U] = Tr( X\u22121U), provided that det(X) is positive if it is real. Indeed,\nusing det(AB) = det(A) det(B) and log(ab) = log(a) + log(b),\nlog(det(X + tU)) = log(det(X(In + tX\u22121U)))\n= log(det(X)) + log(det(In + tX\u22121U))\n= log(det(X)) + t Tr(X\u22121U) + O(t2), (4.37)\nwhere we used det(In + tA) = 1 +t Tr(A) +O(t2) then log(1 +\u03bbt) = \u03bbt + O(t2).\nFor the former claim, check that if \u03bb1, . . . , \u03bbn denote the eigenvalues of A then\ndet(In + tA) = Qn\ni=1(1 +\u03bbit). In particular, if we restrict g to the set of positive", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2012, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a75b1b2e-fd9f-46bf-a9b3-b6fdec328c43": {"__data__": {"id_": "a75b1b2e-fd9f-46bf-a9b3-b6fdec328c43", "embedding": null, "metadata": {"page_label": "76", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f8a1e58-a6cf-4c3f-8a97-93182960a306", "node_type": "4", "metadata": {"page_label": "76", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "fa976cc34669ed01c9428fd0389742b7fb336f1b42c14eb97db0c5e67e94ddc4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n76 First-order optimization algorithms\ndefinite matrices, then g is real valued and we conclude that gradg(X) = X\u22121\nwith respect to the usual inner product. The related function\nh(X) = log(det(X\u22121)) = log(1/det(X)) = \u2212log(det(X)) = \u2212g(X)\nhas derivatives Dh(X)[U] = \u2212Tr(X\u22121U) and gradh(X) = \u2212X\u22121.\nExample 4.29. We now work out a gradient as a full example. Consider the\nfollowing function which maps a pair of square matrices (X, Y) to a real number,\nwith A and B two given real matrices ( A, B, X, Yare all in Rn\u00d7n):\nf(X, Y) = 1\n2\u2225A \u2299 exp(X\u22121B)Y \u22252.\nHere, exp denotes the matrix exponential and \u2299 denotes entrywise multiplication.\nSee Exercise 3.67 for pointers regarding gradients on a product manifold. Define\nQ(X, Y) = A \u2299\n\u0002\nexp(X\u22121B)Y\n\u0003\n, so that\nf(X, Y) = 1\n2\u2225Q(X, Y)\u22252 = 1\n2 \u27e8Q(X, Y), Q(X, Y)\u27e9.\nThen, using the product rule on the inner product \u27e8\u00b7, \u00b7\u27e9, we get the directional\nderivative of f at (X, Y) along the direction ( \u02d9X, \u02d9Y ) (a pair of matrices of the\nsame size as (X, Y)):\nDf(X, Y)[ \u02d9X, \u02d9Y ] =\nD\nDQ(X, Y)[ \u02d9X, \u02d9Y ], Q(X, Y)\nE\n.\nWe focus on the differential of Q for now. Using that A is constant, the product\nrule on exp(\u00b7)Y and the chain rule on exp, we get:\nDQ(X, Y)[ \u02d9X, \u02d9Y ] = A \u2299\nh\nDexp(X\u22121B)[U]Y + exp(X\u22121B) \u02d9Y\ni\n,\nwhere Dexp is the differential of the matrix exponential, and U is the differen-\ntial of (X, Y) 7\u2192 X\u22121B at (X, Y) along ( \u02d9X, \u02d9Y ), that is, U = \u2212X\u22121 \u02d9XX \u22121B.\nCombining and writing W = X\u22121B for short, we find\nDf(X, Y)[ \u02d9X, \u02d9Y ] =\nD\nA \u2299\nh\nDexp(W)[\u2212X\u22121 \u02d9XW ]Y + exp(W) \u02d9Y\ni\n, Q(X, Y)\nE\n.\nWe re-arrange the terms in this expression to reach the form \u27e8 \u02d9X, \u00b7\u27e9+\u27e8 \u02d9Y ,\u00b7\u27e9. This\nmostly requires using the notion of adjoint of linear maps: recall Section 3.1.\nFirst using the adjoint of entrywise multiplication with respect to the usual inner\nproduct as in (3.15), then linearity of the inner product, we find:\nDf(X, Y)[ \u02d9X, \u02d9Y ] =\nD\nDexp(W)[\u2212X\u22121 \u02d9XW ]Y , A\u2299 Q(X, Y)\nE\n+\nD\nexp(W) \u02d9Y , A\u2299 Q(X, Y)\nE\n.\nLet Z = A \u2299 Q(X, Y) for short; using the adjoint of matrix multiplication for\nboth terms:\nDf(X, Y)[ \u02d9X, \u02d9Y ] =\nD\nDexp(W)[\u2212X\u22121 \u02d9XW ], ZY\u22a4\nE\n+\nD\n\u02d9Y ,exp(W)\u22a4Z\nE\n.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2345, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1aea0126-77fe-4035-8db4-9bec3434f5d0": {"__data__": {"id_": "1aea0126-77fe-4035-8db4-9bec3434f5d0", "embedding": null, "metadata": {"page_label": "77", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b3bbf62d-35ef-41ff-b428-ff566db5cf5d", "node_type": "4", "metadata": {"page_label": "77", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "9e4aee6944c5c728c0c688ddb3a2d36853a1d5a1788f6447530d0a142e17d7be", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n4.7 Computing gradients* 77\nThe gradient with respect to Y is readily apparent from the second term. We focus\non the first term. Using the adjoint of the differential of the matrix exponential\nat X\u22121B (denoted by a star), we get:\nD\nDexp(W)[\u2212X\u22121 \u02d9XW ], ZY\u22a4\nE\n=\nD\n\u2212X\u22121 \u02d9XW ,Dexp(W)\u2217[ZY \u22a4]\nE\n=\nD\n\u02d9X, \u2212(X\u22121)\u22a4Dexp(W)\u2217[ZY \u22a4]W\u22a4\nE\n.\nThis reveals the gradient of f with respect to X. We can go one step further\nusing the fact that Dexp(W)\u2217 = Dexp(W\u22a4). To summarize:\ngradf(X, Y) =\n\u0010\n\u2212 (X\u22121)\u22a4Dexp(W\u22a4)[ZY \u22a4]W\u22a4, exp(W)\u22a4Z\n\u0011\n.\nConsidering that W,exp(W) and Q must be computed in order to evaluate f, it\nis clear that computing gradf is not significantly more expensive, and much of\nthe computations can be reused.\nIf A, B, X, Yare in Cn\u00d7n and we use the real inner product over complex\nmatrices (3.17) as in Section 3.1, gradf takes on the same expression except all\ntransposes are replaced by conjugate-transposes, and Z = A \u2299 Q(X, Y). See also\nExample 4.30.\nExample 4.30. Consider the function f : Cn \u2192 R defined by\nf(x) =\nmX\ni=1\n|x\u2217Aix \u2212 bi|2,\nwhere A1, . . . , Am \u2208 Cn\u00d7n and b \u2208 Cm are given. This is a real-valued function\nof a complex vector. As in Section 3.1, we consider Cn to be a real vector space of\ndimension 2n. We further equip Cn with the (real) inner product \u27e8u, v\u27e9 = \u211c{u\u2217v}\nas in (3.16). To work out the gradient of f, as usual, we first work out its\ndirectional derivatives. Define z = z(x) in Cm with zi(x) = x\u2217Aix \u2212 bi; observe\nthat f(x) = Pm\ni=1 |zi|2 and:\nDzi(x)[u] = u\u2217Aix + x\u2217Aiu.\nAbove, we have used the convenient fact that complex conjugation is a linear\nmap on the real vector space Cn; as such, the differential of the conjugate is the\nconjugate of the differential. With the identities |a|2 = aa and ab+ba = 2\u211c{ab},", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1989, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ac214987-e3b6-42f4-9b3e-4419bc179794": {"__data__": {"id_": "ac214987-e3b6-42f4-9b3e-4419bc179794", "embedding": null, "metadata": {"page_label": "78", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8bbda8b-6ab4-4cc1-9229-6c71788eb0f6", "node_type": "4", "metadata": {"page_label": "78", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "d204dedcb02225966657d546c860fbc513e7aebcbf5921e396c35665038b4453", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n78 First-order optimization algorithms\nwe find:\nDf(x)[u] =\nmX\ni=1\nzi \u00b7 Dzi(x)[u] + Dzi(x)[u] \u00b7 zi\n= 2\nmX\ni=1\n\u211c{zi \u00b7 (u\u2217Aix + x\u2217Aiu)}\n= 2\u211c\n( mX\ni=1\nzi \u00b7 (u\u2217Aix)\n)\n+ 2\u211c\n( mX\ni=1\nzi \u00b7 (x\u2217Aiu)\n)\n= 2\n*\nu,\nmX\ni=1\nzi \u00b7 Aix\n+\n+ 2\n* mX\ni=1\nzi \u00b7 A\u2217\ni x, u\n+\n.\nBy identification in the definition \u27e8gradf(x), u\u27e9 = Df(x)[u], we deduce:\ngradf(x) = 2\n mX\ni=1\nziAi + ziA\u2217\ni\n!\nx.\nIn the particular case where Ai = A\u2217\ni and (as one might expect in that case)\nwhere b is real, we also have that z is real and the gradient further simplifies to\ngradf(x) = 4 (Pm\ni=1 ziAi) x.\nThe cheap gradient principle [GW08, p88] asserts that, for a wide class of\nfunctions f, computing the gradient of f at a point requires no more than a\nmultiple (often five or less) of the number of arithmetic operations required to\nevaluate f itself at that point. Furthermore, much of the computations required\nto evaluate the cost function can be reused to evaluate its gradient at the same\npoint. Thus, if it appears that computing the gradient takes inordinately more\ntime than it takes to evaluate the cost, chances are the code can be improved.\nAnticipating the introduction of Hessians, we note that a similar fact holds for\nHessian-vector products [Pea94].\nThe latter principle is at the heart of automatic differentiation (AD): algo-\nrithms that automatically compute the derivatives of a function, based simply\non code to compute that function. AD can significantly speed up development\ntime. Packages such as Manopt offer AD for optimization on manifolds.\nExercise 4.31. Prove rules (4.21), (4.22), (4.23), (4.24) and (4.25).\nExercise 4.32. The (principal) matrix square root function F(X) = X1/2 is\nwell defined provided real eigenvalues of X are positive [Hig08, Thm. 1.29]. Show\nthat DF(X)[U] = E, where E is the matrix which solves the Sylvester equation\nEX1/2 + X1/2E = U. Hint: consider G(X) = X2 and F = G\u22121.\nExercise 4.33. For a matrix function whose Taylor expansion coefficients are\nreal, show that DF(X)[U\u2217] = (DF(X\u2217)[U])\u2217. Combining with (4.36), this yields:\nDF(X)\u2217[U] = DF(X\u2217)[U] = (DF(X)[U\u2217])\u2217.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2332, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3441647d-a11f-4e5b-8420-861873f6c06c": {"__data__": {"id_": "3441647d-a11f-4e5b-8420-861873f6c06c", "embedding": null, "metadata": {"page_label": "79", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bd5c140b-fecc-401a-b29a-32c1adbaa36d", "node_type": "4", "metadata": {"page_label": "79", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "2e75fe58fa575830f8d6fb279b80204b5b9627c32405e9d9bcd9e4e24a607ae6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n4.8 Numerically checking a gradient* 79\n4.8 Numerically checking a gradient*\nAfter writing code to evaluate a cost function f(x) and its Riemannian gradient\ngradf(x), it is often helpful to run numerical tests to catch possible mistakes\nearly. This section describes such tests. In the Matlab toolbox Manopt, they are\nimplemented as checkgradient.\nThe first thing to test is that grad f(x) is indeed in the tangent space at x.\nThis being secured, consider the Taylor expansion (4.3):\nf(Rx(tv)) = f(x) + t \u27e8gradf(x), v\u27e9x + O(t2).\nThis says that, for all x \u2208 Mand v \u2208 TxM, with all retractions,\nE(t) \u225c |f(Rx(tv)) \u2212 f(x) \u2212 t \u27e8gradf(x), v\u27e9x| = O(t2). (4.38)\nTaking the logarithm on both sides, we find that log( E(t)) must grow approxi-\nmately linearly in log( t), with a slope of two (or more 1) when t is small:\nlog(E(t)) \u2248 2 log(t) + constant.\nThis suggests a procedure to check the gradient numerically:\n1. Generate a random point x \u2208 M;\n2. Generate a random tangent vector v \u2208 TxM with \u2225v\u2225x = 1;\n3. Compute f(x) and grad f(x). Check that grad f(x) is in T xM, and compute\n\u27e8gradf(x), v\u27e9x;\n4. Compute E(t) for several values of t logarithmically spaced on the interval\n[10\u22128, 100];\n5. Plot E(t) as a function of t, in a log\u2013log plot;\n6. Check that the plot exhibits a slope of two (or more) over several orders of\nmagnitude.\nWe do not expect to see a slope of two over the whole range. On the one hand,\nfor large t, the Taylor approximation may be poor. On the other hand, for small\nt, floating-point arithmetic strongly affects the computation of E(t) (see also the\ndiscussion in Section 6.4.6). Still, we do expect to see a range of values of t for\nwhich the numerical computation is accurate and the Taylor expansion is valid.\nIf the curve does not exhibit a slope of two over at least one or two orders of\nmagnitude, this is a strong sign that there is a mistake in the computation of\nthe gradient (or the cost function, or the retraction, or the inner product).\nExample 4.34. With some symmetric matrix A and size n, recall the cost func-\ntion f(X) = \u22121\n2 Tr(X\u22a4AX) defined on the Stiefel manifold St(n, p). Its gradient\nis the orthogonal projection of \u2212AX to the tangent space at X. Figure 4.1 plots\nthe numerical gradient check described above, obtained first with an incorrect gra-\ndient (the minus sign was forgotten), then with the correct gradient. Notice how\n1 If the Taylor remainder happens to be O(tk) with k >2, we should get a slope of k. This is\ngood but rare.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2734, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6cce71b0-534f-4f81-aaab-4d8beb6da159": {"__data__": {"id_": "6cce71b0-534f-4f81-aaab-4d8beb6da159", "embedding": null, "metadata": {"page_label": "80", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76ddaffa-ecee-4bad-a52a-d54a1c8d87c1", "node_type": "4", "metadata": {"page_label": "80", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "a1a852841a8c4bdc3dca7cd2697783fc6097f206168f6492d4be3e5a5d3aae07", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n80 First-order optimization algorithms\nWrong gradient\nDashed line:slope 2\nCorrect gradient\nDashed line:slope 2\nFigure 4.1 Example 4.34 illustrates a numerical procedure to check gradient computa-\ntion code. The dashed lines have a slope of two: this serves as a visual reference. The\nsolid curves represent the function E(t) (4.38) in a log\u2013log plot. Part of each solid curve\nis overlaid with a thicker line. The (average) slopes of those thick lines are nearly one\n(left) and two (right), strongly suggesting the left gradient is incorrect, and suggesting\nthe right gradient is correct (as is indeed the case).\nfor the incorrect gradient the solid curve has (mostly) a slope of one, whereas for\nthe correct gradient it has (mostly) a slope of two. This figure is obtained with\nthe following Matlab code, using Manopt.\nn = 50;\nA = randn (n, n);\nA = A + A ' ;\ninner = @(U, V) U (:) ' *V (:) ; % = trace (U ' *V)\nSt = stiefelfactory (n, 3);\nproblem .M = St;\nproblem . cost = @(X) -0.5* inner (X, A*X);\nproblem . grad = @(X) St. proj (X, A*X); % Oops , forgot -\ncheckgradient ( problem ); % First panel\nproblem . grad = @(X) St. proj (X, -A*X); % This is better\ncheckgradient ( problem ); % Second panel\nX = steepestdescent ( problem ); % Call to RGD\nX = trustregions ( problem ); % Call to RTR ( Chapter 6)\n4.9 Notes and references\nAbsil et al. give a thorough treatment and history of Riemannian gradient de-\nscent in [AMS08, \u00a7 4], with references going back to [Lue72, Gab82, Smi94,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1740, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "175b681d-e83e-41b4-9a80-9370d27c4b06": {"__data__": {"id_": "175b681d-e83e-41b4-9a80-9370d27c4b06", "embedding": null, "metadata": {"page_label": "81", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ec2135cb-9306-4b52-bdb4-63450dc6a4bf", "node_type": "4", "metadata": {"page_label": "81", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "322b9889c6b13fee510896a386213dbf4be33c4a4f8c5db9f4e4d47605f20c77", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n4.9 Notes and references 81\nUdr94, HM96, Rap97, EAS98]. Gabay [Gab82] details the important work of\nLichnewsky [Lic79], who generalized Luenberger\u2019s pioneering paper [Lue72] from\nRiemannian submanifolds of Euclidean space to general manifolds, and designed\na Riemannian nonlinear conjugate gradients method for nonlinear eigenvalue\nproblems.\nThe first iteration complexity analyses in the Riemannian setting appear about\nthe same time on public repositories in [ZS16, BAC18, BFM17], under various\nrelated models. The analysis presented here is largely based on [BAC18]. Be-\nfore that, analyses with similar ingredients including Lipschitz-type assumptions\n(but phrased as asymptotic convergence results) appear notably in [dCNdLO98,\nThm. 5.1].\nSeveral first-order optimization algorithms on Riemannian manifolds are avail-\nable, including nonlinear conjugate gradients [AMS08, SI15, Sat16] (pioneered\nby Lichnewsky), BFGS [BM06, QGA10b, RW12, HGA15, HAG16] (pioneered\nby Gabay) and (variance reduced) stochastic gradients [Bon13, ZRS16, KSM18,\nSKM19]. See also the book by Sato [Sat21] which provides a general introduc-\ntion to Riemannian optimization and an in depth treatment of the nonlinear\nconjugate gradients method. Quadratic convergence results for the latter appear\nin [Smi94, \u00a7 5]. Regarding stochastic methods, Hosseini and Sra propose a survey\nin [HS20].\nThere is also recent work focused on nonsmooth cost functions on smooth\nmanifolds, including proximal point methods and subgradient methods [BFM17,\nCMSZ20], gradient sampling [HU17] and ADMM-type algorithms [KGB16].\nMany of these more advanced algorithms require transporters or vector trans-\nports, which we cover in Section 10.5: these are tools to transport tangent vectors\nand linear maps from one tangent space to another.\nIn (4.11), we considered the standard proof that Lipschitz continuity of the\ngradient of f (in the Euclidean case) implies uniform bounds on the truncation\nerror of first-order Taylor expansions off. If f is twice continuously differentiable,\nit is not difficult to show that the converse also holds because the gradient is\nLipschitz continuous if and only if the Hessian is bounded. See [BAJN20, Cor. 5.1]\nfor a more general discussion assuming H\u00a8 older continuity of the gradient.\nSee [BH19] for first-order necessary optimality conditions when x, in addi-\ntion to living on a manifold M, may be further restricted by equality and in-\nequality constraints. Second-order optimality conditions are also investigated in\nSection 6.1 and in [YZS14].\nAfter Proposition 4.7, we observed that (under the stated conditions) the\nstatement guarantees all accumulation points of RGD are critical points, but it\ndoes not guarantee convergence of the iterates: there could be more than one\naccumulation point. This type of behavior is undesirable, and by all accounts\nuncommon. The local convergence results outlined in Section 4.6 exclude such\npathological cases near critical points where the Riemannian Hessian (intro-\nduced in Section 5.5) is positive definite. For more general conditions based on\nanalyticity, see notably [AK06]. See [Lag07] and [BH15, Thm. 4.1, Cor. 4.2] for", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3424, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e7da6d52-d7ea-45b1-9773-3a294d84e23a": {"__data__": {"id_": "e7da6d52-d7ea-45b1-9773-3a294d84e23a", "embedding": null, "metadata": {"page_label": "82", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c861cb9d-72f6-4c6c-9aee-9480abb6151a", "node_type": "4", "metadata": {"page_label": "82", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "48f18071e144eed28222c95565ca9fe6ce6091626c612893e68947cdc463876a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n82 First-order optimization algorithms\nconnections to optimization on manifolds. At their core, these results rely on\nthe Kurdyka\u2013 Lojasiewicz inequality for real analytic functions [ Loj65]. See also\nLemma 11.28 and Theorem 11.29 for the geodesically strongly convex case via\nthe Polyak\u2013 Lojasiewicz inequality.\nThe superlinear convergence claim in Theorem 4.19 also appears with a dis-\ntinction regarding the degree of differentiability of the iteration map in [AMS08,\nThm. 4.5.3].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 741, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e2c74e83-2066-469f-8259-2f152dab1136": {"__data__": {"id_": "e2c74e83-2066-469f-8259-2f152dab1136", "embedding": null, "metadata": {"page_label": "83", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "273e2821-351c-40ae-9c16-900fdaede65e", "node_type": "4", "metadata": {"page_label": "83", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "38ac17c94522a551211405db69bb1ba4ce49d5f0d98a7c5b7e0b96a3e58de1f7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n5 Embedded geometry: second order\nIn previous chapters, we developed a notion of gradient for smooth functions on\nmanifolds. We explored how this notion is useful both to analyze optimization\nproblems and to design algorithms for them. In particular, we found that local\nminimizers are critical points, that is, the gradient vanishes there. Furthermore,\nwe showed under a regularity condition that following the negative gradient\nallows us to find critical points.\nA tool of choice in those developments has been a type of first-order Taylor\nexpansion of the cost function along a curve. Concretely, for a smooth curve c\non M passing through x at t = 0 with velocity c\u2032(0) = v, we considered the\ncomposition g = f \u25e6 c (a smooth function from reals to reals), and its truncated\nTaylor expansion\ng(t) = g(0) + tg\u2032(0) + O(t2) = f(x) + t \u27e8gradf(x), v\u27e9x + O(t2).\nTo gain further control over g, it is natural to ask what happens if we truncate\nthe expansion one term later, that is, if we write\ng(t) = f(x) + t \u27e8gradf(x), v\u27e9x + t2\n2 g\u2032\u2032(0) + O(t3).\nIn the Euclidean case, with the straight curve c(t) = x + tv, we would find the\nwell-known formula\ng(t) = f(x + tv) = f(x) + t \u27e8gradf(x), v\u27e9 + t2\n2 \u27e8Hessf(x)[v], v\u27e9 + O(t3),\nwhere Hessf(x) is the Hessian of f at x.\nThis leads us to ponder: can we define an equivalent of the Hessian of a function\non a Riemannian manifold? To make progress on this question, we first review\nhow Hessians are defined on Euclidean spaces.\nRecall from Section 3.1 the definition of the gradient and Hessian of a smooth\nfunction f : E \u2192R on a Euclidean space E with inner product \u27e8\u00b7, \u00b7\u27e9. The gradient\nof f is the map grad f : E \u2192 Ewhich satisfies \u27e8gradf(x), v\u27e9 = Df(x)[v] for all\nx, v\u2208 E. The Hessian of f at x is the linear map Hess f(x): E \u2192 Edefined by\nHessf(x)[v] = D(gradf)(x)[v] = lim\nt\u21920\ngradf(x + tv) \u2212 gradf(x)\nt .\nThus, Hessf(x)[v] tells us how much the gradient changes if x is perturbed along\nv, up to first order.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2207, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "99e40f34-a0e5-4ae2-9d29-febfd3269635": {"__data__": {"id_": "99e40f34-a0e5-4ae2-9d29-febfd3269635", "embedding": null, "metadata": {"page_label": "84", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9680772c-abe7-4643-af32-3fd6362a69a2", "node_type": "4", "metadata": {"page_label": "84", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "c3aa148e4cebee3b864ef917d73cb72c68905921e0d92c5dd4feebd5f399a0fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n84 Embedded geometry: second order\nFor the special case whereE = Rn is equipped with the standard inner product\n\u27e8u, v\u27e9 = u\u22a4v, we already reasoned that the gradient is the vector of partial\nderivatives of f. In that case, we also recover a familiar form of the Hessian as\nthe symmetric matrix of second-order partial derivatives:\ngradf(x) =\n\uf8ee\n\uf8ef\uf8f0\n\u2202f\n\u2202x1\n(x)\n...\n\u2202f\n\u2202xn\n(x)\n\uf8f9\n\uf8fa\uf8fb, Hessf(x) =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n\u22022f\n\u2202x1\u2202x1\n(x) \u00b7 \u00b7\u00b7 \u22022f\n\u2202xn\u2202x1\n(x)\n... ...\n\u22022f\n\u2202x1\u2202xn\n(x) \u00b7 \u00b7\u00b7 \u22022f\n\u2202xn\u2202xn\n(x)\n\uf8f9\n\uf8fa\uf8fa\uf8fb.\nIndeed, we can confirm this by working out the directional derivative of the\ngradient vector field gradf at x along v \u2208 Rn:\nD(gradf)(x)[v] =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\nD\n\u0010\n\u2202f\n\u2202x1\n\u0011\n(x)[v]\n...\nD\n\u0010\n\u2202f\n\u2202xn\n\u0011\n(x)[v]\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n=\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n\u22022f\n\u2202x1\u2202x1\n(x)v1 + \u00b7 \u00b7\u00b7+ \u22022f\n\u2202xn\u2202x1\n(x)vn\n...\n\u22022f\n\u2202x1\u2202xn\n(x)v1 + \u00b7 \u00b7\u00b7+ \u22022f\n\u2202xn\u2202xn\n(x)vn\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n=\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n\u22022f\n\u2202x1\u2202x1\n(x) \u00b7 \u00b7\u00b7 \u22022f\n\u2202xn\u2202x1\n(x)\n... ...\n\u22022f\n\u2202x1\u2202xn\n(x) \u00b7 \u00b7\u00b7 \u22022f\n\u2202xn\u2202xn\n(x)\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n\uf8ee\n\uf8ef\uf8f0\nv1\n...\nvn\n\uf8f9\n\uf8fa\uf8fb = Hessf(x)[v].\nWhile this special case is important, the general definition of the Hessian as the\nderivative of the gradient vector field is more relevant: it leads the way forward.\nAccordingly, to extend the concept of Hessian to Riemannian manifolds, we\nneed a good notion of derivative of vector fields. As we shall see, the derivative\nwe already have (Definition 3.34) is not appropriate. To overcome this, we intro-\nduce a new notion of derivative for vector fields called a connection or covariant\nderivative. That naturally leads to a notion of covariant derivative of a vector\nfield along a curve. With a particularly apt choice of connection called the Rie-\nmannian connection or Levi-Civita connection, we will be able to complete the\nTaylor expansion above as follows:\nf(c(t)) = f(x) + t \u27e8gradf(x), v\u27e9x + t2\n2 \u27e8Hessf(x)[v], v\u27e9x\n+ t2\n2 \u27e8gradf(x), c\u2032\u2032(0)\u27e9x + O(t3).\nHere, Hessf(x) is the Riemannian Hessian of f at x we are about to define, and\nc\u2032\u2032(t) is the acceleration along c: the covariant derivative of its velocity vector\nfield c\u2032(t). Importantly, Hessf(x) retains familiar properties. For example, it is\nsymmetric as a linear map from T xM to TxM, that is, it is self-adjoint with\nrespect to the Riemannian metric \u27e8\u00b7, \u00b7\u27e9x. For Riemannian submanifolds (a special", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2428, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "72c44ae2-8d3a-4da8-8e95-081e69258e1c": {"__data__": {"id_": "72c44ae2-8d3a-4da8-8e95-081e69258e1c", "embedding": null, "metadata": {"page_label": "85", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d56537dc-afbf-4ee7-9f36-5e08c8fc5286", "node_type": "4", "metadata": {"page_label": "85", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "95638f381850f9500a698466cf8e7c1e5f821ae9f3bea54c31b1531382ee7a75", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n5.1 The case for another derivative of vector fields 85\ncase), we derive a practical expression for Hessf(x)[v] as a type of finite difference\napproximation based on grad f: see Example 5.32. These considerations occupy\nus for most of this chapter.\nWe already encountered the Riemannian Hessian in studying the local con-\nvergence behavior of gradient descent in Section 4.6. In Chapter 6, we use the\nnotion of Riemannian Hessian and the extended Taylor expansion to develop\nso-called second-order optimization algorithms.\n5.1 The case for another derivative of vector fields\nConsider the unit sphere S d\u22121 as a Riemannian submanifold of Rd with the\ncanonical inner product \u27e8u, v\u27e9 = u\u22a4v. For a given symmetric matrix A of size\nd, let f(x) = 1\n2 x\u22a4Ax be defined on S d\u22121. We know from Example 3.62 that the\nRiemannian gradient of f is the following smooth vector field on S d\u22121:\nV (x) = gradf(x) = Ax \u2212 (x\u22a4Ax)x.\nSince V is a smooth map from S d\u22121 to its tangent bundle (two manifolds), we\nalready have a notion of differential for V provided by Definition 3.34. We can\ncompute the latter via (3.28). Explicitly, with the smooth extension\n\u00afV (x) = Ax \u2212 (x\u22a4Ax)x\ndefined on all of Rd, we have for all tangent vectors u \u2208 TxSd\u22121:\nDV (x)[u] = D \u00afV (x)[u] = Au \u2212 (x\u22a4Ax)u \u2212 (u\u22a4Ax + x\u22a4Au)x\n= Projx(Au) \u2212 (x\u22a4Ax)u \u2212 (u\u22a4Ax)x, (5.1)\nwhere Proj x(v) = v \u2212 (x\u22a4v)x is the orthogonal projector from Rd to TxSd\u22121.\nEvidently, DV (x)[u] is not always tangent to S d\u22121 at x: the first two terms\nin (5.1) are tangent, but the third one is not wheneveru\u22a4Ax \u0338= 0. Thus, if we were\nto use that notion of derivative of gradient vector fields to define Hessians, we\nwould find ourselves in the uncomfortable situation where Hessf(x)[u], defined as\nD(gradf)(x)[u], might not be a tangent vector at x. As a result, Hessf(x) would\nnot be a linear map to and from T xSd\u22121, and terms such as \u27e8Hessf(x)[u], u\u27e9x\nwould make no sense. We need a new derivative for vector fields.\n5.2 Another look at differentials of vector fields in linear spaces\nWe aim to define a new derivative for vector fields on manifolds. In so doing, we\nfollow the axiomatic approach, that is, we prescribe properties we would like that\nderivative to have, and later we show there exists a unique operator that satisfies\nthem. Of course, the classical derivative of vector fields on linear spaces should\nqualify: let us have a look at some of its elementary properties for inspiration.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2682, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "524004e3-8023-413e-b6c0-9a22d7ebfda1": {"__data__": {"id_": "524004e3-8023-413e-b6c0-9a22d7ebfda1", "embedding": null, "metadata": {"page_label": "86", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "435c5adf-f14b-4447-9255-552d8181d7e0", "node_type": "4", "metadata": {"page_label": "86", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "0e079ac7f9e6c7fc95033fb63755f9e6b5f516d957fc0d8e889abc6870b82edc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n86 Embedded geometry: second order\nLet E be a linear space. Recall that the differential of a smooth vector field\nV \u2208 X(E) at a point x along u is given by:\nDV (x)[u] = lim\nt\u21920\nV (x + tu) \u2212 V (x)\nt . (5.2)\nGiven three smooth vector fields U, V, W\u2208 X(E), two vectors u, w\u2208 E(we think\nof them as being \u201ctangent at x\u201d), two real numbers a, b\u2208 R and a smooth\nfunction f \u2208 F(E), we know from classical calculus that the following properties\nhold:\n1. D V (x)[au + bw] = aDV (x)[u] + bDV (x)[w];\n2. D( aV + bW)(x)[u] = aDV (x)[u] + bDW(x)[u]; and\n3. D( fV )(x)[u] = Df(x)[u] \u00b7 V (x) + f(x)DV (x)[u].\nFurthermore, the map x 7\u2192 DV (x)[U(x)] is smooth since U and V are smooth,\nand it defines a vector field on E. This constitutes a first set of properties we\nlook to preserve on manifolds.\n5.3 Differentiating vector fields on manifolds: connections\nOur new notion of derivative for vector fields on manifolds is called a connec-\ntion (or affine connection), traditionally denoted by \u2207 (read: \u201cnabla\u201d). Given a\ntangent vector u \u2208 TxM and a vector field V , we think of \u2207uV as a derivative\nof V at x along u. Formally,\u22c6 we should write \u2207(x,u)V , but the base point x is\ntypically clear from context. Note that we do not need a Riemannian metric yet.\nDefinition 5.1. A connection on a manifold M is an operator\n\u2207: TM \u00d7X(M) \u2192 TM: (u, V) 7\u2192 \u2207uV\nsuch that \u2207uV is in TxM whenever u is in TxM and which satisfies four prop-\nerties for all U, V, W\u2208 X(M), u, w\u2208 TxM and a, b\u2208 R:\n0. Smoothness: (\u2207U V )(x) \u225c \u2207U(x)V defines a smooth vector field \u2207U V ;\n1. Linearity in u: \u2207au+bwV = a\u2207uV + b\u2207wV ;\n2. Linearity in V : \u2207u(aV + bW) = a\u2207uV + b\u2207uW; and\n3. Leibniz rule: \u2207u(fV ) = Df(x)[u] \u00b7 V (x) + f(x)\u2207uV .\nThe field \u2207U V is the covariant derivative of V along U with respect to \u2207.\nSee Section 5.6 for a more common (and equivalent) definition.\nThere exist many connections. For example, on a linear space E,\n\u2207uV = DV (x)[u] (5.3)\nis a connection by design. More interestingly, there exist connections on mani-\nfolds. Here is an example for M embedded in a Euclidean space E: based on the\ndiscussion in Section 5.1, one may surmise that a possible fix for the standard", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2402, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "961cf719-bf87-4e77-a2b2-989780ea4f48": {"__data__": {"id_": "961cf719-bf87-4e77-a2b2-989780ea4f48", "embedding": null, "metadata": {"page_label": "87", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "562d087a-40e1-42f9-8e7c-d317123cebc9", "node_type": "4", "metadata": {"page_label": "87", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "69031a3bad7086d5c4682a02cb5c89c613053f8561da056a151468b7a1d94ff1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n5.3 Differentiating vector fields on manifolds: connections 87\nnotion of derivative of vector fields is to project the result to tangent spaces.\nThis can be done as follows:\n\u2207uV = Projx\n\u0000\nD \u00afV (x)[u]\n\u0001\n, (5.4)\nwhere Proj x is the projector from E to TxM\u2014orthogonal with respect to the\nEuclidean metric on E\u2014and \u00afV is any smooth extension of V . That is indeed a\nvalid connection.\nTheorem 5.2. Let M be an embedded submanifold of a Euclidean space E. The\noperator \u2207 defined by (5.4) is a connection on M.\nProof. It is helpful to denote the connection (5.3) on E by \u00af\u2207. Then,\n\u2207uV = Projx\n\u0000\u00af\u2207u \u00afV\n\u0001\n. (5.5)\nIf M is an open submanifold of E, the claim is clear since Projx is identity and we\nmay take \u00afV = V . We now handle M not open in E. Consider U, V, W\u2208 X(M)\ntogether with smooth extensions \u00afU, \u00afV ,\u00afW \u2208 X(O) defined on a neighborhood\nO of M in E. As we just argued, \u00af\u2207 is a connection on O since O is an open\nsubmanifold of E. Also consider a, b\u2208 R and u, w\u2208 TxM. Using consecutively\nthat \u00af\u2207 is a connection and that Proj x is linear, it is straightforward to verify\nlinearity in the first argument:\n\u2207au+bwV = Projx\n\u0000\u00af\u2207au+bw \u00afV\n\u0001\n= Projx\n\u0000\na \u00af\u2207u \u00afV + b \u00af\u2207w \u00afV\n\u0001\n= a\u2207uV + b\u2207wV.\nLikewise, linearity in the second argument holds since:\n\u2207u(aV + bW) = Projx\n\u0000\u00af\u2207u(a \u00afV + b \u00afW)\n\u0001\n= Projx\n\u0000\na \u00af\u2207u \u00afV + b \u00af\u2207u \u00afW\n\u0001\n= a\u2207uV + b\u2207uW.\nTo verify the Leibniz rule, consider an arbitraryf \u2208 F(M) and smooth extension\n\u00aff \u2208 F(O). Then, using that \u00aff \u00afV is a smooth extension for fV on O, it follows\nthat\n\u2207u(fV ) = Projx\n\u0000\u00af\u2207u( \u00aff \u00afV )\n\u0001\n= Projx\n\u0000\nD \u00aff(x)[u] \u00b7 \u00afV (x) + \u00aff(x) \u00af\u2207u \u00afV\n\u0001\n= Df(x)[u] \u00b7 V (x) + f(x)\u2207uV,\nas desired. Finally, we see that \u2207U V is smooth as per Exercise 3.66.\nNot only do connections exist, but actually: there exist infinitely many of them\non any manifold M. For instance, we can consider (5.4) with other projectors.\nAs a result, the connection (5.4) may seem arbitrary. In the next section, we\nshow that, among all connections, exactly one satisfies two additional properties\nonce we add a Riemannian structure on M. As it turns out, the connection (5.4)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2320, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8b1ca6ae-7b2d-4366-8ad8-862b25ed696c": {"__data__": {"id_": "8b1ca6ae-7b2d-4366-8ad8-862b25ed696c", "embedding": null, "metadata": {"page_label": "88", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "90506a4f-15be-48d3-9194-8d35024120a9", "node_type": "4", "metadata": {"page_label": "88", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "01c81fdbd84cfd74621f0c61cad6d29596a9cde59a209f1da95af04069a692d1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n88 Embedded geometry: second order\nsatisfies those additional properties if M is a Riemannian submanifold of E. If\nwe endow M with a different Riemannian metric, there still exists a preferred\nconnection for M but it may differ from (5.4).\nWe close this section with an observation: all connections coincide at critical\npoints of a vector field. For optimization, we will see manifestations of this fact\nwhen applied to the gradient vector field at a critical point of a cost function f.\nThe proof relies on local frames: it can safely be skipped.\nProposition 5.3. Let M be a manifold with arbitrary connection \u2207. Given a\nsmooth vector field V \u2208 X(M) and a point x \u2208 M, if V (x) = 0 then\n\u2207uV = DV (x)[u]\nfor all u \u2208 TxM. In particular, DV (x)[u] is tangent at x.\nProof. Expand V in a local frame W1, . . . , Wn \u2208 X(U) on some neighborhood U\nof x on M (Proposition 3.69):\nV |U = g1W1 + \u00b7 \u00b7\u00b7+ gnWn,\nwhere g1, . . . , gn : U \u2192R are smooth. Given u \u2208 TxM, the properties of connec-\ntions allow us to write the following (see Section 5.6 for a technical point about\nwhy it makes sense to say \u2207uV = \u2207u(V |U)):\n\u2207uV =\nX\ni\n\u2207u(giWi) =\nX\ni\nDgi(x)[u] \u00b7 Wi(x) + gi(x)\u2207uWi.\nMoreover, a direct computation reveals\nDV (x)[u] =\nX\ni\nD(giWi)(x)[u] =\nX\ni\nDgi(x)[u] \u00b7 Wi(x) + gi(x)DWi(x)[u].\nSince V (x) = 0, we know gi(x) = 0 for all i, hence\n\u2207uV =\nX\ni\nDgi(x)[u] \u00b7 Wi(x) = DV (x)[u].\nThis concludes the proof.\nExercise 5.4. Let M1 and M2 be two manifolds, respectively equipped with\nconnections \u2207(1) and \u2207(2). Consider the product manifold M = M1 \u00d7 M2.\nShow that the map \u2207: TM \u00d7X(M) \u2192 TM defined by\n\u2207(u1,u2)(V1, V2) =\n\u0010\n\u2207(1)\nu1 V1(\u00b7, x2) + DV1(x1, \u00b7)(x2)[u2],\n\u2207(2)\nu2 V2(x1, \u00b7) + DV2(\u00b7, x2)(x1)[u1]\n\u0011\n(5.6)\nfor all (u1, u2) tangent to M at (x1, x2) is a connection on M\u2014we call it the\nproduct connection. Notation such as V1(\u00b7, x2) represents the map obtained from\nV1 : M1 \u00d7 M2 \u2192 TM1 by fixing the second input to x2. In particular, V1(\u00b7, x2)\nis a vector field on M1, while V1(x1, \u00b7) is a map from M2 to the linear space\nTx1 M1.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2266, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4de0cf26-73d2-4753-b018-6b57e1b74a51": {"__data__": {"id_": "4de0cf26-73d2-4753-b018-6b57e1b74a51", "embedding": null, "metadata": {"page_label": "89", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a3c5c037-2bac-47ed-b9cb-f4fc212d840d", "node_type": "4", "metadata": {"page_label": "89", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "c4ed3c548269f7e52bf53919d6e8984db4f7686e20a0e54f2b13a7963a1178f1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n5.4 Riemannian connections 89\n5.4 Riemannian connections\nThere exist many connections on a manifold, which means we have leeway to\nbe more demanding. Upon equipping the manifold with a Riemannian metric,\nwe require two further properties so that the connection and the metric interact\nnicely. This is the object of our next theorem, called the fundamental theorem\nof Riemannian geometry. In particular, the two additional properties ensure the\nHessian as defined later in this chapter is a self-adjoint map on each tangent\nspace.\nIn order to state the desired properties, we need to introduce a few notational\ndefinitions. Mind the difference between Uf and fU . \u22c6\nDefinition 5.5. For U, V\u2208 X(M) and f \u2208 F(U) with U open in M, define:\n\u2022 Uf \u2208 F(U) such that (Uf )(x) = Df(x)[U(x)];\n\u2022 [U, V]: F(U) \u2192 F(U) such that [U, V]f = U(V f) \u2212 V (Uf ); and\n\u2022 \u27e8U, V\u27e9 \u2208F(M) such that \u27e8U, V\u27e9(x) = \u27e8U(x), V(x)\u27e9x.\nThe notation Uf captures the action of a smooth vector field U on a smooth\nfunction f through derivation, transforming f into another smooth function. The\ncommutator [U, V] of such action is called the Lie bracket. Even in linear spaces\n[U, V]f is nonzero in general. 1 Notice that\nUf = \u27e8gradf, U\u27e9, (5.7)\nowing to the definitions of Uf , \u27e8V, U\u27e9 and gradf.\nTheorem 5.6. On a Riemannian manifold M, there exists a unique connection\n\u2207 which satisfies two additional properties for all U, V, W\u2208 X(M):\n4. Symmetry: [U, V]f = (\u2207U V \u2212 \u2207V U)f for all f \u2208 F(M); and\n5. Compatibility with the metric: U\u27e8V, W\u27e9 = \u27e8\u2207U V , W\u27e9 + \u27e8V, \u2207U W\u27e9.\nThis connection is called the Levi-Civita or Riemannian connection.\nA connection which satisfies the symmetry property is a symmetric connec-\ntion (also called torsion-free)\u2014this is defined independently of the Riemannian\nstructure. Compatibility with the Riemannian metric is a type of product rule\nfor differentiation through inner products. Unless otherwise stated, we always\nequip a Riemannian manifold with its Riemannian connection.\nBefore we prove Theorem 5.6, let us check its statement against the connections\nwe know. As expected, the Riemannian connection on Euclidean spaces is nothing\nbut classical vector field differentiation (5.3).\nTheorem 5.7. The Riemannian connection on a Euclidean space E with any\nEuclidean metric \u27e8\u00b7, \u00b7\u27e9 is \u2207uV = DV (x)[u]: the canonical Euclidean connection.\n1 In R2, consider U(x) = (1, 0), V (x) = (0, x1x2) and f(x) = x2. Then, [U, V]f = f.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2663, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "591b8ddb-0831-4592-831e-c90e04ecac5e": {"__data__": {"id_": "591b8ddb-0831-4592-831e-c90e04ecac5e", "embedding": null, "metadata": {"page_label": "90", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dc9781bd-bc47-4042-892a-8db01033f748", "node_type": "4", "metadata": {"page_label": "90", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "f6268b74848185bc204ce8dbaece3e95e427968c9a753d5c2348d65e1e03fb39", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n90 Embedded geometry: second order\nProof. We first establish compatibility with the metric, as it will be useful to\nprove symmetry. To this end, we go back to the definition of derivatives as\nlimits. Consider three vector fields U, V, W\u2208 X(E). Owing to smoothness of the\nlatter and to the definition of \u2207,\nV (x + tU(x)) = V (x) + tDV (x)[U(x)] + O(t2)\n= V (x) + t(\u2207U V )(x) + O(t2).\nDefine the function f = \u27e8V, W\u27e9. Using bilinearity of the metric,\n(Uf )(x) = Df(x)[U(x)]\n= lim\nt\u21920\n\u27e8V (x + tU(x)), W(x + tU(x))\u27e9 \u2212 \u27e8V (x), W(x)\u27e9\nt\n= lim\nt\u21920\n\u27e8V (x) + t(\u2207U V )(x), W(x) + t(\u2207U W)(x)\u27e9 \u2212 \u27e8V (x), W(x)\u27e9\nt\n=\n\u0000\n\u27e8\u2207U V , W\u27e9 + \u27e8V, \u2207U W\u27e9\n\u0001\n(x)\nfor all x, as desired.\nTo establish symmetry, we develop the left-hand side first. Recall the definition\nof Lie bracket: [U, V]f = U(V f) \u2212 V (Uf ). Focusing on the first term, note that\n(V f)(x) = Df(x)[V (x)] = \u27e8gradf(x), V(x)\u27e9x .\nWe can now use compatibility with the metric:\nU(V f) = U\u27e8gradf, V\u27e9 = \u27e8\u2207U (gradf), V\u27e9 + \u27e8gradf, \u2207U V \u27e9.\nConsider the term \u2207U (gradf): this is the derivative of the gradient vector field\nof f along U. By definition, this is the (Euclidean) Hessian of f along U. We\nwrite \u2207U (gradf) = Hess f[U], with the understanding that (Hess f[U])(x) =\nHessf(x)[U(x)] = \u2207U(x)(gradf). Overall,\nU(V f) = \u27e8Hessf[U], V\u27e9 + \u27e8gradf, \u2207U V \u27e9.\nLikewise for the other term,\nV (Uf ) = \u27e8Hessf[V ], U\u27e9 + \u27e8gradf, \u2207V U\u27e9.\nIt is a standard fact from multivariate calculus that the Euclidean Hessian is self-\nadjoint, that is, \u27e8Hessf[U], V\u27e9 = \u27e8Hessf[V ], U\u27e9. (This is the Clairaut\u2013Schwarz\ntheorem, which you may remember as the fact that partial derivatives in Rn\ncommute.) Hence,\n[U, V]f = U(V f) \u2212 V (Uf )\n= \u27e8gradf, \u2207U V \u2212 \u2207V U\u27e9\n= (\u2207U V \u2212 \u2207V U)f,\nconcluding the proof.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1958, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e8ba99ed-61e2-4827-ad6f-3c4c82d721d4": {"__data__": {"id_": "e8ba99ed-61e2-4827-ad6f-3c4c82d721d4", "embedding": null, "metadata": {"page_label": "91", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b5deaaca-1c55-46f5-a1c6-d481774fc841", "node_type": "4", "metadata": {"page_label": "91", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "f60438e6b2a8ceb88f4fa94c9c9c3b6821d49df268ca9a2ddd7157bb1f7afe26", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n5.4 Riemannian connections 91\nFor M an embedded submanifold of a Euclidean space E, the connection\n\u2207 (5.4) we defined by projection to tangent spaces is always symmetric, re-\ngardless of any Riemannian structure on M. To show this, it is convenient to\nintroduce notation in analogy with (5.5):\n\u2207U V = Proj\n\u0000\u00af\u2207\u00afU \u00afV\n\u0001\n, (5.8)\nwhere \u00af\u2207 is the canonical Euclidean connection on E; \u00afU, \u00afV are smooth extensions\nof U, V; and Proj takes as input a smooth vector field on a neighborhood of M\nin E and returns a smooth vector field on M obtained by orthogonal projection\nat each point. Thus,\n\u2207U(x)V = (\u2207U V )(x) = Projx\n\u0000\n( \u00af\u2207\u00afU \u00afV )(x)\n\u0001\n= Projx( \u00af\u2207\u00afU(x) \u00afV )\nare all equivalent notations.\nTheorem 5.8. Let M be an embedded submanifold of a Euclidean space E. The\nconnection \u2207 defined by (5.4) is symmetric on M.\nProof. Let \u00af\u2207 denote the canonical Euclidean connection on E. If M is an open\nsubmanifold of E, the claim is clear since \u2207 is then nothing but \u00af\u2207 with restricted\ndomains. We now consider M not open in E. To establish symmetry of \u2207, we\nrely heavily on the fact that \u00af\u2207 is itself symmetric on (any open subset of) the\nembedding space E.\nConsider U, V\u2208 X(M) and f \u2208 F(M) together with smooth extensions \u00afU, \u00afV \u2208\nX(O) and \u00aff \u2208 F(O) to a neighborhood O of M in E. We use the identity\nUf = ( \u00afU \u00aff)|M repeatedly, then the fact that \u00af\u2207 is symmetric on O:\n[U, V]f = U(V f) \u2212 V (Uf )\n= U\n\u0000\n( \u00afV \u00aff)|M\n\u0001\n\u2212 V\n\u0000\n( \u00afU \u00aff)|M\n\u0001\n=\n\u0000\u00afU( \u00afV \u00aff)\n\u0001\n|M \u2212\n\u0000\u00afV ( \u00afU \u00aff)\n\u0001\n|M\n=\n\u0000\n[ \u00afU, \u00afV ] \u00aff\n\u0001\n|M\n=\n\u0000\n( \u00af\u2207\u00afU \u00afV \u2212 \u00af\u2207\u00afV \u00afU) \u00aff\n\u0001\n|M\n= ( \u00afW \u00aff)|M, (5.9)\nwhere we defined \u00afW = \u00af\u2207\u00afU \u00afV \u2212 \u00af\u2207\u00afV \u00afU \u2208 X(O). We know from Section 5.1 that the\nindividual vector fields \u00af\u2207\u00afU \u00afV and \u00af\u2207\u00afV \u00afU need not be tangent along M. Yet, we\nare about to show that their difference is. Assume this for now, that is, assume\n\u00afW is a smooth extension of a vector field W on M. Then,\nW = \u00afW|M = Proj( \u00afW) = Proj\n\u0000\u00af\u2207\u00afU \u00afV \u2212 \u00af\u2207\u00afV \u00afU\n\u0001\n= \u2207U V \u2212 \u2207V U.\nFurthermore, ( \u00afW \u00aff)|M = W f, so that continuing from (5.9) we find:\n[U, V]f = ( \u00afW \u00aff)|M = W f= (\u2207U V \u2212 \u2207V U)f,\nwhich is exactly what we want. Thus, it only remains to show that \u00afW(x) is\nindeed tangent to M for all x \u2208 M.\nTo this end, let x \u2208 Mbe arbitrary and let \u00afh: O\u2032 \u2192 Rk be a local defining", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2456, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2ea63e1d-48c0-4987-968a-a03b849d14ed": {"__data__": {"id_": "2ea63e1d-48c0-4987-968a-a03b849d14ed", "embedding": null, "metadata": {"page_label": "92", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b36931ea-65f1-4711-a5cc-a378faefa500", "node_type": "4", "metadata": {"page_label": "92", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "f309ac5083945081ca7880ad8dc4bbd097ec6dc7a2ffae33482a853f8fbf5924", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n92 Embedded geometry: second order\nfunction for M around x so that M \u2229O\u2032 = \u00afh\u22121(0), and we ensure O\u2032 \u2286 O.\nConsider the restrictionh = \u00afh|M\u2229O\u2032: of course,h is nothing but the zero function.\nApplying (5.9) to h, we find:\n0 = [U, V]h = ( \u00afW \u00afh)|M\u2229O\u2032.\nEvaluate this at x:\n0 = ( \u00afW \u00afh)(x) = D\u00afh(x)\n\u0002 \u00afW(x)\n\u0003\n.\nIn words: \u00afW(x) is in the kernel of D \u00afh(x), meaning it is in the tangent space at\nx. This concludes the proof.\nIn the special case where M inherits the metric from its embedding Euclidean\nspace, \u2207 as defined above is the Riemannian connection.\nTheorem 5.9. Let M be a Riemannian submanifold of a Euclidean space. The\nconnection \u2207 defined by (5.4) is the Riemannian connection on M.\nProof. In light of Theorem 5.8, it remains to check compatibility with the metric,\nthat is, property 5 in Theorem 5.6. Consider U, V, W\u2208 X(M) together with\nsmooth extensions \u00afU, \u00afV ,\u00afW \u2208 X(O) defined on a neighborhood O of M in E.\nLet \u27e8\u00b7, \u00b7\u27e9 denote the metric on the embedding space E (which M inherits). Since\n\u27e8V, W\u27e9 = \u27e8\u00afV ,\u00afW\u27e9|M and Uf = ( \u00afU \u00aff)|M, setting f = \u27e8V, W\u27e9 and \u00aff = \u27e8\u00afV ,\u00afW\u27e9 we\nfind that U\u27e8V, W\u27e9 = ( \u00afU\u27e8\u00afV ,\u00afW\u27e9)|M. Using compatibility of \u00af\u2207 with the metric:\nU\u27e8V, W\u27e9 =\n\u0000\u00afU\u27e8\u00afV ,\u00afW\u27e9\n\u0001\n|M =\n\u0010\n\u00af\u2207\u00afU \u00afV ,\u00afW\n\u000b\n+\n\n\u00afV ,\u00af\u2207\u00afU \u00afW\n\u000b\u0011\f\f\f\nM\n. (5.10)\nPick x \u2208 M. Then, \u00afW(x) = W(x) = Proj x(W(x)). Recall that Proj x is self-\nadjoint (Proposition 3.63), that is, \u27e8u, Projx(v)\u27e9 = \u27e8Projx(u), v\u27e9 for all u, v\u2208 E.\nConsequently,\n\n\u00af\u2207\u00afU \u00afV ,\u00afW\n\u000b\n(x) =\n\n( \u00af\u2207\u00afU \u00afV )(x), Projx(W(x))\n\u000b\n=\n\nProjx\n\u0000\n( \u00af\u2207\u00afU \u00afV )(x)\n\u0001\n, W(x)\n\u000b\nx\n= \u27e8\u2207U V , W\u27e9(x).\nCombining twice with (5.10), we find indeed that\nU\u27e8V, W\u27e9 = \u27e8\u2207U V , W\u27e9 + \u27e8V, \u2207U W\u27e9.\nThis concludes the proof.\nThe previous theorem gives a conveniently clear picture of how to differentiate\nvector fields on a Riemannian submanifold M embedded in a Euclidean space:\nfirst differentiate the vector field in the linear space (a classical derivative), then\northogonally project the result to the tangent spaces of M. More generally, if M\nis not a Riemannian submanifold, then this procedure still defines a symmetric\nconnection, but it may not be the Riemannian connection.\nWe now return to Theorem 5.6. To provide the missing proof, we need a\ntechnical observation: a Lie bracket \u201cis\u201d a smooth vector field.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2479, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1fce0605-88ef-406b-94fc-881d14060f6f": {"__data__": {"id_": "1fce0605-88ef-406b-94fc-881d14060f6f", "embedding": null, "metadata": {"page_label": "93", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "199e26da-eb33-4fac-97e6-d1803764d2f1", "node_type": "4", "metadata": {"page_label": "93", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "83f6797eda027060ea27b10414b420557c3aff9fa850dddd2ef0811e47ee31f1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n5.4 Riemannian connections 93\nProposition 5.10. Let U, V be two smooth vector fields on a manifold M.\nThere exists a unique smooth vector field W on M such that [U, V]f = W f\nfor all f \u2208 F(M). Therefore, we identify [U, V] with that smooth vector field.\nExplicitly, if \u2207 is any symmetric connection, then [U, V] = \u2207U V \u2212 \u2207V U.\nProof. Say M is embedded in the Euclidean space E. In Theorem 5.8 we have\nshown that \u2207 as defined by (5.4) is a symmetric connection for M. Thus,\n[U, V]f = W ffor all f \u2208 F(M) with W = \u2207U V \u2212 \u2207V U. That vector field is\nunique because two vector fields W1, W2 \u2208 X(M) such that W1f = W2f for all\nf \u2208 F(M) are necessarily equal. Indeed, for contradiction, assume W1f = W2f\nfor all f \u2208 F(M) yet W3 = W1 \u2212 W2 \u0338= 0: there exists \u02dc x \u2208 Msuch that\nW3(\u02dcx) \u0338= 0. Consider the linear function \u00aff(x) = \u27e8x, W3(\u02dcx)\u27e9 and its restriction\nf = \u00aff|M; we have\n(W1f)(\u02dcx) \u2212 (W2f)(\u02dcx) = (W3f)(\u02dcx) = Df(\u02dcx)[W3(\u02dcx)] = \u2225W3(\u02dcx)\u22252 \u0338= 0,\nwhich is a contradiction. (Here, \u27e8\u00b7, \u00b7\u27e9 and \u2225 \u00b7 \u2225come from E.)\nA comment is in order. Note that [ U, V] is defined irrespective of any con-\nnection. The above proof shows that [ U, V] is equivalent to \u2207U V \u2212 \u2207V U for\nany symmetric connection \u2207, and relies on Theorem 5.8 for the existence of a\nsymmetric connection. Because of that, the proof here is limited to manifolds\nembedded in a Euclidean space. In Section 8.10, we see a proof that holds for\nmanifolds in general.\nProof sketch of Theorem 5.6. It is easy to verify uniqueness. Indeed, assume \u2207\nis a symmetric connection which is also compatible with the metric. For all\nU, V, W\u2208 X(M), compatibility with the metric implies\nU\u27e8V, W\u27e9 = \u27e8\u2207U V , W\u27e9 + \u27e8V, \u2207U W\u27e9,\nV \u27e8W, U\u27e9 = \u27e8\u2207V W , U\u27e9 + \u27e8W,\u2207V U\u27e9,\nW\u27e8U, V\u27e9 = \u27e8\u2207W U, V\u27e9 + \u27e8U, \u2207W V \u27e9.\nAdd the first two lines and subtract the third: owing to Proposition 5.10 and\nsymmetry of \u2207, we find after some reorganizing that\n2 \u27e8\u2207U V , W\u27e9 = U\u27e8V, W\u27e9 + V \u27e8W, U\u27e9 \u2212W\u27e8U, V\u27e9\n\u2212 \u27e8U, [V, W]\u27e9 + \u27e8V, [W, U]\u27e9 + \u27e8W,[U, V]\u27e9. (5.11)\nThis is the Koszul formula . Notice that the right-hand side is independent of\n\u2207. For fixed U, V, the fact that this identity holds for all W implies that \u2207U V\nis uniquely determined. To see this, consider for each x \u2208 Ma set of vector\nfields W1, . . . , Wn \u2208 X(M) such that W1(x), . . . , Wn(x) form a basis of T xM:\nthis uniquely determines (\u2207U V )(x). Thus, there can be at most one Riemannian\nconnection.\nTo prove existence, we also rely on the Koszul formula but we need more\nadvanced tools. Let U, V\u2208 X(M) be arbitrary (fixed). We can verify that the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2754, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a2ed6b39-753a-49ff-9d60-cd2f7ff93e51": {"__data__": {"id_": "a2ed6b39-753a-49ff-9d60-cd2f7ff93e51", "embedding": null, "metadata": {"page_label": "94", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fe8e008d-ea58-480b-a286-0cdb28d9a604", "node_type": "4", "metadata": {"page_label": "94", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "5c982e9cda0bfec625938d95231bf3397a93c9f8535147371642c821e6dcb44f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n94 Embedded geometry: second order\nright-hand side of (5.11) defines a smooth one-form X : X(M) \u2192 F(M) (recall\nthe definition in Section 3.9). Indeed,\nX(W) = U\u27e8V, W\u27e9 + V \u27e8W, U\u27e9 \u2212W\u27e8U, V\u27e9\n\u2212 \u27e8U, [V, W]\u27e9 + \u27e8V, [W, U]\u27e9 + \u27e8W,[U, V]\u27e9\nlinearly maps a smooth vector field W to a smooth scalar field X(W) with\nthe property that X(fW ) = fX (W) for all W \u2208 X(M) and f \u2208 F(M). This\ncan be verified through direct computation using [U, fV] = f[U, V] + (Uf ) \u00b7V as\nfollows from Proposition 5.10. Then, the musical isomorphism (Proposition 3.71)\nimplies that there exists a unique smooth vector field Z \u2208 X(M) such that\nX(W) = \u27e8Z, W\u27e9. We use this to define an operator \u2207: X(M) \u00d7 X(M) \u2192 X(M)\nas \u2207U V = 1\n2 Z. It then remains to verify that \u2207 is a symmetric connection which\nis compatible with the metric\u2014some details require Section 5.6.\nA comment is in order. In Section 3.9 we only sketched the proof of the musical\nisomorphism. The sketched parts were clear or unnecessary for one-forms such\nas Df. Likewise, one could verify this for the one-form X defined above, though\nthis can be lengthy. It is similarly technical but more instructive to study the\nmissing details outlined later in Section 5.6. For readers who are only interested\nin Riemannian submanifolds, the situation is rather simpler: we already proved\nexistence of the Riemannian connection as a pointwise operator (constructively)\nin Theorem 5.9, and uniqueness is ensured above.\nExercise 5.11. A derivation on M is a map D: F(M) \u2192 F(M) such that, for\nall f, g\u2208 F(M) and a, b\u2208 R, we have:\n1. Linearity: D(af + bg) = aD(f) + bD(g), and\n2. Leibniz rule: D(fg) = gD(f) + fD(g).\nShow that the action of a smooth vector field on a smooth function (as per Defi-\nnition 5.5) is a derivation. (See Section 5.13 for context.)\nExercise 5.12. Show that the Lie bracket [U, V] of two smooth vector fields\nU, V\u2208 X(M) is a derivation, as per the definition in the previous exercise. It is\ninstructive to do so without using connections or Proposition 5.10.\nExercise 5.13. Continuing from Exercise 5.4, show that if \u2207(1), \u2207(2) are the\nRiemannian connections on M1, M2 (respectively), then the product connection\ndefined by (5.6) is the Riemannian connection on the Riemannian product man-\nifold M1 \u00d7 M2 whose metric is defined in Example 3.57. (Concepts from later\nsections may help; specifically, Proposition 5.15 and Theorem 5.29.)\n5.5 Riemannian Hessians\nThe Riemannian Hessian of a function is defined as the covariant derivative of\nits gradient vector field with respect to the Riemannian connection \u2207, which we", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2810, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dc60851e-2c24-4298-94d4-f868bb890351": {"__data__": {"id_": "dc60851e-2c24-4298-94d4-f868bb890351", "embedding": null, "metadata": {"page_label": "95", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a9b94e00-a888-413c-b8f4-3420afa80c5b", "node_type": "4", "metadata": {"page_label": "95", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "b5bd926006ddddebd7112b1652b39fc9fdeee43eef5fb475085522f999ed8cb3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n5.5 Riemannian Hessians 95\ndefined in Theorem 5.6. At any point x on the manifold M, the Hessian defines\na linear map from the tangent space T xM into itself.\nDefinition 5.14. Let M be a Riemannian manifold with its Riemannian con-\nnection \u2207. The Riemannian Hessian of f \u2208 F(M) at x \u2208 Mis the linear map\nHessf(x): T xM \u2192TxM defined as follows:\nHessf(x)[u] = \u2207ugradf.\nEquivalently, Hessf maps X(M) to X(M) as Hessf[U] = \u2207U gradf.\nThe two special properties of the Riemannian connection together lead to\nsymmetry of the Hessian. By the spectral theorem, this implies that the dim M\neigenvalues of Hess f(x) are real, and that corresponding eigenvectors may be\nchosen to form a basis of T xM orthonormal with respect to \u27e8\u00b7, \u00b7\u27e9x (see Theo-\nrem 3.6).\nProposition 5.15. The Riemannian Hessian is self-adjoint with respect to the\nRiemannian metric. That is, for all x \u2208 Mand u, v\u2208 TxM,\n\u27e8Hessf(x)[u], v\u27e9x = \u27e8u, Hessf(x)[v]\u27e9x .\nProof. Pick any two vector fields U, V\u2208 X(M) such that U(x) = u and V (x) =\nv. Recalling the notation for vector fields acting on functions as derivations\n(Definition 5.5) and using compatibility of the Riemannian connection with the\nRiemannian metric, we find:\n\u27e8Hessf[U], V\u27e9 = \u27e8\u2207U gradf, V\u27e9\n= U\u27e8gradf, V\u27e9 \u2212 \u27e8gradf, \u2207U V \u27e9\n= U(V f) \u2212 (\u2207U V )f.\nSimilarly,\n\u27e8U, Hessf[V ]\u27e9 = V (Uf ) \u2212 (\u2207V U)f.\nThus, recalling the definition of Lie bracket, we get\n\u27e8Hessf[U], V\u27e9 \u2212 \u27e8U, Hessf[V ]\u27e9 = U(V f) \u2212 V (Uf ) \u2212 (\u2207U V )f + (\u2207V U)f\n= [U, V]f \u2212 (\u2207U V \u2212 \u2207V U)f\n= 0,\nwhere we were able to conclude owing to symmetry of the connection.\nTo compute the Riemannian Hessian, we must compute the Riemannian con-\nnection. For the particular case of a Riemannian submanifold of a Euclidean\nspace, we know how to do this from Theorem 5.9. In practical terms, we simply\nneed to consider a smooth extension of the Riemannian gradient vector field,\ndifferentiate it in the classical sense, then orthogonally project the result to the\ntangent spaces.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2195, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "982c29a5-f101-4f18-9203-15e1cee46049": {"__data__": {"id_": "982c29a5-f101-4f18-9203-15e1cee46049", "embedding": null, "metadata": {"page_label": "96", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a1870a83-c7fa-4092-9c4e-a9889f501047", "node_type": "4", "metadata": {"page_label": "96", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "2d373464067f768814ed33430d3b1aa9ddf4f177f9e44aea6e6bc93f3f84a546", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n96 Embedded geometry: second order\nCorollary 5.16. Let M be a Riemannian submanifold of a Euclidean space.\nConsider a smooth function f : M \u2192R. Let \u00afG be a smooth extension of gradf\u2014\nthat is, \u00afG is any smooth vector field defined on a neighborhood of M in the\nembedding space such that \u00afG(x) = gradf(x) for all x \u2208 M. Then,\nHessf(x)[u] = Projx\n\u0000\nD \u00afG(x)[u]\n\u0001\n.\nMore can be said about the important special case of Riemannian subman-\nifolds: see Section 5.11. The following example illustrates how to use Corol-\nlary 5.16 in practice.\nExample 5.17. Consider the cost function \u00aff(x) = 1\n2 x\u22a4Ax for some symmetric\nmatrix A \u2208 Rd\u00d7d and its restriction f = \u00aff|Sd\u22121 to the sphere Sd\u22121 as a Rieman-\nnian submanifold of Rd. We already determined the Euclidean and Riemannian\ngradients of \u00aff and f, respectively:\ngrad \u00aff(x) = Ax,\ngradf(x) = Projx(grad \u00aff(x)) = (Id \u2212 xx\u22a4)Ax = Ax \u2212 (x\u22a4Ax)x.\nTo obtain the Riemannian Hessian of f, we must differentiate a smooth exten-\nsion of gradf in Rd and project the result to the tangent spaces of Sd\u22121. As\nis typical, the analytic expression of gradf provides a natural candidate for a\nsmooth extension; we simply pick:\n\u00afG(x) = Ax \u2212 (x\u22a4Ax)x.\nThe differential of \u00afG follows from the product rule (see also Section 4.7):\nD \u00afG(x)[u] = Au \u2212 (u\u22a4Ax + x\u22a4Au)x \u2212 (x\u22a4Ax)u.\nOrthogonally project to the tangent space at x to reveal the Hessian:\nHessf(x)[u] = Projx\n\u0000\nD \u00afG(x)[u]\n\u0001\n= Projx(Au) \u2212 (x\u22a4Ax)u\n= Au \u2212 (x\u22a4Au)x \u2212 (x\u22a4Ax)u.\nThis linear map is formally defined only on TxSd\u22121 (not on all of Rd).\nExercise 5.18. Continuing Example 5.17, show that if gradf(x) is zero and\nHessf(x) is positive semidefinite (i.e., \u27e8u, Hessf(x)[u]\u27e9x \u2265 0 for all u \u2208 TxSd\u22121),\nthen x is a global minimizer of f, that is, x is an eigenvector of A associated\nto its smallest (left-most) eigenvalue. This is an unusual property: we do not\nnormally expect to be able to certify global optimality based on local conditions\nalone. See also Exercise 9.51.\nExample 5.19. Let us derive an expression for the Riemannian Hessian of a\nsmooth function f : M1 \u00d7 M2 \u2192 R on a Riemannian product manifold. Fix a\npoint x = (x1, x2) \u2208 M1 \u00d7 M2. With f(\u00b7, x2): M1 \u2192 R, we denote the function\nobtained from f by fixing its second input to x2; likewise for f(x1, \u00b7): M2 \u2192 R.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2505, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bd916cd9-d390-40dc-8974-8dbc0d8a84ff": {"__data__": {"id_": "bd916cd9-d390-40dc-8974-8dbc0d8a84ff", "embedding": null, "metadata": {"page_label": "97", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dc0ad0ec-e5d6-42fc-984e-6d5691607475", "node_type": "4", "metadata": {"page_label": "97", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "3c36892765b74c14db53c6793e0b74a3b43297b3fc11a4469e0c1ed6b2354b58", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n5.6 Connections as pointwise derivatives* 97\nFrom Exercise 3.67, the Riemannian gradient of f at x is given by\ngradf(x1, x2) =\n\u0000\nG1(x1, x2), G2(x1, x2)\n\u0001\nwith G1(x1, x2) = gradf(\u00b7, x2)(x1),\nG2(x1, x2) = gradf(x1, \u00b7)(x2).\nThen, the Riemannian Hessian of f at x along any tangent vector u = (u1, u2)\nat x follows from Exercise 5.13 as:\nHessf(x1, x2)[u1, u2] =\n\u0000\nHessf(\u00b7, x2)(x1)[u1] + DG1(x1, \u00b7)(x2)[u2],\nHessf(x1, \u00b7)(x2)[u2] + DG2(\u00b7, x2)(x1)[u1]\n\u0001\n. (5.12)\nAbove, G1(x1, \u00b7): M2 \u2192 Tx1 M1 denotes the map obtained from G1 by fixing its\nfirst input to x1, and likewise for G2(\u00b7, x2): M1 \u2192 Tx2 M2.\nAs a side note, if M1, M2 are both Riemannian submanifolds of respective\nembedding spaces E1, E2, then the product manifold M1 \u00d7 M2 is a Riemannian\nsubmanifold of E1 \u00d7 E2. In that case, Corollary 5.16 and Section 5.11 provide\nother ways to get to the Hessian of f.\n5.6 Connections as pointwise derivatives*\nDefinition 5.1 is not standard: the standard definition follows. In this section, we\nargue that they are equivalent. A reader focused on Riemannian submanifolds\ncan safely skip this section.\nDefinition 5.20. A connection on a manifold M is an operator\n\u2207: X(M) \u00d7 X(M) \u2192 X(M): ( U, V) 7\u2192 \u2207U V\nwhich has three properties for all U, V, W\u2208 X(M), f, g\u2208 F(M) and a, b\u2208 R:\n1. F(M)-linearity in U: \u2207fU +gW V = f\u2207U V + g\u2207W V ;\n2. R-linearity in V : \u2207U (aV + bW) = a\u2207U V + b\u2207U W; and\n3. Leibniz rule: \u2207U (fV ) = (Uf )V + f\u2207U V .\nThe field \u2207U V is the covariant derivative of V along U with respect to \u2207.\nIt is clear that if \u2207 is a connection as per Definition 5.1 then it is also a\nconnection as per Definition 5.20, with ( \u2207U V )(x) \u225c \u2207U(x)V . The other way\naround is less clear.\nSpecifically, we must show that a connection in the sense of Definition 5.20\nacts pointwise with respect to U, that is, (\u2207U V )(x) depends on U only through\nU(x). This gives meaning to the notation \u2207uV as being equal to ( \u2207U V )(x) for\narbitrary U \u2208 X(M) such that U(x) = u.\nThat is the object of the following proposition. It is a consequence of F(M)-\nlinearity in U. Note that dependence on V is through more than just V (x) (and\nindeed, connections are not F(M)-linear in V ). The main tool of the proof is the\nexistence of local frames, as introduced in Section 3.9. Furthermore, a technical\npoint requires some extra work, which we defer until after the proof.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2593, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8333a2ee-4513-4858-8497-ddc137a695d6": {"__data__": {"id_": "8333a2ee-4513-4858-8497-ddc137a695d6", "embedding": null, "metadata": {"page_label": "98", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "febe3ae6-916a-4c37-943a-b9c05240c5b7", "node_type": "4", "metadata": {"page_label": "98", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "a90a791bb0bd33b2281ed038dea6e432af4b2f17758ac2deafc96e1e7a583781", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n98 Embedded geometry: second order\nIn the remainder of this section, the word \u2018connection\u2019 refers to Definition 5.20.\nBy the end of the section, we will have established that this is equivalent to\nDefinition 5.1.\nProposition 5.21. For any connection \u2207 and smooth vector fields U, V on a\nmanifold M, the vector field \u2207U V at x depends on U only through U(x).\nProof. It is sufficient to show that if U(x) = 0 then ( \u2207U V )(x) = 0. Indeed, let\nU1, U2 \u2208 X(M) be two vector fields with U1(x) = U2(x). Then, using the claim,\n(\u2207U1 V )(x) \u2212 (\u2207U2 V )(x) = (\u2207U1 V \u2212 \u2207U2 V )(x) = (\u2207U1\u2212U2 V )(x) = 0.\nTo prove the claim, consider a local frame W1, . . . , Wn on a neighborhood U of\nx in M (Proposition 3.69). Given a vector field U \u2208 X(M) with U(x) = 0, there\nexist unique smooth functions g1, . . . , gn \u2208 F(U) such that\nU|U = g1W1 + \u00b7 \u00b7\u00b7+ gnWn.\nClearly, U(x) = 0 implies g1(x) = \u00b7 \u00b7\u00b7= gn(x) = 0. By a technical lemma given\nhereafter (Lemma 5.27), it is legitimate to write:\n(\u2207U V )(x) = (\u2207g1W1+\u00b7\u00b7\u00b7+gnWnV )(x)\n= g1(x)(\u2207W1 V )(x) + \u00b7 \u00b7\u00b7+ gn(x)(\u2207WnV )(x) = 0, (5.13)\nwhich concludes the proof.\nIn the proof above, it is not immediately clear why (5.13) holds, because\n\u2207U|U V is not formally defined: normally, \u2207 is fed two smooth vector fields on\nall of M. To support this notation and the claim that \u2207U V and \u2207U|U V coincide\nat x, we work through a number of lemmas. The first one concerns the existence\nof bump functions in linear spaces. It is an exercise in analysis to build such\nfunctions [Lee12, Lem. 2.22].\nLemma 5.22. Given any real numbers 0 < r1 < r2 and any point x in a\nEuclidean space E with norm \u2225 \u00b7 \u2225, there exists a smooth function b: E \u2192R such\nthat b(y) = 1 if \u2225y \u2212 x\u2225 \u2264r1, b(y) = 0 if \u2225y \u2212 x\u2225 \u2265r2, and b(y) \u2208 (0, 1) if\n\u2225y \u2212 x\u2225 \u2208(r1, r2).\nUsing bump functions, we can show that (\u2207U V )(x) depends on U and V only\nthrough their values in a neighborhood around x. This is the object of the two\nfollowing lemmas.\nLemma 5.23. Let V1, V2 be smooth vector fields on a manifold M equipped with\na connection \u2207. If V1|U = V2|U on some open set U of M, then (\u2207U V1)|U =\n(\u2207U V2)|U for all U \u2208 X(M).\nProof. Pick x \u2208 U. For M an embedded submanifold of a Euclidean space E,\nthere exists an open set O in E such that U = M \u2229O. Furthermore, there exist\n0 < r1 < r2 such that \u00afB(x, r2)\u2014the closed ball of radius r2 around x in E\u2014is\nincluded in O. Hence, by Lemma 5.22 there exists a smooth function \u00afb \u2208 F(E)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2655, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e5a220a6-b5fd-4c8f-93ac-7c463ea40454": {"__data__": {"id_": "e5a220a6-b5fd-4c8f-93ac-7c463ea40454", "embedding": null, "metadata": {"page_label": "99", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7a136fce-67ff-4ddb-b5bf-7a7377ec75ff", "node_type": "4", "metadata": {"page_label": "99", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "c1632ded1d34ce9e6958736029df104442cd7fb8e1f3d7a66fba490a62d48921", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n5.6 Connections as pointwise derivatives* 99\nwhich is constantly equal to 1 on \u00afB(x, r1) and constantly equal to 0 outside of\n\u00afB(x, r2). With b = \u00afb|M \u2208 F(M), it follows that the vector field V = b \u00b7(V1 \u2212V2)\nis the zero vector field on M. Hence, \u2207U V = 0. Using R-linearity of \u2207 in V and\nthe Leibniz rule:\n0 = \u2207U V = \u2207U (b (V1 \u2212 V2)) = (Ub)(V1 \u2212 V2) + b (\u2207U V1 \u2212 \u2207U V2) .\nEvaluating this at x using V1(x) = V2(x) and b(x) = 1, we find ( \u2207U V1)(x) =\n(\u2207U V2)(x). Repeat for all x \u2208 U.\nLemma 5.24. Let U1, U2 be smooth vector fields on a manifold M equipped with\na connection \u2207. If U1|U = U2|U on some open set U of M, then (\u2207U1 V )|U =\n(\u2207U2 V )|U for all V \u2208 X(M).\nProof. Construct b \u2208 F(M) as in the proof of Lemma 5.23. Then,U = b\u00b7(U1\u2212U2)\nis the zero vector field on M. By F(M)-linearity of \u2207 in U,\n0 = \u2207U V = \u2207b (U1\u2212U2)V = b \u00b7 (\u2207U1 V \u2212 \u2207U2 V ).\nEvaluating this at x and using b(x) = 1 yields the result.\nWe now use bump functions to show that a smooth function defined on a\nneighborhood of a point x on a manifold can always be extended into a smooth\nfunction defined on the whole manifold, in such a way that its value at and\naround x is unaffected. This is a weak version of a result known as the extension\nlemma [Lee12, Lem. 2.26].\nLemma 5.25. Let U be a neighborhood of a point x on a manifold M. Given\na smooth function f \u2208 F(U), there exists a smooth function g \u2208 F(M) and a\nneighborhood U\u2032 \u2286 Uof x such that g|U\u2032 = f|U\u2032.\nProof. For M an embedded submanifold of a Euclidean space E, we know from\nProposition 3.23 that U itself is an embedded submanifold of E. Hence, there\nexists a smooth extension \u00aff of f defined on a neighborhood O of x in E. For this\nO, construct \u00afb \u2208 F(E) as in the proof of Lemma 5.23, with 0 < r1 < r2 such that\n\u00afB(x, r2) \u2282 O. Consider \u00afg : E \u2192R defined by\n\u00afg(y) =\n(\u00afb(y) \u00aff(y) if \u2225y \u2212 x\u2225 \u2264r2,\n0 otherwise.\nIt is an exercise in real analysis to verify that \u00afg is smooth in E; hence, g = \u00afg|M\nis smooth on M. Furthermore, \u00afg is equal to \u00aff on \u00afB(x, r1). Set U\u2032 = U \u2229B(x, r1),\nwhere B(x, r1) is the open ball of radius r1 around x in E. This is a neighborhood\nof x on M such that g|U\u2032 = f|U\u2032.\nLikewise, there is a smooth extension lemma for vector fields, and we state\na weak version of it here. The proof is essentially the same as for the previous\nlemma [Lee12, Lem. 8.6].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2561, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "73825b4e-7bd8-4e9f-9609-219503b43506": {"__data__": {"id_": "73825b4e-7bd8-4e9f-9609-219503b43506", "embedding": null, "metadata": {"page_label": "100", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ffb4c078-72e6-408c-ae24-76e5cdd3fd78", "node_type": "4", "metadata": {"page_label": "100", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "8a9c3f5e59def47b94bc86871c846a6581a4d69dde8ddce33f158ceb8146b050", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n100 Embedded geometry: second order\nLemma 5.26. Let U be a neighborhood of a point x on a manifold M. Given a\nsmooth vector field U \u2208 X(U), there exists a smooth vector field V \u2208 X(M) and\na neighborhood U\u2032 \u2286 Uof x such that V |U\u2032 = U|U\u2032.\nEquipped with the last three lemmas, we can finally state the technical result\nnecessary to support the proof of Proposition 5.21.\nLemma 5.27. Let U, Vbe two smooth vector fields on a manifold M equipped\nwith a connection \u2207. Further let U be a neighborhood of x \u2208 Msuch that U|U =\ng1W1 + \u00b7 \u00b7\u00b7+ gnWn for some g1, . . . , gn \u2208 F(U) and W1, . . . , Wn \u2208 X(U). Then,\n(\u2207U V )(x) = g1(x)(\u2207W1 V )(x) + \u00b7 \u00b7\u00b7+ gn(x)(\u2207WnV )(x),\nwhere each vector (\u2207WiV )(x) is understood to mean (\u2207 \u02dcWi\nV )(x) with \u02dcWi any\nsmooth extension of Wi to M around x.\nProof. Combining Lemmas 5.25 and 5.26, we know there exist smooth extensions\n\u02dcg1, . . . ,\u02dcgn \u2208 F(M) and \u02dcW1, . . . ,\u02dcWn \u2208 X(M) that coincide with g1, . . . , gn and\nW1, . . . , Wn on a neighborhood U\u2032 \u2286 Uof x, so that \u02dcU = \u02dcg1 \u02dcW1 + \u00b7 \u00b7\u00b7+ \u02dcgn \u02dcWn is\na smooth vector field on M which agrees with U locally: U|U\u2032 = \u02dcU|U\u2032. Thus, by\nLemma 5.24,\n(\u2207U V )(x) = (\u2207\u02dcU V )(x)\n= (\u2207\u02dcg1 \u02dcW1+\u00b7\u00b7\u00b7+\u02dcgn \u02dcWn\nV )(x)\n= \u02dcg1(x)(\u2207 \u02dcW1\nV )(x) + \u00b7 \u00b7\u00b7+ \u02dcgn(x)(\u2207 \u02dcWn\nV )(x)\n= g1(x)(\u2207W1 V )(x) + \u00b7 \u00b7\u00b7+ gn(x)(\u2207WnV )(x).\nThe stated definition of ( \u2207WiV )(x) is independent of the choice of smooth ex-\ntension owing to Lemma 5.24.\nIn the proofs above, the most important feature of ( U, V) 7\u2192 \u2207U V we have\nused is that it is F(M)-linear in U. With that in mind, it is easy to revisit those\nproofs and fill in the missing parts for the proof of Proposition 3.71.\nAnticipating our needs for Section 5.7, we note that Lemmas 5.23, 5.25 and 5.26\nalso allow us to make sense of the notation\n(\u2207u(gW ))(x) = Dg(x)[u] \u00b7 W(x) + g(x) \u00b7 (\u2207uW)(x), (5.14)\nwhere g \u2208 F(U) and W \u2208 X(U) are merely defined on a neighborhood U of x.\nSpecifically, (\u2207uW)(x) represents ( \u2207u \u02dcW)(x) where \u02dcW \u2208 X(M) is any smooth\nextension of W around x, as justified by Lemmas 5.23 and 5.26.\n5.7 Differentiating vector fields on curves\nRecall that one of our goals in this chapter is to develop second-order Taylor\nexpansions for g = f \u25e6c with a smooth cost function f : M \u2192R evaluated along", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2453, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "044daf39-a8fc-4989-8215-05d7ab27d421": {"__data__": {"id_": "044daf39-a8fc-4989-8215-05d7ab27d421", "embedding": null, "metadata": {"page_label": "101", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b67d5dc-89de-4fd2-8ecc-710be02c9855", "node_type": "4", "metadata": {"page_label": "101", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "de85b4e10db5e451350e7a0b9c6054d3b945031ec5db561af9e8d57f2024624f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n5.7 Differentiating vector fields on curves 101\na smooth curve c: I \u2192 Mdefined on some interval I. We already determined in\nSection 4.1 that the first derivative of g is\ng\u2032(t) = \u27e8gradf(c(t)), c\u2032(t)\u27e9c(t) .\nTo obtain a second-order expansion of g, we must differentiate g\u2032.\nA connection \u2207 does not, in a direct way, tell us how to compute this derivative,\nsince (gradf)\u25e6c and c\u2032 are not vector fields on M. Rather, for all t in the domain\nof c, these maps each provide a tangent vector at c(t), smoothly varying with\nt: they are called smooth vector fields on c. We expect that g\u2032\u2032 should involve a\nkind of derivative of these vector fields on c, through a kind of product rule. In\nshort: we need a derivative for vector fields on c.\nFortunately, a connection\u2207 on a manifold M induces a notion of derivative of\nvector fields along curves, with natural properties. The proof of that statement\nbelow involves local frames, which we discussed in Section 3.9. Readers who\nskipped that section may want to consider Proposition 5.31 instead: that one is\nlimited to Riemannian submanifolds but its proof does not require local frames.\nDefinition 5.28. Let c: I \u2192 Mbe a smooth curve on M defined on an open\ninterval I. A map Z : I \u2192 TM is a vector field on c if Z(t) is in Tc(t)M for all\nt \u2208 I. Moreover, Z is a smooth vector field on c if it is also smooth as a map\nfrom I to TM. The set of smooth vector fields on c is denoted by X(c).\nTheorem 5.29. Let c: I \u2192 Mbe a smooth curve on a manifold equipped with\na connection \u2207. There exists a unique operator D\ndt : X(c) \u2192 X(c) which satisfies\nthe following properties for all Y, Z\u2208 X(c), U \u2208 X(M), g\u2208 F(I), and a, b\u2208 R:\n1. R-linearity: D\ndt (aY + bZ) = a D\ndt Y + b D\ndt Z;\n2. Leibniz rule: D\ndt (gZ) = g\u2032Z + g D\ndt Z;\n3. Chain rule:\n\u0000D\ndt (U \u25e6 c)\n\u0001\n(t) = \u2207c\u2032(t)U for all t \u2208 I.\nWe call D\ndt the induced covariant derivative (induced by \u2207). If moreover M is a\nRiemannian manifold and \u2207 is compatible with its metric \u27e8\u00b7, \u00b7\u27e9 (e.g., if \u2207 is the\nRiemannian connection), then the induced covariant derivative also satisfies:\n4. Product rule: d\ndt \u27e8Y, Z\u27e9 =\n\nD\ndt Y , Z\n\u000b\n+\n\nY, D\ndt Z\n\u000b\n,\nwhere \u27e8Y, Z\u27e9 \u2208F(I) is defined by \u27e8Y, Z\u27e9(t) = \u27e8Y (t), Z(t)\u27e9c(t).\nBefore moving on to the proof, a comment is in order. In light of the chain\nrule (property 3), one may wonder why we need to define D\ndt at all: can it not\nalways be computed through an application of \u2207? The key is that not all vector\nfields Z \u2208 X(c) are of the form U \u25e6 c for some U \u2208 X(M). Indeed, consider a\nsmooth curve c such that c(t1) = c(t2) = x (it crosses itself). It could well be\nthat Z(t1) \u0338= Z(t2). Then, we would not know how to define U(x): should it be\nequal to Z(t1) or Z(t2)? For that reason, we really do need to introduce D\ndt as a\nseparate concept.\nProof of Theorem 5.29. We first prove uniqueness under properties 1\u20133. Pick an", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3092, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cad3c2f8-37af-4b77-a595-1c68e809f1a3": {"__data__": {"id_": "cad3c2f8-37af-4b77-a595-1c68e809f1a3", "embedding": null, "metadata": {"page_label": "102", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eee46f57-c693-448f-9e7c-adb9abb485dd", "node_type": "4", "metadata": {"page_label": "102", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "c85debf9d6592b52f4675749be6a01203dd5846c352bd801d2350c2ad37f845b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n102 Embedded geometry: second order\narbitrary \u00aft \u2208 I. There exists a local frame W1, . . . , Wn \u2208 X(U) defined on a\nneighborhood U of c(\u00aft) in M (see Proposition 3.69). Since c is continuous, J =\nc\u22121(U) is an open subset of I which contains \u00aft. Furthermore, by the properties\nof local frames, there exist unique smooth functions g1, . . . , gn : J \u2192 R such that\n\u2200t \u2208 J, Z (t) = g1(t)W1(c(t)) + \u00b7 \u00b7\u00b7+ gn(t)Wn(c(t)).\nUsing the first two properties of the covariant derivative D\ndt , we get\n\u2200t \u2208 J, D\ndtZ(t) =\nnX\ni=1\ng\u2032\ni(t)Wi(c(t)) + gi(t) D\ndt(Wi \u25e6 c)(t).\nNow using the third property, we find\n\u2200t \u2208 J, D\ndtZ(t) =\nnX\ni=1\ng\u2032\ni(t)Wi(c(t)) + gi(t)\u2207c\u2032(t)Wi. (5.15)\n(As a technicality, see the discussion around eq. (5.14) for how to interpret\n\u2207c\u2032(t)Wi, considering Wi is only defined locally around c(t).) Expression (5.15)\nis fully determined by the connection \u2207. Since this argument can be repeated\non a neighborhood of each \u00aft in I, it follows that D\ndt is uniquely determined by\nthe connection \u2207 and the three stated properties.\nTo prove existence, simply consider (5.15) as the definition of an operator D\ndt\non a neighborhood of each \u00aft. It is an exercise to verify that this definition satisfies\nproperties 1\u20133. Since we have uniqueness, it is clear that definitions obtained on\noverlapping domains J and J\u2032 are compatible, so that (5.15) prescribes a smooth\nvector field on all of c.\nNow consider the case whereM is a Riemannian manifold and\u2207 is compatible\nwith the Riemannian metric. We prove the 4th property. To this end, expand Y\nin the local frame:\n\u2200t \u2208 J, Y (t) = f1(t)W1(c(t)) + \u00b7 \u00b7\u00b7+ fn(t)Wn(c(t)).\nUsing also the expansion of Z, we have the following identity on J:\n\u27e8Y, Z\u27e9 =\nnX\ni,j=1\nfigj \u27e8Wi \u25e6 c, Wj \u25e6 c\u27e9.\nDifferentiate this with respect to t:\nd\ndt\u27e8Y, Z\u27e9 =\nnX\ni,j=1\n(f\u2032\nigj + fig\u2032\nj) \u27e8Wi \u25e6 c, Wj \u25e6 c\u27e9 + figj\nd\ndt\u27e8Wi \u25e6 c, Wj \u25e6 c\u27e9. (5.16)\nOn the other hand, by uniqueness we know that (5.15) is a valid expression for\nD\ndt Z so that\n\u001c\nY, D\ndtZ\n\u001d\n=\nnX\ni,j=1\nfig\u2032\nj \u27e8Wi \u25e6 c, Wj \u25e6 c\u27e9 + figj \u27e8Wi \u25e6 c, \u2207c\u2032Wj\u27e9.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2272, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7f59925b-0470-4fc5-916c-c67299362a2b": {"__data__": {"id_": "7f59925b-0470-4fc5-916c-c67299362a2b", "embedding": null, "metadata": {"page_label": "103", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "43044aa5-2546-47c5-8e1e-f1aeb73e1cfc", "node_type": "4", "metadata": {"page_label": "103", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "738d3e51dc3c188e7cc16e06f22c295c7254c9d1ecbce5fa5b1111614e670503", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n5.7 Differentiating vector fields on curves 103\nSimilarly,\n\u001cD\ndtY , Z\n\u001d\n=\nnX\ni,j=1\nf\u2032\nigj \u27e8Wi \u25e6 c, Wj \u25e6 c\u27e9 + figj \u27e8\u2207c\u2032Wi, Wj \u25e6 c\u27e9.\nSumming up these identities and comparing to (5.16), we find that property 4\nholds if\nd\ndt\u27e8Wi \u25e6 c, Wj \u25e6 c\u27e9 = \u27e8\u2207c\u2032Wi, Wj \u25e6 c\u27e9 + \u27e8Wi \u25e6 c, \u2207c\u2032Wj\u27e9.\nThis is indeed the case owing to compatibility of \u2207 with the metric, since\nd\ndt\n\u0000\n\u27e8Wi, Wj\u27e9 \u25e6c\n\u0001\n(t)\nis the directional derivative of \u27e8Wi, Wj\u27e9 at c(t) along c\u2032(t).\nExample 5.30. Let f be a smooth function on a Riemannian manifold M\nequipped with the Riemannian connection \u2207 and induced covariant derivative\nD\ndt . Applying the chain rule property of Theorem 5.29 to Definition 5.14 for the\nRiemannian Hessian, we get the following expression:\nHessf(x)[u] = \u2207ugradf = D\ndtgradf(c(t))\n\f\f\f\f\nt=0\n, (5.17)\nwhere c: I \u2192 Mis any smooth curve such that c(0) = x and c\u2032(0) = u. This is\ntrue in particular with c(t) = Rx(tu) for any retraction R on M.\nFor the special case where \u2207 is the connection defined by (5.4) on a manifold\nM embedded in a Euclidean space E, the induced covariant derivative admits a\nparticularly nice expression. Consider a smooth curve c: I \u2192 M. We can also\nthink of it as a smooth curve c: I \u2192 E. Thus, a vector field Z along c on M is\nsmooth exactly if it is smooth as a vector field along c in E. As a result, it makes\nsense to write d\ndt Z to denote the classical (or extrinsic) derivative of Z in the\nembedding space E. We are about to show that the operator D\ndt : X(c) \u2192 X(c)\ndefined by\nD\ndtZ(t) = Projc(t)\n\u0012 d\ndtZ(t)\n\u0013\n(5.18)\nis the covariant derivative induced by \u2207. Thus, similarly to \u2207 (5.4), it suffices\nto take a classical derivative in the embedding space, followed by an orthogonal\nprojection to the tangent spaces. In particular, if M is (an open subset of) a\nlinear space, then D\ndt Z = d\ndt Z, as expected.\nProposition 5.31. Let M be an embedded submanifold of a Euclidean space E\nwith connection \u2207 as in (5.4). The operator D\ndt defined by (5.18) is the induced\ncovariant derivative, that is, it satisfies properties 1\u20133 in Theorem 5.29. If M is\na Riemannian submanifold of E, then D\ndt also satisfies property 4 in that same\ntheorem.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2400, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "75012f6a-cf08-4d1d-9fd0-333865667432": {"__data__": {"id_": "75012f6a-cf08-4d1d-9fd0-333865667432", "embedding": null, "metadata": {"page_label": "104", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1d86a1ee-65eb-45b8-afc3-9e57273f4fe8", "node_type": "4", "metadata": {"page_label": "104", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "50d1a78c91dedd7362f2c810c7e59584e3c950cba8ee84dedf483f4e3ff7513a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n104 Embedded geometry: second order\nProof. Properties 1 and 2 follow directly from linearity of projectors. For the\nchain rule, consider U \u2208 X(M) with smooth extension \u00afU, and Z(t) = U(c(t)) =\n\u00afU(c(t)). Then, d\ndt Z(t) = D \u00afU(c(t))[c\u2032(t)] = \u00af\u2207c\u2032(t) \u00afU with \u00af\u2207 the Riemannian con-\nnection on E. It follows from (5.5) that\nD\ndtZ(t) = Projc(t)\n\u0000\u00af\u2207c\u2032(t) \u00afU\n\u0001\n= \u2207c\u2032(t)U,\nas desired for property 3.\nProperty 4 follows as a consequence of Theorem 5.29, but we verify it explicitly\nanyway because this is an important special case and because the proof below\ndoes not require local frames. Let \u27e8\u00b7, \u00b7\u27e9 be the Euclidean metric. Consider two\nvector fields Y, Z\u2208 X(c). Differentiate the function t 7\u2192 \u27e8Y (t), Z(t)\u27e9 treating\nY, Zas vector fields along c in E:\nd\ndt\u27e8Y, Z\u27e9 =\n\u001c d\ndtY , Z\n\u001d\n+\n\u001c\nY, d\ndtZ\n\u001d\n.\nSince Z is tangent to M, Z = Proj cZ (and similarly for Y ). Now using that\nProj is self-adjoint, we have\nd\ndt\u27e8Y, Z\u27e9 =\n\u001c d\ndtY ,ProjcZ\n\u001d\n+\n\u001c\nProjcY ,d\ndtZ\n\u001d\n=\n\u001c\nProjc\nd\ndtY , Z\n\u001d\n+\n\u001c\nY, Projc\nd\ndtZ\n\u001d\n=\n\u001cD\ndtY , Z\n\u001d\n+\n\u001c\nY, D\ndtZ\n\u001d\n.\nConclude using that \u27e8\u00b7, \u00b7\u27e9 is the metric both in the embedding space and on M\nsince M is a Riemannian submanifold of E.\nExample 5.32. For a smooth function f on a Riemannian submanifold M of\na Euclidean space E, we can apply (5.18) to (5.17) to find\nHessf(x)[u] = Projx\n\u0012\nlim\nt\u21920\ngradf(c(t)) \u2212 gradf(c(0))\nt\n\u0013\n= lim\nt\u21920\nProjx(gradf(c(t))) \u2212 gradf(x)\nt , (5.19)\nwhere the subtraction makes sense because gradf(c(t)) is an element of the linear\nembedding space E for all t. This holds for any smooth curve c such that c(0) = x\nand c\u2032(0) = u. Picking a retraction curve for example, this justifies the claim that,\nfor some aptly chosen \u00aft >0,\nHessf(x)[u] \u2248 Projx(gradf(Rx(\u00aftu))) \u2212 gradf(x)\n\u00aft . (5.20)\nThis is a finite difference approximation of the Hessian. Assuming gradf(x) is\nreadily available, it affords us a straightforward way to approximate Hessf(x)[u]\nfor the computational cost of one retraction, one gradient evaluation, and one\nprojection. The parameter \u00aft should be small enough for the mathematical approx-\nimation to be accurate, yet large enough to avoid catastrophic numerical errors.\nWe revisit this concept in more generality in Section 10.6.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2441, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7bd8f7b0-ab98-41ba-9886-896f117f479e": {"__data__": {"id_": "7bd8f7b0-ab98-41ba-9886-896f117f479e", "embedding": null, "metadata": {"page_label": "105", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d02052a-34ea-4819-891d-11a2312c1828", "node_type": "4", "metadata": {"page_label": "105", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "2267ee2c6d2dda3c5234555eef173f2e716155501b28dcab5d0e6c6771071ef9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n5.8 Acceleration and geodesics 105\nExercise 5.33. In the proof of Theorem 5.29, show that the operator (5.15)\nsatisfies properties 1\u20133.\nExercise 5.34. Continuing from Exercise 5.4, show that if \u2207(1), \u2207(2) are con-\nnections on M1, M2 with induced covariant derivatives D\ndt\n(1)\n, D\ndt\n(2)\n(respectively),\nthen the covariant derivative D\ndt induced on M1 \u00d7M2 by the product connection\n\u2207 (5.6) is given simply by:\nD\ndtZ(t) =\n \nD\ndt\n(1)\nZ1(t), D\ndt\n(2)\nZ2(t)\n!\n, (5.21)\nwhere Z = (Z1, Z2) is a smooth vector field along a curve c = (c1, c2) on M1 \u00d7\nM2, so that Z1, Z2 are smooth vector fields along c1, c2 on M1, M2, respectively.\nIn order to verify the chain rule property with a smooth vector field U = (U1, U2)\non M1 \u00d7 M2, it is helpful first to establish that\nD\ndt\n(1)\n(U1 \u25e6 c)(t) = \u2207(1)\nc\u2032\n1(t)U1(\u00b7, c2(t)) + DU1(c1(t), \u00b7)(c2(t))[c\u2032\n2(t)]. (5.22)\n(Likewise for U2.) Hint: expand U1 in a local frame and use Exercise 3.40.\nExercise 5.35. For Z \u2208 X(c), show that D\ndt (Z \u25e6\u03d5)(t) = \u03d5\u2032(t)\n\u0000D\ndt Z\n\u0001\n(\u03d5(t)) where\n\u03d5: R \u2192 I is any smooth reparameterization of c: I \u2192 M.\n5.8 Acceleration and geodesics\nIf the manifold M is equipped with a covariant derivative D\ndt , we can use it to\ndefine the notion of acceleration along a curve on M.\nDefinition 5.36. Let c: I \u2192 Mbe a smooth curve. Its velocity is the vector\nfield c\u2032 \u2208 X(c). The acceleration of c is the smooth vector field c\u2032\u2032 \u2208 X(c) defined\nby:\nc\u2032\u2032 = D\ndtc\u2032.\nWe also call c\u2032\u2032 the intrinsic acceleration of c.\nWhen M is embedded in a linear space E, a curve c on M is also a curve in\nE. It is then convenient to distinguish notationally between the acceleration of\nc on the manifold (as defined above) and the classical acceleration of c in the\nembedding space. We write\n\u00a8c = d2\ndt2 c\nfor the classical or extrinsic acceleration. In that spirit, we use notations c\u2032 and\n\u02d9c interchangeably for velocity since the two notions coincide.\nWhen M is a Riemannian submanifold of E (with the associated Riemannian", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2211, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b3a033e2-b6c1-4305-a6e0-b5e177f68af7": {"__data__": {"id_": "b3a033e2-b6c1-4305-a6e0-b5e177f68af7", "embedding": null, "metadata": {"page_label": "106", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5917bdd7-9ec3-4f32-8bcf-9ff7cc3e279d", "node_type": "4", "metadata": {"page_label": "106", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "b2f4823684aa2e0a533e2a83151cff59486196196f94c9c5949ae44c349397fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n106 Embedded geometry: second order\nconnection), the induced covariant derivative takes on a convenient form (5.18),\nso that\nc\u2032\u2032(t) = Projc(t)(\u00a8c(t)). (5.23)\nWe also state this asc\u2032\u2032 = Projc(\u00a8c) for short. In words: on a Riemannian subman-\nifold, the acceleration of a curve is the tangential part of its extrinsic acceleration\nin the embedding space. See Section 5.11 for a discussion of the normal part.\nExample 5.37. Consider the sphere Sd\u22121 = {x \u2208 Rd : x\u22a4x = 1} equipped with\nthe Riemannian submanifold geometry of Rd with the canonical metric. For a\ngiven x \u2208 Sd\u22121 and v \u2208 TxSd\u22121 (nonzero), consider the curve\nc(t) = cos(t\u2225v\u2225)x + sin(t\u2225v\u2225)\n\u2225v\u2225 v,\nwhich traces a so-called great circle on the sphere. The (extrinsic) velocity and\nacceleration of c in Rd are easily derived:\n\u02d9c(t) = \u2212\u2225v\u2225sin(t\u2225v\u2225)x + cos(t\u2225v\u2225)v,\n\u00a8c(t) = \u2212\u2225v\u22252 cos(t\u2225v\u2225)x \u2212 \u2225v\u2225sin(t\u2225v\u2225)v = \u2212\u2225v\u22252c(t).\nThe velocity c\u2032(t) matches \u02d9c(t). Owing to (5.23), to get the (intrinsic) acceleration\nof c on Sd\u22121, we project:\nc\u2032\u2032(t) = Projc(t)\u00a8c(t) = (Id \u2212 c(t)c(t)\u22a4)\u00a8c(t) = 0.\nThus, c is a curve with zero acceleration on the sphere (even though its acceler-\nation in Rd is nonzero).\nCurves with zero acceleration play a particular role in geometry, as they pro-\nvide a natural generalization of the concept of straight lines t 7\u2192 x + tv from\nlinear spaces to manifolds. Reading the definition below, recall that by default\nwe equip a Riemannian manifold with its Riemannian connection \u2207, which in-\nduces a covariant derivative D\ndt : it is with the latter that c\u2032\u2032 is to be interpreted.\nDefinition 5.38. On a Riemannian manifold M, a geodesic is a smooth curve\nc: I \u2192 Msuch that c\u2032\u2032(t) = 0 for all t \u2208 I, where I is an open interval of R.\nOwing to (5.23), a curvec on a Riemannian submanifold M is a geodesic if and\nonly if its extrinsic acceleration \u00a8c is everywhere normal to M. Geodesics are fur-\nther discussed in Section 10.2\u2014they play a minor role in practical optimization\nalgorithms.\nExercise 5.39. Let c(t) = (c1(t), c2(t)) be a smooth curve on the product man-\nifold M = M1 \u00d7 M2. Its velocity is given by c\u2032(t) = ( c\u2032\n1(t), c\u2032\n2(t)). Equip M1\nand M2 with Riemannian structures, and let M be their Riemannian product\nas in Example 3.57. Argue that c\u2032\u2032(t) = ( c\u2032\u2032\n1(t), c\u2032\u2032\n2(t)), where accelerations are\ndefined with respect to the Riemannian connections. Deduce that c is a geodesic\non M1 \u00d7 M2 if and only if c1, c2 are geodesics on M1, M2, respectively.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2680, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "284a56ea-3c2a-4765-97e8-e97eebca5e7c": {"__data__": {"id_": "284a56ea-3c2a-4765-97e8-e97eebca5e7c", "embedding": null, "metadata": {"page_label": "107", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3196a3f9-f559-4e86-b44c-228d81110463", "node_type": "4", "metadata": {"page_label": "107", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "bed63a7ab7f23b87c6d241d3c8e3ccd450bf535fbc55f438ccd38957f0211865", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n5.9 A second-order Taylor expansion on curves 107\n5.9 A second-order Taylor expansion on curves\nOn a Riemannian manifold M, consider a smooth function f : M \u2192R and a\nsmooth curve c: I \u2192 Mpassing through x at t = 0 with velocity v. In this\nsection, we build a second-order Taylor expansion for the function g = f \u25e6 c, as\nannounced in the introduction of this chapter.\nSince g is a smooth function from I \u2286 R to R, it has a Taylor expansion:\nf(c(t)) = g(t) = g(0) + tg\u2032(0) + t2\n2 g\u2032\u2032(0) + O(t3).\nWe have the tools necessary to investigate the derivatives of g. Indeed,\ng\u2032(t) = Df(c(t))[c\u2032(t)] = \u27e8gradf(c(t)), c\u2032(t)\u27e9c(t) ,\nso that\n(f \u25e6 c)\u2032(0) = g\u2032(0) = \u27e8gradf(x), v\u27e9x . (5.24)\nMoreover, using in turn properties 4 and 3 of Theorem 5.29 regarding the co-\nvariant derivative D\ndt induced by the Riemannian connection \u2207, followed by Def-\ninition 5.14 for the Hessian, we compute:\ng\u2032\u2032(t) = d\ndt\u27e8gradf(c(t)), c\u2032(t)\u27e9c(t)\n(property 4) =\n\u001cD\ndt(gradf \u25e6 c)(t), c\u2032(t)\n\u001d\nc(t)\n+\n\u001c\ngradf(c(t)), D\ndtc\u2032(t)\n\u001d\nc(t)\n(property 3) =\n\n\u2207c\u2032(t)gradf, c\u2032(t)\n\u000b\nc(t) + \u27e8gradf(c(t)), c\u2032\u2032(t)\u27e9c(t)\n= \u27e8Hessf(c(t))[c\u2032(t)], c\u2032(t)\u27e9c(t) + \u27e8gradf(c(t)), c\u2032\u2032(t)\u27e9c(t) .\nEvaluating g\u2032\u2032(t) at t = 0 yields:\n(f \u25e6 c)\u2032\u2032(0) = g\u2032\u2032(0) = \u27e8Hessf(x)[v], v\u27e9x + \u27e8gradf(x), c\u2032\u2032(0)\u27e9x . (5.25)\nThese all combine to form:\nf(c(t)) = f(x) + t \u27e8gradf(x), v\u27e9x + t2\n2 \u27e8Hessf(x)[v], v\u27e9x\n+ t2\n2 \u27e8gradf(x), c\u2032\u2032(0)\u27e9x + O(t3). (5.26)\nTo be clear, formula (5.26) holds for all smooth curves c satisfying c(0) = x and\nc\u2032(0) = v. Of particular interest for optimization is the Taylor expansion of f\nalong a retraction curve. That is the topic of Section 5.10.\nExercise 5.40. For a smooth curve c: [0, 1] \u2192 Mon a Riemannian manifold\nM with c(0) = x and c(1) = y, show that there exists t \u2208 (0, 1) such that\nf(y) = f(x) + \u27e8gradf(x), c\u2032(0)\u27e9x + 1\n2 \u27e8Hessf(c(t))[c\u2032(t)], c\u2032(t)\u27e9c(t)\n+ 1\n2 \u27e8gradf(c(t)), c\u2032\u2032(t)\u27e9c(t) . (5.27)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2104, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ffe91331-33b4-4297-b13b-d924fb5f3f0d": {"__data__": {"id_": "ffe91331-33b4-4297-b13b-d924fb5f3f0d", "embedding": null, "metadata": {"page_label": "108", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "02f2c303-ad6b-47e5-9134-4c25a6e5464f", "node_type": "4", "metadata": {"page_label": "108", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "67011fe05aa3025ec432bd6c09063607f859a66eec7916961fbd1e33bc9dd0ed", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n108 Embedded geometry: second order\n(Hint: use the mean value theorem.) Show that the speed \u2225c\u2032(t)\u2225c(t) of the curve\nc is constant if c is a geodesic. Deduce a proof of Lemma 5.41 below.\nLemma 5.41. Let c(t) be a geodesic connecting x = c(0) to y = c(1), and\nassume Hessf(c(t)) \u2ab0 \u00b5 Id for some \u00b5 \u2208 R and all t \u2208 [0, 1]. Then,\nf(y) \u2265 f(x) + \u27e8gradf(x), v\u27e9x + \u00b5\n2 \u2225v\u22252\nx.\nHere is some context for Lemma 5.41. If x \u2208 Mis such that gradf(x) = 0 and\nHessf(x) \u2ab0 \u00b5\u2032 Id for some \u00b5\u2032 > 0, then by continuity of eigenvalues there exists\na neighborhood U of x in which Hessf(z) \u2ab0 \u00b5 Id for all z \u2208 Uand some \u00b5 >0. If\nU is appropriately chosen, the lemma implies that x is the unique critical point\nin U, and it is the global minimizer in that set (hence an isolated local minimizer\nfor f on all of M). This is relevant in connection with Chapter 11 about geodesic\nconvexity: the neighborhood can be chosen to be a geodesically convex geodesic\nball, and f restricted to that ball is \u00b5-strongly convex, in a geodesic sense. This\ncan ease the study of the local convergence behavior of optimization algorithms\nnear isolated local minimizers by paralleling Section 11.5.\n5.10 Second-order retractions\nContinuing from the Taylor expansion (5.26) established above, we consider the\nimportant case where c is a retraction curve, that is,\nc(t) = Rx(tv)\nfor a point x \u2208 Mand a vector v \u2208 TxM. A direct application of (5.26) yields\nf(Rx(tv)) = f(x) + t \u27e8gradf(x), v\u27e9x + t2\n2 \u27e8Hessf(x)[v], v\u27e9x\n+ t2\n2 \u27e8gradf(x), c\u2032\u2032(0)\u27e9x + O(t3). (5.28)\nThe last term involving the acceleration of c at t = 0 is undesirable, as it is of\norder t2 and depends on the retraction. Fortunately, it vanishes if grad f(x) = 0\nor c\u2032\u2032(0) = 0. The latter happens in particular ifc is a geodesic. Retractions whose\ncurves are geodesics are studied later in Section 10.2: they are called exponential\nmaps. More generally though, notice that we only need the acceleration to vanish\nat t = 0. This suggests the following definition.\nDefinition 5.42. A second-order retraction R on a Riemannian manifold M is\na retraction such that, for all x \u2208 Mand all v \u2208 TxM, the curve c(t) = Rx(tv)\nhas zero acceleration at t = 0, that is, c\u2032\u2032(0) = 0.\nSecond-order retractions are not hard to come by: see Section 5.12 for a com-\nmon construction that works on Riemannian submanifolds. The following exam-\nple illustrates that construction on the sphere.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2643, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b6ac1c45-e50f-422b-a938-6804d4058eb7": {"__data__": {"id_": "b6ac1c45-e50f-422b-a938-6804d4058eb7", "embedding": null, "metadata": {"page_label": "109", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60249dc1-fc98-4ebf-9d18-1f7d43673985", "node_type": "4", "metadata": {"page_label": "109", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "398aeaabbc5d5dc3ee6430338a5feb422319bb78a07c37414ff05ecefcea973c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n5.10 Second-order retractions 109\nExample 5.43. Consider the following retraction on the sphere Sd\u22121:\nRx(v) = x + v\n\u2225x + v\u2225.\nThat retraction is second order. Indeed, with c(t) = Rx(tv):\nc(t) = x + tvp\n1 + t2\u2225v\u22252 =\n\u0012\n1 \u2212 1\n2\u2225v\u22252t2 + O(t4)\n\u0013\n(x + tv),\n\u02d9c(t) = \u2212\u2225v\u22252t(x + tv) +\n\u0012\n1 \u2212 1\n2\u2225v\u22252t2\n\u0013\nv + O(t3),\n\u00a8c(t) = \u2212\u2225v\u22252(x + tv) \u2212 \u2225v\u22252tv \u2212 \u2225v\u22252tv + O(t2)\n= \u2212\u2225v\u22252(x + 3tv) + O(t2).\nOf course, c\u2032(0) = \u02d9c(0) = v. As for acceleration, \u00a8c(0) = \u2212\u2225v\u22252x, so that\nc\u2032\u2032(0) = Projx(\u00a8c(0)) = 0,\nas announced.\nWe summarize two important particular cases of the Taylor expansion (5.28)\ninto a useful statement regarding the pullback f \u25e6 Rx.\nProposition 5.44. Consider a Riemannian manifold M equipped with any re-\ntraction R, and a smooth function f : M \u2192R. If x is a critical point of f (that\nis, if gradf(x) = 0), then\nf(Rx(s)) = f(x) + 1\n2 \u27e8Hessf(x)[s], s\u27e9x + O(\u2225s\u22253\nx). (5.29)\nAlso, if R is a second-order retraction, then for all points x \u2208 Mwe have\nf(Rx(s)) = f(x) + \u27e8gradf(x), s\u27e9x + 1\n2 \u27e8Hessf(x)[s], s\u27e9x + O(\u2225s\u22253\nx). (5.30)\nProof. Simply rewrite (5.28) with s = tv.\nThat the first identity holds for all retractions is useful to study the behavior\nof optimization algorithms at or close to critical points.\nProposition 5.44 suggests an alternative way to compute the Riemannian Hes-\nsian. Indeed, the direct way is to use the definition as we did in Example 5.17.\nThis requires computing with the Riemannian connection, which may not be\nstraightforward for general manifolds. If a second-order retraction is on hand or\nif we are only interested in the Hessian at critical points, an alternative is to\nuse the next result. In practical terms, it suggests to compose f with Rx (which\nyields a smooth function from a linear space to the reals), then to compute the\nHessian of the latter in the usual way. This echoes Proposition 3.59 stating that\ngradf(x) = grad(f \u25e6 Rx)(0).\nProposition 5.45. If the retraction is second order or if gradf(x) = 0, then\nHessf(x) = Hess(f \u25e6 Rx)(0),", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2225, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a394bdc2-adac-475e-b215-93a6d43cf05d": {"__data__": {"id_": "a394bdc2-adac-475e-b215-93a6d43cf05d", "embedding": null, "metadata": {"page_label": "110", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5aa575d7-f034-40fc-b742-8a0b1b04e100", "node_type": "4", "metadata": {"page_label": "110", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "8cf480a937d5f98b5cae73983af4c8b309a2899db401914d33c2bf11f01a7193", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n110 Embedded geometry: second order\nwhere the right-hand side is the Hessian of f \u25e6 Rx : TxM \u2192R at 0 \u2208 TxM.\nThe latter is a \u201cclassical\u201d Hessian since TxM is a Euclidean space. See also\nExercise 10.73 for the Hessian of f \u25e6 Rx away from the origin.\nProof. If R is second order, expand \u02c6fx(s) = f(Rx(s)) using (5.30):\n\u02c6fx(s) = f(x) + \u27e8gradf(x), s\u27e9x + 1\n2 \u27e8Hessf(x)[s], s\u27e9x + O(\u2225s\u22253\nx).\nThe gradient and Hessian of \u02c6fx : TxM \u2192R with respect to s follow easily, using\nthe fact that Hess f(x) is self-adjoint:\ngrad \u02c6fx(s) = gradf(x) + Hessf(x)[s] + O(\u2225s\u22252\nx), and\nHess \u02c6fx(s)[ \u02d9s] = Hessf(x)[ \u02d9s] + O(\u2225s\u2225x\u2225\u02d9s\u2225x).\nEvaluating at s = 0 yields Hess \u02c6fx(0) = Hess f(x), as announced. The proof is\nsimilar if x is a critical point, starting with (5.29).\nWe close this section with a remark. Recall that critical points of a function\nand (first-order) retractions are defined on a manifold M independently of any\nRiemannian structure on M. Proposition 5.45 further tells us that, at a critical\npoint x, the Riemannian Hessian of a function onM depends on the Riemannian\nmetric only through the inner product on T xM. In particular, the signature of\nthe Hessian at x, that is, the number of positive, zero and negative eigenvalues,\nis independent of the Riemannian structure.\nExercise 5.46. Let R be a retraction on a manifold M, and fix x \u2208 M. Consider\na smooth curve w: I \u2192 TxM in the tangent space at x such that w(0) = 0. This\ninduces a smooth curve c(t) = R x(w(t)) on M. Of course, c(0) = x. It is also\neasy to confirm that c\u2032(0) = w\u2032(0).\nShow that if M is Riemannian and R is a second-order retraction, then we\nalso have c\u2032\u2032(0) = w\u2032\u2032(0). Hint: expand the differential DRx(v) for v close to the\norigin using a local frame around x, for example, as provided by Exercise 3.72.\nThen use the properties of D\ndt and \u2207 to compute c\u2032\u2032(t) and work out c\u2032\u2032(0).\nHere is one take-away: for all u, v\u2208 TxM we can create a curve c on M such\nthat c\u2032(0) = u and c\u2032\u2032(0) = v as c(t) = Rx\n\u0010\ntu + t2\n2 v\n\u0011\n. This is always doable since\nthe exponential map is a second-order retraction (Section 10.2).\n5.11 Special case: Riemannian submanifolds*\nThe special case where M is a Riemannian submanifold of a Euclidean space\nE merits further attention. Consider a smooth function f : M \u2192R and a point\nx \u2208 Mtogether with a tangent vector u \u2208 TxM. Let \u00aff be a smooth extension of\nf to a neighborhood of M in E, and let c be any smooth curve on M satisfying\nc(0) = x and c\u2032(0) = u. We know from Proposition 3.61 that\ngradf(c(t)) = Projc(t)(grad \u00aff(c(t))), (5.31)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2784, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e8712ab1-4cff-4e76-ad5f-e08e18190c70": {"__data__": {"id_": "e8712ab1-4cff-4e76-ad5f-e08e18190c70", "embedding": null, "metadata": {"page_label": "111", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d3aea701-7ec6-496a-b362-fa9fe9b299c3", "node_type": "4", "metadata": {"page_label": "111", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "6dd73c1b1ec2dde0f567a72b6aee1fc9fc75a17f1139e9d9603cb6c9670cdc2b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n5.11 Special case: Riemannian submanifolds* 111\nwhere Proj x is the orthogonal projector from E to TxM. We know from Exer-\ncise 3.66 thatx 7\u2192 Projx is smooth. Starting from here, formulas (5.17) and (5.18)\ncombine to yield:\nHessf(x)[u] = D\ndtgradf(c(t))\n\f\f\f\f\nt=0\n= Projx\n\u0012 d\ndtProjc(t)(grad \u00aff(c(t)))\n\f\f\f\f\nt=0\n\u0013\n= Projx\n\u0012 d\ndtProjc(t)\n\f\f\f\f\nt=0\n(grad \u00aff(x))\n\u0013\n+ Projx\n\u0012\nProjx\n\u0012 d\ndtgrad \u00aff(c(t))\n\f\f\f\f\nt=0\n\u0013\u0013\n. (5.32)\nThis simplifies noting that Proj x \u25e6 Projx = Projx. Let us introduce notation for\nthe differentials of the projector, based on Definition 3.34:\nPu \u225c D(x 7\u2192 Projx)(x)[u] = d\ndtProjc(t)\n\f\f\f\f\nt=0\n. (5.33)\nIntuitively, that differential measures how the tangent spaces of M vary, that\nis, how M \u201cbends\u201d in its embedding space. Plugging this notation in (5.32), we\ncan write the Riemannian Hessian as follows:\nHessf(x)[u] = Projx\n\u0000\nPu(grad \u00aff(x))\n\u0001\n+ Projx\n\u0000\nHess \u00aff(x)[u]\n\u0001\n. (5.34)\nIt is instructive to investigatePu more closely. To this end, letP(t) = Projc(t). In\nparticular, P(0) = Projx and P\u2032(0) = Pu. By definition of projectors, P(t)P(t) =\nP(t) for all t. Differentiate with respect to t to find that P\u2032(t)P(t) +P(t)P\u2032(t) =\nP\u2032(t) for all t. At t = 0, this reveals a useful identity:\nPu = Pu \u25e6 Projx + Projx \u25e6 Pu. (5.35)\nLet Proj \u22a5\nx = Id \u2212Projx denote the orthogonal projector to the normal space\nNxM, that is, the orthogonal complement of T xM in E. Then, the identity\nabove can be reorganized in two ways to find:\nPu \u25e6 Proj\u22a5\nx = Projx \u25e6 Pu and Pu \u25e6 Projx = Proj\u22a5\nx \u25e6 Pu. (5.36)\nCombining (5.34) and (5.36) warrants the following statement.\nCorollary 5.47. Let M be a Riemannian submanifold of a Euclidean space E.\nFor a smooth function f : M \u2192R with smooth extension \u00aff to a neighborhood of\nM in E, the Riemannian Hessian of f is given by:\nHessf(x)[u] = Projx(Hess \u00aff(x)[u]) + Pu(Proj\u22a5\nx (grad \u00aff(x))),\nwhere Pu is the differential of x 7\u2192 Projx at x along u and Proj\u22a5\nx = Id \u2212Projx.\nThis casts the Riemannian Hessian as the projected Euclidean Hessian of a\nsmooth extension, plus a correction term which depends on that extension only\nthrough the normal part of its Euclidean gradient.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2367, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "220f2e28-39f5-4311-a3ca-14822f721676": {"__data__": {"id_": "220f2e28-39f5-4311-a3ca-14822f721676", "embedding": null, "metadata": {"page_label": "112", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d15f9ed-7204-4e0b-a1e6-2099182190d9", "node_type": "4", "metadata": {"page_label": "112", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "a5aaa9f32756d06a0b86501ba1a6ed3103477f802b5ae7b01c3f95696f21d6be", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n112 Embedded geometry: second order\nEvidently, the differential of the projector captures important aspects of the\ngeometry of M as judged by the embedding space E. We introduce two new\nobjects that contain the relevant information.\nFrom (5.36), we find that if v \u2208 TxM is a tangent vector at x, then v =\nProjx(v) so that\nPu(v) = Pu(Projx(v)) = Proj\u22a5\nx (Pu(v)).\nNotice how if v is tangent at x then Pu(v) is normal at x. Likewise, if w \u2208 NxM\nis a normal vector at x, then w = Proj\u22a5\nx (w) so that\nPu(w) = Pu(Proj\u22a5\nx (w)) = Projx(Pu(w)).\nThe output is necessarily a tangent vector at x. These considerations motivate\nus to define two special bilinear maps.\nDefinition 5.48. Let M be a Riemannian submanifold of a Euclidean space\nE. At a point x \u2208 M, the normal space NxM is the orthogonal complement of\nTxM in E. The second fundamental form at x is the map:\nII: T xM \u00d7TxM \u2192NxM: (u, v) 7\u2192 II(u, v) = Pu(v). (5.37)\n(Read \u201ctwo\u201d for II.) The Weingarten map at x is the map:\nW : TxM \u00d7NxM \u2192TxM: (u, w) 7\u2192 W(u, w) = Pu(w). (5.38)\nFor both, the map Pu : E \u2192 Eis defined by (5.33).\nThese two objects describe Pu fully. Indeed, for all z \u2208 Ewe can decompose\nPu(z) as follows:\nPu(z) = II(u, Projx(z)) + W(u, Proj\u22a5\nx (z)). (5.39)\nThe maps II and W are further related through the inner product \u27e8\u00b7, \u00b7\u27e9, which\ndenotes both the Euclidean inner product and the Riemannian metric at x since\nM is a Riemannian submanifold of E. Indeed, for all u, v\u2208 TxM and w \u2208 NxM\nit holds that\n\u27e8II(u, v), w\u27e9 = \u27e8Pu(v), w\u27e9 = \u27e8v, Pu(w)\u27e9 = \u27e8v, W(u, w)\u27e9. (5.40)\nThe middle equality above uses that Pu is self-adjoint on E.\nGoing back to Corollary 5.47, we get an identity for the Hessian:\nHessf(x)[u] = Projx(Hess \u00aff(x)[u]) + W(u, Proj\u22a5\nx (grad \u00aff(x))). (5.41)\nIn bilinear form on T xM, combining with (5.40) we also have\n\u27e8v, Hessf(x)[u]\u27e9 =\n\nv, Hess \u00aff(x)[u]\n\u000b\n+ \u27e8v, W(u, Proj\u22a5\nx (grad \u00aff(x)))\u27e9\n=\n\nv, Hess \u00aff(x)[u]\n\u000b\n+ \u27e8II(u, v), grad \u00aff(x)\u27e9. (5.42)\nIn this last identity, it is still clear that only the normal part of grad \u00aff(x) plays\na role since II( u, v) is normal at x.\nWhile it is not obvious from the definition (5.37), we may surmise from (5.42)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2373, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c6ac63f2-d1ca-4840-980d-20b12c9a561b": {"__data__": {"id_": "c6ac63f2-d1ca-4840-980d-20b12c9a561b", "embedding": null, "metadata": {"page_label": "113", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e204b58e-60cb-4908-b049-2c171e8bab72", "node_type": "4", "metadata": {"page_label": "113", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "35a62510404d8350d0b5300fe75daf086c2c319af429ad9f592ea28a805177ca", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n5.11 Special case: Riemannian submanifolds* 113\nthat II is symmetric in its inputs. This is indeed the case, as we show through a\ncouple of lemmas.\nLemma 5.49. Let U, Vbe two smooth vector fields on a manifold M embedded\nin E. Let \u00afU, \u00afV be smooth extensions of U, Vto a neighborhood O of M. Then,\n[ \u00afU, \u00afV ] is a smooth extension of [U, V] to the same neighborhood O.\nProof. This is a by-product of the proof of Theorem 5.8, with the hindsight that\nthe Lie bracket of two vector fields is a vector field (Proposition 5.10).\nLemma 5.50 (Gauss formula) . Let M be a Riemannian submanifold of E,\nrespectively endowed with their Riemannian connections \u2207 and \u00af\u2207. Let V be a\nsmooth vector field on M with smooth extension \u00afV . For all (x, u) \u2208 TM, the\nvector \u00af\u2207u \u00afV splits in a tangent and a normal part to M at x as:\n\u00af\u2207u \u00afV = Projx( \u00af\u2207u \u00afV ) + Proj\u22a5\nx ( \u00af\u2207u \u00afV ) = \u2207uV + II(u, v),\nwhere v = V (x) = \u00afV (x).\nProof. We already know that Projx( \u00af\u2207u \u00afV ) = \u2207uV (Theorem 5.9). It remains to\nshow that Proj\u22a5\nx ( \u00af\u2207u \u00afV ) = II(u, v). This is clear from the following computation,\nwhere we let P(t) = Proj c(t) along a smooth curve c on M with c(0) = x and\nc\u2032(0) = u:\nProj\u22a5\nx ( \u00af\u2207u \u00afV ) = Proj\u22a5\nx\n\u0012 d\ndt\n\u00afV (c(t))\n\f\f\f\f\nt=0\n\u0013\n= Proj\u22a5\nx\n\u0012 d\ndtP(t)( \u00afV (c(t)))\n\f\f\f\f\nt=0\n\u0013\n= Proj\u22a5\nx\n\u0000\nPu(v) + Projx( \u00af\u2207u \u00afV )\n\u0001\n= Pu(Projx(v)))\n= II(u, v).\nIn the second to last step, we used (5.36) and Proj \u22a5\nx \u25e6 Projx = 0.\nProposition 5.51. For all u, v\u2208 TxM it holds that II(u, v) = II(v, u).\nProof. Pick U, V\u2208 X(M) such that U(x) = u and V (x) = v; for example, let\nU(y) = Proj y(u) and V (y) = Proj y(v). Let \u00afU, \u00afV be two smooth extensions for\nthem. Lemma 5.50 yields\nII(u, v) \u2212 II(v, u) = Proj\u22a5\nx ( \u00af\u2207u \u00afV \u2212 \u00af\u2207v \u00afU) = Proj\u22a5\nx ([ \u00afU, \u00afV ](x)).\nConclude with Lemma 5.49 which tells us [ \u00afU, \u00afV ](x) is tangent at x.\nLemma 5.50 has a counter-part for the covariant derivatives of a smooth vector\nfield Z along a curve c on M. The derivative of Z in the embedding space splits\nin a tangent and a normal part:\nd\ndtZ(t) = D\ndtZ(t) + II(c\u2032(t), Z(t)). (5.43)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2282, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "20bd6444-cb51-4a08-b2bf-114a6a6b3900": {"__data__": {"id_": "20bd6444-cb51-4a08-b2bf-114a6a6b3900", "embedding": null, "metadata": {"page_label": "114", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "39767329-45db-4aaf-95f0-a6f2285695dd", "node_type": "4", "metadata": {"page_label": "114", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "fc6a9fc9d8bfa261cbcfdcb8204eb885b0c86b2c69814b0edbc065a368206904", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n114 Embedded geometry: second order\nParticularized to Z = c\u2032, it follows that the extrinsic acceleration of c (denoted\nby \u00a8c) and the intrinsic acceleration of c (denoted by c\u2032\u2032) satisfy\n\u00a8c(t) = c\u2032\u2032(t) + II(c\u2032(t), c\u2032(t)). (5.44)\nThis splits \u00a8c into tangent and normal parts. In particular, it makes clear the fact\nthat c is a geodesic (that is, c\u2032\u2032 is identically zero) if and only if \u00a8c is normal at\nall times. For a geodesic \u03b3 on M satisfying \u03b3(0) = x and \u03b3\u2032(0) = u we know\nthat \u03b3\u2032\u2032(0) = 0 so that \u00a8 \u03b3(0) = II( u, u). This gives meaning to II( u, u) as the\nextrinsic acceleration of the geodesic \u03b3, in the embedding space. This informs us\nregarding extrinsic curvature of M in its embedding space, and may be useful\nto interpret (5.42).\nExercise 5.52. Give a proof for formula (5.43).\n5.12 Special case: metric projection retractions*\nLet E be a Euclidean space with the inner product \u27e8\u00b7, \u00b7\u27e9 and associated norm\n\u2225 \u00b7 \u2225. For an embedded submanifold M, it is natural to consider the following as\na tentative retraction, with ( x, v) \u2208 TM:\nRx(v) = arg min\nx\u2032\u2208M\n\u2225x\u2032 \u2212 (x + v)\u2225. (5.45)\nIn fact, several of the retractions discussed in Chapter 7 are of that form. In this\nsection, we argue that (5.45) indeed defines a retraction (albeit not necessarily\non the whole tangent bundle) and that this retraction is second order if M is a\nRiemannian submanifold of E.\nLet distM : E \u2192R denote the distance from a point of E to M:\ndistM(y) = inf\nx\u2208M\n\u2225x \u2212 y\u2225. (5.46)\nFor a given y \u2208 Ethe set\nPM(y) = {x \u2208 M: \u2225x \u2212 y\u2225 = distM(y)}\nis the metric projection or nonlinear orthogonal projection of y to M. It may be\nempty, or it may contain one or more points.\nLet A \u2286 Ebe the set of points y \u2208 Efor which PM(y) is a singleton, that is, for\nwhich there exists a unique point x \u2208 Mwhich is closest to y. It is an exercise\nto show that A may be neither open nor closed. However, strong properties hold\non the interior of A, that is, on the largest subset of A which is open in E. We\nstate the following theorem without proof: see Section 5.13 for references.\nTheorem 5.53. Let M be an embedded submanifold of E. Let A \u2286 Ebe the\ndomain where PM is single-valued, and let \u2126 denote the interior of A.\n1. For y \u2208 A and x = PM(y), we have that y \u2212 x is orthogonal to TxM and\n{x + t(y \u2212 x) : t \u2208 [0, 1)} \u2282\u2126. In particular, M \u2282\u2126.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2558, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "317d594a-7f69-4672-9d79-4054f4b9196e": {"__data__": {"id_": "317d594a-7f69-4672-9d79-4054f4b9196e", "embedding": null, "metadata": {"page_label": "115", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "31c20893-d85e-4963-a8f5-e5285b504299", "node_type": "4", "metadata": {"page_label": "115", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "e4daa5dadc1b538f0be9e78039427c6420d420514cf3c8db41281761ff9c4b35", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n5.12 Special case: metric projection retractions* 115\n2. \u2126 is dense in A; if M is closed in E, then the closure of \u2126 equals E.\n3. The restriction PM : \u2126 \u2192 Mis smooth, and DPM(x) = Projx for all x \u2208 M\n(the orthogonal projector from E to TxM).\nNow consider the following subset of the tangent bundle of M:\nO = {(x, v) \u2208 TM : x + v \u2208 \u2126}. (5.47)\nIt is open since the map (x, v) 7\u2192 x + v is continuous (in fact, smooth) from TM\nto E. Moreover, O contains all pairs ( x, 0) \u2208 TM since M is included in \u2126. We\nnow argue that formula (5.45) defines a retraction on O.\nProposition 5.54. On an embedded submanifold M of a Euclidean space E with\nnorm \u2225 \u00b7 \u2225, metric projection induces a retraction (5.45) as:\nR: O \u2192 M: (x, v) 7\u2192 R(x, v) = Rx(v) = PM(x + v).\nThis is called the metric projection retraction .\nProof. Clearly, Rx(0) = PM(x) = x for all x \u2208 M. From Theorem 5.53, we see\nthat R is smooth on its domain by composition. By the same theorem, for all\n(x, v) \u2208 TM it holds that\nDRx(0)[v] = DPM(x)[v] = Projx(v) = v.\nThis confirms that DR x(0) is the identity on T xM, as needed.\nAbsil and Malick show that this retraction is part of a large family of second-\norder retractions (recall Definition 5.42) [AM12, Ex. 23]. For the case at hand,\nRazvan-Octavian Radu shared the short proof below.\nProposition 5.55. If M is a Riemannian submanifold of E, the retraction in\nProposition 5.54 is second order.\nProof. For an arbitrary ( x, v) \u2208 TM, consider the retraction curve c(t) =\nRx(tv) = PM(x + tv). From Theorem 5.53, we know that x + tv \u2212c(t) is orthog-\nonal to Tc(t)M for all t. (We could also see this by noting that c(t) is a critical\npoint of x\u2032 7\u2192 \u2225x\u2032 \u2212 (x + tv)\u22252 on M.) This is all we need for our purpose.\nLet P(t) = Projc(t) denote orthogonal projection to T c(t)M: this is smooth in\nt (see Exercise 3.66). Since x + tv \u2212 c(t) is orthogonal to T c(t)M for all t, we\nhave that\ng(t) = P(t)(x + tv \u2212 c(t))\nis identically zero as a function from I \u2286 R (the domain of c) to E. Thus, the\n(classical) derivative g\u2032(t) is also identically zero from I to E:\ng\u2032(t) = P\u2032(t)(x + tv \u2212 c(t)) + P(t)(v \u2212 c\u2032(t)) \u2261 0.\nAt t = 0, we can use c(0) = x to see that 0 = g\u2032(0) = Projx(v\u2212c\u2032(0)). Since v and\nc\u2032(0) are both tangent vectors at x, this simply recovers the fact that c\u2032(0) = v.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2524, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a7bb8614-f306-4ada-b13e-8003783a4c60": {"__data__": {"id_": "a7bb8614-f306-4ada-b13e-8003783a4c60", "embedding": null, "metadata": {"page_label": "116", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6103f60c-7fbf-4be5-94bb-e32239114463", "node_type": "4", "metadata": {"page_label": "116", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "0b861810d232561c4ee44f0d7f3d357a8620668d57306d2911bc1d93b5e4a146", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n116 Embedded geometry: second order\nDifferentiating once more, we have that g\u2032\u2032(t) is also identically zero from I to\nE:\ng\u2032\u2032(t) = P\u2032\u2032(t)(x + tv \u2212 c(t)) + 2P\u2032(t)(v \u2212 c\u2032(t)) \u2212 P(t) d\ndtc\u2032(t) \u2261 0.\nAt t = 0, we use c(0) = x and c\u2032(0) = v to see that\n0 = \u2212g\u2032\u2032(0) = Projx\n\u0012 d\ndtc\u2032(0)\n\u0013\n= D\ndtc\u2032(0) = c\u2032\u2032(0),\nwhere the last two equalities follow (5.23): this is where we use that M is a\nRiemannian submanifold of E.\nThe domain of R x is the open subset Ox = {v \u2208 TxM : (x, v) \u2208 O}. Clearly,\nOx contains the origin, hence it also contains an open ball around the origin.\nHowever, Ox itself is not necessarily star-shaped with respect to the origin, that\nis, it is not true that v \u2208 Ox implies tv \u2208 Ox for all t \u2208 [0, 1]. Indeed, consider\nmetric projection to the set of matrices of fixed rank Rm\u00d7n\nr as defined by (7.49).\nGiven X \u2208 Rm\u00d7n\nr , let \u02d9X = \u2212X: this is a tangent vector to Rm\u00d7n\nr at X. Consider\nthe line t 7\u2192 X +t \u02d9X: projection of X +t \u02d9X to Rm\u00d7n\nr is well defined for all t except\nt = 1. It is an exercise to show that the same issue can arise with an embedded\nsubmanifold which is a closed set as well.\nFor an embedded submanifold M in E, the domain A of PM is all of E if and\nonly if M is an affine subspace of E. However, even if A (and a fortiori \u2126) is\nnot all of E, it can be the case that O = TM. This happens in particular when\nM is the boundary of a non-empty, closed, convex set, as then the sets of the\nform x + TxM are supporting hyperplanes of the convex hull of M: projecting\nan element of x + TxM to M is the same as projecting to the convex hull\nof M, which is globally defined. One example of this is metric projection onto\nthe sphere (7.9), which is the boundary of the unit Euclidean ball. The metric\nprojection retraction (7.24) for the Stiefel manifold St( n, p) with p < nis also\ndefined on the whole tangent bundle. This extends to SO(n) and (with some care\nregarding its two components) to O( n) (Section 7.4).\nIn Section 10.7, we define third-order retractions. Metric projection retractions\nare not third order in general: see Exercise 10.88.\nExercise 5.56. Show that A may be neither open nor closed. Hint: consider\nM = {(t, t2) : t \u2208 R} in R2 and show that A = R2\\{(0, t) : t >1/2}.\nExercise 5.57. Show that Ox (the domain of the metric projection retraction\nrestricted to TxM) can fail to be star-shaped even if M is closed in E. Hint:\nconsider M = {(t, cos(t)) : t \u2208 R} \u2282R2.\n5.13 Notes and references\nDefinition 5.1 for connections is not standard. As explained in Section 5.6, the\nusual approach is to define \u2207 as an operator mapping two smooth vector fields", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2849, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e632d6f2-4f98-4979-9607-9a7303bca4b9": {"__data__": {"id_": "e632d6f2-4f98-4979-9607-9a7303bca4b9", "embedding": null, "metadata": {"page_label": "117", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7e94187b-c817-4cd4-8acc-b437afb94869", "node_type": "4", "metadata": {"page_label": "117", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "afd82d0bbe927396f8111e0da7f74b5ac4b60f762a51d792a8c87fb2f47286b9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n5.13 Notes and references 117\nto a smooth vector field, then to prove that this operator acts pointwise in its\nfirst argument. The latter point confirms that the two definitions are equivalent,\nbut it is technical. Leading with Definition 5.1 makes it possible to skip these\ntechnicalities at first.\nThe pointwise dependence of \u2207 in its first argument is a consequence ofF(M)-\nlinearity, and holds more generally for all tensor fields (see Section 10.7): most\nreferences give the proof at that level of generality. See for example [Lee12,\nLem. 12.24], [Lee18, Prop. 4.5] or the remark after Def. 3.9 as well as Prop. 2.2\nand Cor. 2.3 in [O\u2019N83]. In the same vein, \u2207uV depends on V only locally\nthrough the values of V in a neighborhood of x (as in Lemma 5.23) or along\nany smooth curve passing through x with velocity u (owing to the chain rule\nproperty in Theorem 5.29)\u2014see also [Lee18, Prop. 4.26].\nExistence and uniqueness of the Riemannian connection is proved in most Rie-\nmannian geometry textbooks, e.g., [Lee18, Thm. 5.10] and [O\u2019N83, Thm. 3.11].\nLikewise, for existence and uniqueness of the covariant derivative of vector fields\nalong curves, see [Lee18, Thm. 4.24 and Prop. 5.5] and [O\u2019N83, Prop. 3.18].\nWe showed that the Riemannian connection for a Euclidean space corre-\nsponds to the usual directional derivative, and that the Riemannian connection\non a Riemannian submanifold is obtained through orthogonal projection of the\nRiemannian connection in the embedding space [Lee18, Prop. 5.12], [AMS08,\nProp. 5.3.2]. As part of that proof, we show symmetry in Theorem 5.8. This\ninvolves showing that if \u00afU, \u00afV (smooth vector fields in the embedding space) are\ntangent to a submanifold, then their Lie bracket is also tangent to that subman-\nifold: a similar statement appears as [Lee12, Cor. 8.32].\nIn the proof of Theorem 5.6, we use the fact that the Lie bracket [ U, V] of\ntwo smooth vector fields U, V\u2208 X(M) is itself a smooth vector field (Proposi-\ntion 5.10). Our proof is non-standard and restricted to embedded submanifolds.\nWe provide a general proof in Section 8.10. In the meantime, we get some insight\nalong these lines: Exercise 5.11 introduces derivations, and claims smooth vector\nfields are derivations. In fact, the converse is true as well: smooth vector fields\nare one-to-one with derivations [Lee12, Prop. 8.15]. Then, Exercise 5.12 claims\nLie brackets are derivations, so that Lie brackets are indeed smooth vector fields.\nThe proof in Section 8.10 follows yet another path.\nWe follow the definition of Riemannian Hessian in Absil et al. [AMS08, \u00a7 5.5].\nThe definition of second-order retractions and Proposition 5.45 follow that refer-\nence too. Absil et al. readily stress the importance of the fact that, at a critical\npoint, it does not matter whether the retraction is second order. A broader dis-\ncussion of various types of Hessians and second covariant derivatives of smooth\nfunctions is presented in [AMS08, \u00a7 5.6]. See also Section 10.7.\nThe extension lemmas (Lemmas 5.25 and 5.26) hold for general manifolds.\nThey are stated here to provide extensions in a neighborhood around a single\npoint. More generally, these hold to obtain extensions around any closed set. This\ncan be shown using partitions of unity [Lee12, Lem. 2.26, 8.6]. On this topic,\nbump functions on Euclidean spaces (Lemma 5.22) can be used to construct", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3622, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5e88d5c2-5677-4e43-bb6c-01839c53291b": {"__data__": {"id_": "5e88d5c2-5677-4e43-bb6c-01839c53291b", "embedding": null, "metadata": {"page_label": "118", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b04f224-ff87-4171-b25f-b8556681d8b6", "node_type": "4", "metadata": {"page_label": "118", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "66a33272493ac0894cfb84da522d3701dd9edc7aae9f2f169d6a7215e464be54", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n118 Embedded geometry: second order\npartitions of unity, which in turn can be used to construct bump functions on\nany manifold [Lee12, Lem. 2.22, Thm. 2.23, Prop. 2.25].\nDefinitions (5.37) and (5.38) for the second fundamental form and the Wein-\ngarten map are not standard but they are equivalent to the standard ones given\nin [Lee18, pp225\u2013230]. Moreover, both maps (and their properties as laid out\nin Section 5.11) extend as is to the more general situation of a Riemannian\nsubmanifold of a Riemannian manifold, as defined in Section 8.14. The Hessian\nformula (5.41) involving the Weingarten map\u2014and its construction\u2014appear first\nin [AMT13].\nHere are a few additional references for Section 5.11: The Gauss formula\n(Lemma 5.50) is discussed in [Lee18, Thm. 8.2], symmetry of II (Proposition 5.51)\nis stated in [Lee18, Prop. 8.1], the way covariant derivatives split on submani-\nfolds appears in [O\u2019N83, Prop. 4.8], the implications of the latter for geodesics\non submanifolds is spelled out in [Lee18, Cor. 5.2] and [O\u2019N83, Cor. 4.10], and\nthe relation to extrinsic curvature is pointed out in [Lee18, Prop. 8.10].\nAbsil and Malick study metric projection retractions under the nameprojective\nretraction: they prove Propositions 5.54 and 5.55, and extend the discussion to\nbroad classes of retractions [AM12, \u00a7 3.1,\u00a7 4.3]. Our proof of Proposition 5.55 is\ndifferent. Moreover, the statements here are more specific regarding the domain of\ndefinition of the retractions, building on Theorem 5.53 which is a particular case\nof results presented by Dudek and Holly [DH94, Thms 3.8 and 3.13, Cor. 3.14,\nThm. 4.1]. The discussion of when the domain of the metric projection retraction\nis the whole tangent bundle relies on certain basic facts which appear in [DH94,\nThm. 5.1, 5.3, 6.4]. The two exercises of Section 5.12 parallel [DH94, Ex. 6.1,\n6.4].\nFrom Theorem 5.53 it is fairly direct to build a so-called tubular neighborhood\nfor M in E [Lee18, Thm. 5.25]. The other way around, the proof of Proposi-\ntion 5.55 generalizes easily to show that retractions built from tubular neighbor-\nhoods in a natural way are second order.\nBreiding and Vannieuwenhoven study the sensitivity of metric projection to\nRiemannian submanifolds of Euclidean space in terms of extrinsic curvature, via\nthe Weingarten map [BV21].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2569, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "02a6d677-7315-4066-a942-237b8532519f": {"__data__": {"id_": "02a6d677-7315-4066-a942-237b8532519f", "embedding": null, "metadata": {"page_label": "119", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "265717ab-2465-4c4a-82a8-d340e63a3411", "node_type": "4", "metadata": {"page_label": "119", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "dcb97e5d948ceb97afa2bf9dc9e0b32edea7b1c309579f149f1d54da0fadef7c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n6 Second-order optimization\nalgorithms\nIn Chapter 4, we used the Riemannian gradient of a function to develop Rie-\nmannian gradient descent: a first-order optimization algorithm. Now that we\nhave developed the concept of Riemannian Hessian, we are in a good position to\ndevelop second-order optimization algorithms.\nWe first consider a Riemannian version of Newton\u2019s method: a pillar of both\noptimization and numerical analysis. When initialized close to certain local min-\nimizers, this algorithm enjoys a quadratic local convergence rate. This is signifi-\ncantly faster than gradient descent which converges only linearly to such points,\nbut the speedup comes at a cost:\n1. Each iteration of Newton\u2019s method involves solving a linear system of equa-\ntions in a tangent space: this is more expensive than computing a gradient\nstep.\n2. The global convergence behavior of Newton\u2019s method is erratic: it can easily\ndiverge, whereas gradient descent usually converges.\nTo combine the best of both gradient descent and Newton\u2019s method, we turn to\nthe Riemannian trust-regions method. That algorithm occupies us for most of\nthe chapter. It preserves the favorable global behavior of gradient descent and\nkeeps the per-iteration computational cost under control, while also preserving\nsuperlinear local converge rates to favorable local minimizers. This is arguably\nthe most robust algorithm for smooth optimization on manifolds to date.\nWe open the chapter with a discussion of second-order optimality conditions.\nAfter describing Newton\u2019s method, we look into the conjugate gradients method:\na matrix-free algorithm to solve the type of linear systems that arise in the\ncomputation of Newton steps. That algorithm resurfaces later in the chapter in\na truncated form that is more suitable for the trust-region method. We discuss\nthe latter in detail.\n6.1 Second-order optimality conditions\nBefore we move on to discuss second-order optimization algorithms, we secure\nsecond-order necessary optimality conditions: this is in the same spirit as the\nfirst-order conditions developed in Section 4.2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2348, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "67d59b96-1dbf-4199-aab5-7cec70a00263": {"__data__": {"id_": "67d59b96-1dbf-4199-aab5-7cec70a00263", "embedding": null, "metadata": {"page_label": "120", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "326fa34b-ec7e-4744-ab1d-a0b04c7e2cfe", "node_type": "4", "metadata": {"page_label": "120", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "988b38ae7713fbea7a44308eb40616db9e3dede5314e6f07fe30509f84ee6cf3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n120 Second-order optimization algorithms\nDefinition 6.1. A point x \u2208 Mis second-order critical (or second-order sta-\ntionary) for a smooth function f : M \u2192R if\n(f \u25e6 c)\u2032(0) = 0 and (f \u25e6 c)\u2032\u2032(0) \u2265 0\nfor all smooth curves c on M such that c(0) = x.\nIn words: it is not possible to move away from a second-order critical point\nx and obtain an initial decrease in the value of f with linear or even quadratic\nrate. In particular, second-order critical points are critical points.\nProposition 6.2. Any local minimizer of a smooth function f : M \u2192R is a\nsecond-order critical point of f.\nProof. Let x be a local minimizer of f. We know from Proposition 4.5 that x\nis critical. For contradiction, assume x is not second-order critical. Thus, there\nexists a smooth curve c: I \u2192 Mwith c(0) = x, (f \u25e6c)\u2032(0) = 0 and (f \u25e6c)\u2032\u2032(0) < 0.\nBy continuity of ( f \u25e6 c)\u2032\u2032, there exists \u03b4 > 0 such that ( f \u25e6 c)\u2032\u2032(\u03c4) < 0 for all\n\u03c4 \u2208 [0, \u03b4]. Taylor\u2019s theorem on f \u25e6 c implies that, for each t \u2208 [0, \u03b4], there exists\n\u03c4 \u2208 [0, \u03b4] such that\nf(c(t)) = f(c(0)) + t \u00b7 (f \u25e6 c)\u2032(0) + t2\n2 \u00b7 (f \u25e6 c)\u2032\u2032(\u03c4).\nThus, f(c(t)) < f(x) for all t \u2208 (0, \u03b4]: a contradiction.\nOn a Riemannian manifold, second-order criticality is characterized by gradi-\nents and Hessians. (Recall Definition 3.7 for the symbols \u2ab0 and \u227b.)\nProposition 6.3. Let f : M \u2192R be smooth on a Riemannian manifold M.\nThen, x is a second-order critical point of f if and only if gradf(x) = 0 and\nHessf(x) \u2ab0 0.\nProof. Let c: I \u2192 Mbe any smooth curve onM with c(0) = x, and letv = c\u2032(0),\nu = c\u2032\u2032(0). We know from Section 5.9 that\n(f \u25e6 c)\u2032(0) = \u27e8gradf(x), v\u27e9x and\n(f \u25e6 c)\u2032\u2032(0) = \u27e8gradf(x), u\u27e9x + \u27e8Hessf(x)[v], v\u27e9x .\nIf gradf(x) = 0 and Hessf(x) \u2ab0 0, then x is second-order critical. The other way\naround, assume x is second-order critical. Since the above hold for all v \u2208 TxM,\nwe first find that grad f(x) = 0, and subsequently also that Hess f(x) \u2ab0 0.\nIt is also possible to establish sufficient conditions for local optimality by\nstrengthening the second-order requirements.\nDefinition 6.4. A point x \u2208 Mis strictly second-order critical (or strictly\nsecond-order stationary) for a smooth function f : M \u2192R if\n(f \u25e6 c)\u2032(0) = 0 and (f \u25e6 c)\u2032\u2032(0) > 0\nfor all smooth curves c such that c(0) = x and c\u2032(0) \u0338= 0.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2488, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bad1b73f-a25a-424d-8910-d0d3d790bf18": {"__data__": {"id_": "bad1b73f-a25a-424d-8910-d0d3d790bf18", "embedding": null, "metadata": {"page_label": "121", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9cb4f445-3389-4d7b-9fb0-b9f35e77b145", "node_type": "4", "metadata": {"page_label": "121", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "784a52ca924e5e597acbfe19a321433bd19e8304f0cb8fc952ee2bc4250fcbf3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n6.2 Riemannian Newton\u2019s method 121\nThe proof of the next proposition relies on retractions to provide local param-\neterizations of a manifold. We could also use charts directly, as in Section 8.1.\nProposition 6.5. If x is a strict second-order critical point for f : M \u2192R,\nthen x is a strict local minimizer of f.\nProof. Assume we have a retraction R on M\u2014This is not restrictive: see Sec-\ntion 5.12 for embedded submanifolds or Section 10.2 for the general case. Since\nDRx(0) is invertible (it is the identity map), the inverse function theorem im-\nplies that Rx provides a diffeomorphism between a neighborhood of the origin of\nTxM and a neighborhood of x in M: see Corollary 4.17. As a result, x is a strict\nlocal minimizer for f on M if and only if the origin is a strict local minimizer\nfor \u02c6fx = f \u25e6 Rx on TxM. Since TxM is a linear space, we can endow it with an\ninner product \u27e8\u00b7, \u00b7\u27e9, so that \u02c6fx has a (Euclidean) gradient and Hessian. For some\nnonzero v \u2208 TxM, let c(t) = Rx(tv). Then, \u02c6fx(tv) = f(c(t)) and therefore:\n\u27e8grad \u02c6fx(0), v\u27e9 = d\ndt\n\u02c6fx(tv)\n\f\f\f\f\nt=0\n= (f \u25e6 c)\u2032(0) = 0,\n\u27e8Hess \u02c6fx(0)[v], v\u27e9 = d2\ndt2\n\u02c6fx(tv)\n\f\f\f\f\nt=0\n= (f \u25e6 c)\u2032\u2032(0) > 0.\nThe above hold for all v \u0338= 0, so that grad \u02c6fx(0) = 0 and Hess \u02c6fx(0) \u227b 0. The\nclaim now follows from the Euclidean case [NW06, Thm. 2.4].\nThe following proposition is clear. Its proof is a slight modification of that for\nProposition 6.3.\nProposition 6.6. Let f : M \u2192R be smooth on a Riemannian manifold M.\nThen, x is a strict second-order critical point of f if and only if gradf(x) = 0\nand Hessf(x) \u227b 0.\n6.2 Riemannian Newton\u2019s method\nAll optimization algorithms we consider are retraction based, in the sense that\nthey iterate\nxk+1 = Rxk (sk)\nfor some step sk. Thus, the change in cost function value from one iterate to the\nnext can be understood through the pullbacks \u02c6fx = f \u25e6 Rx:\nf(xk+1) = f(Rxk (sk)) = \u02c6fxk (sk).\nAccordingly, a strategy to design algorithms is to pick a modelmxk : Txk M \u2192R\nwhich suitably approximates \u02c6fxk , and to choose sk as an (approximate) mini-\nmizer of mxk . Given our work building Taylor expansions (recall equation (5.28)),", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2373, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8b3445e7-a970-4bc4-94c7-410510901028": {"__data__": {"id_": "8b3445e7-a970-4bc4-94c7-410510901028", "embedding": null, "metadata": {"page_label": "122", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8d976c50-a93f-4676-97d2-cf4d09bc4f77", "node_type": "4", "metadata": {"page_label": "122", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "1d00b66c8232e96b8167736bbc8c1cca8fbe532cc9a7ddeabdd088c5d0a5b418", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n122 Second-order optimization algorithms\nwe know that close to critical points it holds that\n\u02c6fx(s) \u2248 mx(s) \u225c f(x) + \u27e8gradf(x), s\u27e9x + 1\n2 \u27e8Hessf(x)[s], s\u27e9x .\nThe model mx is a quadratic function of s on the linear space TxM. A minimizer\nof mx, if one exists, must be a critical point of mx. To determine the gradient of\nmx, we use the fact that Hess f(x) is self-adjoint to compute:\n\u27e8grad mx(s), u\u27e9x = Dmx(s)[u] = \u27e8gradf(x), u\u27e9x + \u27e8Hessf(x)[s], u\u27e9x .\nThe above holds for all u \u2208 TxM, hence by identification we find:\ngrad mx(s) = gradf(x) + Hessf(x)[s].\nThus, a tangent vector s \u2208 TxM is a critical point of mx if and only if\nHessf(x)[s] = \u2212gradf(x). (6.1)\nThis defines a linear system of equations called the Newton equations for the\nunknown s \u2208 TxM. So long as Hess f(x) is invertible, there exists a unique\nsolution called the Newton step : we use it to define Algorithm 6.1. It is an easy\nexercise to show that if Hessf(x) is positive definite then the Newton step is the\nminimizer of mx. In contrast, if the Hessian is invertible but not positive definite,\nthen the Newton step does not correspond to a minimizer of mx: following that\nstep may lead us astray.\nAlgorithm 6.1 Riemannian Newton\u2019s method\nInput: x0 \u2208 M\nFor k = 0, 1, 2, . . .\nSolve Hessf(xk)[sk] = \u2212gradf(xk) for sk \u2208 Txk M\nxk+1 = Rxk (sk)\nRecall the various notions of local convergence rates introduced in Section 4.6.\nAs we now show, Newton\u2019s method may converge locally quadratically (Def-\ninition 4.15). This is much faster than the typical linear local convergence of\ngradient descent, though we should bear in mind that (a) Newton steps are\nmore expensive to compute, and (b) the global convergence behavior of New-\nton\u2019s method can be unwieldy. The proof below relies on the local contraction\nmapping theorem from Section 4.6.\nTheorem 6.7. Let f : M \u2192R be smooth on a Riemannian manifold M. If\nx\u22c6 \u2208 Mis such that gradf(x\u22c6) = 0 and Hessf(x\u22c6) is invertible, then there exists\na neighborhood of x\u22c6 on M such that, for all x0 in that neighborhood, Newton\u2019s\nmethod (Algorithm 6.1) generates an infinite sequence of iterates x0, x1, x2, . . .\nwhich converges at least quadratically to x\u22c6.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2415, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4e082aa5-5fc1-45a2-9fec-907b77c96540": {"__data__": {"id_": "4e082aa5-5fc1-45a2-9fec-907b77c96540", "embedding": null, "metadata": {"page_label": "123", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f93b4a25-2d76-4d92-9baa-a8c705f8c976", "node_type": "4", "metadata": {"page_label": "123", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "1a98d07e3d780863550504fe120c38bf32e094b4a656c1ab4b438d390b48f327", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n6.2 Riemannian Newton\u2019s method 123\nProof. Let U = {x \u2208 M: det(Hess f(x)) \u0338= 0} be the subset of M where the\nRiemannian Hessian of f is invertible. This is a neighborhood of x\u22c6. Indeed, it\ncontains x\u22c6 by assumption, and it is open because its complement is closed (the\ndeterminant of the Hessian is a continuous function). Newton\u2019s method iterates\nthe map F : U \u2192 Mgiven by\nF(x) = R(x, \u2212V (x)) with V (x) = Hessf(x)\u22121[gradf(x)].\nAt a critical point x\u22c6, we have V (x\u22c6) = 0 hence Lemma 4.21 provides the fol-\nlowing for all u \u2208 Tx\u22c6M:\nDF(x\u22c6)[u] = DR(x\u22c6, 0)[u, \u2212DV (x\u22c6)[u]] = u \u2212 DV (x\u22c6)[u]. (6.2)\nMoreover, Proposition 5.3 provides D V (x\u22c6)[u] = \u2207uV with the Riemannian\nconnection \u2207, and\n\u2207uV = \u2212\n\u0000\nHessf(x\u22c6)\u22121 \u25e6 \u2207uHessf \u25e6 Hessf(x\u22c6)\u22121\u0001\n[gradf(x\u22c6)]\n+ Hessf(x\u22c6)\u22121[\u2207ugradf]\n= u. (6.3)\nIn the intermediate expression above, the first term involves the covariant deriva-\ntive of the Hessian tensor field: see Section 10.7. Its precise definition does not\nmatter in the end since that linear operator is applied to the vector grad f(x\u22c6)\nwhich is zero. The second term evaluates to u since \u2207ugradf = Hessf(x\u22c6)[u] by\ndefinition. The above results combined establish that D F(x\u22c6) = 0. All claims\nnow follow from the local contraction mapping theorem (Theorem 4.19).\nNotice that the above theorem does not require the retraction to be second\norder. Essentially, this is due to Proposition 5.44 and the fact that x\u22c6 is a critical\npoint.\nFrom an optimization perspective, Theorem 6.7 is only beneficial if Hess f(x\u22c6)\nis positive definite. Indeed, by Proposition 6.2, critical points with an invertible\nHessian which is not positive definite are certainly not local minimizers (in fact,\nthey could be local maximizers). Yet, this theorem tells us Newton\u2019s method\nmay converge to such points.\nPartly because of this, given an initialization x0, it is hard to predict where\nNewton\u2019s method may converge (if it converges at all). After all, the neighbor-\nhood in Theorem 6.7 may be arbitrarily small. To compensate for such issues,\nwe add safeguards and other enhancements to this bare algorithm in Section 6.4.\nStill, owing to its fast local convergence, Newton\u2019s method is relevant for favor-\nable problems, or more generally to refine approximate solutions (for example,\nobtained through gradient descent). Thus, before moving on entirely, we discuss\na practical algorithm to compute the Newton step sk in the next section. This\nproves useful for the safeguarded algorithm as well.\nExercise 6.8. Let g(v) = 1\n2 \u27e8v, Hv\u27e9x \u2212\u27e8b, v\u27e9x be a quadratic function defined on\na tangent space TxM of a Riemannian manifold. Assume H is positive definite", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2879, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c5ca9bf6-0cb9-4c5d-a0f6-226f38d70f69": {"__data__": {"id_": "c5ca9bf6-0cb9-4c5d-a0f6-226f38d70f69", "embedding": null, "metadata": {"page_label": "124", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "75f8bf61-daeb-4328-9617-0e75e519b03d", "node_type": "4", "metadata": {"page_label": "124", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "cbb4b75b24024d07b84015d5f8fc50a47475adcc9f46484b3a1c6e740245fea2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n124 Second-order optimization algorithms\non TxM. Show that g has a unique minimizer which coincides with its unique\ncritical point, that is, the solution of the linear system Hv = b. More generally,\nshow that v minimizes g over a linear subspace of TxM if and only if gradg(v)\nis orthogonal to that subspace, and that this minimizer exists and is unique.\nExercise 6.9. Theorem 6.7 controls the local convergence of Newton\u2019s method to\na zero of a gradient vector field. More generally, let U \u2208 X(M) be a smooth vector\nfield (not necessarily the gradient of some function) on a manifold equipped with\na connection \u2207. Let Jx : TxM \u2192TxM denote the Jacobian of U at x, defined\nby Jx(u) = \u2207uU for all u \u2208 TxM. Newton\u2019s method for the vector field U\niterates xk+1 = F(xk) with F(x) = R x(\u2212(Jx)\u22121[U(x)]), using some retraction\nR. Highlight the small changes needed in the proof of Theorem 6.7 to see that\nif U(x\u22c6) = 0 and Jx\u22c6 is invertible for some x\u22c6 \u2208 Mthen Newton\u2019s method\ninitialized in a sufficiently small neighborhood of x\u22c6 converges to x\u22c6 at least\nquadratically.\n6.3 Computing Newton steps: conjugate gradients\nConsider a cost function f : M \u2192R. Let x \u2208 Mbe such that the Riemannian\nHessian of f at x is positive definite. Then, to compute Newton\u2019s step at x\nwe minimize a quadratic approximation of f lifted to the tangent space at x.\nExplicitly, we seek v \u2208 TxM to minimize\ng(v) = 1\n2 \u27e8v, Hv\u27e9x \u2212 \u27e8b, v\u27e9x , (6.4)\nwhere we let H = Hessf(x) and b = \u2212gradf(x) for short. Since H is positive\ndefinite by assumption, g has a unique minimizer which coincides with its unique\ncritical point (Exercise 6.8). As\ngradg(v) = Hv \u2212 b, (6.5)\nthat minimizer is the unique solution s \u2208 TxM of the linear system Hs = b.\nBelow, we assume b \u0338= 0 as otherwise the task is trivial.\u22c6\nSince H is a linear map on the linear space TxM, we could in principle do the\nfollowing: choose a basis for T xM, represent H as a matrix and b as a vector\nwith respect to that basis (see Exercise 3.9), and solve the resulting linear system\nin matrix form using any standard solver (e.g., based on LU, QR or Cholesky\ndecomposition). However, that would be impractical because we seldom have\naccess to a preferred basis of a tangent space (we would need to generate one),\nand computing the representation of H in that basis would be expensive.\nIt is far more fruitful to resort to a matrix-free solver, that is, an algorithm\nwhich only requires access to H as a linear map v 7\u2192 Hv. This is indeed what we\nusually have at our disposal in applications. Such solvers do not require access\nto H in matrix form.\nThe most famous matrix-free solver for systems with a positive definite map", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2902, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c5e7b92b-6c6d-4698-bd8d-6b90e058e1df": {"__data__": {"id_": "c5e7b92b-6c6d-4698-bd8d-6b90e058e1df", "embedding": null, "metadata": {"page_label": "125", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f528974-17ce-4df6-b72f-b402932b594a", "node_type": "4", "metadata": {"page_label": "125", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "9ec8da1c5a6df162b7ef33ad532eb913144ac6b2456b7ac9a41a1d347778d55e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n6.3 Computing Newton steps: conjugate gradients 125\nis the conjugate gradients method (CG): see Algorithm 6.2. Using exactly one\ncomputation of the form v 7\u2192 Hv per iteration, this method generates three\nfinite sequences of tangent vectors in T xM:\n1. p0, p1, p2, . . .are linearly independent: they span a subspace of T xM of in-\ncreasing dimension;\n2. v0, v1, v2, . . .are increasingly better approximations of the minimizer of g; and\n3. r0, r1, r2, . . .are the residues: the smaller rn, the better vn approximates the\nsought solution.\nAlgorithm 6.2 CG: Conjugate gradients on a tangent space\nInput: positive definite map H on TxM and b \u2208 TxM, b \u0338= 0\nSet v0 = 0, r0 = b, p0 = r0\nFor n = 1, 2, . . .\nCompute Hpn\u22121 (this is the only call to H)\n\u03b1n = \u2225rn\u22121\u22252\nx\n\u27e8pn\u22121,Hpn\u22121\u27e9x\nvn = vn\u22121 + \u03b1npn\u22121\nrn = rn\u22121 \u2212 \u03b1nHpn\u22121\nIf rn = 0, output s = vn: the solution of Hs = b\n\u03b2n = \u2225rn\u22252\nx\n\u2225rn\u22121\u22252x\npn = rn + \u03b2npn\u22121\nWe begin with a simple fact clarifying how the residue rn informs us about\nthe quality of vn as a candidate minimizer for g.\nLemma 6.10. If Algorithm 6.2 generates the vectors v0, . . . , vn and r0, . . . , rn\nbefore termination, then\nrn = \u2212gradg(vn) = b \u2212 Hvn. (6.6)\nThus, the algorithm terminates with vn if and only if vn minimizes g.\nProof. The proof is by induction. Clearly, r0 = b = \u2212gradg(v0) since v0 = 0.\nAssume rn\u22121 = \u2212gradg(vn\u22121) = b \u2212 Hvn\u22121. Then, by construction in Al-\ngorithm 6.2, we have rn = rn\u22121 \u2212 \u03b1nHpn\u22121 = b \u2212 Hvn\u22121 \u2212 \u03b1nHpn\u22121 =\nb \u2212 H(vn\u22121 + \u03b1npn\u22121) = b \u2212 Hvn = \u2212gradg(vn). The last part holds since\nthe algorithm terminates with vn if and only if rn = 0.\nThe key fact about the CG algorithm is that the vectorsp0, p1, . . .are orthogo-\nnal with respect to a special inner product. The standard proof is by induction to\nshow simultaneously Lemmas 6.11, 6.12 and 6.13 below: see for example [TB97,\nThm. 38.1]. For exposition, we state Lemma 6.11 without proof, then we use it\nto prove the two subsequent lemmas. The intent is to clarify how the properties\nof p0, p1, . . .unlock all the other important features of CG.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2296, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7f838087-58df-4ffd-beb1-14c33816da8f": {"__data__": {"id_": "7f838087-58df-4ffd-beb1-14c33816da8f", "embedding": null, "metadata": {"page_label": "126", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e35e1b96-8216-480e-89d5-8dad2ecd1959", "node_type": "4", "metadata": {"page_label": "126", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "a55dbadb0fe635f3fa2aa8e9660f604f07b01d840654d5a29970ffd64d1a551c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n126 Second-order optimization algorithms\nLemma 6.11. If Algorithm 6.2 generates the vectors p0, p1, . . . , pn\u22121 before ter-\nmination, then they are H-conjugate, that is, they are nonzero and\n\u2200i \u0338= j, \u27e8pi, Hpj\u27e9x = 0.\nIn particular, p0, . . . , pn\u22121 are linearly independent.\nThe above lemma states that the vectors p0, p1, . . .form an orthogonal basis\nwith respect to the special inner product \u27e8u, v\u27e9H = \u27e8u, Hv\u27e9x. The fact that each\npn\u22121 is nonzero also confirms that \u03b1n is well defined since \u27e8pn\u22121, Hpn\u22121\u27e9x is\nthen positive by positive definiteness of H.\nThe remarkable feature of H-conjugacy is that it makes minimizing g trivial\nin that basis. The CG method exploits this to build the sequence v0, v1, . . ., as\nstated in the next lemma. A more constructive proof would start with the fact\nthat any vector v in span(p0, . . . , pn\u22121) expands as v = y1p0 + \u00b7 \u00b7\u00b7+ ynpn\u22121 for\nsome coefficients y1, . . . , yn, then observing that g(v) is a quadratic function of\nthose coefficients with a diagonal Hessian matrix owing to H-conjugacy.\nLemma 6.12. If Algorithm 6.2 generates the vectors p0, . . . , pn\u22121 and v0, . . . , vn\nbefore termination, then\nvn = arg min\nv\u2208span(p0,...,pn\u22121)\ng(v). (6.7)\nIn particular, gradg(vn) is orthogonal to span(p0, . . . , pn\u22121).\nProof. Unrolling the recursion for vn in Algorithm 6.2 with v0 = 0, we see that\nvn = vn\u22121 + \u03b1npn\u22121 = \u00b7 \u00b7\u00b7= \u03b11p0 + \u00b7 \u00b7\u00b7+ \u03b1npn\u22121. (6.8)\nThus, it is clear that vn is in the span of p0, . . . , pn\u22121. To show thatvn minimizes\ng in that span, we proceed by induction. For n = 0, we see that v0 = 0 is valid\nsince that is the only vector in the trivial span. Assume vn\u22121 minimizes g in\nthe span of p0, . . . , pn\u22122. Equivalently, gradg(vn\u22121) is orthogonal to p0, . . . , pn\u22122\n(Exercise 6.8). By Lemma 6.10, this means rn\u22121 is orthogonal to p0, . . . , pn\u22122.\nFor the same reason, to show that vn minimizes g in the span of p0, . . . , pn\u22121\nwe must show that rn is orthogonal to those vectors. Consider the following for\ni = 0, . . . , n\u2212 1:\n\u27e8rn, pi\u27e9x = \u27e8rn\u22121 \u2212 \u03b1nHpn\u22121, pi\u27e9x = \u27e8rn\u22121, pi\u27e9x \u2212 \u03b1n \u27e8pn\u22121, Hpi\u27e9x .\nFor i \u2264 n\u22122, both terms on the right-most side are zero by induction hypothesis\n(for the firm term) and by H-conjugacy (for the second term, see Lemma 6.11).\nFor i = n \u2212 1, the right-most side is zero by definition of \u03b1n. Indeed,\n\u27e8rn\u22121, pn\u22121\u27e9x = \u27e8rn\u22121, rn\u22121 + \u03b2n\u22121pn\u22122\u27e9x = \u27e8rn\u22121, rn\u22121\u27e9x ,\nwhere the last equality holds by orthogonality of rn\u22121 and pn\u22122.\nFrom the above lemma, we can infer the following:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2720, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "428ede47-ac3d-43fb-8ca1-be48062d4246": {"__data__": {"id_": "428ede47-ac3d-43fb-8ca1-be48062d4246", "embedding": null, "metadata": {"page_label": "127", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0218830a-93cd-4ddf-9b70-1531207acffb", "node_type": "4", "metadata": {"page_label": "127", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "84cb567851c0e186f97864b4c6dd81a5093bda9c507f9d4011bbc4eb0eb2a514", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n6.3 Computing Newton steps: conjugate gradients 127\n1. We get steady progress in the sense that g(vn) \u2264 g(vn\u22121) \u2264 \u00b7\u00b7 \u00b7 \u2264g(v0)\nbecause each vi is obtained by minimizing g over a subspace that contains all\nprevious subspaces.\n2. The algorithm terminates with the minimizer of g after at most dim M it-\nerations because if n reaches dim M then p0, . . . , pn\u22121 span the whole space\nTxM hence rn = \u2212gradg(vn) = 0.\nTo quantify how much progress we can hope to achieve as iterations progress,\nit is instructive to study the sequence of subspaces spanned by the H-conjugate\ndirections. The following lemma gives a convenient characterization.\nLemma 6.13. If Algorithm 6.2 generates the vectors p0, . . . , pn\u22121 before termi-\nnation, then\nspan(p0, . . . , pn\u22121) = span(b, Hb, H2b, . . . , Hn\u22121b). (6.9)\nWe call the right-hand side the Krylov subspace Kn.\nProof. The proof is by induction on n. The identity (6.9) certainly holds for\nn = 1 since p0 = b. Now assume (6.9) holds. Under that induction hypothesis,\nwe aim to show that if the algorithm generates pn then\nspan(p0, . . . , pn) = Kn+1.\nTo show this equality, it is sufficient to show both of the following:\n1. dim Kn+1 \u2264 dim span(p0, . . . , pn), and\n2. span( p0, . . . , pn) \u2286 Kn+1.\nWe know from Lemma 6.11 that p0, . . . , pn are linearly independent, so that\ndim span(p0, . . . , pn) = n + 1. Moreover, dim Kn+1 \u2264 n + 1 because Kn+1 is\ngenerated by n + 1 vectors. Thus, the first part is clear. For the second part, we\nalready know by induction hypothesis that p0, . . . , pn\u22121 are included in Kn+1. It\nremains to show the same for pn. To this end, consider the following where we\nuse (6.6) for rn:\npn = rn + \u03b2npn\u22121 = b \u2212 Hvn + \u03b2npn\u22121.\nOf course, b is in Kn+1. From (6.8) we know vn is in span( p0, . . . , pn\u22121). By\ninduction hypothesis, this means vn \u2208 Kn. By definition, for all u \u2208 Kn it holds\nthat Hu is in Kn+1. Thus, Hvn \u2208 Kn+1. The induction hypothesis also provides\npn\u22121 \u2208 Kn+1. Hence, pn is in Kn+1.\nLet s \u2208 TxM be our target, that is, the unique solution to Hs = b. We wish\nto assess the size of the error vector vn \u2212 s at iteration n. We could do so in the\nnorm \u2225 \u00b7 \u2225x we already have on TxM, but we choose to use the norm associated\nto the inner product \u27e8u, v\u27e9H = \u27e8u, Hv\u27e9x instead, namely, the norm\n\u2225u\u2225H =\nq\n\u27e8u, u\u27e9H =\nq\n\u27e8u, Hu\u27e9x. (6.10)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2560, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c33af5cd-5c40-4c0a-bf66-a12c59d3c134": {"__data__": {"id_": "c33af5cd-5c40-4c0a-bf66-a12c59d3c134", "embedding": null, "metadata": {"page_label": "128", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3bd46432-3105-408b-b5fb-1df51633d112", "node_type": "4", "metadata": {"page_label": "128", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "bc235c117790e36db04cf44a4a5a9ec26a575712258c48197321113c8b07a9c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n128 Second-order optimization algorithms\nExplicitly, we aim to bound\u2225vn\u2212s\u2225H. To this end, notice that the approximation\nerror \u2225v \u2212 s\u2225H for a vector v \u2208 TxM obeys:\n\u2225v \u2212 s\u22252\nH = \u27e8v \u2212 s, H(v \u2212 s)\u27e9x\n= \u27e8v, Hv\u27e9x \u2212 \u27e8s, Hv\u27e9x \u2212 \u27e8v, Hs\u27e9x + \u27e8s, Hs\u27e9x\n= \u27e8v, Hv\u27e9x \u2212 2 \u27e8v, b\u27e9x + \u27e8s, Hs\u27e9x\n= 2g(v) + \u27e8s, Hs\u27e9x , (6.11)\nwhere we used that H is self-adjoint and Hs = b. Since the last term on the\nright-hand side is independent of v, we conclude that minimizing \u2225v\u2212s\u2225H over a\nsubset of TxM is equivalent to minimizing g(v) over that same subset. Therefore,\nLemma 6.12 tells us that vn is the vector in span(p0, . . . , pn\u22121) which minimizes\n\u2225vn \u2212 s\u2225H: this is why the H-norm is particularly relevant. Further combining\nwith Lemma 6.13 reveals the following key fact about the CG algorithm:\nvn = arg min\nv\u2208Kn\n\u2225v \u2212 s\u2225H, (6.12)\nwhere Kn is the Krylov subspace. Let us reformulate this once more: Lemmas 6.12\nand 6.13 combined with the observation (6.11) reveal that\nvn = a0b + a1Hb + a2H2b + \u00b7 \u00b7\u00b7+ an\u22121Hn\u22121b\n=\n\u0000\na0I + a1H + \u00b7 \u00b7\u00b7+ an\u22121Hn\u22121\u0001\nb, (6.13)\nwith coefficients a0, . . . , an\u22121 \u2208 R such that \u2225vn \u2212s\u2225H is minimized. Substituting\nHs for b in (6.13), we deduce that the error vector at iteration n is\nvn \u2212 s =\n\u0000\na0I + a1H + \u00b7 \u00b7\u00b7+ an\u22121Hn\u22121\u0001\nHs \u2212 s\n=\n\u0000\na0H + a1H2 + \u00b7 \u00b7\u00b7+ an\u22121Hn \u2212 I\n\u0001\ns. (6.14)\nThe parenthesized expression on the right-hand side is a polynomial inH. Specif-\nically, it is qn(H) with the polynomial\nqn(z) = \u22121 + a0z + a1z2 + \u00b7 \u00b7\u00b7+ an\u22121zn. (6.15)\nThus,\nvn \u2212 s = qn(H)s. (6.16)\nThe polynomial qn has degree at most n and satisfies qn(0) = \u22121. Let Qn denote\nthe set of such polynomials. Since every polynomial in Qn can be written in the\nform (6.15) for some choice of coefficients a0, . . . , an\u22121, and since the CG method\ngenerates vn such that \u2225vn \u2212 s\u2225H is minimized, it follows that the CG method\nguarantees\n\u2225vn \u2212 s\u2225H = min\nq\u2208Qn\n\u2225q(H)s\u2225H. (6.17)\nTo turn this conclusion into an interpretable bound on the error after n CG\niterations, we now investigate the effect of applying a polynomial to the map\nH. To this end, let u1, . . . , ud be a basis of eigenvectors of H, orthonormal with", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2339, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "84f18cf7-c9db-4866-8f9d-3cefd3e16302": {"__data__": {"id_": "84f18cf7-c9db-4866-8f9d-3cefd3e16302", "embedding": null, "metadata": {"page_label": "129", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c65ed8ec-dcef-46ad-81ae-e3bfab4e096a", "node_type": "4", "metadata": {"page_label": "129", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "bca2c00e2180efd4733a3849d02b17374ba1cad3484fb6f168d673cb006dd4fc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n6.3 Computing Newton steps: conjugate gradients 129\nrespect to \u27e8\u00b7, \u00b7\u27e9x (we write d = dim M for short). These exist since H is self-\nadjoint. Moreover, let \u03bb1, . . . , \u03bbd be associated eigenvalues: Hui = \u03bbiui. The\nunknown vector s expands as\ns =\ndX\ni=1\n\u27e8ui, s\u27e9x ui.\nHence, t applications of H to this vector yield:\nHts =\ndX\ni=1\n\u03bbt\ni \u27e8ui, s\u27e9x ui.\nMore generally, applying q(H) to s for some polynomial q yields:\nq(H)s =\ndX\ni=1\nq(\u03bbi) \u27e8ui, s\u27e9x ui.\nWe conclude that, for any polynomial q,\n\u2225q(H)s\u22252\nH\n\u2225s\u22252\nH\n= \u27e8q(H)s, Hq(H)s\u27e9x\n\u27e8s, Hs\u27e9x\n=\nPd\ni=1 q(\u03bbi)2\u03bbi \u27e8ui, s\u27e92\nxPd\ni=1 \u03bbi \u27e8ui, s\u27e92\nx\n\u2264 max\n1\u2264i\u2264d\nq(\u03bbi)2,\nwhere the inequality is due to positivity of the eigenvalues. Combined with (6.17),\nit follows that\n\u2225vn \u2212 s\u2225H \u2264 \u2225s\u2225H \u00b7 min\nq\u2208Qn\nmax\n1\u2264i\u2264d\n|q(\u03bbi)|. (6.18)\nIn words: the relative error after n iterations, in the H-norm, is controlled by\nthe existence of a polynomial q in Qn with small absolute value when evaluated\nat each of the eigenvalues of H.\nBased on these considerations, it follows easily that ifH has only k \u2264 d distinct\neigenvalues then CG terminates in k iterations. To verify this, it suffices to\nconstruct a polynomial q of degree k with single roots at the distinct eigenvalues\nand such that q(0) = \u22121. More generally, if \u03bbmin and \u03bbmax denote the smallest\nand largest eigenvalues of H, then \u03ba = \u03bbmax\n\u03bbmin\nis the condition number of H, and\nit can be shown that\n\u2225vn \u2212 s\u2225H \u2264 \u2225s\u2225H \u00b7 2\n\u0012\u221a\u03ba \u2212 1\u221a\u03ba + 1\n\u0013n\n\u2264 \u2225s\u2225H \u00b7 2e\u2212n/\u221a\u03ba, (6.19)\nso that the error decreases exponentially fast as CG iterates (hence linear conver-\ngence as per Definition 4.14). This is shown by exhibiting an appropriate polyno-\nmial q with small absolute value over the whole interval [\u03bbmin, \u03bbmax]: see [TB97,\nThm. 38.5] for a classical construction based on Chebyshev polynomials, as il-\nlustrated in Figure 6.1.\nWe close with a few comments.\n1. That CG terminates in at most dimM iterations is of little practical relevance,\nin part because numerical round-off errors typically prevent this (specifically,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2238, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9eda0a4b-7866-4603-a7fc-256ac3c4a833": {"__data__": {"id_": "9eda0a4b-7866-4603-a7fc-256ac3c4a833", "embedding": null, "metadata": {"page_label": "130", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fcc414ee-c97c-44d1-ac15-83403f0816e0", "node_type": "4", "metadata": {"page_label": "130", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "87c8c96df4a614957799b365ec7d49459766217d2819956ffa1b05f3f377cb72", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n130 Second-order optimization algorithms\n0 2 4 6 8 10-1\n-0.5\n0\n0.5\n0 2 4 6 8 10-1\n-0.5\n0\n0.5\n0 2 4 6 8 10-1\n-0.5\n0\n0.5\n0 2 4 6 8 10-1\n-0.5\n0\n0.5\nFigure 6.1 The convergence rate of CG is governed by the existence of special poly-\nnomials, as shown by (6.18). For illustration of (6.19), let [ \u03bbmin, \u03bbmax] = [1 , 9] and\n\u03ba = \u03bbmax\n\u03bbmin\n= 9. For n = 2 , 3, 4, 5, the plots show the polynomial qn(x) = \u2212Tn(\u2113(x))\nTn(\u2113(0)) ,\nwhere \u2113(x) = 2x\u2212\u03bbmax\u2212\u03bbmin\n\u03bbmax\u2212\u03bbmin\nmaps [\u03bbmin, \u03bbmax] to [ \u22121, 1] and Tn(x) = cos(n arccos(x))\ndefines the Chebyshev polynomial of the first kind and of degree n on [\u22121, 1]. One can\ncheck that qn is in Qn (that is, qn is a polynomial of degree n with qn(0) = \u22121) and,\nas depicted, that |qn(x)| \u22642\n\u0010\u221a\u03ba\u22121\u221a\u03ba+1\n\u0011n\nfor all x \u2208 [\u03bbmin, \u03bbmax].\nbecause numerically the vectors pi are not exactly H-conjugate). However,\nthe progressive improvement of the iterates vn as predicted by (6.19) is borne\nout empirically, and the role of the condition number \u03ba is indeed critical. In\npractice, CG is terminated after a set number of iterations, or when a target\nrelative tolerance is met. For example, we may replace the stopping criterion\nrn = 0 with \u2225rn\u2225x \u2264 \u03b5tolerance\u2225b\u2225x.\n2. Reconsidering the bigger picture, we want to keep in mind that the goal is to\nminimize f(x): solving the linear system which arises in Newton\u2019s method is\nonly a means to an end. Since CG can produce adequate approximate solutions\nto the linear system in few iterations, it is often beneficial to terminate CG\nearly and proceed with an approximate Newton step: this is at the heart of\nthe developments regarding the trust-region method in the next section.\n3. In practice, Hess f(x) may not be positive definite. If such is the case, we\nought to be able to detect it. For example, the inner product \u27e8pn\u22121, Hpn\u22121\u27e9x\nmay turn out to be negative. In the trust-region method, such events are\nmonitored and appropriate actions are taken.\n4. Regarding numerical errors again, in Algorithm 6.2, the vectors pi may slowly\nbuild-up a non-tangent component (even though this cannot happen mathe-\nmatically). Experience shows that it is sometimes beneficial to ensure pn\u22121 is\ntangent (up to machine precision) before computing Hpn\u22121. For embedded\nsubmanifolds, this can be done through orthogonal projection for example.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2545, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9361f137-c8b7-4337-a132-a6a6c03df00f": {"__data__": {"id_": "9361f137-c8b7-4337-a132-a6a6c03df00f", "embedding": null, "metadata": {"page_label": "131", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "699d186a-2014-4281-8fa5-d46c478de3b4", "node_type": "4", "metadata": {"page_label": "131", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "d1bf4d69a6434522892cf962ca996629ffff4c39a743d33b21d27121a157d886", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n6.4 Riemannian trust regions 131\nDoing this at every iteration appears to be sufficient to ensure the other se-\nquences (namely, ri and vi) also remain numerically tangent.\nExercise 6.14. An alternative to CG is to run gradient descent on g(v) (6.4)\nin the tangent space. Since g is a quadratic, it is easy to check that it has L-\nLipschitz continuous gradient with L = \u03bbmax(H). Show that running vn+1 =\nvn \u2212 1\nL gradg(vn) with v0 = 0 leads to \u2225vn \u2212s\u2225x \u2264 e\u2212n/\u03ba\u2225s\u2225x, where \u03ba = \u03bbmax(H)\n\u03bbmin(H) .\nContrast this with the role of \u03ba in (6.19) for CG.\n6.4 Riemannian trust regions\nThe trust-region method addresses the fundamental shortcomings of Newton\u2019s\nmethod, while preserving its fast local convergence properties under favorable\ncircumstances. The premise is the same: around a point x, we approximate the\npullback f \u25e6 Rx with a simpler model in the tangent space:\nf(Rx(s)) \u2248 mx(s) = f(x) + \u27e8gradf(x), s\u27e9x + 1\n2 \u27e8Hx(s), s\u27e9x .\nHere, Hx is allowed to be any self-adjoint linear map on T xM (in fact, we will\nrelax this even further). Of course, the model is a better match for f \u25e6 Rx if Hx\nis chosen to be the Hessian of f \u25e6Rx. From Proposition 5.44, we also know that,\nclose to critical points, this is essentially the same as Hessf(x) (exactly the same\nfor second-order retractions).\nIn a key departure from Newton\u2019s method however, we do not select the step\nby blindly jumping to the critical point of the model (which might not even\nexist). Rather, we insist on reducing the value of mx. Moreover, since the model\nis only a local approximation of the pullback, we only trust it in a ball around\nthe origin in the tangent space: the trust region. Specifically, at the iterate xk,\nwe define the model\nmk(s) = f(xk) + \u27e8gradf(xk), s\u27e9xk\n+ 1\n2 \u27e8Hk(s), s\u27e9xk\n(6.20)\nfor some map Hk : Txk M \u2192Txk M to be specified, and we pick the tentative\nnext iterate x+\nk as Rxk (sk) such that the step sk approximately solves the trust-\nregion subproblem:\nmin\ns\u2208Txk M\nmk(s) subject to \u2225s\u2225xk \u2264 \u2206k, (6.21)\nwhere \u2206k is the radius of the trust region at iteration k. Specific requirements\nare discussed later, but at the very least mk(sk) should be smaller than mk(0).\nThe step is accepted ( xk+1 = x+\nk ) or rejected ( xk+1 = xk) based on the perfor-\nmance of x+\nk as judged by the actual cost function f, compared to the expected\nimprovement as predicted by the model. Depending on how the two compare,\nthe trust-region radius may also be adapted. See Algorithm 6.3 for details: it is\ncalled the Riemannian trust-region method (RTR).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2769, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5333567e-19ce-4535-b99d-256b75f3f906": {"__data__": {"id_": "5333567e-19ce-4535-b99d-256b75f3f906", "embedding": null, "metadata": {"page_label": "132", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "15f2eb40-eab0-44b4-b3d1-9ca18e860200", "node_type": "4", "metadata": {"page_label": "132", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "e42b051e611e3a13b36f7f998345b45b8d3694fb55757f6aa96419726bece4ab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n132 Second-order optimization algorithms\nAlgorithm 6.3 RTR: the Riemannian trust-region method\nParameters: maximum radius \u00af\u2206 > 0, threshold \u03c1\u2032 \u2208 (0, 1/4)\nInput: x0 \u2208 M, \u22060 \u2208 (0, \u00af\u2206]\nFor k = 0, 1, 2, . . .\nPick a map Hk : Txk M \u2192Txk M to define mk (6.20).\nApproximately solve the subproblem (6.21), yielding sk.\nThe tentative next iterate is x+\nk = Rxk (sk).\nCompute the ratio of actual to model improvement:\n\u03c1k = f(xk) \u2212 f(x+\nk )\nmk(0) \u2212 mk(sk). (6.22)\nAccept or reject the tentative next iterate:\nxk+1 =\n(\nx+\nk if \u03c1k > \u03c1\u2032 (accept),\nxk otherwise (reject).\n(6.23)\nUpdate the trust-region radius:\n\u2206k+1 =\n\uf8f1\n\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3\n1\n4 \u2206k if \u03c1k < 1\n4 ,\nmin(2\u2206k, \u00af\u2206) if \u03c1k > 3\n4 and \u2225sk\u2225xk = \u2206k,\n\u2206k otherwise.\n(6.24)\nRunning RTR, we hope to find a minimizer of f, but that is too much to ask\nin general. More realistically, we hope to find a point x = xk such that\n\u2225gradf(x)\u2225x \u2264 \u03b5g and Hess f(x) \u2ab0 \u2212\u03b5H Id, (6.25)\nwhere Id is the identity map on T xM, and \u03b5H may be infinite if we only care\nabout first-order optimality conditions. One of the main goals of this chapter\nis to show that, regardless of initialization, under suitable assumptions, RTR\nprovides such a point in a bounded number of iterations.\nOf course, to provide such guarantees we must specify conditions on the maps\nHk, requirements on how well the trust-region subproblems are to be solved,\nand regularity conditions on the pullbacks f \u25e6Rxk . We do this in the subsections\nbelow. In Section 6.7, we discuss more restrictive settings which make it possible\nto verify all assumptions discussed below in a straightforward manner.\n6.4.1 Conditions on the model\nThe model mxk is determined by a choice of map Hk from T xk M to itself.\nThe simpler this map, the easier it may be to solve the trust-region subprob-\nlem (6.21). In choosing Hk, we aim to strike a balance between model accuracy,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2090, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ecc20ef3-fe28-492d-9ee6-795858e0a4b1": {"__data__": {"id_": "ecc20ef3-fe28-492d-9ee6-795858e0a4b1", "embedding": null, "metadata": {"page_label": "133", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b59756c7-e125-4dc5-8310-7740c56d2132", "node_type": "4", "metadata": {"page_label": "133", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "8416bc8bcdbd929eca9b1822a9b00969c0637627ac15bd5a5c97fb91d9fc2b1d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n6.4 Riemannian trust regions 133\ncomputational efficiency, and convenience. With the goal (6.25) determined by\n\u03b5g, \u03b5H > 0 in mind, we introduce the following requirements.\nFor iterations with large gradient, the conditions are particularly mild. In\nessence, this is because for such iterations the main focus is on reducing the\ngradient norm, which can be done with any first-order accurate model.\nA 6.1. For all iterations k such that \u2225gradf(xk)\u2225xk > \u03b5g, we require that:\n1. Hk is radially linear, that is,\n\u2200s \u2208 Txk M, \u03b1\u2265 0, H k(\u03b1s) = \u03b1Hk(s); and (6.26)\n2. Hk is uniformly bounded, that is, there exists c0 \u2265 0, independent of k, such\nthat\n\u2200s \u2208 Txk M,\n\f\f\u27e8s, Hk(s)\u27e9xk\n\f\f \u2264 c0\u2225s\u22252\nxk . (6.27)\n(We can gain insight into the latter through Corollary 10.47.)\nAn extreme case consists in selecting Hk = L \u00b7 Id for some L > 0. This is\nconvenient, computationally inexpensive, and allows us to solve the subprob-\nlem (6.21) in closed form: RTR then takes gradient steps. However, the model\ndoes not capture second-order information at all, which may slow down conver-\ngence. Alternatively, a convenient radially linear (but not linear) map Hk can\nbe obtained from finite difference approximations of the Hessian using gradients,\nsee Section 10.6. Naturally, if it is practical to use the Hessian of f (or that\nof f \u25e6 Rxk ) itself for Hk, then the enhanced accuracy of the model is a strong\nincentive to do so.\nFor iterations with small gradient, if there is a desire to reach approximate\nsatisfaction of second-order necessary optimality conditions ( \u03b5H < \u221e), we need\nthe model to be (at least approximately) second-order accurate.\nA 6.2. For all iterations k such that \u2225gradf(xk)\u2225xk \u2264 \u03b5g, we require Hk to be\nlinear and self-adjoint. Furthermore, there must exist c1 \u2265 0 independent of k\nsuch that\n\u2225Hess(f \u25e6 Rxk )(0) \u2212 Hk\u2225 \u2264c1\u2206k\n3 , (6.28)\nwhere \u2225 \u00b7 \u2225denotes the operator norm of a self-adjoint map, that is, the largest\nmagnitude of any of its eigenvalues.\nThe convergence results below guarantee Hk is, eventually, almost positive\nsemidefinite. This is only meaningful if Hk is close to Hess f(xk) in operator\nnorm. In turn, Hessf(xk) is equal to Hess( f \u25e6 Rxk )(0) if the retraction is second\norder (and for a general retraction they are close if xk is nearly critical): see\nPropositions 5.44 and 5.45 (and Exercise 10.73 for first-order retractions). Over-\nall, the conceptually simplest situation is that for which we use a second-order\nretraction and a quadratically-accurate model, in which case:\nHk = Hess(f \u25e6 Rxk )(0) = Hessf(xk). (6.29)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2798, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "257f66d5-7976-455c-b861-b226b36d6f05": {"__data__": {"id_": "257f66d5-7976-455c-b861-b226b36d6f05", "embedding": null, "metadata": {"page_label": "134", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "99b8fbff-ceaf-400a-8790-f450481a81c5", "node_type": "4", "metadata": {"page_label": "134", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "043615257b281b133328d769f8615097f8be5b99e4a5f44b93c75f81eeb8a675", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n134 Second-order optimization algorithms\nThen, A6.2 holds with c1 = 0.\n6.4.2 Requirements on solving the subproblem\nOnce a model is selected through a choice of map Hk, the key (and typically\nmost computationally expensive) part of an iteration of RTR is to solve the\ntrust-region subproblem (6.21) approximately, producing a step sk. Numerous\nefficient algorithms have been proposed over the past few decades: we detail one\nthat is particularly well suited to the Riemannian case in Section 6.5. For now, we\nmerely specify minimum requirements on how well the task ought to be solved.\nWe require sufficient decrease in the value of themodel, similar to but different\nfrom the analysis of Riemannian gradient descent in Section 4.3 which required\nsufficient decrease in the value of the actual cost function. So long as first-order\ncriticality has not been approximately attained, sufficient decrease is defined with\nrespect to the gradient norm. The subproblem solver we discuss in Section 6.5\nsatisfies the assumption below, see Exercise 6.26.\nA 6.3. There exists c2 > 0 such that, for all k with \u2225gradf(xk)\u2225xk > \u03b5g, the\nstep sk satisfies\nmk(0) \u2212 mk(sk) \u2265 c2 min\n\u0012\n\u2206k, \u2225gradf(xk)\u2225xk\nc0\n\u0013\n\u2225gradf(xk)\u2225xk , (6.30)\nwhere c0 is the constant in A6.1.\nThis condition is easily satisfied by computing the so-called Cauchy point : the\nminimizer of the subproblem when restricted to the negative gradient direction.\nGiven the gradient atxk, it can be computed with one call toHk. It is an exercise\nto establish the following lemma.\nLemma 6.15. Let gk = grad f(xk) for convenience. The Cauchy point is the\ntangent vector sC\nk = \u2212tgradf(xk) with t \u2265 0 such that mk(sC\nk ) is minimal under\nthe constraint \u2225sC\nk \u2225xk \u2264 \u2206k. Under A6.1, we can compute the corresponding\noptimal t explicitly as:\nt =\n\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3\nmin\n\u0012\n\u2225gk\u22252\nxk\n\u27e8gk,Hk(gk)\u27e9xk\n, \u2206k\n\u2225gk\u2225xk\n\u0013\nif \u27e8gk, Hk(gk)\u27e9xk\n> 0,\n\u2206k\n\u2225gk\u2225xk\notherwise.\nFurthermore, setting sk = sC\nk in RTR satisfies A6.3 with c2 = 1\n2 .\nOnce the gradient is small and if \u03b5H < \u221e, it becomes necessary to focus on\nsecond-order optimality conditions.\nA 6.4. There exists c3 > 0 such that, for all k with \u2225gradf(xk)\u2225xk \u2264 \u03b5g and\n\u03bbmin(Hk) < \u2212\u03b5H, the step sk satisfies\nmk(0) \u2212 mk(sk) \u2265 c3\u22062\nk\u03b5H. (6.31)\n(Note that Hk has real eigenvalues owing to A6.2.)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2515, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "901dac7e-973f-4814-bd3a-5dfdca4ee665": {"__data__": {"id_": "901dac7e-973f-4814-bd3a-5dfdca4ee665", "embedding": null, "metadata": {"page_label": "135", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52a8bac8-ba8e-43cd-9994-6757ed38ca84", "node_type": "4", "metadata": {"page_label": "135", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "8a1dcb5a59c866b76b5a320dc22349cae908c986b5c28ea56aae1b804af15c48", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n6.4 Riemannian trust regions 135\nThis condition too can be satisfied with explicit, finite procedures by computing\neigensteps: moving up to the boundary of the trust region along a direction which\ncertifies that the smallest eigenvalue of Hk is strictly smaller than \u2212\u03b5H. Proving\nthe next lemma is an exercise.\nLemma 6.16. Under A6.2, if \u03bbmin(Hk) < \u2212\u03b5H then there exists a tangent\nvector u \u2208 Txk M satisfying\n\u2225u\u2225xk = 1, \u27e8gradf(xk), u\u27e9xk\n\u2264 0, and \u27e8u, Hk(u)\u27e9xk\n< \u2212\u03b5H.\nSetting sk = \u2206ku (called an eigenstep) in RTR satisfies A6.4 with c3 = 1\n2 .\nEigensteps are rarely (if ever) computed in practice. More pragmatically, the\nexistence of eigensteps serves to show that a global minimizer of the subproblem\nalso satisfies A6.4.\nCorollary 6.17. If Hk is linear and self-adjoint for every iteration k, then\nsetting sk to be a global minimizer of the subproblem (6.21) at every iteration\nsatisfies both A6.3 and A6.4 with c2 = c3 = 1\n2 . Likewise, setting sk to achieve at\nleast a fraction \u03b1 \u2208 (0, 1] of the optimal model decrease satisfies the assumptions\nwith c2 = c3 = \u03b1\n2 .\nExercise 6.18. Give a proof of Lemma 6.15.\nExercise 6.19. Give a proof of Lemma 6.16.\n6.4.3 Regularity conditions\nAs we did when analyzing the Riemannian gradient method, we require that the\ncost function be lower-bounded.\nA 6.5. There exists flow \u2208 R such that f(xk) \u2265 flow for all iterates x0, x1, . . .\nLikewise, we still require a first-order, Lipschitz-type condition on the pull-\nbacks of f for the given retraction R. The set Sg is specified later on.\nA 6.6. For a given subset Sg of the tangent bundle TM, there exists a constant\nLg > 0 such that, for all (x, s) \u2208 Sg,\nf(Rx(s)) \u2264 f(x) + \u27e8gradf(x), s\u27e9x + Lg\n2 \u2225s\u22252\nx.\nIn addition to these, we now also include a second-order Lipschitz-type condi-\ntion. When M is a Euclidean space and Rx(s) = x+s, this one holds in particular\nif Hessf is Lipschitz continuous with constant LH. The set SH is specified later\non; it is empty if \u03b5H = \u221e.\nA 6.7. For a given subset SH of the tangent bundle TM, there exists a constant\nLH > 0 such that, for all (x, s) \u2208 SH,\nf(Rx(s)) \u2264 f(x) + \u27e8gradf(x), s\u27e9x + 1\n2\u27e8s, Hess(f \u25e6 Rx)(0)[s]\u27e9x + LH\n6 \u2225s\u22253\nx.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2415, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ea0a0344-b7e7-4c10-bb20-d9f7fb149701": {"__data__": {"id_": "ea0a0344-b7e7-4c10-bb20-d9f7fb149701", "embedding": null, "metadata": {"page_label": "136", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "46e36a31-3fa3-42e8-8c36-6858c5d24a05", "node_type": "4", "metadata": {"page_label": "136", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "5281291de5b17135203401069be5e9017b2c39ed290c288fdc8aea8330f6c763", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n136 Second-order optimization algorithms\nWe note that, in particular, the sets Sg and SH will not be required to contain\nany tangent vectors of norm larger than \u00af\u2206, since this is the largest trust-region\nradius ever considered. This is useful notably when the retraction is not globally\ndefined (or well behaved). In addition, all root points of elements in Sg and\nSH are iterates x0, x1, x2, . . .generated by RTR. This can be helpful when the\niterates are easily shown to lie in a compact subset of M, for example if the\nsublevel sets of f are compact, as then A6.6 and A6.7 hold by Lemma 10.57: see\nSection 6.7.\nWe gain further insight into the regularity assumptions from Corollary 10.54\nand Exercise 10.58 (for A6.6) and from Corollary 10.56 and Exercise 10.87 (for\nA6.7).\n6.4.4 Iteration complexity\nGiven tolerances \u03b5g > 0 and \u03b5H > 0, we show that RTR produces an iterate\nxk which satisfies the following termination conditions in a bounded number of\niterations:\n\u2225gradf(xk)\u2225xk \u2264 \u03b5g and \u03bbmin(Hk) \u2265 \u2212\u03b5H. (6.32)\nWe stress that \u03b5H may be set to infinity if only first-order optimality conditions\nare targeted. Accordingly, we separate the theorem statement in two scenar-\nios. See the discussion around eq. (6.29) to relate the guarantees on Hk to the\neigenvalues of Hessf(xk).\nFollowing the standard proofs for trust regions in Euclidean space, the analysis\nis based on three supporting lemmas which we state and prove below. In a\nnutshell, they show that:\n1. The trust-region radius cannot become arbitrarily small. Essentially, this is\nbecause regularity of the cost function ensures the model mk is sufficiently\naccurate for small steps, which ultimately ensures step acceptance. This pre-\nvents trust-region radius reductions beyond a certain point.\n2. Combining the latter with our sufficient decrease assumptions, successful steps\ninitiated from iterates with large gradient produce large decrease in the cost\nfunction value (and similarly at iterates where Hk has a \u201clarge\u201d negative\neigenvalue). Yet, the total amount of cost decrease is bounded byf(x0)\u2212flow,\nso that there cannot be arbitrarily many successful steps.\n3. The number of successful steps as above is at least a fraction of the total\nnumber of iterations, because a large number of consecutive failures would\neventually violate the fact that the trust-region radius is lower-bounded: every\nso often, there must be a successful step.\nWe state the main theorem\u2014the proof comes later in this section.\nTheorem 6.20. Let S = {(x0, s0), (x1, s1), . . .} be the pairs of iterates and ten-\ntative steps generated by RTR under A6.1, A6.2, A6.3, A6.4 and A6.5. Further", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2890, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "10687a53-502d-419b-9cbf-5f4f8bac0be9": {"__data__": {"id_": "10687a53-502d-419b-9cbf-5f4f8bac0be9", "embedding": null, "metadata": {"page_label": "137", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b5f51d4-1b43-4a4c-a9ab-6b7694ddacf8", "node_type": "4", "metadata": {"page_label": "137", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "5a96a67f933fb313bd7b9b7938f641093ea915052a309b7d1b287158546d1bfc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n6.4 Riemannian trust regions 137\nassume A6.6 and A6.7 hold with constants Lg and LH on the sets\nSg = {(xk, sk) \u2208 S : \u2225gradf(xk)\u2225xk > \u03b5g}, and\nSH = {(xk, sk) \u2208 S : \u2225gradf(xk)\u2225xk \u2264 \u03b5g and \u03bbmin(Hk) < \u2212\u03b5H}.\nDefine\n\u03bbg = 1\n4 min\n\u0012 1\nc0\n, c2\nLg + c0\n\u0013\nand \u03bbH = 3\n4\nc3\nLH + c1\n. (6.33)\nWe consider two scenarios, depending on whether second-order optimality con-\nditions are targeted or not:\n1. If \u03b5g \u2264 \u22060\n\u03bbg\nand \u03b5H = \u221e, there exists t with \u2225gradf(xt)\u2225xt \u2264 \u03b5g and\nt \u2264 3\n2\nf(x0) \u2212 flow\n\u03c1\u2032c2\u03bbg\n1\n\u03b52g\n+ 1\n2 log2\n\u0012 \u22060\n\u03bbg\u03b5g\n\u0013\n= O\n\u0012 1\n\u03b52g\n\u0013\n. (6.34)\n(In this scenario, A6.2, A6.4 and A6.7 are irrelevant.)\n2. If \u03b5g \u2264 \u22060\n\u03bbg\n, \u03b5g \u2264 c2\nc3\n\u03bbH\n\u03bb2g\nand \u03b5H < c2\nc3\n1\n\u03bbg\n, there exists t\u2032 \u2265 t such that\n\u2225gradf(xt\u2032)\u2225xt\u2032 \u2264 \u03b5g and \u03bbmin(Ht\u2032) \u2265 \u2212\u03b5H with\nt\u2032 \u2264 3\n2\nf(x0) \u2212 flow\n\u03c1\u2032c3\u03bb2\n1\n\u03b52\u03b5H\n+ 1\n2 log2\n\u0012\u22060\n\u03bb\u03b5\n\u0013\n= O\n\u0012 1\n\u03b52\u03b5H\n\u0013\n, (6.35)\nwhere (\u03bb, \u03b5) = (\u03bbg, \u03b5g) if \u03bbg\u03b5g \u2264 \u03bbH\u03b5H, and (\u03bb, \u03b5) = (\u03bbH, \u03b5H) otherwise.\nSince the algorithm is a descent method, f(xt\u2032) \u2264 f(xt) \u2264 f(x0).\nTo build a proof of the theorem above, we work through a sequence of three\nlemmas. This first one lower-bounds the trust-region radius.\nLemma 6.21. Under the assumptions of Theorem 6.20, let x0, . . . , xn be iterates\ngenerated by RTR. If none of them satisfy the termination conditions (6.32), then\n\u2206k \u2265 min(\u22060, \u03bbg\u03b5g, \u03bbH\u03b5H) (6.36)\nfor k = 0, . . . , n.\nProof. Our goal is to show that if \u2206 k is small, then \u03c1k must be large. By the\nmechanism of RTR (specifically, eq. (6.24)), this guarantees \u2206k cannot decrease\nfurther. By definition of \u03c1k (6.22), using mk(0) = f(xk),\n1 \u2212 \u03c1k = 1 \u2212 f(xk) \u2212 f(Rxk (sk))\nmk(0) \u2212 mk(sk) = f(Rxk (sk)) \u2212 mk(sk)\nmk(0) \u2212 mk(sk) .\nConsider an iteration k such that \u2225gradf(xk)\u2225xk > \u03b5g. Then, the numerator is\nupper-bounded owing to A6.6 and A6.1:\nf(Rxk (sk)) \u2212 mk(sk)\n= f(Rxk (sk)) \u2212 f(xk) \u2212 \u27e8gradf(xk), sk\u27e9xk\n\u2212 1\n2 \u27e8Hk(sk), sk\u27e9xk\n\u2264 Lg + c0\n2 \u2225sk\u22252\nxk .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2083, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "086e127e-d1a3-4b9d-9e05-090d2992fa2b": {"__data__": {"id_": "086e127e-d1a3-4b9d-9e05-090d2992fa2b", "embedding": null, "metadata": {"page_label": "138", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "84a8ec6f-e69a-4b5d-9355-ce958d5d9aff", "node_type": "4", "metadata": {"page_label": "138", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "d43e5145013c173bfc2f8806ca33243f3ed9ef546bb6ff365a1009ba337ba23c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n138 Second-order optimization algorithms\nFurthermore, the denominator is lower-bounded by A6.3:\nmk(0) \u2212 mk(sk) \u2265 c2 min\n\u0012\n\u2206k, \u03b5g\nc0\n\u0013\n\u03b5g.\nHence, using \u2225sk\u2225xk \u2264 \u2206k, we have\n1 \u2212 \u03c1k \u2264 1\n2\nLg + c0\nc2\u03b5g\n\u22062\nk\nmin\n\u0010\n\u2206k, \u03b5g\nc0\n\u0011.\nIf \u2206 k \u2264 \u03b5g\nc0\n, the last factor is equal to \u2206 k. If additionally \u2206 k \u2264 c2\u03b5g\nLg+c0\n, then\n1 \u2212 \u03c1k \u2264 1\n2 . Using (6.33), we summarize this as: if \u2206 k \u2264 4\u03bbg\u03b5g, then \u03c1k \u2265 1\n2 and\nthe mechanism of RTR implies \u2206 k+1 \u2265 \u2206k.\nNow, consider k such that \u2225gradf(xk)\u2225xk \u2264 \u03b5g and \u03bbmin(Hk) < \u2212\u03b5H. Then,\nthe numerator is upper-bounded by A6.7, A6.2 and \u2225sk\u2225xk \u2264 \u2206k:\nf(Rxk (sk)) \u2212 mk(sk)\n= f(Rxk (sk)) \u2212 f(xk) \u2212 \u27e8gradf(xk), sk\u27e9xk\n\u2212 1\n2 \u27e8Hess(f \u25e6 Rxk )(0)[sk], sk\u27e9xk\n+ 1\n2 \u27e8(Hess(f \u25e6 Rxk (0) \u2212 Hk)[sk], sk\u27e9xk\n\u2264 LH + c1\n6 \u22063\nk,\nand the denominator is lower-bounded by A6.4:\nmk(0) \u2212 mk(sk) \u2265 c3\u22062\nk\u03b5H. (6.37)\nCombining, we get\n1 \u2212 \u03c1k \u2264 LH + c1\n6c3\u03b5H\n\u2206k.\nAgain, considering (6.33), we find that if \u2206 k \u2264 4\u03bbH\u03b5H, then \u03c1k \u2265 1\n2 and as a\nresult \u2206k+1 \u2265 \u2206k.\nWe have established that if \u2206 k \u2264 4 min(\u03bbg\u03b5g, \u03bbH\u03b5H) then \u2206 k+1 \u2265 \u2206k. Since\nRTR does not reduce the radius by more than a factor four per iteration, the\nclaim follows.\nThe second lemma upper-bounds the total number of successful (that is, ac-\ncepted) steps before termination conditions are met.\nLemma 6.22. Under the assumptions of Theorem 6.20, let x0, . . . , xn be iterates\ngenerated by RTR. If none of them satisfy the termination conditions (6.32),\ndefine the set of successful steps among those as\nSn =\n\b\nk \u2208 {0, . . . , n} : \u03c1k > \u03c1\u2032\t\n,\nand let Un designate the unsuccessful steps, so that Sn and Un form a partition", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1832, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f296e3eb-36b6-4292-8e14-6b8e2ca9ece8": {"__data__": {"id_": "f296e3eb-36b6-4292-8e14-6b8e2ca9ece8", "embedding": null, "metadata": {"page_label": "139", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6ce1cfde-5fa9-46ef-a905-d1d867cac1c4", "node_type": "4", "metadata": {"page_label": "139", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "e7906ed097e3e561e09242bbf4cf873e412408742642c8e7a7f14b9cb18276f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n6.4 Riemannian trust regions 139\nof {0, . . . , n}. In the first scenario of Theorem 6.20, the number of successful\nsteps is bounded as\n|Sn| \u2264f(x0) \u2212 flow\n\u03c1\u2032c2\n1\n\u03bbg\u03b52g\n. (6.38)\nSimilarly, in the second scenario we have\n|Sn| \u2264f(x0) \u2212 flow\n\u03c1\u2032c3\n1\nmin(\u03bbg\u03b5g, \u03bbH\u03b5H)2\u03b5H\n. (6.39)\nProof. Clearly, if k \u2208 Un, then f(xk) = f(xk+1). On the other hand, if k \u2208 Sn,\nthen the definition of \u03c1k (6.22) combined with A6.3 and A6.4 ensures:\nf(xk) \u2212 f(xk+1) = \u03c1k\n\u0000\nmk(0) \u2212 mk(sk)\n\u0001\n\u2265 \u03c1\u2032 min\n\u0012\nc2 min\n\u0012\n\u2206k, \u03b5g\nc0\n\u0013\n\u03b5g , c3\u22062\nk\u03b5H\n\u0013\n.\nBy Lemma 6.21 and the assumption \u03bbg\u03b5g \u2264 \u22060, it holds that\n\u2206k \u2265 min (\u03bbg\u03b5g, \u03bbH\u03b5H) .\nFurthermore, using \u03bbg \u2264 1/c0 reveals that\nmin(\u2206k, \u03b5g/c0) \u2265 min(\u2206k, \u03bbg\u03b5g) \u2265 min (\u03bbg\u03b5g, \u03bbH\u03b5H) .\nHence,\nf(xk) \u2212 f(xk+1) \u2265 \u03c1\u2032 min\n\u0000\nc2\u03bbg\u03b52\ng, c2\u03bbH\u03b5g\u03b5H, c3\u03bb2\ng\u03b52\ng\u03b5H, c3\u03bb2\nH\u03b53\nH\n\u0001\n. (6.40)\nIn the first scenario, \u03b5H = \u221e and the above simplifies to:\nf(xk) \u2212 f(xk+1) \u2265 \u03c1\u2032c2\u03bbg\u03b52\ng.\nSum over iterations up to n and use A6.5 (lower-bounded f):\nf(x0) \u2212 flow \u2265 f(x0) \u2212 f(xn+1) =\nX\nk\u2208Sn\nf(xk) \u2212 f(xk+1) \u2265 |Sn|\u03c1\u2032c2\u03bbg\u03b52\ng.\nHence,\n|Sn| \u2264f(x0) \u2212 flow\n\u03c1\u2032c2\u03bbg\n1\n\u03b52g\n.\nSimilarly, in the second scenario, starting over from (6.40) and assuming both\nc3\u03bb2\ng\u03b52\ng\u03b5H \u2264 c2\u03bbH\u03b5g\u03b5H and c3\u03bb2\ng\u03b52\ng\u03b5H \u2264 c2\u03bbg\u03b52\ng (which is equivalent to \u03b5g \u2264\nc2\u03bbH/c3\u03bb2\ng and \u03b5H \u2264 c2/c3\u03bbg), the same telescoping sum yields\nf(x0) \u2212 flow \u2265 |Sn|\u03c1\u2032c3 min(\u03bbg\u03b5g, \u03bbH\u03b5H)2\u03b5H.\nReorganize this as a bound on |Sn| to conclude.\nOur third and last lemma lower-bounds the number of successful steps before\ntermination as a fraction of the total number of iterations before termination. It\ncaptures the fact that we cannot have arbitrarily long strings of rejections.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1851, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cfeff32b-d4c7-4417-8369-09350ef5e306": {"__data__": {"id_": "cfeff32b-d4c7-4417-8369-09350ef5e306", "embedding": null, "metadata": {"page_label": "140", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7fe67618-9175-4136-a06f-776277c64206", "node_type": "4", "metadata": {"page_label": "140", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "6e403b2bf328e3125e91b73048f0226a43380da856b6ff6344b426c33c5a94c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n140 Second-order optimization algorithms\nLemma 6.23. Under the assumptions of Theorem 6.20, let x0, . . . , xn be iterates\ngenerated by RTR. If none of them satisfy the termination conditions (6.32),\nusing the notation Sn and Un of Lemma 6.22, it holds that\n|Sn| \u22652\n3(n + 1) \u2212 1\n3 max\n\u0012\n0, log2\n\u0012 \u22060\n\u03bbg\u03b5g\n\u0013\n, log2\n\u0012 \u22060\n\u03bbH\u03b5H\n\u0013\u0013\n. (6.41)\nProof. The proof rests on the lower-bound for \u2206 k from Lemma 6.21. For all\nk \u2208 Sn, it holds that \u2206 k+1 \u2264 2\u2206k. For all k \u2208 Uk, it holds that \u2206 k+1 \u2264 2\u22122\u2206k.\nHence,\n\u2206n \u2264 2|Sn|2\u22122|Un|\u22060.\nOn the other hand, Lemma 6.21 gives\n\u2206n \u2265 min(\u22060, \u03bbg\u03b5g, \u03bbH\u03b5H) .\nCombine, divide by \u22060 and take the log in base 2 to see that:\n|Sn| \u22122|Un| \u2265min\n\u0012\n0, log2\n\u0012\u03bbg\u03b5g\n\u22060\n\u0013\n, log2\n\u0012\u03bbH\u03b5H\n\u22060\n\u0013\u0013\n.\nUse |Sn| + |Un| = n + 1 to conclude.\nWith these lemmas available, the main theorem follows easily.\nProof of Theorem 6.20. For each scenario, Lemmas 6.22 and 6.23 provide an\nupper-bound and a lower-bound on |Sn|, and it suffices to combine them to\nproduce an upper-bound on n. For example, in the first scenario, if n is such\nthat none of the iterates x0, . . . , xn have gradient smaller than \u03b5g, then\nn \u2264 3\n2\nf(x0) \u2212 flow\n\u03c1\u2032c2\n1\n\u03bbg\u03b52g\n+ 1\n2 log2\n\u0012 \u22060\n\u03bbg\u03b5g\n\u0013\n\u2212 1.\nThus, by contraposition, after a number of iterations larger than the right-hand\nside, an iterate with sufficiently small gradient must have been found. The same\nargument applies in the second scenario.\n6.4.5 Critical accumulation points\nBuilding on Theorem 6.20 above, it is also possible to show that all accumulation\npoints of RTR are critical points. We start with a straightforward corollary of\nthis theorem that ensures RTR keeps generating points with small gradient, then\nwe strengthen that corollary with an additional assumption.\nCorollary 6.24. Let S = {(x0, s0), (x1, s1), . . .} be the pairs of iterates and\ntentative steps generated by RTR under A6.1, A6.3 and A6.5 with \u03b5g = 0 (we\naim for first-order criticality). Further assume A6.6 holds on S. Then,\nlim inf\nk\u2192\u221e\n\u2225gradf(xk)\u2225xk = 0, (6.42)\nthat is, for all \u03b5 >0 and K there exists k \u2265 K such that \u2225gradf(xk)\u2225xk \u2264 \u03b5.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2306, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1d2e305e-8b22-444d-a461-f4283953d971": {"__data__": {"id_": "1d2e305e-8b22-444d-a461-f4283953d971", "embedding": null, "metadata": {"page_label": "141", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "002bd546-8fb4-433c-9df6-152aa8a5ff19", "node_type": "4", "metadata": {"page_label": "141", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "f0a0cc1b35fcdc41bd4d9f7458e9f556b123c2d95818ff16d3a9b62de24b357a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n6.4 Riemannian trust regions 141\nProof. Let \u03b5 > 0 and K \u2208 {0, 1, 2, . . .} be arbitrary. Our assumptions imply\nthat SK = {(xK, sK), (xK+1, sK+1), . . .} is a sequence of pairs of iterates and\ntentative steps generated by RTR under A6.1, A6.3 and A6.5 with \u03b5g = \u03b5 and\n\u03b5H = \u221e, and that A6.6 holds on SK. Thus, Theorem 6.20 guarantees that there\nexists k \u2265 K such that \u2225gradf(xk)\u2225xk \u2264 \u03b5.\nTo strengthen the above corollary, we introduce a new assumption. If the\nfunction x 7\u2192 \u2225gradf(x)\u2225x is Lipschitz continuous (see Section 10.4; this notably\nholds if Hess f is continuous and bounded), then that assumption is satisfied\nin particular if the retraction does not unduly distort distances, that is, if the\nRiemannian distance between x and Rx(s) is bounded by some constant times\n\u2225s\u2225x (see also A6.9 below). The latter holds for the exponential retraction (Sec-\ntion 10.2). The assumption below also holds if S is contained in a compact set,\nsee Proposition 6.31.\nA 6.8. For a given subset S of the tangent bundle TM, there exists a constant\nLgn > 0 such that, for all (x, s) \u2208 S,\n\f\f\u2225gradf(Rx(s))\u2225Rx(s) \u2212 \u2225gradf(x)\u2225x\n\f\f \u2264 Lgn\u2225s\u2225x.\nUnder that assumption it is possible to show that all accumulation points of\nRTR are critical points. The statement below is similar in spirit to [AMS08,\nThm. 7.4.4], though the precise assumptions are different, hence the proof is also\nadapted.\nProposition 6.25. Let S = {(x0, s0), (x1, s1), . . .} be the pairs of iterates and\ntentative steps generated by RTR under A6.1, A6.3 and A6.5 with \u03b5g = 0 (we\naim for first-order criticality). Further assume A6.6 and A6.8 hold on S. Then,\nlim\nk\u2192\u221e\n\u2225gradf(xk)\u2225xk = 0. (6.43)\nIn particular, all accumulation points of x0, x1, . . .(if any) are critical points.\nProof. If iteration k is unsuccessful ( \u03c1k \u2264 \u03c1\u2032), then xk+1 = xk. If iteration k is\nsuccessful, then A6.8 guarantees\n\u2225gradf(xk+1)\u2225xk+1 \u2265 \u2225gradf(xk)\u2225xk \u2212 Lgn\u2225sk\u2225xk .\nFix an arbitrary index m such that gradf(xm) \u0338= 0. For all \u2113 \u2265 m, we have\n\u2225gradf(x\u2113+1)\u2225x\u2113+1 \u2265 \u2225gradf(xm)\u2225xm \u2212 Lgn\nX\nm\u2264k\u2264\u2113\n\u03c1k>\u03c1\u2032\n\u2225sk\u2225xk .\nPick the smallest \u2113 \u2265 m such that \u2225gradf(x\u2113+1)\u2225x\u2113+1 \u2264 1\n2 \u2225gradf(xm)\u2225xm: we\nknow such \u2113 exists owing to Corollary 6.24. Then,\n\u2225gradf(xm)\u2225xm \u2264 2Lgn\nX\nm\u2264k\u2264\u2113\n\u03c1k>\u03c1\u2032\n\u2225sk\u2225xk \u2264 2Lgn\nX\nm\u2264k\u2264\u2113\n\u03c1k>\u03c1\u2032\n\u2206k. (6.44)\nGiven our choice of \u2113, we have \u2225gradf(xk)\u2225xk > 1\n2 \u2225gradf(xm)\u2225xm for k =", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2561, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3f732582-6365-46c3-ab8b-08477bc89495": {"__data__": {"id_": "3f732582-6365-46c3-ab8b-08477bc89495", "embedding": null, "metadata": {"page_label": "142", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c463ac14-cb93-49be-9759-3ca4874c7d4a", "node_type": "4", "metadata": {"page_label": "142", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "5de27f240c7fa5607bd9e1ba9cc875f6670c97d713df769b79328af714fca44f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n142 Second-order optimization algorithms\nm, . . . , \u2113. Also, xk+1 = R xk (sk) for all k such that \u03c1k > \u03c1\u2032. It thus follows\nfrom A6.3 and from the definition of \u03c1k (6.22) that\nf(xm) \u2212 f(x\u2113+1) =\nX\nm\u2264k\u2264\u2113\n\u03c1k>\u03c1\u2032\nf(xk) \u2212 f(xk+1)\n\u2265\nX\nm\u2264k\u2264\u2113\n\u03c1k>\u03c1\u2032\n\u03c1\u2032c2\n2 min\n\u0012\n\u2206k, \u2225gradf(xm)\u2225xm\n2c0\n\u0013\n\u2225gradf(xm)\u2225xm.\n(6.45)\nThere are two scenarios to consider. Either \u2225gradf(xm)\u2225xm\n2c0\n\u2264 \u2206k for some k in the\nsummation range, in which case we use the corresponding term to lower-bound\nthe sum:\nf(xm) \u2212 f(x\u2113+1) \u2265 \u03c1\u2032c2\n4c0\n\u2225gradf(xm)\u22252\nxm. (6.46)\nOr \u2225gradf(xm)\u2225xm\n2c0\n> \u2206k for all k in the summation range, in which case we use\nboth (6.45) and (6.44) to see that\nf(xm) \u2212 f(x\u2113+1) \u2265 \u03c1\u2032c2\n2 \u2225gradf(xm)\u2225xm\nX\nm\u2264k\u2264\u2113\n\u03c1k>\u03c1\u2032\n\u2206k\n\u2265 \u03c1\u2032c2\n4Lgn\n\u2225gradf(xm)\u22252\nxm. (6.47)\nThe sequence of function values f(x0), f(x1), . . .is lower-bounded by A6.5 and\nnon-increasing, hence it converges to somef\u221e. Combining the results above with\nf(xm) \u2212 f\u221e \u2265 f(xm) \u2212 f(x\u2113+1), we find for all m that\nf(xm) \u2212 f\u221e \u2265 \u03c1\u2032c2\n4 max(Lgn, c0)\u2225gradf(xm)\u22252\nxm. (6.48)\nTake the limit m \u2192 \u221eto conclude, using f(xm) \u2212 f\u221e \u2192 0.\n6.4.6 Practical aspects\nWe list some practical considerations in a nutshell:\n1. A typical value for \u03c1\u2032 is 1\n10 .\n2. Possible default settings for \u00af\u2206 are\n\u221a\ndim M or the diameter of the manifold\nif it is bounded; and \u2206 0 = 1\n8\n\u00af\u2206.\n3. Hk is often taken to be Hess f(xk) when available, regardless of whether or\nnot the retraction is second order. This does not affect local convergence rates\nsince close to critical points the distinction between first- and second-order\nretraction is irrelevant for us.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1808, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "789440ce-5414-4224-8d3c-68243b6a2a81": {"__data__": {"id_": "789440ce-5414-4224-8d3c-68243b6a2a81", "embedding": null, "metadata": {"page_label": "143", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a0169162-c9aa-4f25-a0d8-aa7f91f9aa5d", "node_type": "4", "metadata": {"page_label": "143", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "02b50368ad0e6f073c4a67cc4c931a8abaf840a8fcba944879638cd3fca709f1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n6.4 Riemannian trust regions 143\n4. Practical stopping criteria for RTR typically involve an upper-bound on the\ntotal number of iterations and a threshold on the gradient norm such as:\nterminate if \u2225gradf(xk)\u2225xk \u2264 \u03b5g. Typically,\u03b5g = 10\u22128\u2225gradf(x0)\u2225x0 is a good\nvalue. It is rare that one would explicitly check the eigenvalues of Hess f(xk)\nbefore termination.\n5. Computing \u03c1k (6.22) can be delicate close to convergence, as it involves the\ncomputation of f(xk) \u2212 f(x+\nk ): a difference of two potentially large numbers\nthat could be dangerously close to one another. Specifically, say we compute\nf(xk) and we store it in memory in the variable f1. Even if f(xk) is computed\nwith maximal accuracy, it must eventually be rounded to one of the real\nnumbers that are exactly representable in, say, double precision, that is, on\n64 bits following the IEEE 754 standard. This standard guarantees a relative\naccuracy of \u03b5M \u2248 10\u221216, so that f1 = f(xk)(1 + \u03b51) with |\u03b51| \u2264\u03b5M. This is a\nrelative accuracy guarantee since\n|f1 \u2212 f(xk)|\n|f(xk)| \u2264 \u03b5M.\n(In practice, computing f(xk) would involve further errors leading to a larger\nright-hand side.) Likewise, f2 = f(x+\nk )(1 + \u03b52) with |\u03b52| \u2264\u03b5M.\nAssuming the difference between f1 and f2 is exactly representable in mem-\nory,1 in computing the numerator for \u03c1k we truly compute\nf1 \u2212 f2 = f(xk) \u2212 f(x+\nk ) + \u03b51f(xk) \u2212 \u03b52f(x+\nk ).\nThe best we can claim in general about the relative error is:\n|(f1 \u2212 f2) \u2212 (f(xk) \u2212 f(x+\nk ))|\n|f(xk) \u2212 f(x+\nk )| \u2264 \u03b5M\n|f(xk)| + |f(x+\nk )|\n|f(xk) \u2212 f(x+\nk )| .\nThe right-hand side can be catastrophically large. Indeed, if f(xk) and f(x+\nk )\nare large in absolute value yet their difference is very small (which may happen\nnear convergence), the relative error on the computation of the numerator of\n\u03c1k may make it useless. For example, with f(xk) = 104 and f(xk) \u2212 f(x+\nk ) =\n10\u221212, the relative error bound is close to 1, meaning none of the digits in the\ncomputed numerator can be trusted. In turn, this can lead to wrong decisions\nin RTR regarding step rejections and trust-region radius updates.\nNo such issues plague the denominator, provided it is appropriately com-\nputed. Indeed,\nmk(0) \u2212 mk(sk) = \u2212 \u27e8sk, gradf(xk)\u27e9xk\n\u2212 1\n2\u27e8sk, Hk(sk)\u27e9xk . (6.49)\nUsing the right-hand side for computation, if the step sk is small and the\ngradient is small, then we combine two small real numbers, which is not as\ndangerous as computation of the left-hand side.\n1 By the Sterbenz lemma, this is true if f1, f2 are within a factor 2 of each other.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2761, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bd6618b0-d6ba-48e2-9e37-bfd3bc0b7092": {"__data__": {"id_": "bd6618b0-d6ba-48e2-9e37-bfd3bc0b7092", "embedding": null, "metadata": {"page_label": "144", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9060b0b9-b204-4e80-a786-77048b462699", "node_type": "4", "metadata": {"page_label": "144", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "4be976f7e3cabe596305a3988780f1948622282a5e001388799ae833fcb49f38", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n144 Second-order optimization algorithms\nA standard fix [CGT00, \u00a7 17.4.2] to these numerical issues is to regularize\nthe computation of \u03c1k as\n\u03c1k = f(xk) \u2212 f(x+\nk ) + \u03b4k\n\u2212 \u27e8sk, gradf(xk)\u27e9xk\n\u2212 1\n2 \u27e8sk, Hk(sk)\u27e9xk + \u03b4k\n, (6.50)\nwith\n\u03b4k = max(1, |f(xk)|)\u03b5M\u03c1reg. (6.51)\nThe parameter \u03c1reg can be set to 103 for example. When both the true numer-\nator and denominator of \u03c1k become very small near convergence, the regular-\nization nudges (6.50) toward 1, which leads to step acceptance as expected.\nThis is a heuristic to (try to) address an inescapable limitation of inexact\narithmetic, though a detailed analysis by Sun and Nocedal provides insight\ninto what one may reasonably guarantee with it [SN22].\n6. Care should be put in implementations to minimize the number of calls to the\nmap Hk. For example, in the subproblem solver described in Section 6.5 below,\nexactly one call to Hk is needed per iteration, and furthermore the vector\nHk(sk) is a by-product of that algorithm when Hk is linear (Exercise 6.28),\nso that computing the denominator of \u03c1k does not require further calls to Hk.\n6.5 The trust-region subproblem: truncated CG\nThe trust-region subproblem (6.21) consists in approximately solving a problem\nof the form\nmin\ns\u2208TxM\nm(s) subject to \u2225s\u2225x \u2264 \u2206 where m(s) = 1\n2\u27e8s, Hs\u27e9x \u2212 \u27e8b, s\u27e9x , (6.52)\nwith a map H : TxM \u2192TxM, a tangent vector b \u2208 TxM and a radius \u2206 > 0.\nAt iteration k of RTR, these objects are H = Hk, b = \u2212gradf(xk) and \u2206 = \u2206k.\nWe consider the important particular case where H is a linear, self-adjoint\nmap (e.g., Hk = Hess f(xk)). Then, m: TxM \u2192R is a quadratic function.\nAside from the constraint \u2225s\u2225x \u2264 \u2206, if H is furthermore positive definite, then\nwe know from Section 6.3 that conjugate gradients (CG, Algorithm 6.2) can be\nused to compute a global minimizer of m: simply compare functions in (6.4)\nand (6.52).\nThe general idea of the truncated CG method (tCG), Algorithm 6.4, is to run\nCG on m(s) (6.52) while\n1. Keeping an eye out for signs that H may not be positive definite;\n2. Checking whether we left the trust region; and\n3. Looking for opportunities to terminate early even if neither of those events\nhappen.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2402, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2a37e9d8-92f9-412f-a2eb-5a99985a592b": {"__data__": {"id_": "2a37e9d8-92f9-412f-a2eb-5a99985a592b", "embedding": null, "metadata": {"page_label": "145", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4d06aa7c-a9f3-434c-9426-ac9b12d5e2d2", "node_type": "4", "metadata": {"page_label": "145", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "c67fe010fe548937fd7c1b480c2c958d7bdaf7383f5be2449a2acd934ada6f84", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c969372-a9bf-4e6c-ae2a-5e35d8440a5f", "node_type": "1", "metadata": {}, "hash": "1b388593ed898f6e3304b00255bb3c8fc1ed639fd8c6a0c747a69a8f57c36915", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n6.5 The trust-region subproblem: truncated CG 145\nRecall that CG generates directions pi. If the scalars \u27e8pi, Hpi\u27e9x are positive\nfor i = 0, . . . , n\u22122, then p0, . . . , pn\u22122 are linearly independent hence they form a\nbasis for a subspace of T xM. Moreover, H is positive definite on that subspace.\nThus, up to that point, all the properties of CG hold. If, however, upon consid-\nering pn\u22121 we determine that \u27e8pn\u22121, Hpn\u22121\u27e9x is non-positive, then this is proof\nthat H is not positive definite. In such situation, tCG computes the next step\nvn by moving away from vn\u22121 along pn\u22121 so as to minimize the model m, that\nis, tCG sets vn = vn\u22121 + tpn\u22121 with t such that m(vn) is minimized, under the\nconstraint \u2225vn\u2225x \u2264 \u2206. There are two candidates for the value of t, namely, the\ntwo roots of the quadratic\n\u2225vn\u22121 + tpn\u22121\u22252\nx \u2212 \u22062 = \u2225pn\u22121\u22252\nxt2 + 2t \u27e8vn\u22121, pn\u22121\u27e9x + \u2225vn\u22121\u22252\nx \u2212 \u22062. (6.53)\nThe product of these roots is negative since \u2225vn\u22121\u2225x < \u2206 (otherwise we would\nhave already terminated), hence one root is positive and the other is negative. It\ncan be shown that selecting the positive root leads to the smallest value in the\nmodel [ABG07, \u00a7 3].\nNow assuming \u27e8pn\u22121, Hpn\u22121\u27e9x is positive, we consider the tentative new step\nv+\nn\u22121 = vn\u22121 + \u03b1npn\u22121. If this step lies outside the trust region, it seems at first\nthat we face a dilemma. Indeed, a priori, it might happen that later iterates re-\nenter the trust region, in which case it would be unwise to stop. Fortunately, this\ncannot happen. Specifically, it can be shown that steps grow in norm, so that if\none iterate leaves the trust region, then no future iterate re-enters it [CGT00,\nThm. 7.5.1], [NW06, Thm. 7.3]. Thus, it is reasonable to act now: tCG proceeds\nby reducing how much we move along pn\u22121, setting vn = vn\u22121 + tpn\u22121 instead\nwith t \u2265 0 being the largest value that fulfills the trust-region constraint. This\nhappens to correspond exactly to the positive root of the quadratic in eq. (6.53).\nIn the unlikely event thatv+\nn\u22121 lies exactly on the boundary of the trust region, it\nmakes sense to stop by the same argument: this is why we test for \u2225v+\nn\u22121\u2225x \u2265 \u2206\nwith a non-strict inequality.\nFinally, if neither non-positive \u27e8pi, Hpi\u27e9x are encountered nor do the steps\nleave the trust region, we rely on a stopping criterion to terminate tCG early.\nThe principle is that we should only work hard to solve the subproblem when\nRTR is already close to convergence. Specifically, withr0 = b = \u2212gradf(xk), the\nchosen stopping criterion with parameters \u03b8 and \u03ba allows tCG to terminate if\n\u2225rn\u2225xk \u2264 \u2225gradf(xk)\u2225xk \u00b7 min(\u2225gradf(xk)\u2225\u03b8\nxk , \u03ba). (6.54)\nIt is only when the gradient of f is small that tCG puts in the extra effort to\nreach residuals as small as \u2225gradf(xk)\u22251+\u03b8\nxk . This is key to obtain superlinear\nconvergence, of order min(1 + \u03b8, 2) (in particular, quadratic convergence for \u03b8 =\n1), see Theorem 6.30 below.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3123, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2c969372-a9bf-4e6c-ae2a-5e35d8440a5f": {"__data__": {"id_": "2c969372-a9bf-4e6c-ae2a-5e35d8440a5f", "embedding": null, "metadata": {"page_label": "145", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4d06aa7c-a9f3-434c-9426-ac9b12d5e2d2", "node_type": "4", "metadata": {"page_label": "145", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "c67fe010fe548937fd7c1b480c2c958d7bdaf7383f5be2449a2acd934ada6f84", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a37e9d8-92f9-412f-a2eb-5a99985a592b", "node_type": "1", "metadata": {"page_label": "145", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "f6878d94fb44b8252c11d82d1701f7cd3643a224ec0c9b1886d8146cc372adab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The principle is that we should only work hard to solve the subproblem when\nRTR is already close to convergence. Specifically, withr0 = b = \u2212gradf(xk), the\nchosen stopping criterion with parameters \u03b8 and \u03ba allows tCG to terminate if\n\u2225rn\u2225xk \u2264 \u2225gradf(xk)\u2225xk \u00b7 min(\u2225gradf(xk)\u2225\u03b8\nxk , \u03ba). (6.54)\nIt is only when the gradient of f is small that tCG puts in the extra effort to\nreach residuals as small as \u2225gradf(xk)\u22251+\u03b8\nxk . This is key to obtain superlinear\nconvergence, of order min(1 + \u03b8, 2) (in particular, quadratic convergence for \u03b8 =\n1), see Theorem 6.30 below. Intuitively, superlinear convergence occurs because\nwhen xk is close to a critical point with positive definite Hessian, and with\nHk = Hess f(xk), steps produced by tCG are increasingly similar to Newton\nsteps.\nThe comments at the end of Section 6.3 regarding how to run CG in practice", "mimetype": "text/plain", "start_char_idx": 2561, "end_char_idx": 3409, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4c136cfd-eecb-4cec-a3cd-e95aac4e4eef": {"__data__": {"id_": "4c136cfd-eecb-4cec-a3cd-e95aac4e4eef", "embedding": null, "metadata": {"page_label": "146", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb7f030c-667a-4363-b213-aaee39cf1873", "node_type": "4", "metadata": {"page_label": "146", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "fa0c9295a2397d8252e15027a8adc584e9b6e0edb3587914b3157f46711eab8a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n146 Second-order optimization algorithms\nAlgorithm 6.4 tCG: truncated conjugate gradients on a tangent space\nParameters: \u03ba \u2265 0, \u03b8\u2208 (0, 1], e.g., \u03ba = 1\n10 , \u03b8 = 1\nInput: self-adjoint H on TxM, b \u2208 TxM and radius \u2206 > 0\nOutput: approximate minimizer of m(s) = 1\n2 \u27e8s, Hs\u27e9x \u2212 \u27e8b, s\u27e9x subject to\n\u2225s\u2225x \u2264 \u2206\nSet v0 = 0, r0 = b, p0 = r0\nIf r0 = 0\noutput s = v0\nFor n = 1, 2, . . .\nCompute Hpn\u22121 (this is the only call to H)\nCompute \u27e8pn\u22121, Hpn\u22121\u27e9x\n\u03b1n = \u2225rn\u22121\u22252\nx\n\u27e8pn\u22121,Hpn\u22121\u27e9x\nv+\nn\u22121 = vn\u22121 + \u03b1npn\u22121\nIf \u27e8pn\u22121, Hpn\u22121\u27e9x \u2264 0 or \u2225v+\nn\u22121\u2225x \u2265 \u2206\nSet vn = vn\u22121 + tpn\u22121 with t \u2265 0 such that \u2225vn\u2225x = \u2206\n(t is the positive root of the quadratic in (6.53).)\noutput s = vn\nvn = v+\nn\u22121\nrn = rn\u22121 \u2212 \u03b1nHpn\u22121\nIf \u2225rn\u2225x \u2264 \u2225r0\u2225x min(\u2225r0\u2225\u03b8\nx, \u03ba)\noutput s = vn\n\u03b2n = \u2225rn\u22252\nx\n\u2225rn\u22121\u22252x\npn = rn + \u03b2npn\u22121\napply to tCG as well. Specifically, it is common to set a hard limit on the maxi-\nmum number of iterations, and it is beneficial to ensure tangent vectors remain\ntangent numerically.\nJust like regular CG, tCG can be preconditioned [CGT00, \u00a7 5.1.6]: this can\nimprove performance dramatically. In a precise sense, preconditioning tCG is\nequivalent to changing the Riemannian metric [MS16].\nFinally, it is good to know that the trust-region subproblem, despite being non-\nconvex, can be solved to global optimality efficiently. See [Vav91] and [CGT00,\n\u00a7 7] for pointers to a vast literature.\nExercise 6.26. Show that v1 as computed by Algorithm 6.4 is the Cauchy point\nas constructed in Lemma 6.15. Since iterates monotonically improvem(vn) (6.52)\nthis implies that tCG guarantees A6.3 (p134) with c2 = 1\n2 .\nExercise 6.27. Consider using tCG within RTR, so that b = \u2212gradf(xk) and\nH = Hk at iteration k of RTR. If b = 0, tCG terminates immediately with s = 0\n(this leads RTR to set sk = 0, so that \u03c1k = 0\n0 (not-a-number); standard extended\narithmetic conventions then lead RTR to set xk+1 = xk and \u2206k+1 = \u2206k). Check", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2139, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "74b7c3f8-124d-4411-9ab5-2ee690f93785": {"__data__": {"id_": "74b7c3f8-124d-4411-9ab5-2ee690f93785", "embedding": null, "metadata": {"page_label": "147", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ae5806c0-979f-448e-a6ed-7e428fcd265a", "node_type": "4", "metadata": {"page_label": "147", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "1cce04b5e2b84cfcd5cce70ff692544f798f66b57c55c73b2b54e3ab241dd923", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n6.6 Local convergence of RTR with tCG* 147\nthat this may violate A6.4 (p134) if Hk has negative eigenvalues (in particular,\ntCG does not compute a global minimum of the trust-region subproblem in this\ncase). Explain why it is necessary for tCG to terminate immediately if b = 0, that\nis, explain why even if we skip the initial \u201cif\u201d statement the rest of the algorithm\nwould not be able to exploit the negative eigenvalues of Hk. See [CGT00, \u00a7 7.5.4]\nfor a fix based on Lanczos iterations.\nExercise 6.28. Algorithm 6.4 terminates with a vector s as output. Show that\nthe same algorithm can also output Hs as a by-product without requiring ad-\nditional calls to H. Explicitly, if the second \u201coutput\u201d statement triggers, then\nHs = b \u2212 rn\u22121 + tHpn\u22121; and if the third \u201coutput\u201d statement triggers, then\nHs = b \u2212 rn. This is useful to compute the denominator of \u03c1k (6.22) in the\ntrust-region method via (6.49).\n6.6 Local convergence of RTR with tCG*\nUnder suitable assumptions, once iterates of RTR are close enough to a critical\npoint where the Hessian is positive definite, RTR converges superlinearly to that\npoint provided subproblems are solved with sufficient accuracy (for example,\nusing tCG). The two theorems below make this precise. They are (in some ways,\nrestricted) variations of claims found in [ABG07] and [AMS08, \u00a7 7]. The proofs\nare omitted.\nThe first result is a variation of [AMS08, Thm. 7.4.10]: it is a type of capture\ntheorem for RTR with tCG. It involves a special assumption on the retraction\nthat prevents undue distance distortions. It holds in particular if the retraction\nis the exponential map (with c5 = 1), and it also holds if M is compact (see\nLemma 6.32 below).\nA 6.9. There exist positive constants c4, c5 such that, for all (x, v) \u2208 TM, if\n\u2225v\u2225x \u2264 c4 then dist(x, Rx(v)) \u2264 c5\u2225v\u2225x.\nBelow, we require that Hess f is continuous.\nTheorem 6.29. Let S = {(x0, s0), (x1, s1), . . .} be the pairs of iterates and\ntentative steps generated by RTR with tCG as subproblem solver, with models\nHk = Hess( f \u25e6 Rxk )(0) or Hk = Hess f(xk). Assume \u2225Hk\u2225 \u2264c0 with some\nconstant c0 for all k, so that A6.1 and A6.3 hold, and also assume f is lower-\nbounded as in A6.5. Further assume A6.6 and A6.8 hold on S. (In particular,\nthe assumptions of Proposition 6.25 hold, so that \u2225gradf(xk)\u2225xk \u2192 0.) Let the\nretraction satisfy A6.9.\nLet x\u22c6 \u2208 Msatisfy gradf(x\u22c6) = 0 and Hessf(x\u22c6) \u227b 0\u2014in particular, it is a\nlocal minimizer. There exists a neighborhood U of x\u22c6 such that, if xk is in U for\nsome k, then all subsequent iterates are in U and they converge to x\u22c6.\nThe second result is a restriction of [AMS08, Thm. 7.4.11]. It establishes su-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2901, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "035c3369-ba1b-4ddf-941a-1008af940660": {"__data__": {"id_": "035c3369-ba1b-4ddf-941a-1008af940660", "embedding": null, "metadata": {"page_label": "148", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "42659385-bfb6-4873-b34e-7525fa958db3", "node_type": "4", "metadata": {"page_label": "148", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "3b54a70c6a6c1813b4d777824bb2772620a47e4befb7d505cd0bbd3b2734681a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n148 Second-order optimization algorithms\nperlinear local convergence (recall Definitions 4.14 and 4.15). We require that\nHessf is continuously differentiable.\nTheorem 6.30. Let S = {(x0, s0), (x1, s1), . . .} be the pairs of iterates and ten-\ntative steps generated by RTR with tCG as subproblem solver, either with models\nHk = Hess(f \u25e6 Rxk )(0) or with models Hk = Hessf(xk). If the latter, assume\nthere exists a constant c6 such that \u2225c\u2032\u2032(0)\u2225x \u2264 c6 for all curves of the type\nc(t) = R x(ts) with s \u2208 TxM of unit norm and x = xk for some k\u2014this holds\nwith c6 = 0 if the retraction is second order.\nLet x\u22c6 \u2208 Msatisfy gradf(x\u22c6) = 0 and Hessf(x\u22c6) \u227b 0. If the sequence\nx0, x1, x2, . . .converges to x\u22c6 (as Theorem 6.29 might provide), then there exist\na constant c7 > 0 and an index K such that, for all k \u2265 K, we have\ndist(xk+1, x\u22c6) \u2264 c7 dist(xk, x\u22c6)min(\u03b8+1,2),\nwhere \u03b8 >0 is a parameter in the stopping criterion of tCG. In particular, with\n\u03b8 = 1 convergence is at least quadratic.\n6.7 Simplified assumptions for RTR with tCG*\nThe main theorems of Sections 6.4 and 6.6 involve a number of assumptions that\nneed to be checked in order to claim convergence guarantees for RTR. In this\nsection, we restrict the discussion to RTR with tCG as subproblem solver and\ninclude simple assumptions that simplify the process of verifying that all other\nassumptions hold. The resulting statements are more restrictive than above, but\nthey can often be applied directly in applications. This is especially simple if M\nis compact, as is the case for the Stiefel and the Grassman manifolds for example.\nThroughout, we require that Hess f is continuously differentiable.\nProposition 6.31. Let S = {(x0, s0), (x1, s1), . . .} be the pairs of iterates and\ntentative steps generated by RTR with models Hk = Hess(f \u25e6 Rxk )(0) and tCG\nas subproblem solver. (If the retraction is second order, the models coincide with\nHessf(xk).) Assume the iterates x0, x1, . . .are contained in a compact subset\nof M. (This holds in particular if M is compact, or if any of the sublevel sets\n{x \u2208 M: f(x) \u2264 f(xk)} is compact.) Then, A6.1 holds with some c0 \u2265 0, A6.2\nholds with c1 = 0, A6.3 holds with c2 = 1\n2 (Exercise 6.26), A6.4 may not hold\n(Exercise 6.27), A6.5 holds with flow = inf k f(xk) > \u2212\u221e, A6.6 and A6.7 hold\non S with some constants Lg and LH (Lemma 10.57), and A6.8 holds on S with\nsome constant Lgn.\nIf the subproblem solver is replaced by one which solves the trust-region sub-\nproblem to optimality, then all of the above remain true except A6.4 is also\nsatisfied with c3 = 1\n2 (Corollary 6.17).\nProof. All claims are clear except for the gradient-norm Lipschitz-type assump-\ntion A6.8 which we now verify explicitly. In so doing, we use concepts from", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2990, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "95a5da59-6f84-4a97-92cb-00b54a29db60": {"__data__": {"id_": "95a5da59-6f84-4a97-92cb-00b54a29db60", "embedding": null, "metadata": {"page_label": "149", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0732527c-dd95-441a-b4fe-24ba2ec7b5df", "node_type": "4", "metadata": {"page_label": "149", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "d2d9231449aaaaf1ff346efa597ac69a308775da424d4fadf45e4de81a444c99", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b65aa98e-e119-4792-8384-d198d5bf3f6f", "node_type": "1", "metadata": {}, "hash": "cdf8a407f2ca04ec75de3ea24fb2339333be8fc54dd355d4bfeb9ab9dace1c29", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n6.7 Simplified assumptions for RTR with tCG* 149\nSections 10.1 (length of curves), 10.3 (parallel transport) and 10.4 (Lipschitz\ncontinuity).\nSince the iterates xk are contained in a compact set K we know that S is\nincluded in T = {(x, s) : x \u2208 Kand \u2225s\u2225x \u2264 \u00af\u2206} which is compact in T M\n(Exercise 10.31). For each ( x, s) \u2208 TM, the map DR x(s) is linear from T xM\nto TRx(s)M. Its operator norm is continuous as a function of ( x, s) since R is\nsmooth, that is, the function ( x, s) 7\u2192 \u2225DRx(s)\u2225 is continuous on T . Since T is\ncompact, we deduce that there exists a constant r such that \u2225DRx(s)\u2225 \u2264r for\nall (x, s) \u2208 T. Consequently, with (x, s) \u2208 Tarbitrary and c(t) = Rx(ts), we find\nthat the length of the curve c on the interval [0, 1] satisfies:\nL(c) =\nZ 1\n0\n\u2225c\u2032(t)\u2225c(t)dt\n=\nZ 1\n0\n\u2225DRx(ts)[s]\u2225c(t)dt \u2264\nZ 1\n0\nr\u2225s\u2225xdt = r\u2225s\u2225x. (6.55)\nThe set R( T ) = {Rx(s) : ( x, s) \u2208 T }is compact in M since it is the image\nof a compact set through a continuous map. Thus, Hess f is continuous hence\nbounded (in operator norm) by some constant q on R(T ). Writing PT c\n1\u21900 for\nparallel transport along a curve c from t = 0 to t = 1 (this is an isometry from\nTc(0)M to Tc(1)M), it follows with c(t) = R x(ts) and using Proposition 10.46\nthat, for all ( x, s) \u2208 T,\n\u2225gradf(Rx(s))\u2225Rx(s) = \u2225gradf(Rx(s)) \u2212 PTc\n1\u21900gradf(x) + PTc\n1\u21900gradf(x)\u2225Rx(s)\n\u2264 \u2225gradf(Rx(s)) \u2212 PTc\n1\u21900gradf(x)\u2225Rx(s) + \u2225gradf(x)\u2225x\n\u2264 qL(c) + \u2225gradf(x)\u2225x\n\u2264 qr\u2225s\u2225x + \u2225gradf(x)\u2225x.\nThus, \u2225gradf(Rx(s))\u2225Rx(s) \u2212 \u2225gradf(x)\u2225x \u2264 qr\u2225s\u2225x. A similar argument shows\nthat \u2225gradf(x)\u2225x \u2212\u2225gradf(Rx(s))\u2225Rx(s) \u2264 qr\u2225s\u2225x, so that A6.8 holds on T with\nLgn = qr.\nLemma 6.32. Any retraction R on a compact manifold M satisfies A6.9.\nProof. For all c4 > 0 the set T = {(x, v) \u2208 TM : \u2225v\u2225x \u2264 c4} is compact\n(Exercise 10.31) hence there existsc5 > 0 such that\u2225DRx(v)\u2225 \u2264c5 for all (x, v) \u2208\nT (by continuity of the operator norm and smoothness of the retraction). It then\nfollows from (6.55) and from the definitions of distance and length (Section 10.1)\nthat\ndist(x, Rx(v)) \u2264 L(c) \u2264 c5\u2225v\u2225x, (6.56)\nwhere c(t) = Rx(tv) is a curve from c(0) = x to c(1) = Rx(v).\nCorollary 6.33. Let S = {(x0, s0), (x1, s1), . . .}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2389, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b65aa98e-e119-4792-8384-d198d5bf3f6f": {"__data__": {"id_": "b65aa98e-e119-4792-8384-d198d5bf3f6f", "embedding": null, "metadata": {"page_label": "149", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0732527c-dd95-441a-b4fe-24ba2ec7b5df", "node_type": "4", "metadata": {"page_label": "149", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "d2d9231449aaaaf1ff346efa597ac69a308775da424d4fadf45e4de81a444c99", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95a5da59-6f84-4a97-92cb-00b54a29db60", "node_type": "1", "metadata": {"page_label": "149", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "6ac288dda8de1f2cedfa2706b6c5ea4ea9803c652668bcf1d65161f68e2eb7cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For all c4 > 0 the set T = {(x, v) \u2208 TM : \u2225v\u2225x \u2264 c4} is compact\n(Exercise 10.31) hence there existsc5 > 0 such that\u2225DRx(v)\u2225 \u2264c5 for all (x, v) \u2208\nT (by continuity of the operator norm and smoothness of the retraction). It then\nfollows from (6.55) and from the definitions of distance and length (Section 10.1)\nthat\ndist(x, Rx(v)) \u2264 L(c) \u2264 c5\u2225v\u2225x, (6.56)\nwhere c(t) = Rx(tv) is a curve from c(0) = x to c(1) = Rx(v).\nCorollary 6.33. Let S = {(x0, s0), (x1, s1), . . .} be the pairs of iterates and\ntentative steps generated by RTR with models Hk = Hess(f \u25e6 Rxk )(0) and tCG\nas subproblem solver.\nIf the sublevel set {x \u2208 M: f(x) \u2264 f(x0)} is compact (which holds if M is", "mimetype": "text/plain", "start_char_idx": 1923, "end_char_idx": 2590, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "88c1e8a8-353e-4a70-a512-a77d6be0d856": {"__data__": {"id_": "88c1e8a8-353e-4a70-a512-a77d6be0d856", "embedding": null, "metadata": {"page_label": "150", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0c5b937c-d14e-4a49-b616-b9550e7661e7", "node_type": "4", "metadata": {"page_label": "150", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "6ef70ca5c4ae7ef922dcb41ad23cbb9da090fed543e9079b21900f1f29ab77fe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n150 Second-order optimization algorithms\ncompact), then the sequence of iterates x0, x1, x2, . . .has at least one accumula-\ntion point and all of its accumulation points are critical points.\nFurther assume the retraction satisfies A6.9 (this holds if M is compact by\nLemma 6.32, or if M is complete and the retraction is the exponential map). If\none of the accumulation points has a positive definite Hessian, then the sequence\nconverges to that point with a superlinear local convergence rate (quadratic if\n\u03b8 = 1 in tCG).\nProof. RTR is a descent method (f(xk+1) \u2264 f(xk) for all k) hence the sequence\nx0, x1, . . .is contained in a compact set: this ensures that it has at least one\naccumulation point. All of these accumulation points are critical points owing\nto Proposition 6.25, whose assumptions are satisfied owing to Proposition 6.31.\nIf A6.9 holds too, then Theorem 6.29 applies, guaranteeing that if any of the\naccumulation points has a positive definite Hessian then that critical point is\nattractive: eventually, the sequence enters any neighborhood of that point and\nconverges to it as a result. The rate of convergence follows from Theorem 6.30.\n6.8 Numerically checking a Hessian*\nIn Section 4.8, we considered a numerical method to check whether code to\ncompute the Riemannian gradient is correct. Similarly, we now describe a method\nto check code for the Riemannian Hessian. In the Matlab toolbox Manopt, this\nmethod is implemented as checkhessian.\nThe two first points to check are:\n1. That Hess f(x) indeed maps T xM to TxM linearly, and\n2. That it is indeed a self-adjoint map.\nThis can be done numerically by generating a random x \u2208 Mand two ran-\ndom tangent vectors u, v\u2208 TxM, computing both Hessf(x)[u] and Hessf(x)[v],\nverifying that these are tangent, checking that\nHessf(x)[au + bv] = aHessf(x)[u] + bHessf(x)[v]\nfor some random scalars a, b, and finally confirming that\n\u27e8u, Hessf(x)[v]\u27e9x = \u27e8Hessf(x)[u], v\u27e9x ,\nall up to machine precision.\nThis being secured, consider the Taylor expansion (5.28): if R is a second-order\nretraction, or if x is a critical point, then\nf(Rx(tv)) = f(x) + t \u27e8gradf(x), v\u27e9x + t2\n2 \u27e8Hessf(x)[v], v\u27e9x + O(t3). (6.57)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2427, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "31ff4633-8026-45f0-bfed-e36f193bc65d": {"__data__": {"id_": "31ff4633-8026-45f0-bfed-e36f193bc65d", "embedding": null, "metadata": {"page_label": "151", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1fc2926b-98cc-4241-b175-fa310ee487b1", "node_type": "4", "metadata": {"page_label": "151", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "1652eeb4c7d47fdfb02ff268e14570e0a2729024dd460d62a41ac3b83bdfe96c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n6.9 Notes and references 151\nThis says that, under the stated conditions,\nE(t) \u225c\n\f\f\f\ff(Rx(tv)) \u2212 f(x) \u2212 t \u27e8gradf(x), v\u27e9x \u2212 t2\n2 \u27e8Hessf(x)[v], v\u27e9x\n\f\f\f\f = O(t3).\nTaking the logarithm on both sides, we find that log( E(t)) must grow approxi-\nmately linearly in log( t), with a slope of three (or more) when t is small:\nlog(E(t)) \u2248 3 log(t) + constant.\nThis suggests a procedure to check the Hessian numerically:\n1. Check that the gradient is correct (Section 4.8);\n2. Run the preliminary checks (tangency, linearity and symmetry);\n3. If using a second-order retraction, generate a random point x \u2208 M; otherwise,\nfind an (approximate) critical point x \u2208 M, for example using Riemannian\ngradient descent;\n4. Generate a random tangent vector v \u2208 TxM with \u2225v\u2225x = 1;\n5. Compute f(x), \u27e8gradf(x), v\u27e9x and \u27e8Hessf(x)[v], v\u27e9x;\n6. Compute E(t) for several values of t logarithmically spaced on the interval\n[10\u22128, 100];\n7. Plot E(t) as a function of t, in a log\u2013log plot;\n8. Check that the plot exhibits a slope of three (or more) over several orders of\nmagnitude.\nAgain, we do not expect to see a slope of three over the whole range, but we do\nexpect to see this over a range of values of t covering at least one or two orders\nof magnitude. Of course, the test is less conclusive if it has to be run at a critical\npoint. Even if computing second-order retractions turns out to be expensive for\nthe manifold at hand, its use here as part of a diagnostics tool is worthwhile: we\nare still free to use any other retraction for the optimization algorithm.\n6.9 Notes and references\nFirst- and second-order optimality conditions are further studied in [YZS14,\nBH19], notably to include the case of constrained optimization on manifolds.\nNewton\u2019s method on manifolds is analyzed in most treatments of optimization\non manifolds; see for example [AMS08, \u00a7 6] and the many references therein,\nincluding [ADM+02, Man02]. In particular, the convergence result Theorem 6.7\nand Exercise 6.9 correspond to [AMS08, Thm. 6.3.2]. The reference material for\nthe discussion of conjugate gradients in Section 6.3 is [TB97, Lect. 38].\nAssumption A6.9 in Section 6.6 parallels an assumption made for the same\nreasons in [AMS08, eq. (7.25)].\nTrust-region methods in Euclidean space are discussed in great detail by Conn\net al. [CGT00]; see also [NW06] for a shorter treatment. Absil et al. [ABG07] in-\ntroduced the Riemannian version of the trust-region method. Their analysis also", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2702, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6c3e6c17-d5d2-467c-8f5a-9b0e8f38ea0f": {"__data__": {"id_": "6c3e6c17-d5d2-467c-8f5a-9b0e8f38ea0f", "embedding": null, "metadata": {"page_label": "152", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b560c667-42e6-4dae-a4a0-68ad5f748dc1", "node_type": "4", "metadata": {"page_label": "152", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "510e738ca4e22adef8474678710f19c56673161c1e996968005ff074d262c9dc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n152 Second-order optimization algorithms\nappears in [AMS08, \u00a7 7]. The truncated CG algorithm is also called Steihaug-\nToint CG [Ste83, Toi81]. The global convergence analysis which shows RTR\ncomputes approximate first- and second-order critical points in a bounded num-\nber of iterations is mostly the same as in [BAC18]. Certain parts appear almost\nverbatim in that reference (in particular, the proofs of Lemmas 6.22 and 6.23). It\nis itself based on a similar analysis of the Euclidean version proposed by Cartis\net al. [CGT12], who also show examples for which the worst-case is attained. The\nglobal convergence results in terms of limit inferior and limit of gradient norm\n(Corollary 6.24 and Proposition 6.25) appear with somewhat different assump-\ntions as [AMS08, Thm. 7.4.2, Thm. 7.4.4]: the proofs are adapted accordingly.\nThe RTR method presented here generates sequences whose accumulation\npoints are first-order critical (under some assumptions). It can also find approx-\nimate second-order critical points up to any tolerance, but the theory does not\nguarantee accumulation at exact second-order critical points. A somewhat more\ntheoretical variant of RTR presented in [LKB22b] does accumulate at second-\norder critical points. It mirrors a Euclidean construction by Curtis et al. [CLR18].\nFor local convergence results, the capture theorem (Theorem 6.29) and the su-\nperlinear local convergence result (Theorem 6.30) appear with proofs as [AMS08,\nThm. 7.4.10, Thm. 7.4.11]. The statements here are somewhat different but\nthe same proofs apply. In particular, for Theorem 6.30, the reference state-\nment [AMS08, Thm. 7.4.11] makes the two following assumptions (among oth-\ners). First, there exists c6 > 0 such that, for all k,\n\u2225Hk \u2212 Hess(f \u25e6 Rxk )(0)\u2225 \u2264c6\u2225gradf(xk)\u2225xk . (6.58)\nThis is clear if Hk = Hess( f \u25e6 Rxk )(0). If Hk = Hess f(xk) the above follows\nfrom the assumptions in Theorem 6.30 and from the following formula (see for\nexample Exercise 10.73):\n\u27e8v, Hess(f \u25e6 Rx)(0)[v]\u27e9x = \u27e8v, Hessf(x)[v]\u27e9x + \u27e8gradf(x), c\u2032\u2032(0)\u27e9x (6.59)\nwhere (x, v) \u2208 TM is arbitrary and c(t) = Rx(tv). Second, there exist positive\nc8, c9, c10 such that, for all ( x, v) \u2208 TM with dist(x, x\u22c6) \u2264 c8 and \u2225v\u2225x \u2264 c9 it\nholds\n\u2225Hess(f \u25e6 Rx)(v) \u2212 Hess(f \u25e6 Rx)(0)\u2225 \u2264c10\u2225v\u2225x. (6.60)\nThis always holds if Hess f is continuously differentiable, by Lemma 10.57.\nTo some extent, the trust-region method is a fix of Newton\u2019s method to make\nit globally convergent. At its core, it is based on putting a hard limit on how\nfar one trusts a certain quadratic model for the (pullback of the) cost function.\nAlternatively, one may resort to a soft limit by adding a cubic regularization\nterm to a quadratic model. In the same way that the trust-region radius is\nupdated adaptively, the weight of the regularization term can also be updated\nadaptively, leading to the adaptive regularization with cubics (ARC) method.\nIn the Euclidean case, it dates back to seminal work by Griewank [Gri81] and\nNesterov and Polyak [NP06]. Cartis et al. give a thorough treatment including", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3303, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "76cc272c-e7d7-492a-af83-7806603b7990": {"__data__": {"id_": "76cc272c-e7d7-492a-af83-7806603b7990", "embedding": null, "metadata": {"page_label": "153", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6ad8d613-51a5-4b6b-a904-ba28c148e829", "node_type": "4", "metadata": {"page_label": "153", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "cebf8732aec612bf59dec7ad77020770ae953aac121f2a140223fef56c46bfdd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n6.9 Notes and references 153\ncomplexity bounds [CGT11b, CGT11a]. Qi proposed a first extension of ARC to\nRiemannian manifolds [Qi11]. Iteration complexity analyses akin to the one we\ngive here for RTR appear in [ZZ18, ABBC20]. As a theoretical strength, ARC\nis an optimal method for cost functions with Lipschitz continuous gradient and\nHessian.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 603, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3304653f-3c48-46a4-8fb9-0cc13a887bb7": {"__data__": {"id_": "3304653f-3c48-46a4-8fb9-0cc13a887bb7", "embedding": null, "metadata": {"page_label": "154", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "51c0b513-82a8-4b17-963d-cbe03130de46", "node_type": "4", "metadata": {"page_label": "154", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "ed6de85042ae1f43566b1e7e5ca86eb035d0925b151442358cd4da4542711845", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n7 Embedded submanifolds: examples\nIn this chapter, we describe several embedded submanifolds of linear spaces that\noccur in applications. For each one, we rely on Chapters 3 and 5 to derive the geo-\nmetric tools that are relevant to optimize over them. See Table 7.1 for a list of the\nmanifolds discussed in this chapter (and a few more), together with pointers to\nMatlab implementations in the toolbox Manopt [BMAS14]. PyManopt [TKW16]\nand Manopt.jl [Ber22] provide similar implementations in Python and Julia. All\nthree toolboxes are available from manopt.org.\nRemember from Section 3.2 that products of embedded submanifolds are em-\nbedded submanifolds. This extends to general manifolds. Throughout the book,\nwe show how to build the geometric toolbox of a product using the geometric\ntoolboxes of its parts. See Table 7.2 for pointers. Manopt builds these toolboxes\nautomatically for products M1 \u00d7\u00b7 \u00b7 \u00b7\u00d7Mk and powers Mk = M\u00d7\u00b7\u00b7 \u00b7\u00d7Mwith\nthe tools productmanifold and powermanifold.\nThis chapter is meant to be consulted periodically for illustration while reading\nearlier chapters.\n7.1 Euclidean spaces as manifolds\nOptimization on manifolds generalizes unconstrained optimization: the tools and\nalgorithms we develop here apply just as well to optimization on linear spaces.\nFor good measure, we spell out the relevant geometric tools.\nLet E be a real linear space, such as Rn, Rm\u00d7n, Cn, Cm\u00d7n, etc.: see Section 3.1.\nWe think of E as a (linear) manifold. Its dimension as a manifold is the same as\nits dimension as a linear space. All tangent spaces are the same: for x \u2208 E,\nTxE = E. (7.1)\nAn obvious (and reasonable) choice of retraction is\nRx(v) = x + v, (7.2)\nthough Definition 3.47 allows for more exotic choices as well.\nEquipped with an inner product, E is a Euclidean space, and also a (linear)\nRiemannian manifold. The orthogonal projector from E to a tangent space is of", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2141, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9c7e80f6-e6c0-49ef-b443-f0687a3e93c2": {"__data__": {"id_": "9c7e80f6-e6c0-49ef-b443-f0687a3e93c2", "embedding": null, "metadata": {"page_label": "155", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9dd8f58b-5aee-4377-9be3-8a887b277c9c", "node_type": "4", "metadata": {"page_label": "155", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "c236570dd559cbb20b77e193538be9dada39b52be42ca54261d398c1d6f41e0e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n7.1 Euclidean spaces as manifolds 155\nM Set Manopt tools Section\nE Rn, Rm\u00d7n, . . . euclideanfactory 7.1\nCn, Cm\u00d7n, . . . euclideancomplexfactory\nSym(n) symmetricfactory\nSkew(n) skewsymmetricfactory\nSubspace euclideansubspacefactory\nSd\u22121 Sphere in Rm\u00d7n spherefactory 7.2\nSphere in Cm\u00d7n spherecomplexfactory\nOB(d, n) Oblique manifold obliquefactory\nComplex oblique obliquecomplexfactory\nCn\n1 n complex phases complexcirclefactory\nSt(n, p) Stiefel stiefelfactory 7.3\nComplex Stiefel stiefelcomplexfactory\nO(n) Orthogonal group (see St( n, n) or SO(n)) 7.4\nSO(n) Rotation group rotationsfactory\nU(n) Unitary group unitaryfactory\nRm\u00d7n\nr Fixed rank fixedrankembeddedfactory 7.5\nHn Hyperbolic space hyperbolicfactory 7.6\n{x \u2208 E: h(x) = 0} 7.7\nGr(n, p) Set of subspaces grassmannfactory 9.16\nin Rn or Cn grassmanncomplexfactory\nSym(n)+ Positive definite sympositivedefinitefactory 11.7\nM1 \u00d7 M2 Product manifold productmanifold\nMk Power manifold powermanifold\nTable 7.1 List of manifolds described in this chapter (and a few more), with pointers to\nimplementations in Manopt (Matlab). The toolbox offers more, as documented on the web-\nsite manopt.org. The latter also points to PyManopt and Manopt.jl with implementations\nin Python and Julia. Section 7.8 points to additional manifolds of interest. Details regarding\nproduct manifolds are given throughout the book: see Table 7.2.\ncourse the identity map:\nProjx(u) = u. (7.3)\nSmoothness of a function f : E \u2192R is defined in the usual sense; its classical\ngradient and its Riemannian gradient coincide.\nMore generally, we may consider a linear manifoldM embedded in a Euclidean\nspace E, that is, M is a linear subspace of E. For example, we may consider\nSym(n)\u2014the space of real symmetric matrices of size n\u2014to be a submanifold of\nRn\u00d7n. It still holds that T xM = M for all x \u2208 M, and Rx(v) = x + v is still a\ngood choice for a retraction. Numerically, points and tangent vectors of M are\ntypically stored as elements of E. In this more general setup, Proj x denotes the\northogonal projection from E to TxM, that is, orthogonal projection from E to\nM. In particular, it does not depend on x: we write ProjM. If we make M into a", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2425, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3500af81-67ee-4043-ada8-ae193aacf7e8": {"__data__": {"id_": "3500af81-67ee-4043-ada8-ae193aacf7e8", "embedding": null, "metadata": {"page_label": "156", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e00e6f10-9ba3-41ec-9218-fe0353c7302d", "node_type": "4", "metadata": {"page_label": "156", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "0c56f7d7506fe39cb9ca69caa22e450f0deabf962f80ad5f2dc0d729f11fc6bf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n156 Embedded submanifolds: examples\nProduct of manifolds is a manifold (embedded) Proposition 3.20\nProduct of manifolds is a manifold (general) Exercise 8.31\nDifferential of F : M1 \u00d7 M2 \u2192 N Exercise 3.40\nTangent bundle of M1 \u00d7 M2 Equation (3.31)\nRetraction for M1 \u00d7 M2 Exercise 3.50\nProduct of Riemannian metrics is Riemannian Example 3.57\nGradient of f : M1 \u00d7 M2 \u2192 R Exercise 3.67\nProduct connection \u2207 on M1 \u00d7 M2 Exercise 5.4\nProduct of Riemannian connections is Riemannian Exercise 5.13\nHessian of f : M1 \u00d7 M2 \u2192 R Example 5.19\nCovariant derivative D\ndt induced by product \u2207 Exercise 5.34\nGeodesics on Riemannian product manifold Exercise 5.39\nRiemannian distance on Riemannian product Exercise 10.14\nExponential map on Riemannian product manifold Exercise 10.32\nParallel transport on product manifold Exercise 10.39\nTable 7.2 The product M1 \u00d7 M2 of two manifolds is a manifold. Moreover, if we know\nhow to work on M1 and M2 separately, then it is easy to work on their product as well.\nThis table points to the relevant facts to do that in various places of this book.\nRiemannian submanifold of E, that is, if the inner product on M is the same as\nthe inner product on E (appropriately restricted), then Proposition 3.61 states\nthe following: given a smooth f : M \u2192R with smooth extension \u00aff : U \u2192 R\ndefined on a neighborhood U of M in E,\ngradf(x) = ProjM(grad \u00aff(x)). (7.4)\nFor example, with the usual inner product on E = Rn\u00d7n (3.14), with M =\nSym(n) as a Riemannian submanifold, Proj M(Z) = Z+Z\u22a4\n2 so that the gradient\nof a function on Sym(n) is simply the symmetric part of its classical gradient on\nall of Rn\u00d7n.\nOf course, we could endow E with a non-Euclidean Riemannian metric, that\nis, with a Riemannian metric which varies from point to point: see Exercise 7.1.\nSecond-order tools\nCovariant derivatives (\u2207 and D\ndt ) on a Euclidean space E coincide with the usual\nvector field derivatives. The Riemannian Hessian of a function f : E \u2192R coin-\ncides with its Euclidean Hessian. The retraction R x(v) = x + v is a second-order\nretraction (Definition 5.42). In fact, it is the exponential map (Section 10.2).\nFurther consider the case where M is a linear subspace and a Riemannian\nsubmanifold of E. Then, continuing with the same notation as above, \u2207 and D\ndt\nare still the usual vector field derivatives, and the Hessian of f is related to that\nof \u00aff through\nHessf(x)[v] = ProjM\n\u0000\nHess \u00aff(x)[v]\n\u0001\n(7.5)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2668, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e67475c4-ab22-42b4-9e66-d2f76ecdde76": {"__data__": {"id_": "e67475c4-ab22-42b4-9e66-d2f76ecdde76", "embedding": null, "metadata": {"page_label": "157", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9a38bf55-d1f8-43db-9135-b8a5eb7234bd", "node_type": "4", "metadata": {"page_label": "157", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "df49fd6317efa06e64324b6b93a308af0c08ccf364cbe155b372d2a4393730a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n7.2 The unit sphere in a Euclidean space 157\nfor all x, v\u2208 M. We can also write this symmetrically as:\nHessf(x) = ProjM \u25e6 Hess \u00aff(x) \u25e6 ProjM. (7.6)\nThe retraction Rx(v) = x + v retains the aforementioned properties on M.\nExercise 7.1. We could endow a linear space with a non-Euclidean Riemannian\nmetric, that is, with a Riemannian metric which varies from point to point. To\nbe explicit, let M denote the manifold Rn with the Riemannian metric\n\u27e8u, v\u27e9x = u\u22a4G(x)v,\nwhere G(x) \u2208 Sym(n) is a positive definite matrix which varies smoothly with\nx. The retraction Rx(v) = x + v is still acceptable since retractions are defined\nindependently of the Riemannian structure.\nGiven a smooth function \u00aff : Rn \u2192 R, we can formally define f : M \u2192R\nthrough f(x) = \u00aff(x) for all x. This way, gradf denotes the Riemannian gradient\nof f on M and grad \u00aff denotes the Euclidean gradient of \u00aff on Rn, where the\nlatter is equipped with the canonical inner product \u27e8u, v\u27e9 = u\u22a4v. Give a formula\nfor gradf(x) in terms of grad \u00aff(x).\nConsider the special case where the Hessian of \u00aff is everywhere positive definite\n( \u00aff is strictly convex) and we let G(x) = Hess \u00aff(x). Compare the classical Newton\nmethod on \u00aff and Riemannian gradient descent on f.\n7.2 The unit sphere in a Euclidean space\nLet E be a Euclidean space endowed with an inner product \u27e8\u00b7, \u00b7\u27e9 and associated\nnorm \u2225 \u00b7 \u2225. For example, this could be Rd with the metric \u27e8u, v\u27e9 = u\u22a4v, or it\ncould be Rn\u00d7p with the metric \u27e8U, V\u27e9 = Tr(U\u22a4V ). With d = dim E, we define\nthe unit sphere in E as\nSd\u22121 = {x \u2208 E: \u2225x\u2225 = 1}. (7.7)\nA defining function is h(x) = \u27e8x, x\u27e9 \u22121. Its differential is D h(x)[v] = 2 \u27e8x, v\u27e9, so\nthat\nTxSd\u22121 = {v \u2208 E: \u27e8x, v\u27e9 = 0}, (7.8)\nand dim Sd\u22121 = dim E \u22121 = d \u2212 1. One possible retraction is\nRx(v) = x + v\n\u2225x + v\u2225 = x + vp\n1 + \u2225v\u22252 . (7.9)\nThe orthogonal projector to the tangent space at x is\nProjx : E \u2192TxSd\u22121 : u 7\u2192 Projx(u) = u \u2212 \u27e8x, u\u27e9x. (7.10)\nEquip Sd\u22121 with the induced Riemannian metric to turn it into a Riemannian\nsubmanifold. Then, for a smooth function f : Sd\u22121 \u2192 R with smooth extension", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2307, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "63c347ae-64e6-4b5e-aea5-9f5332dbbcea": {"__data__": {"id_": "63c347ae-64e6-4b5e-aea5-9f5332dbbcea", "embedding": null, "metadata": {"page_label": "158", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3a14a873-0aa6-4f64-93e7-700cabe07a43", "node_type": "4", "metadata": {"page_label": "158", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "08bc28ec9c289befcc4057d097c9d0822b915147c346a1adb0ca46a8697ed898", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n158 Embedded submanifolds: examples\n\u00aff : U \u2192 R in a neighborhood U of S d\u22121 in E, the gradient of f is given by\nProposition 3.61 as\ngradf(x) = Projx(grad \u00aff(x)) = grad \u00aff(x) \u2212\n\nx, grad \u00aff(x)\n\u000b\nx. (7.11)\nIn particular, x is a critical point of f if and only if grad \u00aff(x) is parallel to x.\nIn Manopt, formulas such as (7.11) which convert the Euclidean gradient of a\nsmooth extension into a Riemannian gradient are available for each manifold as\negrad2rgrad.\nA product of k spheres is called an oblique manifold. For example, the product\nof k spheres in Rd is denoted by OB( d, k) = (Sd\u22121)k. Its elements are typically\nrepresented using matrices in Rd\u00d7k (or Rk\u00d7d) whose columns (or rows) have unit\nnorm. The same can be done for complex matrices. An often useful particular\ncase is the complex circle, which consists of all complex numbers of unit modulus\n(called phases): this is nothing but an alternative way of representing S 1.\nSecond-order tools\nWith S d\u22121 as a Riemannian submanifold of the Euclidean space E, covariant\nderivatives (\u2207 and D\ndt ) on S d\u22121 coincide with the usual vector field derivatives\n(of smooth extensions) in E, followed by orthogonal projection to tangent spaces\n(Theorem 5.9, Proposition 5.31).\nWe can use this to obtain a formula for the Riemannian Hessian of f : Sd\u22121 \u2192\nR, with smooth extension \u00aff : U \u2192 R defined on a neighborhood U of Sd\u22121 in E.\nFollowing Example 5.17, we let\n\u00afG(x) = grad \u00aff(x) \u2212\n\nx, grad \u00aff(x)\n\u000b\nx\ndenote a smooth extension of the vector field grad f to a neighborhood of S d\u22121\nin E. Then,\nHessf(x)[v] = \u2207vgradf\n= Projx\n\u0000\nD \u00afG(x)[v]\n\u0001\n= Projx\n\u0010\nHess \u00aff(x)[v] \u2212\n\u0002\nv, grad \u00aff(x)\n\u000b\n+\n\nx, Hess \u00aff(x)[v]\n\u000b\u0003\nx\n\u2212\n\nx, grad \u00aff(x)\n\u000b\nv\n\u0011\n= Projx(Hess \u00aff(x)[v]) \u2212\n\nx, grad \u00aff(x)\n\u000b\nv. (7.12)\nIn Manopt, formulas such as (7.12) which convert the Euclidean gradient and\nHessian of a smooth extension into a Riemannian Hessian are available for each\nmanifold as ehess2rhess.\nThe retraction (7.9) is a second-order retraction (see Definition 5.42, Exam-\nple 5.43 and Proposition 5.55). Geodesics on S d\u22121 are given in Example 5.37.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2324, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d0a612c4-a28f-4543-88d1-0c52abaeb4bc": {"__data__": {"id_": "d0a612c4-a28f-4543-88d1-0c52abaeb4bc", "embedding": null, "metadata": {"page_label": "159", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e21103d5-3542-4241-84a8-f90830408018", "node_type": "4", "metadata": {"page_label": "159", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "134fc50080ec7957ee4b28427bcdcea08cf6897129b2fbfa9b31508caa542927", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n7.3 The Stiefel manifold: orthonormal matrices 159\n7.3 The Stiefel manifold: orthonormal matrices\nFor p \u2264 n, let Rn\u00d7p be endowed with the standard inner product \u27e8U, V\u27e9 =\nTr(U\u22a4V ). The (compact)1 Stiefel manifold is the set of matrices in Rn\u00d7p whose\ncolumns are orthonormal in Rn with respect to the inner product \u27e8u, v\u27e9 = u\u22a4v.\nThis can be written conveniently as: 2\nSt(n, p) =\n\b\nX \u2208 Rn\u00d7p : X\u22a4X = Ip\n\t\n, (7.13)\nwhere Ip is the identity matrix of size p. In particular, St(n, 1) is the unit sphere\nin Rn. We call matrices in St(n, p) orthonormal matrices and we reserve the word\northogonal matrix for square orthonormal matrices.\nConsider the following function:\nh: Rn\u00d7p \u2192 Sym(p): X 7\u2192 h(X) = X\u22a4X \u2212 Ip, (7.14)\nwhere Sym( p) is the linear space of symmetric matrices of size p. The latter\nhas dimension k = p(p+1)\n2 , so that we may identify it with Rk if desired. We\ncan verify that h is a defining function for St( n, p). Indeed, h is smooth and\nh\u22121(0) = St(n, p): it remains to check that the differential of h has rank k for all\nX \u2208 St(n, p). To this end, consider D h(X): Rn\u00d7p \u2192 Sym(p):\nDh(X)[V ] = lim\nt\u21920\nh(X + tV ) \u2212 h(X)\nt\n= lim\nt\u21920\n(X + tV )\u22a4(X + tV ) \u2212 X\u22a4X\nt\n= X\u22a4V + V \u22a4X. (7.15)\nTo show Dh(X) has rankk, we must show its image (or range) is a linear subspace\nof dimension k. Since the codomain Sym(p) has dimension k, we must show that\nthe image of D h(X) is all of Sym( p), that is, D h(X) is surjective. To do so,\nconsider V = 1\n2 XA with A \u2208 Sym(p) arbitrary. Then,\nDh(X)[V ] = 1\n2X\u22a4XA + 1\n2A\u22a4X\u22a4X = A.\nIn other words: for any matrix A \u2208 Sym(p), there exists a matrix V \u2208 Rn\u00d7p such\nthat Dh(X)[V ] = A. This confirms the image of D h(X) is all of Sym( p), so that\nit has rank k. Thus, h is a defining function for St(n, p), making it an embedded\nsubmanifold of Rn\u00d7p of dimension\ndim St(n, p) = dim Rn\u00d7p \u2212 dim Sym(p) = np \u2212 p(p + 1)\n2 . (7.16)\nThe tangent spaces are subspaces of Rn\u00d7p:\nTXSt(n, p) = ker Dh(X) =\n\b\nV \u2208 Rn\u00d7p : X\u22a4V + V \u22a4X = 0\n\t\n. (7.17)\n1 The non-compact Stiefel manifold refers to the open subset of matrices of rank p in Rn\u00d7p.\nWe always mean compact.\n2 Many authors use the notation St( p, n) for the same set\u2014we prefer the notation St( n, p) as\nit is reminiscent of the size of the matrices.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2474, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "94c63026-b261-45ba-a2f2-c390d2423e05": {"__data__": {"id_": "94c63026-b261-45ba-a2f2-c390d2423e05", "embedding": null, "metadata": {"page_label": "160", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c904c305-b9ff-43e2-bf65-aa00c4b32d47", "node_type": "4", "metadata": {"page_label": "160", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "791e7106d24123bc217edaa2457dfa8b000dfe48c8f34bbd7d0e95a31c3be6f0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n160 Embedded submanifolds: examples\nIt is sometimes convenient to parameterize tangent vectors in explicit form. First,\ncomplete3 the orthonormal basis formed by the columns of X with a matrix\nX\u22a5 \u2208 Rn\u00d7(n\u2212p) such that [ X X\u22a5 ] \u2208 Rn\u00d7n is orthogonal:\nX\u22a4X = Ip, X \u22a4\n\u22a5X\u22a5 = In\u2212p, and X\u22a4X\u22a5 = 0. (7.18)\nSince [ X X\u22a5 ] is, in particular, invertible, any matrix V \u2208 Rn\u00d7p can be written\nas\nV =\n\u0002\nX X \u22a5\n\u0003\u0014\u2126\nB\n\u0015\n= X\u2126 + X\u22a5B, (7.19)\nfor a unique choice of \u2126 \u2208 Rp\u00d7p and B \u2208 R(n\u2212p)\u00d7p. Using this decomposition,\nV is a tangent vector at X if and only if\n0 = Dh(X)[V ] = X\u22a4(X\u2126 + X\u22a5B) + (X\u2126 + X\u22a5B)\u22a4X = \u2126 + \u2126\u22a4.\nIn other words, \u2126 must be skew-symmetric, while B is free. Thus,\nTXSt(n, p) =\nn\nX\u2126 + X\u22a5B : \u2126 \u2208 Skew(p), B\u2208 R(n\u2212p)\u00d7p\no\n, (7.20)\nwhere we used the decomposition (7.19) with respect to an arbitrary choice of\nX\u22a5 \u2208 Rn\u00d7(n\u2212p) satisfying (7.18), and\nSkew(p) = {\u2126 \u2208 Rp\u00d7p : \u2126\u22a4 = \u2212\u2126} (7.21)\nis the set of skew-symmetric matrices of size p.\nOne popular retraction for St( n, p) is the Q-factor retraction:4\nRX(V ) = Q, (7.22)\nwhere QR = X + V is a (thin) QR decomposition: Q \u2208 St(n, p) and R \u2208 Rp\u00d7p\nupper triangular with nonnegative diagonal entries. This is well defined since,\nfor a tangent vector V \u2208 TXSt(n, p),\n(X + V )\u22a4(X + V ) = Ip + V \u22a4V (7.23)\nis positive definite, showing X +V has full rank p: under that condition, the QR\ndecomposition is indeed unique. This retraction can be computed in \u223c np2 basic\narithmetic operations (+ , \u2212, \u00d7, /,\u221a\u00b7) using the modified Gram\u2013Schmidt algo-\nrithm or a Householder triangularization. The defining properties of a retraction\nare satisfied: Surely, RX(0) = X; Furthermore, inspecting the Gram\u2013Schmidt al-\ngorithm reveals that it maps full-rank matrices inRn\u00d7p to their Q-factor through\na sequence of smooth operations, so that R is smooth (by composition); Finally,\nan expression for DR X(V ) is derived in [AMS08, Ex. 8.1.5], from which it is\nstraightforward to verify that DRX(0) is the identity map.\n3 The matrix X\u22a5 is never built explicitly: it is merely a useful mathematical tool.\n4 Some software packages offer a built-in qr routine which may not enforce nonnegativity of\ndiagonal entries of R\u2014this is the case of Matlab for example. It is important to flip the\nsigns of the columns of Q accordingly. In Manopt, call qr unique.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2516, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c269f3ab-72b0-4f4f-b763-25ffd47d0fa1": {"__data__": {"id_": "c269f3ab-72b0-4f4f-b763-25ffd47d0fa1", "embedding": null, "metadata": {"page_label": "161", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "268728d4-1915-441c-9749-728825864547", "node_type": "4", "metadata": {"page_label": "161", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "012e60cce309414160e45e2c604d03e055aba01bf48b401d1e41ecbba7c33fef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n7.3 The Stiefel manifold: orthonormal matrices 161\nAnother popular retraction for St( n, p) is the polar retraction:\nRX(V ) = (X + V )\n\u0000\n(X + V )\u22a4(X + V )\n\u0001\u22121/2\n= (X + V )(Ip + V \u22a4V )\u22121/2, (7.24)\nwhere M\u22121/2 denotes the inverse matrix square root ofM. This can be computed\nthrough eigenvalue decomposition of the matrix Ip + V \u22a4V , or (better) through\nSVD of X +V . Indeed, if X +V = U\u03a3W\u22a4 is a thin singular value decomposition,\nthe polar factor of X + V is UW \u22a4 and that is equivalent to (7.24). Clearly,\nRX(0) = X and R is smooth. It is straightforward to check that DR X(0) is the\nidentity map. In fact, the polar retraction is the metric projection retraction (see\nSection 5.12 and [Sch66]), globally well defined since X + V has full rank for all\n(X, V) \u2208 TM as argued above [AM12, Prop. 7].\nYet another often-used retraction for the Stiefel manifold is the Cayley trans-\nform [WY13, JD15].\nThe orthogonal projector to a tangent space of St( n, p) must be such that\nU \u2212ProjX(U) is orthogonal to TXSt(n, p), that is, the difference must be in the\northogonal complement of the tangent space in Rn\u00d7p. The latter is called the\nnormal space to St(n, p) at X:\nNXSt(n, p) = (TXSt(n, p))\u22a5\n=\n\b\nU \u2208 Rn\u00d7p : \u27e8U, V\u27e9 = 0 for all V \u2208 TXSt(n, p)\n\t\n=\n\b\nU \u2208 Rn\u00d7p : \u27e8U, X\u2126 + X\u22a5B\u27e9 = 0\nfor all \u2126 \u2208 Skew(p), B\u2208 R(n\u2212p)\u00d7p\t\n.\nExpand normal vectors as U = XA + X\u22a5C with some A \u2208 Rp\u00d7p and C \u2208\nR(n\u2212p)\u00d7p; then:\nNXSt(n, p) =\n\b\nU \u2208 Rn\u00d7p : \u27e8XA + X\u22a5C, X\u2126 + X\u22a5B\u27e9 = 0\nfor all \u2126 \u2208 Skew(p), B\u2208 R(n\u2212p)\u00d7p\t\n=\n\b\nU \u2208 Rn\u00d7p : \u27e8A, \u2126\u27e9 = 0 and \u27e8C, B\u27e9 = 0\nfor all \u2126 \u2208 Skew(p), B\u2208 R(n\u2212p)\u00d7p\t\n= {XA : A \u2208 Sym(p)}, (7.25)\nwhere we used that the orthogonal complement of Skew( p) in Rp\u00d7p is Sym(p).\nThus, orthogonal projection of U \u2208 Rn\u00d7p satisfies\nU \u2212 ProjX(U) = XA\nfor some symmetric matrix A. Furthermore, the projected vector must lie in\nTXSt(n, p), hence\nProjX(U)\u22a4X + X\u22a4ProjX(U) = 0.\nPlugging the former into the latter yields\n(U \u2212 XA)\u22a4X + X\u22a4(U \u2212 XA) = 0,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2170, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3a11db6c-f849-47ae-99e3-81dd44b2072b": {"__data__": {"id_": "3a11db6c-f849-47ae-99e3-81dd44b2072b", "embedding": null, "metadata": {"page_label": "162", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b94964ad-a356-472c-a3b6-6513e5cd1399", "node_type": "4", "metadata": {"page_label": "162", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "f43fe5cb5cd3f494ec08e99db76f4d4b01208accfa00943decee213aee6c313a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n162 Embedded submanifolds: examples\nthat is, U\u22a4X + X\u22a4U = 2A. Hence,\nProjX(U) = U \u2212 X X\u22a4U + U\u22a4X\n2 (7.26)\n= (I \u2212 XX \u22a4)U + X X\u22a4U \u2212 U\u22a4X\n2 . (7.27)\nOne convenient way to turn St(n, p) into a Riemannian manifold is to make it a\nRiemannian submanifold of Rn\u00d7p, in which case the projector yields a convenient\nformula for the gradient of a smooth function f in terms of a smooth extension\n\u00aff, by Proposition 3.61:\ngradf(X) = ProjX(grad \u00aff(X)) = grad \u00aff(X) \u2212 X sym(X\u22a4grad \u00aff(X)), (7.28)\nwhere sym(M) = M+M\u22a4\n2 extracts the symmetric part of a matrix.\nOther Riemannian metrics are sometimes used: see for example the so-called\ncanonical metric in [EAS98].\nSecond-order tools\nWith St(n, p) as a Riemannian submanifold of Rn\u00d7p, covariant derivatives ( \u2207\nand D\ndt ) on St( n, p) coincide with the usual vector field derivatives (of smooth\nextensions) in Rn\u00d7p, followed by orthogonal projection to tangent spaces (The-\norem 5.9, Proposition 5.31).\nWe use this to obtain a formula for the Riemannian Hessian off : St(n, p) \u2192 R,\nwith smooth extension \u00aff defined on a neighborhood of St( n, p) in Rn\u00d7p. Let\n\u00afG(X) = grad \u00aff(X) \u2212 X sym(X\u22a4grad \u00aff(X))\ndenote a smooth extension of the vector field gradf to a neighborhood of St(n, p)\nin Rn\u00d7p. Then,\nHessf(X)[V ] = \u2207V gradf\n= ProjX\n\u0000\nD \u00afG(X)[V ]\n\u0001\n= ProjX\n\u0010\nHess \u00aff(X)[V ] \u2212 V sym(X\u22a4grad \u00aff(X)) \u2212 XS\n\u0011\n= ProjX\n\u0010\nHess \u00aff(X)[V ] \u2212 V sym(X\u22a4grad \u00aff(X))\n\u0011\n, (7.29)\nwhere S = sym( V \u22a4grad \u00aff(X) + X\u22a4Hess \u00aff(X)[V ]), and XS vanishes through\nProjX. The polar retraction (7.24) is a second-order retraction (Definition 5.42)\nbecause it is the metric projection retraction (Proposition 5.55), but the Q-factor\nretraction (7.22) is not. Geodesics on St( n, p) are given in [AMS08, eq. (5.26)].\nExercise 7.2. Show that the polar retraction R on M = St(n, p) (7.24) is such\nthat E(X, V) = (X, RX(V )) from TM to E(TM) has a smooth inverse.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2111, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4fc199c9-9697-49ed-8789-9839c3ae40b4": {"__data__": {"id_": "4fc199c9-9697-49ed-8789-9839c3ae40b4", "embedding": null, "metadata": {"page_label": "163", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1659456d-6c48-4485-b66d-29a8e888b5e3", "node_type": "4", "metadata": {"page_label": "163", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "18ee3033c703bc5a6ba2cc97c6eaf7fdc6d9439293da061bb2b474d3d13f2f0b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n7.4 The orthogonal group and rotation matrices 163\n7.4 The orthogonal group and rotation matrices\nAs a special case of the Stiefel manifold, matrices in St(n, n) form the orthogonal\ngroup, that is, the set of orthogonal matrices in Rn\u00d7n:\nO(n) = {X \u2208 Rn\u00d7n : X\u22a4X = XX \u22a4 = In}. (7.30)\nIt is a group equipped with matrix multiplication as its group operation. Being a\nspecial case of the Stiefel manifold, O( n) is also an embedded submanifold, this\ntime of Rn\u00d7n. As a set which is both a manifold and a group, it is known as a\nLie group (more about this in Section 9.2). It has dimension\ndim O(n) = n2 \u2212 n(n + 1)\n2 = n(n \u2212 1)\n2 , (7.31)\nand tangent spaces given by\nTXO(n) =\n\b\nX\u2126 \u2208 Rn\u00d7n : \u2126 \u2208 Skew(n)\n\t\n= XSkew(n). (7.32)\nNotice how TInO(n) = Skew(n), so that T XO(n) = XTInO(n): tangent spaces\nare essentially \u201ctranslated\u201d versions of the tangent space at the identity matrix,\nwhich is also the identity element of O( n) as a group. In Lie group parlance, we\ncall TInO(n) the Lie algebra of O(n).\nNumerically, it is convenient to represent tangent vectors atX simply by their\nskew-symmetric factor \u2126, keeping in mind that we mean to represent the tangent\nvector X\u2126. More generally, it is important to mind the distinction between how\nwe represent points and vectors in the ambient space, and how we represent\npoints and tangent vectors on the manifold.\nBoth the Q-factor and the polar retractions of St( n, p) are valid retractions\nfor O(n).\nThe orthogonal projector is given by\nProjX(U) = X X\u22a4U \u2212 U\u22a4X\n2 = X skew(X\u22a4U), (7.33)\nwhere skew(M) = M\u2212M\u22a4\n2 extracts the skew-symmetric part of a matrix. Turning\nO(n) into a Riemannian submanifold of Rn\u00d7n with the standard Euclidean met-\nric, this once more gives a direct formula for the gradient of a smooth function\non O(n), through Proposition 3.61:\ngradf(X) = X skew(X\u22a4grad \u00aff(X)). (7.34)\nOf course, this is equivalent to the corresponding formula (7.28) for Stiefel.\nAn important feature of O( n), relevant for optimization, is that it is discon-\nnected. Specifically, it has two components, corresponding to orthogonal matrices\nof determinant +1 and \u22121:\n1 = det(In) = det(XX \u22a4) = det(X)2.\nIndeed: since the determinant is a continuous function from Rn\u00d7n to R, by", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2465, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a4f4d368-31e5-4d07-8c41-16f578178710": {"__data__": {"id_": "a4f4d368-31e5-4d07-8c41-16f578178710", "embedding": null, "metadata": {"page_label": "164", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecfee4e3-1bc3-4036-998d-8d6ac11dc233", "node_type": "4", "metadata": {"page_label": "164", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "5ee639e5229821c90c861e0b7d4895d7cf406526aee1504c0df54e34cf45d1ca", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n164 Embedded submanifolds: examples\nthe intermediate value theorem, any continuous curve connecting a matrix with\ndeterminant +1 to a matrix with determinant \u22121 must pass through a matrix\nwith determinant zero and hence must leave O( n).\nOur optimization algorithms move along continuous curves (retraction curves).\nAs a result, when we initialize such an algorithm in a certain connected com-\nponent, it cannot \u201cjump\u201d to another connected component. Therefore, it is im-\nportant to initialize in the appropriate component. Geometrically, orthogonal\nmatrices of size n correspond to rotations of Rn, possibly composed with a re-\nflection for those matrices that have determinant \u22121. In situations where only\nrotations are relevant, it makes sense to consider the special orthogonal group,\nalso known as the group of rotations :\nSO(n) = {X \u2208 O(n) : det(X) = +1}. (7.35)\nThis is still an embedded submanifold of Rn\u00d7n of course. To verify it, consider\nthe defining function h(X) = X\u22a4X \u2212 In defined on {X \u2208 Rn\u00d7n : det(X) > 0},\nwhich is an open subset of Rn\u00d7n.\nAs a connected component of O( n), all the tools we developed so far apply\njust as well to SO( n). This includes eq. (7.34) for gradients as well as\ndim SO(n) = n(n \u2212 1)\n2 , (7.36)\nTXSO(n) = XSkew(n), (7.37)\nProjX(U) = X skew(X\u22a4U). (7.38)\nIt is clear that retractions on O( n) yield retractions on SO( n) since, being\nsmooth, they cannot leave a connected component.\nSecond-order tools\nWith O(n) and SO(n) as Riemannian submanifolds of the Euclidean spaceRn\u00d7n,\ncovariant derivatives (\u2207 and D\ndt ) coincide with the usual vector field derivatives\n(of smooth extensions) in Rn\u00d7n, followed by orthogonal projection to tangent\nspaces (Theorem 5.9, Proposition 5.31).\nWe use this to obtain a formula for the Riemannian Hessian of a real function\nf on O(n) or SO( n), with smooth extension \u00aff in Rn\u00d7n. Of course, exactly the\nsame developments as for the Stiefel manifold hold, so that by (7.29) we get:\nHessf(X)[V ] = ProjX\n\u0000\nHess \u00aff(X)[V ] \u2212 V sym(X\u22a4grad \u00aff(X))\n\u0001\n. (7.39)\nWriting V = X\u2126 for some \u2126 \u2208 Skew(n), this also reads\nHessf(X)[X\u2126] = X skew\n\u0000\nX\u22a4Hess \u00aff(X)[V ] \u2212 \u2126 sym(X\u22a4grad \u00aff(X))\n\u0001\n,\nmaking the skew-symmetric representation of Hess f(X)[X\u2126] clearer.\nFor both O(n) and SO(n), the polar retraction (7.24) is the metric projection\nretraction (because it was so for the Stiefel manifold) hence it is a second-order", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2629, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f0472724-0c08-406e-9987-96334d28d087": {"__data__": {"id_": "f0472724-0c08-406e-9987-96334d28d087", "embedding": null, "metadata": {"page_label": "165", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8279567-0178-450d-99fe-7663cc70a647", "node_type": "4", "metadata": {"page_label": "165", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "9ac6b5198f8bfbb920e42c8f514ecdfd21b9575069becedd64ec99cc1c193a83", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n7.5 Fixed-rank matrices 165\nretraction (see Section 5.12), but the Q-factor retraction (7.22) is not. It is an\nexercise to show that\nc(t) = X exp(t\u2126) (7.40)\nis a geodesic on O(n) (or SO(n)) such that c(0) = X and c\u2032(0) = X\u2126. (This hap-\npens because the Riemannian metric is bi-invariant, so that the Lie exponential\nmap and the Riemannian exponential map coincide, and it is known that the\nLie exponential map is given by the matrix exponential exp.)\nExercise 7.3. Show that c(t) as defined by (7.40) is indeed a curve on O(n), and\nverify that d\ndt c(t) = c(t)\u2126. Deduce that d\ndt\n\u0000 d\ndt c(t)\n\u0001\n= c(t)\u21262 and, eventually, that\nc\u2032\u2032(t) = D\ndt c\u2032(t) = 0, which confirms c is a geodesic. Hint: use (4.33) to express\nthe differential of the matrix exponential, and use the fact that exp(A + B) =\nexp(A) exp(B) if A and B commute.\nExercise 7.4. Work out a geometric toolbox for the unitary group\nU(n) = {X \u2208 Cn\u00d7n : X\u2217X = In} (7.41)\nas a Riemannian submanifold of Cn\u00d7n with the usual inner product (3.17).\n7.5 Fixed-rank matrices\nThe set of real matrices of size m \u00d7 n and rank r,\nRm\u00d7n\nr = {X \u2208 Rm\u00d7n : rank(X) = r}, (7.42)\nis an embedded submanifold of Rm\u00d7n, as we now show. Importantly, this is only\ntrue for fixed rank r: the set of matrices in Rm\u00d7n with rank up to r is not\nan embedded submanifold of Rm\u00d7n. It is, however, an algebraic variety and a\nstratified space\u2014we do not consider optimization on such spaces. Moreover, in\ncontrast to the examples discussed earlier in this chapter, Rm\u00d7n\nr is neither open\nnor closed in Rm\u00d7n.\nFor an arbitraryX \u2208 Rm\u00d7n\nr , we now build a local defining function. We cannot\nuse h(X) = rank(X) \u2212 r as a defining function because it is not continuous, let\nalone smooth. Instead, we proceed as follows. Since X has rank r, it contains an\ninvertible submatrix of size r \u00d7 r, that is, it is possible to extract r columns and\nr rows of X such that the resulting matrix in Rr\u00d7r is invertible. For notational\nconvenience, assume for now that this is the case for the firstr rows and columns,\nso that X can be written in block form as\nX =\n\u0014X11 X12\nX21 X22\n\u0015\nwith X11 \u2208 Rr\u00d7r invertible, and X12 \u2208 Rr\u00d7(n\u2212r), X21 \u2208 R(m\u2212r)\u00d7r and X22 \u2208", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2409, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "340c6769-f912-4752-a1ca-854069b21453": {"__data__": {"id_": "340c6769-f912-4752-a1ca-854069b21453", "embedding": null, "metadata": {"page_label": "166", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b9b8f471-9334-454e-b476-81ba82380962", "node_type": "4", "metadata": {"page_label": "166", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "b2f732cb9fe0867ca91117332a489031fa0f25863be957c7d836ead8d60debae", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n166 Embedded submanifolds: examples\nR(m\u2212r)\u00d7(n\u2212r). Since X has rank r, its n \u2212 r last columns must be linear combi-\nnations of its r first columns, that is, there exists W \u2208 Rr\u00d7(n\u2212r) such that\n\u0014X12\nX22\n\u0015\n=\n\u0014X11\nX21\n\u0015\nW.\nConsequently, W = X\u22121\n11 X12 and X22 = X21W = X21X\u22121\n11 X12. Under our as-\nsumption that X11 is invertible, this relationship between the blocks of X is\nnecessary and sufficient for X to have rank r.\nThis suggests a candidate local defining function. Let U be the subset of Rm\u00d7n\nconsisting of all matrices whose upper-left submatrix of size r\u00d7r is invertible: X\nis in U, and U is open in Rm\u00d7n since its complement\u2014the set of matrices whose\nupper-left submatrix has determinant equal to zero\u2014is closed. Consider\nh: U \u2192R(m\u2212r)\u00d7(n\u2212r) : Y =\n\u0014Y11 Y12\nY21 Y22\n\u0015\n7\u2192 h(Y ) = Y22 \u2212 Y21Y \u22121\n11 Y12,\nwith the same block-matrix structure as before. By the above, h\u22121(0) = Rm\u00d7n\nr \u2229\nU. Furthermore, h is smooth in U. Finally, its differential at Y is (V \u2208 Rm\u00d7n\nhas the same block structure as Y ):\nDh(Y )[V ] = V22 \u2212 V21Y \u22121\n11 Y12 + Y21Y \u22121\n11 V11Y \u22121\n11 Y12 \u2212 Y21Y \u22121\n11 V12,\nwhere we used the following identity for the differential of the matrix inverse\n(recall Example 4.24):\nD\n\u0000\nM 7\u2192 M\u22121\u0001\n(M)[H] = \u2212M\u22121HM \u22121. (7.43)\nThe codomain of D h(Y ) is R(m\u2212r)\u00d7(n\u2212r). Any matrix in that codomain can be\nattained with some input V (simply consider setting V11, V12, V21 to zero, so that\nDh(Y )[V ] = V22). Thus, the differential of h is surjective everywhere in U: it is\na local defining function for Rm\u00d7n\nr around X. If the upper-left submatrix of size\nr \u00d7 r of X is not invertible, we can construct another local defining function\nusing the same procedure: one for each choice of submatrix.\nTogether, these local defining functions cover the whole set, showing that\nRm\u00d7n\nr is an embedded submanifold of Rm\u00d7n with dimension\ndim Rm\u00d7n\nr = dim Rm\u00d7n \u2212 dim R(m\u2212r)\u00d7(n\u2212r)\n= mn \u2212 (m \u2212 r)(n \u2212 r)\n= r(m + n \u2212 r). (7.44)\nNotice that, for a given rank r, the dimension of Rm\u00d7n\nr grows linearly with\nm + n, as opposed to the dimension of the embedding space Rm\u00d7n which grows\nmuch faster, as mn. To exploit this key feature in numerical algorithms, we\nmust represent X appropriately in memory: this should make it possible to\nstore matrices with an amount of memory that grows linearly in m + n even\nthough their size is m \u00d7 n. One convenient choice is as a thin singular value", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2620, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d923e7d1-8a81-43b8-8f54-a8f68cb4f129": {"__data__": {"id_": "d923e7d1-8a81-43b8-8f54-a8f68cb4f129", "embedding": null, "metadata": {"page_label": "167", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8bd1feeb-d8b9-4dba-b07e-daaa351a87f6", "node_type": "4", "metadata": {"page_label": "167", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "604f200b2b6c44180782e372253d4cbee10a35c6881e10a26c0011e4326732c2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n7.5 Fixed-rank matrices 167\ndecomposition:\nX = U\u03a3V \u22a4, U \u2208 St(m, r), \u03a3 =\n\uf8ee\n\uf8ef\uf8f0\n\u03c31\n...\n\u03c3r\n\uf8f9\n\uf8fa\uf8fb, (7.45)\nV \u2208 St(n, r),\nwhere \u03c31 \u2265 \u00b7\u00b7 \u00b7 \u2265\u03c3r > 0 are the singular values of X. To identify X uniquely, it\nis only necessary to store U, \u03a3, Vin memory. We stress that this is only about\nrepresentation: the use of orthonormal matrices is only for convenience, and has\nno bearing on the geometry of Rm\u00d7n\nr .\nThe tangent space to Rm\u00d7n\nr at X is given by the kernel of D h(X), with an\nappropriate h as constructed above. However, this characterization is impractical\nbecause it requires one to identify an invertible submatrix of X in order to\ndetermine which local defining function to use. Besides, it is more convenient to\naim for a representation of the tangent space T XRm\u00d7n\nr that is compatible with\nthe practical representation of X (7.45).\nSince we know that each tangent space has dimension as in (7.44), it is suf-\nficient to exhibit a linear subspace of that dimension which is included in the\ntangent space. Going back to the definition of tangent space (3.23), we do so by\nexplicitly constructing smooth curves on Rm\u00d7n\nr .\nGiven X = U\u03a3V \u22a4 as above, let U(t) be a smooth curve on St( m, r) such that\nU(0) = U, let V (t) be a smooth curve on St( n, r) such that V (0) = V , and let\n\u03a3(t) be a smooth curve in the set of invertible matrices of size r \u00d7 r (this is an\nopen submanifold of Rr\u00d7r) such that \u03a3(0) = \u03a3. Then,\nc(t) = U(t)\u03a3(t)V (t)\u22a4\nis a smooth curve on Rm\u00d7n\nr such that c(0) = X. Hence, its velocity at zero is a\ntangent vector at X:\nc\u2032(0) = U\u2032(0)\u03a3V \u22a4+ U\u03a3\u2032(0)V \u22a4+ U\u03a3V \u2032(0)\u22a4 \u2208 TXRm\u00d7n\nr .\nSince U(t) is a smooth curve on St( m, r) through U, its velocity U\u2032(0) is in\nthe tangent space to St( m, r) at U. The other way around, for any vector in\nTU St(m, r), there is a smooth curveU(t) with that velocity att = 0. From (7.20),\nthis means that for any \u2126 \u2208 Skew(r) and B \u2208 R(m\u2212r)\u00d7r we can arrange to have\nU\u2032(0) = U\u2126 + U\u22a5B,\nwhere U\u22a5 is such that [ U U\u22a5 ] is orthogonal. Likewise, for any \u2126 \u2032 \u2208 Skew(r) and\nC \u2208 R(n\u2212r)\u00d7r we can arrange to have\nV \u2032(0) = V \u2126\u2032 + V\u22a5C,\nwith V\u22a5 such that [V V\u22a5 ] is orthogonal. Finally, since \u03a3(t) is a smooth curve in an\nopen submanifold of Rr\u00d7r, we can arrange for \u03a3\u2032(0) to be any matrix A \u2208 Rr\u00d7r.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2479, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb958323-1122-4b3d-95b6-297ec99b78ad": {"__data__": {"id_": "bb958323-1122-4b3d-95b6-297ec99b78ad", "embedding": null, "metadata": {"page_label": "168", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9a287a74-41ae-414d-9f0e-230784312d87", "node_type": "4", "metadata": {"page_label": "168", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "cb453f4cc35b7268fc2ccc4508a1d0000226806e67b6ba0dfb8e0842afdf7cc4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n168 Embedded submanifolds: examples\nOverall, this shows that all of the following velocities are in the tangent space of\nRm\u00d7n\nr at X:\nc\u2032(0) = (U\u2126 + U\u22a5B)\u03a3V \u22a4+ UAV \u22a4+ U\u03a3(V \u2126\u2032 + V\u22a5C)\u22a4\n= U(\u2126\u03a3 + A \u2212 \u03a3\u2126\u2032\n| {z }\nM\n)V \u22a4+ U\u22a5B\u03a3| {z }\nUp\nV \u22a4+ U(V\u22a5C\u03a3\u22a4\n| {z }\nVp\n)\u22a4. (7.46)\nSince \u03a3 is invertible, we find that any matrix of the form\nUMV \u22a4+ UpV \u22a4+ UV \u22a4\np\nwith M \u2208 Rr\u00d7r arbitrary and Up \u2208 Rm\u00d7r, Vp \u2208 Rn\u00d7r such that U\u22a4Up = V \u22a4Vp =\n0 is tangent at X. The conditions on Up and Vp amount to 2r2 linear constraints,\nhence we have found a linear subspace of T XRm\u00d7n\nr of dimension\nr2 + mr + nr \u2212 2r2 = r(m + n \u2212 r).\nThis coincides with the dimension of T XRm\u00d7n\nr by (7.44). Thus, we have found\nthe whole tangent space:\nTXRm\u00d7n\nr =\n\b\nUMV \u22a4+ UpV \u22a4+ UV \u22a4\np :\nM \u2208 Rr\u00d7r, Up \u2208 Rm\u00d7r, Vp \u2208 Rn\u00d7r, and\nU\u22a4Up = 0, V\u22a4Vp = 0\n\t\n. (7.47)\nNotice how, if X is already identified by the triplet (U, \u03a3, V), then to represent a\ntangent vector at X we only need small matrices M, Up, Vp. These require essen-\ntially the same amount of memory as for storing X. Sometimes, it is convenient\n(for analysis, not computation) to write tangent vectors as follows:\nTXRm\u00d7n\nr =\n\u001a\u0002\nU U \u22a5\n\u0003\u0014A B\nC 0\n\u0015\u0002\nV V \u22a5\n\u0003\u22a4\n: A, B, Care arbitrary\n\u001b\n. (7.48)\nThis reveals the dimension of the tangent space even more explicitly.\nTo build a retraction for Rm\u00d7n\nr , one possibility is to use metric projection\n(Section 5.12): make the step in the ambient space, then project back to the\nmanifold. To project from Rm\u00d7n to Rm\u00d7n\nr , we first need to endow Rm\u00d7n with\na Euclidean metric: we choose the standard inner product, \u27e8U, V\u27e9 = Tr(U\u22a4V ),\nwith its induced norm \u2225U\u2225 =\np\n\u27e8U, U\u27e9 (the Frobenius norm). Then, we construct\nthe retraction as:\nRX(H) = arg min\nY \u2208Rm\u00d7n\nr\n\u2225X + H \u2212 Y \u22252. (7.49)\nFollowing the well-known Eckart\u2013Young\u2013Mirsky theorem, the solution to this\noptimization problem (when it exists) is given by the singular value decomposi-\ntion of X + H truncated at rank r. With X and H represented as above, this", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2200, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2057b859-a0ba-4af9-993b-a8c4a499763e": {"__data__": {"id_": "2057b859-a0ba-4af9-993b-a8c4a499763e", "embedding": null, "metadata": {"page_label": "169", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "898ab30b-be46-499e-9e5e-e82f786c2943", "node_type": "4", "metadata": {"page_label": "169", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "2087ea653675657d52cdc6f08349f1a646fcec97fbb848b1d0b05b87d3630f9c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n7.5 Fixed-rank matrices 169\ncan be computed efficiently. Indeed, consider\nX + H = U(\u03a3 + M)V \u22a4+ UpV \u22a4+ UV \u22a4\np\n=\n\u0002\nU U p\n\u0003\u0014\u03a3 + M I r\nIr 0\n\u0015\u0002\nV V p\n\u0003\u22a4\n.\nThis notably reveals that X + H has rank at most 2 r. Compute5 thin QR fac-\ntorizations of the left and right matrices:\nQU RU =\n\u0002\nU U p\n\u0003\n, Q V RV =\n\u0002\nV V p\n\u0003\n,\nwith QU \u2208 St(m, 2r), QV \u2208 St(n, 2r) and RU , RV \u2208 R2r\u00d72r upper triangular\n(assuming 2 r \u2264 m, n; otherwise, the procedure is easily adapted). This costs\n\u223c 8(m + n)r2 arithmetic operations. Then,\nX + H = QU RU\n\u0014\u03a3 + M I r\nIr 0\n\u0015\nR\u22a4\nV\n| {z }\n\u2248\u02dcU \u02dc\u03a3 \u02dcV \u22a4\nQ\u22a4\nV .\nCompute a singular value decomposition \u02dcU \u02dc\u03a3 \u02dcV \u22a4 of the middle part as indicated,\ntruncated at rank r: \u02dcU, \u02dcV \u2208 St(2r, r), and \u02dc\u03a3 \u2208 Rr\u00d7r diagonal with decreas-\ning, nonnegative diagonal entries. This costs essentially some multiple of \u223c r3\narithmetic operations, and reveals the truncated singular value decomposition of\nX + H:\nRX(H) = (QU \u02dcU)\u02dc\u03a3(QV \u02dcV )\u22a4. (7.50)\nComputing the products QU \u02dcU and QV \u02dcV costs \u223c 4(m + n)r2 arithmetic opera-\ntions. The triplet ( QU \u02dcU, \u02dc\u03a3, QV \u02dcV ) represents the retracted point on Rm\u00d7n\nr .\nNotice that if we wish to compute R X(tH) for several different values of t (as\nwould happen in a line-search procedure), then we can save the QR computa-\ntions and replace the matrix\n\u0002\u03a3+M Ir\nIr 0\n\u0003\nwith\n\u0002\u03a3+tM tIr\ntIr 0\n\u0003\n. After a first retraction,\nsubsequent retractions along the same direction could be up to three times faster.\nIt is clear that R X(0) = X. That this retraction is indeed well defined and\nsmooth (locally) and that DR X(0) is the identity map follow from general prop-\nerties of metric projection retractions (Section 5.12). See also [AO15] for details\non this and several other retractions on Rm\u00d7n\nr .\nThere \u22c6is an important caveat with the retraction detailed above. Specifically,\nprojection to Rm\u00d7n\nr is not globally well defined. In part, this is because Rm\u00d7n\nr is\nnot (the shell of) a convex set in Rm\u00d7n. This fact is apparent in the step where\nwe compute the rank- r truncated singular value decomposition of a matrix of\nsize 2r \u00d7 2r: depending on the vector being retracted, that operation may not\nhave a solution (if the matrix has rank less than r), or the solution may not be\n5 Here, the signs on the diagonals of RU , RV are irrelevant. In principle, some work can be\nsaved using that U has orthonormal columns and that the columns of Up are orthogonal to\nthose of U, but this is numerically delicate when Up is ill conditioned; likewise for V, Vp.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2729, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d0217b4d-5dcb-463b-9985-2c00fce93e25": {"__data__": {"id_": "d0217b4d-5dcb-463b-9985-2c00fce93e25", "embedding": null, "metadata": {"page_label": "170", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "39696ee9-ce77-4e48-8b45-51c188dfc290", "node_type": "4", "metadata": {"page_label": "170", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "5506dfdb9c86e28cc8f33738364c91f23245838df9f5f94822aef11c7d5216ab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n170 Embedded submanifolds: examples\nunique (if its rth and ( r + 1)st singular values are positive and equal). Overall,\nthis means we must be careful when we use this retraction.\nWith Rm\u00d7n still endowed with the standard inner product, we now turn to\nthe orthogonal projectors of Rm\u00d7n\nr . From (7.48), it is clear that the normal space\nat X = U\u03a3V \u22a4 is given by:\nNXRm\u00d7n\nr =\nn\nU\u22a5W V\u22a4\n\u22a5 : W \u2208 R(m\u2212r)\u00d7(n\u2212r)\no\n. (7.51)\nThen, the orthogonal projection of Z \u2208 Rm\u00d7n to TXRm\u00d7n\nr satisfies both\nZ \u2212 ProjX(Z) = U\u22a5W V\u22a4\n\u22a5\nfor some W and, following (7.47),\nProjX(Z) = UMV \u22a4+ UpV \u22a4+ UV \u22a4\np (7.52)\nfor some M, Up, Vp with U\u22a4Up = V \u22a4Vp = 0. Combined, these state\nZ = UMV \u22a4+ UpV \u22a4+ UV \u22a4\np + U\u22a5W V\u22a4\n\u22a5.\nDefine PU = UU \u22a4, PV = V V\u22a4, P\u22a5\nU = Im \u2212PU and P\u22a5\nV = In \u2212PV . Then, we find\nin turn:\nPU ZPV = UMV \u22a4, P \u22a5\nU ZPV = UpV \u22a4, and PU ZP \u22a5\nV = UV \u22a4\np .\nHence,\nProjX(Z) = PU ZPV + P\u22a5\nU ZPV + PU ZP \u22a5\nV (7.53)\n= U(U\u22a4ZV )V \u22a4+ (Im \u2212 UU \u22a4)ZV V\u22a4+ UU \u22a4Z(In \u2212 V V\u22a4).\nIn the notation of (7.52), this is a tangent vector at X represented by\nM = U\u22a4ZV, U p = ZV \u2212 UM, and Vp = Z\u22a4U \u2212 V M\u22a4. (7.54)\nIf Z is structured so that U\u22a4Z and ZV can be computed efficiently, its projection\ncan also be computed efficiently: this is crucial in practice.\nTurning Rm\u00d7n\nr into a Riemannian submanifold of Rm\u00d7n with the standard\nEuclidean metric, the gradient of a smoothf : Rm\u00d7n\nr \u2192 R with smooth extension\n\u00aff to a neighborhood of Rm\u00d7n\nr in Rm\u00d7n is given by Proposition 3.61 as\ngradf(X) = ProjX(grad \u00aff(X)),\nto be computed using (7.52) and (7.54). In applications, grad \u00aff(X) is often a\nsparse matrix, or a low-rank matrix available in factored form, or a sum of such\nstructured matrices. In those cases, the projection can (and should) be computed\nefficiently.\nLet X = U\u03a3V \u22a4 \u2208 Rm\u00d7n\nr be a matrix represented by the triplet ( U, \u03a3, V),\nand let \u02d9X, \u02d9X\u2032 be two tangent vectors at X represented as in (7.47) by triplets", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2122, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c8db6d4f-7ed0-4045-b7ac-5200402e3e5f": {"__data__": {"id_": "c8db6d4f-7ed0-4045-b7ac-5200402e3e5f", "embedding": null, "metadata": {"page_label": "171", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0aaf3074-b4ab-415d-9eee-e021be762ddc", "node_type": "4", "metadata": {"page_label": "171", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "747bc23a89c3c98e75b9a3caa8162610fe4dceb8c21ab24cf5304190dca7ca90", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n7.5 Fixed-rank matrices 171\n(M, Up, Vp) and ( M\u2032, U\u2032\np, V\u2032\np) (respectively). With the stated Riemannian struc-\nture on Rm\u00d7n\nr , we can compute the inner product \u02d9X and \u02d9X\u2032 as follows:\n\u27e8 \u02d9X, \u02d9X\u2032\u27e9X = \u27e8UMV \u22a4+ UpV \u22a4+ UV \u22a4\np , UM\u2032V \u22a4+ U\u2032\npV \u22a4+ U(V \u2032\np)\u22a4\u27e9\n= \u27e8M, M\u2032\u27e9 + \u27e8Up, U\u2032\np\u27e9 + \u27e8Vp, V\u2032\np\u27e9, (7.55)\nwhere \u27e8\u00b7, \u00b7\u27e9 refers to the usual Frobenius inner products over the appropriate ma-\ntrix spaces. Notice how the cancellations that occurred above make it possible to\ncompute inner products of tangent vectors using only the triplets that represent\nthem, for a moderate computational cost.\nSecond-order tools\nWith Rm\u00d7n\nr as a Riemannian submanifold of the Euclidean space Rm\u00d7n, covari-\nant derivatives ( \u2207 and D\ndt ) coincide with the usual vector field derivatives (of\nsmooth extensions), followed by orthogonal projection to tangent spaces (Theo-\nrem 5.9, Proposition 5.31).\nWe use this to obtain a formula for the Riemannian Hessian of f : Rm\u00d7n\nr \u2192 R\nwith smooth extension \u00aff. Let O be the subset of Rm\u00d7n containing all matrices\nwhose rth and ( r + 1)st singular values are distinct: this is a neighborhood of\nRm\u00d7n\nr . Given a matrix X in O, let PU be the orthogonal projector from Rm\nto the subspace spanned by the r dominant left singular vectors of X: this is\nsmooth in X. In particular, if X = U\u03a3V \u22a4 has rank r (with factors as in (7.53)),\nthen PU = UU \u22a4. Likewise, let PV be the orthogonal projector from Rn to the\nsubspace spanned by the r dominant right singular vectors of X, also smooth in\nX, so that for X = U\u03a3V \u22a4 \u2208 Rm\u00d7n\nr we have PV = V V\u22a4. The projectors to the\northogonal complements are P\u22a5\nU = Im\u2212PU and P\u22a5\nV = In\u2212PV . Then, we define a\nsmooth extension of gradf(X) to O in Rm\u00d7n with the shorthand Z = grad \u00aff(X)\nas\n\u00afG(X) = PU ZPV + P\u22a5\nU ZPV + PU ZP \u22a5\nV\n= PU ZPV + ZPV \u2212 PU ZPV + PU Z \u2212 PU ZPV\n= ZPV + PU Z \u2212 PU ZPV .\nIn order to differentiate \u00afG(X), we must determine the differentials of PU and\nPV as a function of X. To this end, consider any tangent vector H = UMV \u22a4+\nUpV \u22a4+ UV \u22a4\np at X \u2208 Rm\u00d7n\nr . We aim to design a smooth curve c on Rm\u00d7n\nr such\nthat c(0) = X and c\u2032(0) = H. Then, we can use\nD \u00afG(X)[H] = ( \u00afG \u25e6 c)\u2032(0)\nto reach our conclusion.\nTaking inspiration from (7.46), pick a smooth curveU(t) on St(m, r) such that\nU(0) = U and U\u2032(0) = Up\u03a3\u22121. Similarly, pick a smooth curve V (t) on St( n, r)\nsuch that V (0) = V and V \u2032(0) = Vp\u03a3\u22121, and set \u03a3( t) = \u03a3 + tM. By design,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2646, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6d59ef69-ded6-4bae-aa74-00e064708238": {"__data__": {"id_": "6d59ef69-ded6-4bae-aa74-00e064708238", "embedding": null, "metadata": {"page_label": "172", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "314e1552-438a-482f-9f47-bc3b6c06c590", "node_type": "4", "metadata": {"page_label": "172", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "e965f362c3d26ddbdf46db39d8f8341b63d6868935216689f50a7be9ef6f923d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "072f1939-10ae-44f5-9e13-b260f3b47148", "node_type": "1", "metadata": {}, "hash": "3d0b09a4a36151e4b0db215e214fa4d0ef79094cc7bc0f66cd8dc929f742214b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n172 Embedded submanifolds: examples\nthis ensures that c(t) = U(t)\u03a3(t)V (t)\u22a4 satisfies c(0) = X and c\u2032(0) = H. Define\n\u02d9PU \u2014the derivative of PU at X along H\u2014through:\nPU(t) = U(t)U(t)\u22a4, and\n\u02d9PU \u225c d\ndtPU(t)\n\f\f\f\f\nt=0\n= U(0)U\u2032(0)\u22a4+ U\u2032(0)U(0)\u22a4 = U\u03a3\u22121U\u22a4\np + Up\u03a3\u22121U\u22a4.\nLikewise, define \u02d9PV through\nPV (t) = V (t)V (t)\u22a4, and \u02d9PV \u225c d\ndtPV (t)\n\f\f\f\f\nt=0\n= V \u03a3\u22121V \u22a4\np + Vp\u03a3\u22121V \u22a4.\nWith \u02d9Z = Hess \u00aff(X)[H] for short, this allows us to write\nD \u00afG(X)[H] = \u02d9ZPV + Z \u02d9PV + \u02d9PU Z + PU \u02d9Z \u2212 \u02d9PU ZPV \u2212 PU \u02d9ZPV \u2212 PU Z \u02d9PV\n= (PU + P\u22a5\nU ) \u02d9ZPV + PU \u02d9Z(PV + P\u22a5\nV ) \u2212 PU \u02d9ZPV + P\u22a5\nU Z \u02d9PV + \u02d9PU ZP \u22a5\nV\n= PU \u02d9ZPV + P\u22a5\nU ( \u02d9ZPV + Z \u02d9PV ) + (PU \u02d9Z + \u02d9PU Z)P\u22a5\nV .\nWe can now use the fact that Rm\u00d7n\nr is a Riemannian submanifold of Rm\u00d7n\ntogether with (7.52) to claim\nHessf(X)[H] = ProjX\n\u0000\nD \u00afG(X)[H]\n\u0001\n= U \u02dcMV \u22a4+ \u02dcUpV \u22a4+ U \u02dcV \u22a4\np , (7.56)\nfor matrices \u02dcM, \u02dcUp, \u02dcVp given as in (7.54). Explicitly,\n\u02dcM = U\u22a4D \u00afG(X)[H]V = U\u22a4 \u02d9ZV,\n\u02dcUp = D \u00afG(X)[H]V \u2212 U \u02dcM = P\u22a5\nU\n\u0010\n\u02d9ZV + ZVp\u03a3\u22121\n\u0011\n,\n\u02dcVp = (D \u00afG(X)[H])\u22a4U \u2212 V \u02dcM\u22a4 = P\u22a5\nV\n\u0010\n\u02d9Z\u22a4U + Z\u22a4Up\u03a3\u22121\n\u0011\n, (7.57)\nwhere Z = grad \u00aff(X), \u02d9Z = Hess \u00aff(X)[H], X = U\u03a3V \u22a4 and H is represented by\nthe triplet (M, Up, Vp).\nOnce more, Z and \u02d9Z are matrices in Rm\u00d7n whose structure (if any) should be\nexploited to compute the products ZVp, Z\u22a4Up, \u02d9ZV and \u02d9Z\u22a4U efficiently.\nWe may reorganize the above as:\nHessf(X)[H] = ProjX(Hess \u00aff(X)[H])\n+\n\u0002\nP\u22a5\nU grad \u00aff(X)Vp\u03a3\u22121\u0003\nV \u22a4+ U\n\u0002\nP\u22a5\nV (grad \u00aff(X))\u22a4Up\u03a3\u22121\u0003\u22a4\n. (7.58)\nThis highlights the Riemannian Hessian as the projection of the Euclidean Hes-\nsian with additional corrections to \u02dcUp and \u02dcVp (between brackets): compare with\nCorollary 5.47.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1826, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "072f1939-10ae-44f5-9e13-b260f3b47148": {"__data__": {"id_": "072f1939-10ae-44f5-9e13-b260f3b47148", "embedding": null, "metadata": {"page_label": "172", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "314e1552-438a-482f-9f47-bc3b6c06c590", "node_type": "4", "metadata": {"page_label": "172", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "e965f362c3d26ddbdf46db39d8f8341b63d6868935216689f50a7be9ef6f923d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d59ef69-ded6-4bae-aa74-00e064708238", "node_type": "1", "metadata": {"page_label": "172", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "ae5232bfcaa87ad3192db2de8313c31ea158f6e24ec6121e3ed9e1f9e701b653", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Once more, Z and \u02d9Z are matrices in Rm\u00d7n whose structure (if any) should be\nexploited to compute the products ZVp, Z\u22a4Up, \u02d9ZV and \u02d9Z\u22a4U efficiently.\nWe may reorganize the above as:\nHessf(X)[H] = ProjX(Hess \u00aff(X)[H])\n+\n\u0002\nP\u22a5\nU grad \u00aff(X)Vp\u03a3\u22121\u0003\nV \u22a4+ U\n\u0002\nP\u22a5\nV (grad \u00aff(X))\u22a4Up\u03a3\u22121\u0003\u22a4\n. (7.58)\nThis highlights the Riemannian Hessian as the projection of the Euclidean Hes-\nsian with additional corrections to \u02dcUp and \u02dcVp (between brackets): compare with\nCorollary 5.47. Notice the \u03a3\u22121 factors: these indicate that Riemannian Hessians\nare likely to behave poorly close to the \u201cbrink\u201d, that is, if some of the top r\nsingular values of X are near zero.\nIn closing, we note that the retraction (7.50) is second order because it is the\nmetric projection retraction (Proposition 5.55).", "mimetype": "text/plain", "start_char_idx": 1367, "end_char_idx": 2136, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ff8be358-f666-4b55-99b6-7ea055e73c39": {"__data__": {"id_": "ff8be358-f666-4b55-99b6-7ea055e73c39", "embedding": null, "metadata": {"page_label": "173", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "93283ef8-2219-4af8-b8bb-e01ec47b8324", "node_type": "4", "metadata": {"page_label": "173", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "68cd474e4d95de4939a50f777fc8ca9a9a773272f212748f95a56b95f18da1e0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n7.6 The hyperboloid model 173\nExercise 7.5. Taking inspiration from the discussion of retraction (7.50), pro-\npose an algorithm to compute an SVD representation of a tangent vector. More\nprecisely: given a point X \u2208 Rm\u00d7n\nr represented by a triplet (U, \u03a3, V) as in (7.45)\nand a tangent vector \u02d9X \u2208 TXRm\u00d7n\nr represented by a triplet (M, Up, Vp) as\nin (7.47), explain how you compute a triplet ( \u02dcU, \u02dc\u03a3, \u02dcV ) which is a representation\nof \u02d9X = \u02dcU \u02dc\u03a3 \u02dcV \u22a4, where \u02dcU, \u02dcV have 2r orthonormal columns and \u02dc\u03a3 has nonnegative\n(but not necessarily positive) diagonal entries, with overall complexity linear in\nm + n. (In Manopt, such functions are called tangent2ambient.)\nExercise 7.6. In this section, we have developed representations of points and\ntangent vectors on the manifold Rm\u00d7n\nr which allow for efficient computation.\nTo develop theory however, it is sometimes more convenient to work with the\npoints and tangent vectors directly, rather than in terms of particular matrix\ndecompositions. It is indeed possible to find such expressions.\nLet PX and P\u22a5\nX denote orthogonal projectors to the image (the range) of X\nand to its orthogonal complement, respectively. Thus, if X = U\u03a3V \u22a4 is an SVD\nof X \u2208 Rm\u00d7n\nr , then\nPX = UU \u22a4, P \u22a5\nX = Im \u2212 UU \u22a4, P X\u22a4 = V V\u22a4, P \u22a5\nX\u22a4 = In \u2212 V V\u22a4.\nVerify the following:\nTXRm\u00d7n\nr = { \u02d9X \u2208 Rm\u00d7n : P\u22a5\nX \u02d9XP \u22a5\nX\u22a4 = 0},\nProjX(Z) = Z \u2212 P\u22a5\nX ZP \u22a5\nX\u22a4\n= PXZ + ZPX\u22a4 \u2212 PXZPX\u22a4.\nWith \u00aff a smooth extension of f : Rm\u00d7n\nr \u2192 R, further verify that\ngradf(X) = ProjX(grad \u00aff(X)) and\nHessf(X)[ \u02d9X] = ProjX\n\u0010\nHess \u00aff(X)[ \u02d9X] + N \u02d9X\u22a4(X\u2020)\u22a4+ (X\u2020)\u22a4 \u02d9X\u22a4N\n\u0011\n,\nwhere X\u2020 is the Moore\u2013Penrose pseudo-inverse of X, and N is shorthand for\nthe normal part of the gradient of \u00aff at X:\nN = P\u22a5\nX grad \u00aff(X)P\u22a5\nX\u22a4.\nOf course, X is second-order critical for f if and only if gradf(X) = 0 and\nHessf(X) \u2ab0 0. The latter is equivalent to the condition that\nD\n\u02d9X, Hess \u00aff(X)[ \u02d9X] + N \u02d9X\u22a4(X\u2020)\u22a4+ (X\u2020)\u22a4 \u02d9X\u22a4N\nE\n\u2265 0\nfor all \u02d9X \u2208 TXRm\u00d7n\nr .\n7.6 The hyperboloid model\nConsider the bilinear map \u27e8\u00b7, \u00b7\u27e9M on Rn+1 defined by\n\u27e8u, v\u27e9M = \u2212u0v0 + u1v1 + \u00b7 \u00b7\u00b7+ unvn = u\u22a4Jv (7.59)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2304, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "75f2998f-9c75-4881-b9ac-68903762e4dc": {"__data__": {"id_": "75f2998f-9c75-4881-b9ac-68903762e4dc", "embedding": null, "metadata": {"page_label": "174", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "83647637-a00d-407e-9dee-4e415aba5bfc", "node_type": "4", "metadata": {"page_label": "174", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "65d32f9a109f45a41a64104c07bfcf3dd02589199d0b926b0ff5f3adb6acdbea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n174 Embedded submanifolds: examples\nwith J = diag(\u22121, 1, . . . ,1). This is not a Euclidean inner product because J has\none negative eigenvalue, but it is a pseudo-inner product because all eigenvalues\nof J are nonzero. It is called the Minkowski pseudo-inner product on Rn+1.\nConsider the following subset of Rn+1 (sometimes denoted by Hn):\nM =\n\b\nx \u2208 Rn+1 : \u27e8x, x\u27e9M = \u22121 and x0 > 0\n\t\n=\n\b\nx \u2208 Rn+1 : x2\n0 = 1 + x2\n1 + \u00b7 \u00b7\u00b7+ x2\nn and x0 > 0\n\t\n. (7.60)\nThe equation \u27e8x, x\u27e9M = \u22121 defines two connected components determined by\nthe sign of x0. The condition x0 > 0 selects one of them. The defining function\nh(x) = \u27e8x, x\u27e9M + 1 has differential\nDh(x)[u] = 2 \u27e8x, u\u27e9M = (2Jx)\u22a4u.\nNotice that x0 \u0338= 0 for all x \u2208 M; hence, 2Jx \u0338= 0 for all x \u2208 M. We deduce that\nM is an embedded submanifold of Rn+1 of dimension n with tangent spaces\nTxM =\n\b\nu \u2208 Rn+1 : \u27e8x, u\u27e9M = 0\n\t\n. (7.61)\nFor n = 2, the manifold M is one sheet of a hyperboloid of two sheets in R3.\nWhile \u27e8\u00b7, \u00b7\u27e9M is only a pseudo-inner product on Rn+1, it is an inner product\nwhen restricted to the tangent spaces of M. Indeed, for all ( x, u) \u2208 TM,\n\u27e8u, u\u27e9M = u2\n1 + \u00b7 \u00b7\u00b7+ u2\nn \u2212 u2\n0\n= u2\n1 + \u00b7 \u00b7\u00b7+ u2\nn \u2212 1\nx2\n0\n(x1u1 + \u00b7 \u00b7\u00b7+ xnun)2\n\u2265 u2\n1 + \u00b7 \u00b7\u00b7+ u2\nn \u2212 1\nx2\n0\n(x2\n1 + \u00b7 \u00b7\u00b7+ x2\nn)(u2\n1 + \u00b7 \u00b7\u00b7+ u2\nn)\n= (u2\n1 + \u00b7 \u00b7\u00b7+ u2\nn)\n\u0012\n1 \u2212 x2\n0 \u2212 1\nx2\n0\n\u0013\n= 1\nx2\n0\n(u2\n1 + \u00b7 \u00b7\u00b7+ u2\nn)\n\u2265 0.\nAbove, we used in turn: \u27e8x, u\u27e9M = 0 to eliminate u0, then Cauchy\u2013Schwarz, then\n\u27e8x, x\u27e9M = \u22121 to claim x2\n1 + \u00b7 \u00b7\u00b7+ x2\nn = x2\n0 \u2212 1. As a result, \u2225u\u2225M =\np\n\u27e8u, u\u27e9M is\na well-defined norm on any tangent space. This is despite the fact that \u27e8u, u\u27e9M\ncan be negative if u does not belong to any tangent space of M.\nIt is easy to check that the restriction of \u27e8\u00b7, \u00b7\u27e9M to each tangent space T xM\ndefines a Riemannian metric on M, turning it into a Riemannian manifold. With\nthis Riemannian structure, we callM a hyperbolic spacein the hyperboloid model.\nThe main geometric trait of M with n \u2265 2 is that its sectional curvatures are\nconstant, equal to \u22121. Manifolds with that property are called hyperbolic spaces.\nThere are several other models that share this trait, namely the Beltrami\u2013Klein\nmodel, the Poincar\u00b4 e ball model and the Poincar\u00b4 e half-space model. For more\nabout curvature and these models, see [Lee18, p62].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2499, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b289b273-67c9-4472-a5da-e6edb8270f29": {"__data__": {"id_": "b289b273-67c9-4472-a5da-e6edb8270f29", "embedding": null, "metadata": {"page_label": "175", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5eb4f8f5-22b1-49ca-bcc3-a3d4d93df0d0", "node_type": "4", "metadata": {"page_label": "175", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "a09065fc1267e9ede3cf75de73544bdc93c4e1adb2725fb854f7aeb48081c183", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n7.6 The hyperboloid model 175\nThe tangent space TxM is an n-dimensional subspace of Rn+1. Its orthogonal\ncomplement with respect to \u27e8\u00b7, \u00b7\u27e9M is the one-dimensional normal space\nNxM =\n\b\nv \u2208 Rn+1 : \u27e8u, v\u27e9M = 0 for all u \u2208 TxM\n\t\n= span(x). (7.62)\nThus, orthogonal projection from Rn+1 to TxM with respect to \u27e8\u00b7, \u00b7\u27e9M takes the\nform Projx(z) = z + \u03b1x with \u03b1 \u2208 R chosen so that z + \u03b1x is in TxM, that is, so\nthat 0 = \u27e8x, z+ \u03b1x\u27e9M = \u27e8x, z\u27e9M \u2212 \u03b1. In other words:\nProjx(z) = z + \u27e8x, z\u27e9M \u00b7 x. (7.63)\nWith this tool in hand, we can construct a useful formula to compute gradients\nof functions on M.\nProposition 7.7. Let \u00aff : Rn+1 \u2192 R be a smooth function on the Euclidean space\nRn+1 with the usual inner product \u27e8u, v\u27e9 = u\u22a4v. Let f = \u00aff|M be the restriction\nof \u00aff to M with the Riemannian structure as described above. The gradient of f\nis related to that of \u00aff as follows:\ngradf(x) = Projx\n\u0000\nJgrad \u00aff(x)\n\u0001\n, (7.64)\nwhere J = diag(\u22121, 1, . . . ,1) and Projx is defined by (7.63).\nProof. By definition, gradf(x) is the unique vector in TxM such that Df(x)[u] =\n\u27e8gradf(x), u\u27e9M for all u \u2208 TxM. Since \u00aff is a smooth extension of f, we can\ncompute\nDf(x)[u] = D \u00aff(x)[u]\n=\n\ngrad \u00aff(x), u\n\u000b\n=\n\nJgrad \u00aff(x), u\n\u000b\nM\n=\n\nJgrad \u00aff(x), Projx(u)\n\u000b\nM\n=\n\nProjx\n\u0000\nJgrad \u00aff(x)\n\u0001\n, u\n\u000b\nM .\nAbove, the second line is by definition of grad \u00aff(x); the third by definition of\n\u27e8\u00b7, \u00b7\u27e9M ; the fourth because u is tangent at x; and the fifth because Proj x is self-\nadjoint with respect to \u27e8\u00b7, \u00b7\u27e9M , as are all orthogonal projectors. The claim follows\nby uniqueness.\nAs a remark, note thatJgrad \u00aff(x) which appears in (7.64) is the gradient of\u00aff in\nthe Minkowski spaceRn+1 with pseudo-inner product \u27e8\u00b7, \u00b7\u27e9M . See O\u2019Neill [O\u2019N83]\nfor a general treatment of submanifolds of spaces equipped with pseudo-inner\nproducts.\nSecond-order tools\nFor all smooth vector fields V on M and all ( x, u) \u2208 TM, define the operator\n\u2207 as\n\u2207uV = Projx(D \u00afV (x)[u]), (7.65)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2168, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "016379d4-5297-4ccd-a9c9-a9d572dff0f0": {"__data__": {"id_": "016379d4-5297-4ccd-a9c9-a9d572dff0f0", "embedding": null, "metadata": {"page_label": "176", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0546b4e7-9792-4b2f-bfd7-3580bddab0dc", "node_type": "4", "metadata": {"page_label": "176", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "2bb20ed8075d1860ac02b35117630deb5178484b93350bf31d01e103cba27293", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n176 Embedded submanifolds: examples\nwhere \u00afV is any smooth extension of V to a neighborhood of M in Rn+1 and\nD \u00afV (x)[u] is the usual directional derivative. It is an exercise to check that\u2207 is the\nRiemannian connection for M. It is instructive to compare this with Theorem 5.9\nwhere we make the same claim under the assumption that the embedding space\nis Euclidean. Here, the embedding space is not Euclidean, but the result stands.\nAgain, see O\u2019Neill [O\u2019N83] for a general treatment.\nThe covariant derivative D\ndt (induced by \u2207) for a smooth vector field Z along\na smooth curve c: I \u2192 Mis given by\nD\ndtZ(t) = Projc(t)\n\u0012 d\ndtZ(t)\n\u0013\n, (7.66)\nwhere d\ndt Z(t) is the usual derivative of Z understood as a map from I to Rn+1\u2014\nthis makes use of the fact that Z(t) \u2208 Tc(t)M \u2282Rn+1. Compare this with\nProposition 5.31.\nIt is an exercise to check that, for arbitrary ( x, u) \u2208 TM,\nc(t) = Expx(tu) \u225c cosh(\u2225tu\u2225M )x + sinh(\u2225tu\u2225M )\n\u2225tu\u2225M\ntu\n= cosh(t\u2225u\u2225M )x + sinh(t\u2225u\u2225M )\n\u2225u\u2225M\nu (7.67)\ndefines the unique geodesic on M such that c(0) = x and c\u2032(0) = u. Notice\nthat this is defined for all t: Exp is a second-order retraction defined on the\nwhole tangent bundle (see also Section 10.2). Compare with the geodesics on the\nsphere, Example 5.37.\nWe proceed to construct a formula for the Hessian of a function on M based\non the gradient and Hessian of a smooth extension.\nProposition 7.8. (Continued from Proposition 7.7.) The Hessian of f is related\nto that of \u00aff as follows:\nHessf(x)[u] = Projx\n\u0000\nJHess \u00aff(x)[u]\n\u0001\n+\n\nx, Jgrad \u00aff(x)\n\u000b\nM \u00b7 u, (7.68)\nwhere J = diag(\u22121, 1, . . . ,1) and Projx is defined by (7.63).\nProof. Consider the following smooth vector field in Rn+1:\n\u00afG(x) = Jgrad \u00aff(x) +\n\nJgrad \u00aff(x), x\n\u000b\nM \u00b7 x.\nThis is a smooth extension of gradf from M to Rn+1. Thus, for all (x, u) \u2208 TM\nwe have\nHessf(x)[u] = \u2207ugradf\n= Projx\n\u0000\nD \u00afG(x)[u]\n\u0001\n= Projx\n\u0000\nJHess \u00aff(x)[u] + qx +\n\nJgrad \u00aff(x), x\n\u000b\nM \u00b7 u\n\u0001\n= Projx\n\u0000\nJHess \u00aff(x)[u]\n\u0001\n+\n\nJgrad \u00aff(x), x\n\u000b\nM \u00b7 u,\nwhere q is the derivative of\n\nJgrad \u00aff(x), x\n\u000b\nM at x along u\u2014and we do not need", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2285, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "965fc71d-0187-49b4-84f3-9a8cbabe1b6b": {"__data__": {"id_": "965fc71d-0187-49b4-84f3-9a8cbabe1b6b", "embedding": null, "metadata": {"page_label": "177", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6fa991ff-1293-486d-8f1a-7cde0bee63a9", "node_type": "4", "metadata": {"page_label": "177", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "3481e1f4887c1b72b6682d44f69ff60140a1aa1e3078b9993a16dd895ba281d6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n7.7 Manifolds defined by h(x) = 0 177\nto compute it since qx is in the normal space, hence it vanishes through the\nprojector.\nExercise 7.9. Check that \u27e8\u00b7, \u00b7\u27e9M indeed defines a Riemannian metric on M.\nVerify that \u2207 (7.65) is the Riemannian connection for M, that D\ndt (7.66) is\nthe covariant derivative induced by \u2207 and that c(t) (7.67) is a geodesic on M\nsatisfying c(0) = x and c\u2032(0) = u (that this is the unique such geodesic is a\nconsequence of general results, see Section 10.2).\n7.7 Manifolds defined by h(x) = 0\nLet h: E \u2192Rk be a smooth function on a Euclidean space of dimension strictly\nlarger than k with inner product \u27e8\u00b7, \u00b7\u27e9 and induced norm \u2225 \u00b7 \u2225. If Dh(x) has full\nrank k for all x such that h(x) = 0, the set\nM = {x \u2208 E: h(x) = 0} (7.69)\nis an embedded submanifold of E of dimension dim E \u2212k. We assume so here. In\ncontrast with Definition 3.10, we require the whole manifold to be defined with\na single defining function h. Notwithstanding, everything below still holds if M\nis only locally defined by h. We focus on the case of a global h for notational\nsimplicity and because it covers several of the examples we have encountered.\nWith the notation h(x) = ( h1(x), . . . , hk(x))\u22a4 to highlight the k constraint\nfunctions hi : E \u2192R, we can spell out the linear map\nDh(x)[v] =\n\u0000\n\u27e8gradh1(x), v\u27e9, . . . ,\u27e8gradhk(x), v\u27e9\n\u0001\u22a4\n(7.70)\nand its adjoint\nDh(x)\u2217[\u03b1] =\nkX\ni=1\n\u03b1igradhi(x). (7.71)\nThe tangent spaces are given by\nTxM = ker Dh(x) = {v \u2208 E: \u27e8gradhi(x), v\u27e9 = 0 for all i}. (7.72)\nThe fact that D h(x) has full rank k means that the gradients of the constraints\nat x are linearly independent. In other words, they form a basis for the normal\nspace at x:\nNxM = (ker Dh(x))\u22a5 = span\n\u0000\ngradh1(x), . . . ,gradhk(x)\n\u0001\n. (7.73)\nLet Projx : E \u2192TxM denote orthogonal projection from E to TxM. Then, for\nany vector v in E there exists a unique choice of coefficients \u03b1 \u2208 Rk such that\nv = Projx(v) + Dh(x)\u2217[\u03b1]. (7.74)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2175, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a4dd0396-5b7f-4d05-ab88-1247e00ed4d9": {"__data__": {"id_": "a4dd0396-5b7f-4d05-ab88-1247e00ed4d9", "embedding": null, "metadata": {"page_label": "178", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "56eca053-5fb4-40e5-a33a-2e8a956f63e4", "node_type": "4", "metadata": {"page_label": "178", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "a4e394088e3d93bfae19636a914fdec49c400d53cd7b61e770ade965e3063c1c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n178 Embedded submanifolds: examples\nThis decomposes v into its tangent and normal parts at x. Explicitly, \u03b1 is the\nunique solution to the following least-squares problem:\n\u03b1 = arg min\n\u03b1\u2208Rk\n\u2225v \u2212 Dh(x)\u2217[\u03b1]\u22252 = (Dh(x)\u2217)\u2020 [v],\nwhere the dagger \u2020 denotes Moore\u2013Penrose pseudo-inversion, so that\nProjx(v) = v \u2212 Dh(x)\u2217\nh\n(Dh(x)\u2217)\u2020 [v]\ni\n. (7.75)\nThis formula is expected since im D h(x)\u2217 = (ker Dh(x))\u22a5 = NxM.\nOne possible retraction for M is metric projection as studied in Section 5.12.\nIt relies on the Euclidean metric to define:\nRx(v) = arg min\ny\u2208E\n\u2225x + v \u2212 y\u2225 subject to h(y) = 0. (7.76)\nThis is well defined for small enough v, but Rx(v) may not be uniquely defined\nfor all v. It may be difficult to compute in general.\nLet M be a Riemannian submanifold of E. Then, R (7.76) is a second-order\nretraction. Given a smooth function \u00aff : E \u2192R and its restriction f = \u00aff|M, the\nRiemannian gradient follows from (7.75) as\ngradf(x) = Projx(grad \u00aff(x)) = grad \u00aff(x) \u2212\nkX\ni=1\n\u03bbi(x)gradhi(x), (7.77)\nwith \u03bb(x) = (Dh(x)\u2217)\u2020 [grad \u00aff(x)].\nSecond-order tools\nWith M as a Riemannian submanifold of the Euclidean spaceE, covariant deriva-\ntives ( \u2207 and D\ndt ) coincide with the usual vector field derivatives (of smooth\nextensions), followed by orthogonal projection to tangent spaces (Theorem 5.9,\nProposition 5.31). We use this to determine the Riemannian Hessian off = \u00aff|M.\nNotice that\n\u03bb(x) = (Dh(x)\u2217)\u2020[grad \u00aff(x)] (7.78)\nis a smooth function on the open subset of E consisting of all points x where\nDh(x) has full rank k. Thus, we can differentiate grad f(x) (7.77) as follows:\nDgradf(x)[v] = Hess \u00aff(x)[v]\n\u2212\nkX\ni=1\nD\u03bbi(x)[v] \u00b7 gradhi(x) \u2212\nkX\ni=1\n\u03bbi(x)Hesshi(x)[v].\nThen, since Hessf(x)[v] is nothing but the orthogonal projection of Dgradf(x)[v]\nto TxM and since each gradhi(x) is orthogonal to T xM, it follows that\nHessf(x)[v] = Projx\n \nHess \u00aff(x)[v] \u2212\nkX\ni=1\n\u03bbi(x)Hesshi(x)[v]\n!\n. (7.79)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2137, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5bfc91b2-9d10-4647-bf6a-787aefcde9e9": {"__data__": {"id_": "5bfc91b2-9d10-4647-bf6a-787aefcde9e9", "embedding": null, "metadata": {"page_label": "179", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dfd9824b-4139-4220-a085-6e79fadeb1b0", "node_type": "4", "metadata": {"page_label": "179", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "4cd0a8111bd127d91c73a7d09121e71ee72aa76ceaa5d98db9115b020ddd019f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n7.7 Manifolds defined by h(x) = 0 179\nThis can be summarized with pleasantly symmetric identities:\ngradf(x) = grad \u00aff(x) \u2212\nkX\ni=1\n\u03bbi(x)gradhi(x), (7.80)\nHessf(x) = Projx \u25e6\n \nHess \u00aff(x) \u2212\nkX\ni=1\n\u03bbi(x)Hesshi(x)\n!\n\u25e6 Projx, (7.81)\nwith \u03bb(x) as defined in (7.78), and with the understanding that the linear map on\nthe right-hand side of (7.81) is restricted to T xM. Notice that \u03bb(x) depends on\ngrad \u00aff(x) only through its normal component: compare with the Hessian formulas\nin Section 5.11. The work above easily yields an expression for the Weingarten\nmap (5.38) of M.\nExercise 7.10. Consider the equality constrained optimization problem\nmin\nx\u2208E\n\u00aff(x) subject to h(x) = 0, (7.82)\nwhere \u00aff : E \u2192R and h: E \u2192Rk are smooth on a Euclidean space E with dim E >\nk. The Lagrangian function L: E \u00d7Rk \u2192 R for this problem is:\nL(x, \u03bb) = \u00aff(x) \u2212 \u27e8\u03bb, h(x)\u27e9.\nA classical result is that if x \u2208 Eis such that Dh(x) has rank k and x is a local\nminimizer for (7.82) then x satisfies KKT conditions of order one and two;\nexplicitly: there exists a unique \u03bb \u2208 Rk such that\n1. gradL(x, \u03bb) = 0, and\n2. \u27e8HessxL(x, \u03bb)[v], v\u27e9 \u22650 for all v \u2208 ker Dh(x),\nwhere the Hessian of L is taken with respect to x only. These are the classical\nfirst- and second-order necessary optimality condition for (7.82).\nThe full-rank requirement on Dh(x) is known as the linear independence con-\nstraint qualification (LICQ), because it amounts to the requirement that the gra-\ndients of the constraints at x be linearly independent.\nWe know M = {x \u2208 E: h(x) = 0} is an embedded submanifold of E if Dh(x)\nhas rank k for all x \u2208 M. Assuming this holds, show that x \u2208 Esatisfies the\nfirst-order KKT conditions if and only if x is in M and gradf(x) = 0 , where\nf = \u00aff|M is restricted to M equipped with the Riemannian submanifold structure.\nAdditionally, show that x satisfies both first- and second-order KKT conditions\nif and only if x \u2208 M, gradf(x) = 0 and Hessf(x) \u2ab0 0.\nThis confirms that the classical necessary optimality conditions are equivalent\nto the conditions we established in Sections 4.2 and 6.1 when LICQ holds globally.\n(Of course, this reasoning can also be applied locally around any point x.) This\ngives KKT conditions and Lagrange multipliers a geometric interpretation. These\nconsiderations form part of the basis of Luenberger\u2019s seminal paper [Lue72] which\nstarted the field of optimization on manifolds.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2635, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "27389124-f307-4325-85fa-dcc8353c863b": {"__data__": {"id_": "27389124-f307-4325-85fa-dcc8353c863b", "embedding": null, "metadata": {"page_label": "180", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1cd8ec23-45b6-4031-a37b-b6008799a3e7", "node_type": "4", "metadata": {"page_label": "180", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "a84622ed273e205a904ac11d324e4e210d24039cae5aa154357f45aed85779ea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n180 Embedded submanifolds: examples\n7.8 Notes and references\nMuch of the material in this chapter is standard, though some of it rarely appears\nin as much detail.\nFor the Stiefel manifold in particular, we follow mostly [AMS08].\nThe construction of tools for optimization on Rm\u00d7n\nr as a Riemannian subman-\nifold of Rm\u00d7n follows work by Vandereycken [Van13]. Similarly, one can derive\ntools for optimization over fixed-rank tensors in tensor train (TT) and Tucker\nformat [UV13, KSV14, HS18, UV20]. Fine properties of curves generated by\nthe metric projection retraction to the real algebraic variety of matrices of rank\nupper-bounded by r appear in [Lev20, Thm. 3.1, Cor. 3.3]. One popular tech-\nnique to optimize over matrices of rank up to r (rather than equal to r) is to\nset X = AB\u22a4 and to optimize over the factors A \u2208 Rm\u00d7r, B \u2208 Rn\u00d7r (this is an\nover-parameterization since the factorization is not unique). There exist other\nsuch smooth over-parameterizations of the variety of bounded rank matrices, see\nfor example [LKB22b].\nApplications of optimization on hyperbolic space in machine learning include\nhierarchical embeddings [NK17, JMM19, KMU +20].\nHere are a few other manifolds of interest for applications:\n\u2022 The Stiefel manifold with the canonical metric [EAS98];\n\u2022 The Grassmann manifold Gr(n, p) of subspaces of dimensionp in Rn. It can be\nviewed as a quotient manifold of St( n, p), or as an embedded submanifold\nof Rn\u00d7n where each subspace is identified with an orthogonal projector of\nrank p: see the discussion around eq. (9.90) in Section 9.16;\n\u2022 Matrices with positive entries (see Section 11.6);\n\u2022 Positive definite matrices (see Section 11.7);\n\u2022 Positive semidefinite matrices with a fixed rank [VAV09, JBAS10, MA20]\u2014see\nalso Example 9.57;\n\u2022 Multinomial manifolds; the simplex (Exercise 3.65); stochastic matrices [DH19];\n\u2022 The rigid motion group (special Euclidean group) SE(n): this is a manifold as\nthe product of the manifolds Rn and SO(n), providing a parameterization\nof all possible rigid motions in Rn as a combination of a translation and a\nrotation (to add reflections, use O( n) instead of SO( n));\n\u2022 The essential manifold for camera pose descriptions (epipolar constraint be-\ntween projected points in two perspective views) [TD14];\n\u2022 Shape space as the manifold of shapes in R2, R3, . . .(see [FCPJ04] or [MS20]\nand many references therein).\nAt times, the search space of an optimization problem in a linear space E\nis defined through two sets of equality constraints, h(x) = 0 and g(x) = 0, in\nsuch a way that {x : h(x) = 0} defines an embedded submanifold of E but the\nintersection {x : h(x) = 0 and g(x) = 0} does not. Then, it may be beneficial to\noptimize over the manifold{x : h(x) = 0} and to move the constraintg(x) = 0 to\nthe cost function as a penalty. This can be done in several ways, for example using", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3108, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "46619238-b8bd-4861-9c09-b1471ad4d84d": {"__data__": {"id_": "46619238-b8bd-4861-9c09-b1471ad4d84d", "embedding": null, "metadata": {"page_label": "181", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95b41ca9-87bf-4121-b8be-c939626b7ee6", "node_type": "4", "metadata": {"page_label": "181", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "edd4b435d529b7ecbfbf7274d4263b453f24c72b114890ee041692648499dbe8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n7.8 Notes and references 181\na quadratic penalty\u2014f(x)+ \u03bb\u2225g(x)\u22252\u2014or using a type of augmented Lagrangian\nmethod [LB20]. One can also attempt to handle inequality constraints in this\nfashion.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 447, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "32f44fc2-63af-491e-9a64-e66979d9e29d": {"__data__": {"id_": "32f44fc2-63af-491e-9a64-e66979d9e29d", "embedding": null, "metadata": {"page_label": "182", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "32ca113b-746c-403d-bee1-5c64f59a2049", "node_type": "4", "metadata": {"page_label": "182", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "e3a7d1e5db424ee605bcd7a585810edbbbbeaa7e1ab054713dc15dd71f845846", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n8 General manifolds\nIn this chapter, we consider the general definition of a (smooth) manifold. Follow-\ning Brickell and Clark [BC70], we initially give a (too) broad definition, devoid\nof topological considerations. To avoid confusion, we refer to these objects as\nmanifolds*, with a star. Promptly after that, in order to exclude topological cu-\nriosities that are of little interest to optimization, we restrict the definition and\ncall the remaining objects manifolds. This final definition is standard.\nOf course, embedded submanifolds of linear spaces\u2014as we have considered\nso far\u2014are manifolds: we shall verify this. Interestingly, the general perspective\nenables us to consider new manifolds. In particular, we touch upon the Grass-\nmann manifold which consists of all linear subspaces of a given dimension in\nsome linear space. Chapter 9 discusses such manifolds in more depth.\nWe then revisit our geometric toolbox to generalize smooth maps, tangent\nspaces, vector fields, retractions, Riemannian metrics, gradients, connections,\nHessians, etc. By design, Chapters 4 and 6 regarding optimization algorithms\napply verbatim to the general setting.\n8.1 A permissive definition\nGiven a set M (without any particular structure so far), the first step toward\ndefining a smooth manifold structure on M is to model M after Rd. To do so, we\nintroduce the concept of chart. A chart establishes a one-to-one correspondence\nbetween a subset of M and an open subset of Rd. This allows us to leverage the\npowerful tools we have at our disposal on Rd to work on M.\nAs the terms chart and (later) atlas suggest, it helps to think of M as the\nEarth (a sphere), of charts as two-dimensional, flat maps of parts of the Earth,\nand of atlases as collections of maps that cover the Earth.\nDefinition 8.1. A d-dimensional chart on a set M is a pair (U, \u03c6) consisting\nof a subset U of M (called the domain) and a map \u03c6: U \u2192Rd such that:\n1. \u03c6(U) is open in Rd, and\n2. \u03c6 is invertible between U and \u03c6(U).\nThe numbers (\u03c6(x)1, . . . , \u03c6(x)d) are the coordinates of the point x \u2208 Uin the\nchart \u03c6. The map \u03c6\u22121 : \u03c6(U) \u2192 Uis a local parameterization of M.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2390, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a6bda2d8-1e04-4b7d-a422-45f122c73917": {"__data__": {"id_": "a6bda2d8-1e04-4b7d-a422-45f122c73917", "embedding": null, "metadata": {"page_label": "183", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1cab42b5-27ed-4a89-843a-9d2a65abacc3", "node_type": "4", "metadata": {"page_label": "183", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "c28fc4f3bdbef3d6bf1a9a9e0dc3f989a0f489b60191468926975e2cf0b8a56d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n8.1 A permissive definition 183\nf\n\u03c6(U)\nU\n\u03c6\u22121\n\u02dcf = f \u25e6 \u03c6\u22121\nRM\n\u03c6(U) \u2286 Rd\nM R\n\u03c6\u22121\n\u02dcf\nf\nFigure 8.1 Illustration and matching commutative diagram for eq. (8.1) expressing a\nreal function f on a set M through a chart (U, \u03c6).\nWhen the domain is clear, we often call \u03c6 itself a chart. Given a point x in\nM, we say \u03c6 is a chart around x if x is in the domain of \u03c6.\nFor a function from (an open subset of) Rd to R, we readily have a notion of\nsmoothness: it is smooth at x if it is infinitely differentiable at x, in the usual\nsense. One of the goals of differential geometry is to generalize this notion to\nfunctions f : M \u2192 R on a more general class of sets M. Let ( U, \u03c6) be a d-\ndimensional chart around x \u2208 M. Then, as illustrated in Figure 8.1,\n\u02dcf = f \u25e6 \u03c6\u22121 : \u03c6(U) \u2192 R (8.1)\nis called a coordinate representative of f in this chart. Since \u03c6(U) is open in Rd,\nit makes sense to talk of differentiability of \u02dcf. In particular, we may want to\ndefine that, with respect to this chart, f is smooth at x if \u02dcf is smooth at \u03c6(x).\nTwo d-dimensional charts ( U, \u03c6) and ( V, \u03c8) on M around x are compatible\nif they yield the same conclusions regarding smoothness of functions at x. Re-\nstricted to the appropriate domains, the coordinate representatives\n\u02dcf = f \u25e6 \u03c6\u22121 and \u02c6f = f \u25e6 \u03c8\u22121\nare related by\n\u02dcf = \u02c6f \u25e6 (\u03c8 \u25e6 \u03c6\u22121) and \u02c6f = \u02dcf \u25e6 (\u03c6 \u25e6 \u03c8\u22121).\nThus, the differentiability properties of \u02dcf and \u02c6f are the same if the domains\ninvolved are open in Rd and if \u03c8 \u25e6 \u03c6\u22121 and its inverse are smooth. This is made\nprecise in the following definition illustrated by Figure 8.2. There, we could allow\nnon-overlapping charts to have different dimensions, but this serves little purpose\nin optimization. Accordingly, we require all charts to have the same dimension.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1996, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b99a3e6b-d4ac-4e38-bf03-1bd36b383c4c": {"__data__": {"id_": "b99a3e6b-d4ac-4e38-bf03-1bd36b383c4c", "embedding": null, "metadata": {"page_label": "184", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2e90e617-26c7-427d-ab7f-6840dbd88823", "node_type": "4", "metadata": {"page_label": "184", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "8f4c874dde3fde8c19369a4102dfc70ab6d082e7699baba6cd9858f0cf1c1843", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n184 General manifolds\nM\nRd Rd\nU\nV\n\u03c6(U)\n\u03c8(V)\n\u03c6\u22121 \u03c8\n\u03c8 \u25e6 \u03c6\u22121\nFigure 8.2 Overlapping charts ( U, \u03c6) and ( V, \u03c8) on a manifold of dimension d. The\ndarker area on the manifold corresponds to the intersection U \u2229Vof the chart domains.\nIn the coordinate spaces (bottom), the darker areas correspond to the open images\n\u03c6(U \u2229V) and \u03c8(U \u2229V): the coordinate change map \u03c8\u25e6\u03c6\u22121 is a diffeomorphism between\nthese two.\nDefinition 8.2. Two charts (U, \u03c6) and (V, \u03c8) of M are compatible if they have\nthe same dimension d and either U \u2229 V= \u2205, or U \u2229 V \u0338= \u2205 and:\n1. \u03c6(U \u2229 V) is open in Rd;\n2. \u03c8(U \u2229 V) is open in Rd; and\n3. \u03c8 \u25e6 \u03c6\u22121 : \u03c6(U \u2229 V) \u2192 \u03c8(U \u2229 V) is a smooth invertible function whose inverse\nis also smooth (i.e., it is a diffeomorphism, see Definition 3.11).\nA collection of charts is compatible if each pair of charts in that collection is\ncompatible. Compatible charts that cover the whole set M form an atlas.\nDefinition 8.3. An atlas A on a set M is a compatible collection of charts on\nM whose domains cover M. In particular, for every x \u2208 M, there is a chart\n(U, \u03c6) \u2208 Asuch that x \u2208 U.\nGiven an atlas A, it is an exercise to show that the collection A+ of all charts\nof M which are compatible with A is itself an atlas of M, called a maximal atlas.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1499, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6c0ca391-979b-41e0-b869-b945156897e0": {"__data__": {"id_": "6c0ca391-979b-41e0-b869-b945156897e0", "embedding": null, "metadata": {"page_label": "185", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e2d6d3ca-a1db-4c39-a81d-6bdb49cb8006", "node_type": "4", "metadata": {"page_label": "185", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "bb5c921902f6de688380281124814fe58ac23bafa56afdee181a34cf5b61c985", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n8.1 A permissive definition 185\nU \u2286 M V \u2286 M\u2032\n\u03c6(U) \u2286 Rd\n\u03c8(V) \u2286 Rd\u2032\u02dcF\n\u03c6\u22121\nF\n\u03c8\nFigure 8.3 Commutative diagram for Definition 8.5 expressing a map through charts.\nThus, any atlas uniquely defines a maximal atlas: we use the latter to define\nmanifolds* (the star is a reminder that topological concerns are delayed to a\nlater section.) We say that the maximal atlas defines a smooth structure on M.\nDefinition 8.4. A manifold* is a pair M = (M, A+), consisting of a set M\nand a maximal atlas A+ on M. The dimension of M is the dimension of any of\nits charts. When the atlas is clear from context, we often conflate notation for\nM and M.\nWe can now define smoothness of maps between manifolds*. Below, smooth-\nness of \u02dcF is understood in the usual sense for maps between open subsets of\nlinear spaces (see Section 3.1). See also Figure 8.3.\nDefinition 8.5. A map F : M \u2192 M\u2032 is smooth at x \u2208 Mif\n\u02dcF = \u03c8 \u25e6 F \u25e6 \u03c6\u22121 : \u03c6(U) \u2192 \u03c8(V)\nis smooth at \u03c6(x), where (U, \u03c6) is a chart of M around x and (V, \u03c8) is a chart\nof M\u2032 around F(x). The map F is smooth if it is smooth at every point x in\nM. We call \u02dcF a coordinate representative of F.\nRemark 8.6. By extension, we say a map F : M \u2192 M\u2032 is k times (contin-\nuously) differentiable if its coordinate representatives are so. Smoothness cor-\nresponds to k = \u221e. Later, we endow M with a Riemannian metric so that\nf : M \u2192R is (continuously) differentiable if and only if it has a (continuous)\nRiemannian gradient, and f is twice (continuously) differentiable if and only if\nit has a (continuous) Riemannian Hessian.\nIt is an exercise to verify that Definition 8.5 is independent of the choice of\ncharts, and that composition preserves smoothness.\nExample 8.7. Let E be a linear space of dimension d. We can equip E with a\nsmooth structure as follows: choose a basis for E; set U = E and let \u03c6(x) \u2208 Rd\ndenote the coordinates of x in the chosen basis; the maximal atlas generated by\n(U, \u03c6) yields the usual smooth structure on E. For example, if E = Rd, we can\nchoose \u03c6(x) = x. By default, we always use this smooth structure on Rd.\nExample 8.8. Let M be an open subset of a linear space E of dimension d.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2390, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "88c63a3f-078c-4c93-a2c2-37568d828d2b": {"__data__": {"id_": "88c63a3f-078c-4c93-a2c2-37568d828d2b", "embedding": null, "metadata": {"page_label": "186", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7db4c44f-e699-4b87-a87d-a71b1282bfd3", "node_type": "4", "metadata": {"page_label": "186", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "078b83bc0bb7c0a26942ee294b3d6f3737b7800e9f919f6ffb9e0cd2a62bb06f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n186 General manifolds\nWith the same chart as in the previous example, only restricted to U = M, it\nis clear that \u03c6(M) is open in Rd, so that (U, \u03c6) is a chart for M, and it covers\nall of M hence it defines an atlas on M. We conclude that any open subset of a\nlinear space is a manifold* with a natural atlas. By default, we always use this\nsmooth structure on open subsets of linear spaces.\nExample 8.9. The local parameterization\u03c6\u22121 : \u03c6(U) \u2192 Massociated to a chart\n(U, \u03c6) is a smooth map. Likewise, with the previous example in mind, the chart\n\u03c6: U \u2192Rd is a smooth map. Indeed, in both cases, we can arrange for their\ncoordinate representative to be the identity map.\nExample 8.10. Consider the unit circle, S1 = {x \u2208 R2 : x2\n1 + x2\n2 = 1}. One\npossible atlas is made of four charts, each defined on a half circle\u2014dubbed North,\nEast, South and West\u2014as follows:\nUN = {x \u2208 S1 : x2 > 0}, \u03c6 N (x) = x1,\nUE = {x \u2208 S1 : x1 > 0}, \u03c6 E(x) = x2,\nUS = {x \u2208 S1 : x2 < 0}, \u03c6 S(x) = x1,\nUW = {x \u2208 S1 : x1 < 0}, \u03c6 W (x) = x2.\nIt is clear that these are one-dimensional charts. For example, checking the North\nchart we find that \u03c6N : UN \u2192 \u03c6N (UN ) is invertible and \u03c6N (UN ) = ( \u22121, 1) is\nopen in R, as required. Furthermore, these charts are compatible. For example,\nchecking for the North and East charts, we find that:\n1. UN \u2229 UE = {x \u2208 S1 : x1 > 0 and x2 > 0};\n2. \u03c6N (UN \u2229 UE) = (0, 1) is open;\n3. \u03c6E(UN \u2229 UE) = (0, 1) is open; and\n4. \u03c6\u22121\nE (z) = (\n\u221a\n1 \u2212 z2, z), so that (\u03c6N \u25e6\u03c6\u22121\nE )(z) =\n\u221a\n1 \u2212 z2, which is smooth and\nsmoothly invertible on (0, 1).\nThe charts also cover the whole set S1, so that together they form an atlas A for\nS1. As a result, (S1, A+) is a manifold*.\nEarlier, using Definition 3.10, we called S1 an embedded submanifold of R2.\nIn Section 8.3, we argue more generally that embedded submanifolds of linear\nspaces (as per that early definition) are manifolds*.\nExample 8.11. We now discuss a new example: the (n \u2212 1)-dimensional real\nprojective space, RPn\u22121. This is the set of lines through the origin (that is, one-\ndimensional linear subspaces) of Rn. To any nonzero point x \u2208 Rn, we associate\na linear subspace as follows:\n\u03c0 : Rn\\{0} \u2192RPn\u22121 : x 7\u2192 \u03c0(x) = {\u03b1x : \u03b1 \u2208 R}.\nThe classical atlas for RPn\u22121 is built from the following charts. For a given i in\n{1, . . . , n}, consider the following subset of RPn\u22121:\nUi = {\u03c0(x) : x \u2208 Rn and xi \u0338= 0}.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2612, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "48ef7ccc-4fee-4c52-a9c5-5e35c1b1070a": {"__data__": {"id_": "48ef7ccc-4fee-4c52-a9c5-5e35c1b1070a", "embedding": null, "metadata": {"page_label": "187", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fe71d34e-096f-44f8-bd51-d16c6d9f942b", "node_type": "4", "metadata": {"page_label": "187", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "122b9eae2e74cf614df240e46b32791892d2c7c38a1d0ff6379db1ab2bf9767e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n8.1 A permissive definition 187\nThis is the set of lines through the origin that are not parallel to the plane Pi\ndefined by xi = 1. In other words, this is the set of lines through the origin that\nintersect that plane. This allows us to define the map \u03c6i on the domain Ui into\nRn\u22121, as the coordinates of the intersection of the line \u03c0(x) with the plane Pi:\n\u03c6i(\u03c0(x)) =\n\u0012x1\nxi\n, . . . ,xi\u22121\nxi\n, xi+1\nxi\n, . . . ,xn\nxi\n\u0013\n.\nThe map \u03c6i is indeed well defined because the right-hand side depends only on\n\u03c0(x) and not on x itself\u2014this is key. The range \u03c6i(Ui) is all of Rn\u22121 (since\nthere exists a line through the origin and any point of Pi), hence it is open.\nFurthermore, \u03c6i is invertible:\n\u03c6\u22121\ni (z1, . . . , zi\u22121, zi+1, . . . , zn) = \u03c0(z1, . . . , zi\u22121, 1, zi+1, . . . , zn).\nThus, {(Ui, \u03c6i)}i=1,...,n are charts for RPn\u22121. They cover RPn\u22121 since no line\ncan be parallel to all planes P1, . . . , Pn. Thus, it remains to verify that the charts\nare compatible. For all pairs i \u0338= j, consider the following:\n1. Ui \u2229 Uj = {\u03c0(x) : x \u2208 Rn, xi \u0338= 0 and xj \u0338= 0};\n2. \u03c6i(Ui \u2229Uj) and \u03c6j(Ui \u2229Uj) are both subsets of Rn\u22121 defined by one coordinate\nbeing nonzero: they are indeed open;\n3. Without loss of generality, consider i < j. Then,\n(\u03c6j \u25e6 \u03c6\u22121\ni )(z1, . . . , zi\u22121, zi+1, . . . , zn)\n=\n\u0012z1\nzj\n, . . . ,zi\u22121\nzj\n, 1\nzj\n, zi+1\nzj\n, . . . ,zj\u22121\nzj\n, zj+1\nzj\n, . . . ,zn\nzj\n\u0013\nis indeed smooth on the appropriate domain, and similarly for \u03c6i \u25e6 \u03c6\u22121\nj .\nAs a result, the charts form an atlas for RPn\u22121, turning it into a manifold*.\nIn Chapter 9, we discuss a generalization of this idea: the Grassmann manifold,\nwhich consists of all linear subspaces of a given dimension.\nIt is important to note that, in general, a set M may admit two (or more)\ndistinct atlases A and A\u2032 that are not compatible (their union is not an atlas),\nso that their corresponding maximal atlases are distinct. These two atlases then\nlead to different smooth structures on M, which shows that it is not sufficient\nto specify the set M: an atlas must also be specified\u2014see Exercise 8.14.\nExercise 8.12. Given an atlas A for a set M, show that the collection A+ of\nall charts of M which are compatible with A is a well-defined atlas of M.\nExercise 8.13. Show Definition 8.5 is independent of the choice of charts. Fur-\nthermore, show that if F : M \u2192 M\u2032 and G: M\u2032 \u2192 M\u2032\u2032 are smooth, then their\ncomposition G \u25e6 F is smooth. More broadly, establish the smoothness rules from\nExercises 3.37, 3.38, 3.39 and 3.40 for general manifolds. We study the claims\nabout differentials later in Exercise 8.40.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2808, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3746a184-0d81-42f3-aa0a-901acbfa9265": {"__data__": {"id_": "3746a184-0d81-42f3-aa0a-901acbfa9265", "embedding": null, "metadata": {"page_label": "188", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bcaeee74-de9b-48d3-b6a5-0c39dfd2b997", "node_type": "4", "metadata": {"page_label": "188", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "85e2fede133cda1ee04f38ffa94acf56e1db6aad5c1fffe34805e7ad30ee8832", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n188 General manifolds\nExercise 8.14. For the set M = R, consider the two following charts, both\ndefined on all of M: \u03c6(x) = x and \u03c8(x) = 3\u221ax. Verify that these are indeed\ncharts, and that they are not compatible. Let A+ be the maximal atlas generated\nby \u03c6 and let M = (M, A+) denote R with the resulting smooth structure (this\nis the usual structure on R). Likewise, let B+ be the maximal atlas generated by\n\u03c8 and write M\u2032 = (M, B+). Give an example of a function f : R \u2192 R which is\nnot smooth as a function from M to R yet which is smooth as a function from\nM\u2032 to R. What about the other way around?\n8.2 The atlas topology, and a final definition\nIn the above section, we have equipped a set M with a smooth structure. This\naffords us the notion of smooth functions between properly endowed sets. As we\nnow show, this structure further induces a topology on M, that is, a notion of\nopen sets, called the atlas topology. In turn, having a topology on M is useful in\noptimization to define concepts such as local optima and convergence.\nWe start with a few reminders. After discussing two desirable properties of\ntopologies, we restrict the definition of manifold to those whose atlas topology\nenjoy those properties.\nThe usual notion of open sets in Rd can be abstracted to arbitrary sets as\ntopologies. Essentially, in defining a topology, we declare certain subsets to be\nopen, while making sure that certain basic properties hold, as specified below.\nDefinition 8.15. A topology on a set M is a collection T of subsets of M with\nthe following properties. A subset of M is called open if and only if it is in T ,\nand:\n1. M and \u2205 are open;\n2. The union of any collection of open sets is open; and\n3. The intersection of any finite collection of open sets is open.\nA subset C of M is called closed if it is the complement of an open set in M,\nthat is, M\\C is open. In particular, M and \u2205 are both open and closed. Some\nsubsets of M may be neither open nor closed.\nA topological space is a pair (M, T ) consisting of a set with a topology. Given\ntwo topological spaces (M, T ), (M\u2032, T \u2032) and a map F : M \u2192 M\u2032, we define that\nF is continuous if for every open set O\u2032 in M\u2032 the pre-image\nF\u22121(O\u2032) = {x \u2208 M : F(x) \u2208 O\u2032}\nis open in M.\nIn defining a topology on a manifold* M = (M, A+), it is natural to require\nthat the chart functions be continuous in that topology. In particular, since for\nany chart (U, \u03c6) of M we have that \u03c6(U) is open in Rd (assuming dim M = d),\nwe should require that \u03c6\u22121(\u03c6(U)) = U be open, that is, chart domains should be", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2799, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "78e4731a-0e0c-445e-89d3-34e831db10a9": {"__data__": {"id_": "78e4731a-0e0c-445e-89d3-34e831db10a9", "embedding": null, "metadata": {"page_label": "189", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ee6f8217-0f1b-4a54-b1ce-25d66344683d", "node_type": "4", "metadata": {"page_label": "189", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "47aab9bbe3fd0df98fcdc4c8ace4e59ecd31d7f88e4df105a9cee78b6c1033a1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n8.2 The atlas topology, and a final definition 189\ndeemed open. It is easy to check with the following definitions that this collection\nof sets forms a basis for a topology consisting in the collection of all unions of\nchart domains [BC70, Prop. 2.4.2].\nDefinition 8.16. A collection B of subsets of a set M is a basis for a topology\non M if\n1. For each x \u2208 M, there is a set B \u2208 Bsuch that x \u2208 B; and\n2. If x \u2208 B1 \u2229 B2 for B1, B2 \u2208 B, there exists B3 \u2208 Bsuch that x \u2208 B3 and\nB3 \u2286 B1 \u2229 B2.\nThe topology T defined by B is the collection of all unions of elements of B.\nIn the following definition, it is important to consider the maximal atlas as\notherwise we may miss some open sets.\nDefinition 8.17. Given a maximal atlas A+ on a set M, the atlas topology on\nM states that a subset of M is open if and only if it is the union of a collection\nof chart domains.\nA subset S of a topological space T inherits a topology called the subspace\ntopology: it consists in the collection of all open sets of T intersected with S.\nBy default, when we consider a subset of a topological space, we tacitly equip it\nwith the subspace topology. With this in mind, we get the following convenient\nfact, true by design [BC70, Prop. 2.4.3].\nProposition 8.18. In the atlas topology, any chart \u03c6: U \u2192\u03c6(U) is continuous\nand its inverse is also continuous (i.e., it is a homeomorphism).\nA welcome consequence of the latter proposition is that, with the atlas topolo-\ngies on manifolds* M and M\u2032, any function F : M \u2192 M\u2032 which is smooth in\nthe sense of Definition 8.5 is also continuous in the topological sense [BC70,\nProp. 2.4.4].\nOne of the reasons we need to discuss topologies in some detail is that, in\ngeneral, atlas topologies may lack certain desirable properties: we must require\nthem explicitly. The first such property is called Hausdorff (or T2).\nDefinition 8.19. A topology on a set M is Hausdorff if all pairs of distinct\npoints have disjoint neighborhoods, that is, for all x, x\u2032 distinct in M there exist\nopen sets O and O\u2032 such that x \u2208 O, x\u2032 \u2208 O\u2032 and O \u2229 O\u2032 = \u2205.\nRecall that a sequence x0, x1, . . .on a topological space is said to converge to x\nif, for every neighborhood U of x, there exists an index k such that xk, xk+1, . . .\nare all in U: we then say that the sequence is convergent and that x is its limit.\nCrucially for optimization, in a Hausdorff topology, any convergent sequence of\npoints has a unique limit [Lee12, p600]. This may not be the case otherwise\n(consider for example the trivial topology, in which the only open sets are the\nempty set and the set itself.)\nThe second desirable property is called second-countable.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2892, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "18bafec4-2c17-4c4a-873b-0b4ad3a63590": {"__data__": {"id_": "18bafec4-2c17-4c4a-873b-0b4ad3a63590", "embedding": null, "metadata": {"page_label": "190", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8845a7ca-e9f0-4e95-87c5-f97e8f16352c", "node_type": "4", "metadata": {"page_label": "190", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "4cd0f15a684c658f1363fb44cd0d67a0da73cb807d6f274dceac184b56bb2d83", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n190 General manifolds\nDefinition 8.20. A topology is second-countable if there is a countable basis\nfor its topology.\nAt last, we can give a proper definition of manifolds.\nDefinition 8.21. A manifold is a pair M = ( M, A+) consisting of a set M\nand a maximal atlas A+ on M such that the atlas topology is Hausdorff and\nsecond-countable.\nA manifold* is indeed not always a manifold: the atlas topology is not always\nHausdorff (see Examples 3.2.1\u20133 in [BC70]), and it may also not be second-\ncountable (see Example 3.3.2 in the same reference). The following proposition\ngives a convenient way of ensuring a (not necessarily maximal) atlas induces a\nsuitable topology [Lee12, Lem. 1.35].\nProposition 8.22. Let A be an atlas for the set M. Assume both:\n1. For all x, y\u2208 M distinct, either both x and y are in the domain of some\nchart, or there exist two disjoint chart domains U and V such that x \u2208 Uand\ny \u2208 V; and\n2. Countably many of the chart domains suffice to cover M.\nThen, the atlas topology of A+ is Hausdorff (by property 1) and second-countable\n(by property 2), so that M = (M, A+) is a manifold.\nThe following proposition provides yet another way of assessing the atlas topol-\nogy [BC70, Prop. 3.1.1]. We use it in Section 8.3. The \u201conly if\u201d direction is a\ndirect consequence of Proposition 8.18.\nProposition 8.23. Let the set M be equipped with both a maximal atlas A+\nand a topology T . The atlas topology on M coincides with T if and only if the\ncharts of one atlas of M in A+ are homeomorphisms with respect to T .\nOpen subsets of manifolds are manifolds in a natural way by restriction of\nthe chart domains, called open submanifolds. Unless otherwise specified, when\nworking with an open subset of a manifold (often, a chart domain), we implicitly\nmean to use the open submanifold geometry. See also Section 8.14 for further\nfacts about open submanifolds.\nDefinition 8.24. Let M be a manifold and let V be open in M in the atlas\ntopology. For any chart (U, \u03c6) of M such that U\u2229V \u0338= \u2205, build the chart (U\u2229V , \u03c6)\non V. The collection of these charts forms an atlas for V, turning it into a\nmanifold in its own right. Equipped with this atlas, we call V an open submanifold\nof M.\nExample 8.25. In all examples from Section 8.1, we have constructed atlases\nwith a finite number of charts. Hence, by Proposition 8.22, their atlas topologies\nare second-countable. Furthermore, for linear spaces and open subsets of linear\nspaces, we have used only one chart, so that the same proposition guarantees the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2768, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "42bcfd59-5be0-41e4-b115-4756ececde8d": {"__data__": {"id_": "42bcfd59-5be0-41e4-b115-4756ececde8d", "embedding": null, "metadata": {"page_label": "191", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5722fd4b-957c-4761-9d5d-b6796708cdb4", "node_type": "4", "metadata": {"page_label": "191", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "d7552e151e6130bb5514cba0c31a37f760c19ef8324f2c4212815410eb0561ec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n8.2 The atlas topology, and a final definition 191\nresulting topologies are Hausdorff. We conclude that linear spaces and their open\nsubsets are manifolds.\nPart of the motivation for the topological restrictions introduced in this section\nis that a manifold* carries partitions of unity if (and essentially only if) the\ntopology is as prescribed\u2014see [BC70, \u00a7 3.4]. Partitions of unity are useful in\nparticular to show existence of Riemannian metrics (see Section 8.9). In short:\nevery manifold can be turned into a Riemannian manifold [Lee12, Prop. 13.3].\nWe close this section with the definition of compact manifolds, for which we\nfirst recall a few topological notions. (See also Theorem 10.8.)\nDefinition 8.26. Let M = (M, T ) be a topological space (for example, a man-\nifold with its atlas topology). An open cover of a subset S of M is a collection\nof open sets of M whose union contains S. We say S is compact if, for each\nopen cover of S, one can select a finite number of open sets from that open cover\nwhose union still contains S (called a finite subcover). The space M itself is\ncompact if S = M is compact.\nDefinition 8.27. A compact manifold is a manifold which is compact as a\ntopological space with its atlas topology.\nExample 8.28. This example anticipates concepts from Section 8.3. An embed-\nded submanifold M of a Euclidean space E is a compact manifold if and only if\nM is a compact subset of E, that is, M is closed and bounded as a subset of E.\nThis is because M inherits its topology from E. In particular, the unit sphere,\nthe Stiefel manifold, the orthogonal group and the special orthogonal group as\ndiscussed in Chapter 7 all are compact manifolds.\nExercise 8.29. To show that the circle S1 and the real projective space RPn\u22121\nare manifolds, it remains to verify that their atlases (as constructed in Sec-\ntion 8.1) induce Hausdorff topologies. Do this using Proposition 8.22. You may\nneed to add a few charts to the atlases.\nExercise 8.30. Check that Definition 8.24 is legitimate, that is, show that the\nproposed charts are indeed charts, that they form an atlas, and that the atlas\ntopology is Hausdorff and second-countable.\nExercise 8.31. Let M and N be two manifolds. For any pair of charts (U, \u03c6)\nand (V, \u03c8) of M and N, respectively, consider the map \u03d5 defined on U \u00d7 V\nby \u03d5(x, y) = ( \u03c6(x), \u03c8(y)). Show that these maps define a smooth structure on\nthe product space M \u00d7 N, called the product manifold structure . Deduce that\ndim(M \u00d7 N) = dim M + dimN, and that open subsets of the product manifold\nare unions of products of open subsets of M and N.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2842, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "83a644a9-f0ab-4173-a4b4-2b9bd51603d9": {"__data__": {"id_": "83a644a9-f0ab-4173-a4b4-2b9bd51603d9", "embedding": null, "metadata": {"page_label": "192", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8d687fc6-126f-49fa-bc37-2069d9cf8125", "node_type": "4", "metadata": {"page_label": "192", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "b1d0e296043fc9b010bd2d634a16f0f5ebb35a767088259359f0252b0db4b646", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n192 General manifolds\n8.3 Embedded submanifolds are manifolds\nAll the way back in Chapter 3, we defined embedded submanifolds of linear\nspaces with Definition 3.10. In this section, we show that all sets we have thus\nfar called embedded submanifolds are indeed manifolds. To do so, we equip them\nwith an atlas, and we confirm that the corresponding atlas topology coincides\nwith the topology we have been using so far. In Section 8.14, we shall also see\nthat our early notion of smooth maps between embedded submanifolds of linear\nspaces agrees with the more general notion of smooth maps between manifolds.\nProposition 8.32. A subset M of a linear space E which is an embedded sub-\nmanifold as per Definition 3.10 admits an atlas which makes it a manifold in\nthe sense of Definition 8.21. The corresponding atlas topology coincides with the\nsubspace topology as given in Definition 3.21.\nProof. Let d = dim E and n = dim M = d \u2212 k. The claim has two parts.\nPart 1.\nWe construct an atlas for M to make it a manifold*. Let x \u2208 Mbe arbitrary.\nBy Theorem 3.12, there exists a neighborhood U of x in E, an open set W in\nRd and a diffeomorphism F : U \u2192 W such that F(M \u2229U) = E \u2229 W where\nE = {y \u2208 Rd : yn+1 = \u00b7 \u00b7\u00b7= yd = 0} is a linear subspace of Rd. We use F to\npropose a tentative chart ( U, \u03c6) for M around x. Let\nU = M \u2229U and \u03c6: U \u2192\u03c6(U): y 7\u2192 \u03c6(y) = trim(F(y)), (8.2)\nwhere trim: Rd \u2192 Rn discards the last k components of a vector. This map is\ninvertible since the k entries removed by trim are identically zero on U, so that\n\u03c6\u22121(z) = F\u22121(zpad(z)), (8.3)\nwhere zpad: Rn \u2192 Rd pads a vector with k zeros at the end. The composition\ntrim \u25e6zpad is identity on Rn while zpad \u25e6trim is identity on E. Notice that W\nis open in Rd and\n\u03c6(U) = trim(F(M \u2229U)) = trim(E \u2229 W).\nOne can then verify that \u03c6(U) is open in Rn using standard properties of the\ntopologies on Rn and Rd. Thus, (U, \u03c6) is an n-dimensional chart for M around\nx. Such a chart can be constructed around every point x \u2208 M, so that we cover\nthe whole set. The last step is to verify that the charts are compatible. To this\nend, consider two charts as above, ( U, \u03c6) and (V, \u03c8), with overlapping domains\nand associated diffeomorphisms F : U \u2192 F(U) \u2286 Rd and G: V \u2192 G(V ) \u2286 Rd.\nThen, the change of coordinates map is\n\u03c8 \u25e6 \u03c6\u22121 = trim \u25e6G \u25e6 F\u22121 \u25e6 zpad,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2554, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f58823a9-2594-44d0-aa5d-0032ad9ce8fc": {"__data__": {"id_": "f58823a9-2594-44d0-aa5d-0032ad9ce8fc", "embedding": null, "metadata": {"page_label": "193", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "97f812bd-d27a-46d1-9106-dda1f768a5a9", "node_type": "4", "metadata": {"page_label": "193", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "deafd50fc380a4770f183160060fcf909ae997dba09e9bc6add32e4c42bde2fe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n8.3 Embedded submanifolds are manifolds 193\nfrom \u03c6(U \u2229 V) to \u03c8(U \u2229 V). These domains are open because U \u2229 V is open,\nhence so are F(U \u2229 V ) and G(U \u2229 V ) and we have\n\u03c6(U \u2229 V) = trim(E \u2229 F(U \u2229 V )) , and\n\u03c8(U \u2229 V) = trim(E \u2229 G(U \u2229 V )) .\nTo check the first identity, verify that F(U \u2229V) = E \u2229F(U \u2229V ) as follows, then\ncompose with trim:\n\u2022 F(U \u2229 V) \u2286 E \u2229 F(U \u2229 V ) as F(U \u2229 V) = F(M \u2229U \u2229 V ) \u2286 F(U \u2229 V ) and\nF(U \u2229 V) \u2286 F(U) = E \u2229 F(U) \u2286 E, and\n\u2022 E \u2229F(U \u2229V ) \u2286 F(U \u2229V) since for all y \u2208 E \u2229F(U \u2229V ) there exists x \u2208 U \u2229V\nsuch that F(x) = y, hence x is in U and F(x) is in E, which implies that\nx is in U = M\u2229 U; since x is also in V we deduce that x is in U \u2229V, hence\ny is in F(U \u2229 V).\nOverall, we find that the change of coordinates map \u03c8 \u25e6 \u03c6\u22121 is smooth (by\ncomposition) and its inverse \u03c6 \u25e6 \u03c8\u22121 = trim \u25e6F \u25e6 G\u22121 \u25e6 zpad is also smooth,\nso that the charts are compatible. This finishes the construction of our atlas,\nturning M into a manifold*.\nPart 2.\nThat the atlas and subspace topologies coincide follows from Proposition 8.23.\nIndeed, we only need to show that the charts constructed above are homeomor-\nphisms with respect to the subspace topology on M. By definition, U = M \u2229U\nis open in that topology. Furthermore, \u03c6(U) is open in Rn as we argued above.\nSince the map \u03c6: U \u2192\u03c6(U) is invertible, it remains to argue that it and its\ninverse are continuous in the subspace topology. That \u03c6 is continuous is clear\nsince it is the restriction of the continuous map trim \u25e6F from U to U. That \u03c6\u22121\nis continuous is also clear since it is equal to the continuous map F\u22121 \u25e6 zpad,\nonly with the codomain restricted to U.\nThe topology on E is Hausdorff and second-countable, and it is easy to see\nthat the subspace topology inherits these properties. Thus, we conclude that M\nequipped with the above atlas is a manifold.\nAdditionally, the constructed atlas yields the unique smooth structure on M\nfor which (a) the atlas topology coincides with the subspace topology and (b)\nthe notion of smooth functions on M is the same as judged through charts or\nas judged by the existence of smooth extensions\u2014see Section 8.14. This is why, \u22c6\neven though in general it does not make sense to say that a set is or is not a\nmanifold, it does make sense to say that a subset of a linear space is or is not an\nembedded submanifold of that linear space.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2585, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6bc3bf95-7e63-4fe7-a184-296a59bd4b0b": {"__data__": {"id_": "6bc3bf95-7e63-4fe7-a184-296a59bd4b0b", "embedding": null, "metadata": {"page_label": "194", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "daa48086-e858-46db-9e9c-2b1318fb72ea", "node_type": "4", "metadata": {"page_label": "194", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "c4925cc51a6ec9bc68f5bfbd303466088c3da1251f5db84ebf0396cb01ccdf27", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n194 General manifolds\n8.4 Tangent vectors and tangent spaces\nIn defining tangent vectors to a manifold in Section 3.2, we relied heavily on\nthe linear embedding space. In the general setting however, we do not have this\nluxury. We must turn to a more general, intrinsic definition. Here, we present\none general definition of tangent vectors on manifolds as equivalence classes of\ncurves. Another (equivalent) definition is through the notion of derivation (at a\npoint): we do not discuss it.\nLet x be a point on a d-dimensional manifold M. Consider the set Cx of\nsmooth curves on M passing through x at t = 0:\nCx = {c | c: I \u2192 Mis smooth and c(0) = x}.\nSmoothness of c on an open interval I \u2286 R around 0 is to be understood through\nDefinition 8.5.\nWe define an equivalence relation on Cx denoted by \u223c. Let (U, \u03c6) be a chart\nof M around x and consider c1, c2 \u2208 Cx. Then, c1 \u223c c2 if and only if \u03c6 \u25e6 c1 and\n\u03c6 \u25e6 c2 have the same derivative at t = 0, that is,\nc1 \u223c c2 \u21d0 \u21d2 (\u03c6 \u25e6 c1)\u2032(0) = (\u03c6 \u25e6 c2)\u2032(0). (8.4)\nThese derivatives are well defined as \u03c6\u25e6ci is a smooth function (by composition)\nfrom some open interval around 0 to an open subset of Rd. It is an exercise to\nprove that this equivalence relation is independent of the choice of chart.\nThe equivalence relation partitions Cx into equivalence classes: we call them\ntangent vectors. The rationale is that all the curves in a same equivalence class\n(and only those) pass through x with the same \u201cvelocity,\u201d as judged by their\nvelocities through \u03c6(x) in coordinates.\nDefinition 8.33. The equivalence class of a curve c \u2208 Cx is the set of curves\nthat are equivalent to c as per (8.4):\n[c] = {\u02c6c \u2208 Cx : c \u223c \u02c6c}.\nEach equivalence class is called a tangent vector to M at x. The tangent space\nto M at x, denoted by TxM, is the quotient set\nTxM = Cx/\u223c = {[c] : c \u2208 Cx},\nthat is, the set of all equivalence classes.\nGiven a chart (U, \u03c6) around x, the map\n\u03b8\u03c6\nx : TxM \u2192Rd : [c] 7\u2192 \u03b8\u03c6\nx ([c]) = (\u03c6 \u25e6 c)\u2032(0) (8.5)\nis well defined by construction: the expression ( \u03c6 \u25e6 c)\u2032(0) does not depend on\nthe choice of representative c in [c]. It is an exercise to show that \u03b8\u03c6\nx is bijective.\nThis bijection naturally induces a linear space structure over T xM, by copying", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2458, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7139a2b5-8f27-4c9e-82a8-29f2537eea5e": {"__data__": {"id_": "7139a2b5-8f27-4c9e-82a8-29f2537eea5e", "embedding": null, "metadata": {"page_label": "195", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c4d7bfbe-4e35-4d2c-9bf2-3153f0973629", "node_type": "4", "metadata": {"page_label": "195", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "83717f2e211af9e66b96471e28e8a5a84592e1492ec36f62b7499f66a8c5184f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n8.4 Tangent vectors and tangent spaces 195\nthe linear structure of Rd:\na \u00b7 [c1] + b \u00b7 [c2] \u225c (\u03b8\u03c6\nx )\u22121\u0000\na \u00b7 \u03b8\u03c6\nx ([c1]) + b \u00b7 \u03b8\u03c6\nx ([c2])\n\u0001\n. (8.6)\nThis structure, again, is independent of the choice of chart. Thus, the tangent\nspace is a linear space in its own right.\nTheorem 8.34. Tangent spaces are linear spaces of dimension dim M with the\nlinear structure given through (8.6).\nWhen M is an embedded submanifold of a linear space, the two definitions\nof tangent spaces we have seen are compatible in the sense that they yield the\nsame vector space structure, so that we always use the simpler one. In particular,\nthe tangent spaces of (an open subset of) a linear space E (for example, Rd) are\nidentified with E itself.\nTheorem 8.35. For M embedded in a linear space E, there exists a linear space\nisomorphism (that is, an invertible linear map) showing that Definitions 3.14\nand 8.33 are compatible.\nProof. Pick x \u2208 M. Let ( U, \u03c6) be a chart around x as built in (8.2) from a\ndiffeomorphism F so that \u03c6 = trim \u25e6F|U and \u03c6\u22121 = F\u22121 \u25e6 zpad |\u03c6(U). Pick\nan arbitrary smooth curve c on M satisfying c(0) = x. This is also a curve\nin E. Let v = c\u2032(0) \u2208 E. Passing to coordinates, define \u02dc c(t) = \u03c6(c(t)). Write\nc = \u03c6\u22121 \u25e6 \u02dcc = F\u22121 \u25e6 zpad \u25e6\u02dcc to see that F \u25e6 c = zpad \u25e6\u02dcc. Thus,\nF(x) = F(c(0)) = zpad(\u02dcc(0)), and\nDF(x)[v] = (F \u25e6 c)\u2032(0) = zpad(\u02dcc\u2032(0)).\nMoreover, \u03b8\u03c6\nx ([c]) = (\u03c6 \u25e6 c)\u2032(0) = \u02dcc\u2032(0). Therefore, with v = c\u2032(0),\nv = DF(x)\u22121[zpad(\u03b8\u03c6\nx ([c]))]. (8.7)\nThis is a linear map converting the tangent vector [ c] in the sense of Defini-\ntion 8.33 to the tangent vector v in the sense of Definition 3.14. This map is\none-to-one, with inverse given by:\n[c] = (\u03b8\u03c6\nx )\u22121(trim(DF(x)[v])). (8.8)\nThus, the two definitions of tangent spaces are compatible.\nExercise 8.36. Show that the equivalence relation (8.4) is independent of the\nchoice of chart (U, \u03c6) around x. Show that \u03b8\u03c6\nx (8.5) is bijective. Show that the\nlinear structure on TxM defined by (8.6) is independent of the choice of chart,\nso that it makes sense to talk of linear combinations of tangent vectors without\nspecifying a chart.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2348, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "72ee9c1e-bc51-48b6-b09b-491ce95a2c1d": {"__data__": {"id_": "72ee9c1e-bc51-48b6-b09b-491ce95a2c1d", "embedding": null, "metadata": {"page_label": "196", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7c9955c0-b712-4e08-8630-56445c97aa45", "node_type": "4", "metadata": {"page_label": "196", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "0310f58f08bb50fe004344f589d03eb920d58e77b542ce88600e9225fc53636c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n196 General manifolds\n8.5 Differentials of smooth maps\nBy design, the notion of tangent vector induces a notion of directional deriva-\ntives. Let F : M \u2192 M\u2032 be a smooth map. For any tangent vectorv \u2208 TxM, pick\na representative curve c (formally, c \u2208 v) and consider the map t 7\u2192 F(c(t)): this\nis a smooth curve on M\u2032 passing through F(x) at t = 0. The equivalence class\nof that curve is a tangent vector to M\u2032 at F(x). The equivalence relation (8.4)\nis specifically crafted so that this map between tangent spaces does not depend\non the choice of c in v. This yields a notion of differential for maps between\nmanifolds. In equation (8.9) below, brackets on the right-hand side select an\nequivalence class of curves, whereas brackets on the left-hand side merely distin-\nguish between x (the point at which we differentiate) and v (the direction along\nwhich we differentiate) as per usual.\nDefinition 8.37. Given manifolds M and M\u2032, the differential of a smooth map\nF : M \u2192 M\u2032 at x is a linear map DF(x): T xM \u2192TF(x)M\u2032 defined by:\nDF(x)[v] = [t 7\u2192 F(c(t))], (8.9)\nwhere c is a smooth curve on M passing through x at t = 0 such that v = [c].\nWhen the codomain of F is (an embedded submanifold of) a linear space,\nTheorem 8.35 provides an identification of the abstract tangent spaces of that\ncodomain with the concrete tangent spaces from Chapter 3. In this way, we can\nconfirm that Definitions 8.37 and 3.34 are compatible.\nProposition 8.38. For a smooth map F : M \u2192 Nwhere N is an embedded\nsubmanifold of a linear space E, we identify the tangent spaces of N to subspaces\nof E as provided by Theorem 8.35. Then, with v = [c] a tangent vector at x \u2208 M,\nwe can write\nDF(x)[v] = (F \u25e6 c)\u2032(0), (8.10)\nwhere F \u25e6 c is seen as a map into E.\nIn particular, let F(M) denote the set of smooth scalar fields on M, that is,\nthe set of smooth functions f : M \u2192R. Then, identifying the tangent spaces of\nR with R itself, we write\nDf(x)[v] = (f \u25e6 c)\u2032(0) (8.11)\nfor the differential Df(x): T xM \u2192R, where v = [c].\nProof. This is essentially a tautology. Let us write the proof for a map G: M \u2192\nN so we can use F to denote the diffeomorphism appearing in the conversion\nformula (8.7) for a chart \u03c6 of N around G(x). On the one hand, since G \u25e6 c is a\ncurve on N passing through G(x), formula (8.7) provides\n(G \u25e6 c)\u2032(0) = DF(G(x))\u22121[zpad(\u03b8\u03c6\nG(x)([G \u25e6 c]))]. (8.12)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2602, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6894e621-273c-4c5f-aaf0-4dd7eef21385": {"__data__": {"id_": "6894e621-273c-4c5f-aaf0-4dd7eef21385", "embedding": null, "metadata": {"page_label": "197", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a1dfc68b-6bd4-4957-83c0-f6f4a6cfe3c3", "node_type": "4", "metadata": {"page_label": "197", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "8db0fce9f41fde385f986145f34918530214240f52125b03578e6e8f7750e44e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n8.6 Tangent bundles and vector fields 197\nOn the other hand, Definition 8.37 states that D G(x)[v] = [ G \u25e6 c], and the\nconcrete representation of [G \u25e6 c] is obtained through (8.7) as:\nDF(G(x))\u22121[zpad(\u03b8\u03c6\nG(x)([G \u25e6 c]))].\nThus, the concrete representation of D G(x)[v] is (G \u25e6 c)\u2032(0).\nExercise 8.39. Verify that equation (8.9) is well defined, that is, the right-hand\nside does not depend on the choice of c representing v. Additionally, show that\nDF(x) is indeed a linear map with respect to the linear structure (8.6) on tangent\nspaces.\nExercise 8.40. (Continued from Exercise 8.13.) For smooth mapsF1, F2 : M \u2192\nE (with E a linear space) and real numbers a1, a2, show that F : x 7\u2192 a1F1(x) +\na2F2(x) is smooth and we have linearity:\nDF(x) = a1DF1(x) + a2DF2(x).\nFor smooth maps f : M \u2192R and G: M \u2192 E, show that the product map\nfG : x 7\u2192 f(x)G(x) is smooth from M to E and we have a product rule:\nD(fG)(x)[v] = G(x)Df(x)[v] + f(x)DG(x)[v].\nLet F : M \u2192 M\u2032 and G: M\u2032 \u2192 M\u2032\u2032 be smooth. Establish the chain rule for the\ndifferential of their composition:\nD(G \u25e6 F)(x)[v] = DG(F(x))[DF(x)[v]].\nGeneralize the claim of Exercise 3.40 too.\n8.6 Tangent bundles and vector fields\nIdentically to Definition 3.42, we define the tangent bundle as the disjoint union\nof all tangent spaces, now provided by Definition 8.33.\nDefinition 8.41. The tangent bundle of a manifold M is the set:\nTM = {(x, v) : x \u2208 Mand v \u2208 TxM}.\nWe often conflate notation for (x, v) and v when the context is clear.\nDefinition 8.42. The projection \u03c0 : TM \u2192 Mextracts the base of a vector,\nthat is, \u03c0(x, v) = x. At times, we may write \u03c0(v) = x.\nJust like tangent bundles of embedded submanifolds are themselves embedded\nsubmanifolds (Theorem 3.43), tangent bundles of manifolds are manifolds in a\nnatural way. (Smoothness of \u03c0 is understood through Definition 8.5.)\nTheorem 8.43. For any manifold M of dimension d, the tangent bundle TM is\nitself a manifold of dimension 2d, in such a way that the projection \u03c0 : TM \u2192 M\nis smooth.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2246, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8ae06f2b-498f-47df-8854-b9357df4a29c": {"__data__": {"id_": "8ae06f2b-498f-47df-8854-b9357df4a29c", "embedding": null, "metadata": {"page_label": "198", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "75c67807-ec76-42e0-8ed1-54faea0f7543", "node_type": "4", "metadata": {"page_label": "198", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "185ca379f52f29eb054b4c5a22c69b90cc2040ef25a59a2ecebcaa718e939d95", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n198 General manifolds\nProof. From any chart (U, \u03c6) of M, we construct a chart (\u02dcU, \u02dc\u03c6) of TM as follows.\nDefine the domain \u02dcU = \u03c0\u22121(U) to be the set of all tangent vectors to any point\nin U. Then, define \u02dc\u03c6: \u02dcU \u2192\u02dc\u03c6( \u02dcU) \u2286 R2d as\n\u02dc\u03c6(x, v) = (\u03c6(x), \u03b8\u03c6\nx (v)), (8.13)\nwhere \u03b8\u03c6\nx is defined by (8.5). See [Lee12, Prop. 3.18] for details.\nThe smooth structure on a tangent bundle is such that the differential of a\nsmooth map is itself a smooth map.\nProposition 8.44. Consider a smooth map F : M \u2192 M\u2032 and its differential\nDF : TM \u2192TM\u2032 defined by DF(x, v) = D F(x)[v]. With the natural smooth\nstructures on TM and TM\u2032, the map DF is smooth.\nProof. Write DF in coordinates using charts from Theorem 8.43, then use Propo-\nsition 8.51 below. Details in [Lee12, Prop. 3.21].\nThe manifold structure on TM makes it possible to define smooth vector fields\non manifolds as smooth maps from M to TM.\nDefinition 8.45. A vector field V is a map from M to TM such that \u03c0 \u25e6 V is\nthe identity map. The vector at x is written V (x) and lies in TxM. If V is also\na smooth map, then it is a smooth vector field . The set of smooth vector fields\non M is denoted by X(M).\nIn Section 8.8, we use the following characterization of smooth vector fields to\nconstruct coordinate vector fields.\nProposition 8.46. A vector field V on M is smooth if and only if, for every\nchart (U, \u03c6) of M, the map x 7\u2192 \u03b8\u03c6\nx (V (x)) is smooth on U.\nProof. Using Definition 8.5 about smooth maps and the charts of T M defined\nby (8.13), we conclude that V is smooth if and only if, for every chart ( U, \u03c6) of\nM,\n\u02dcV = \u02dc\u03c6 \u25e6 V \u25e6 \u03c6\u22121 : \u03c6(U) \u2286 Rd \u2192 R2d\nis smooth, where \u02dc\u03c6(x, v) = (\u03c6(x), \u03b8\u03c6\nx (v)). For z = \u03c6(x), we have\n\u02dcV (z) = (z, \u03b8\u03c6\nx (V (x)))\nso that \u02dcV is smooth if and only if x 7\u2192 \u03b8\u03c6\nx (V (x)) is smooth on U.\nLet V be a vector field on M. As we did in Definition 5.5, we define the\naction of V on a smooth function f \u2208 F(U) with U open in M as the function\nV f: U \u2192R determined by\n(V f)(x) = Df(x)[V (x)]. (8.14)\nBased on the latter, we mention a characterization of smooth vector fields which\nis sometimes useful. The proof in the direction we need is an exercise in Sec-\ntion 8.8. See [Lee12, Prop. 8.14] for the other one.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2438, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b243197a-f233-49c7-8cfb-d1e641eda111": {"__data__": {"id_": "b243197a-f233-49c7-8cfb-d1e641eda111", "embedding": null, "metadata": {"page_label": "199", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "78afd476-d315-42fe-9d5d-8d6acd3df7cb", "node_type": "4", "metadata": {"page_label": "199", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "8e2cffc8f910751ebcc643eb656285c614bf576dbaaebd0edf13c5de3cea3f01", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n8.7 Retractions and velocity of a curve 199\nProposition 8.47. A vector field V on a manifold M is smooth if and only if\nV fis smooth for all f \u2208 F(M).\nExercise 8.48. Show that for V, W\u2208 X(M) and f, g\u2208 F(M) the vector field\nfV + gW is smooth.\n8.7 Retractions and velocity of a curve\nNow equipped with broader notions of smooth maps, tangent vectors and tan-\ngent bundles for a manifold M, we can generalize the notion of retraction from\nDefinition 3.47.\nDefinition 8.49. A retraction on a manifold M is a smooth map\nR: T M \u2192 M: (x, v) 7\u2192 Rx(v)\nsuch that for each (x, v) \u2208 TM the curve c(t) = Rx(tv) satisfies v = [c], where\n[c] is the equivalence class of the curve c as per Definition 8.33.\nThe latter definition is somewhat abstract. We can give it a more familiar look\nby defining the notion of velocity of a curve on a general manifold.\nDefinition 8.50. Let c: I \u2192 Mbe a smooth curve. The velocity of c at t,\ndenoted by c\u2032(t), is the tangent vector in Tc(t)M given by\nc\u2032(t) = [\u03c4 7\u2192 c(t + \u03c4)],\nwhere the brackets on the right-hand side take the equivalence class of the shifted\ncurve, as per Definition 8.33.\nObserve that c\u2032(0) = [ c]. Thus, a smooth map R: T M \u2192 Mis a retraction\nexactly if each curve c(t) = R x(tv) satisfies c\u2032(0) = [ c] = v and (as implicitly\nrequired by the latter) c(0) = x. This characterization matches Definition 3.47.\nMoreover, it is equivalent still to define retractions as smooth maps R: TM \u2192\nM: (x, v) 7\u2192 Rx(v) such that, for all ( x, v) \u2208 TM, we have\n1. R x(0) = x, and\n2. DR x(0): T xM \u2192TxM is the identity map: DR x(0)[v] = v.\nTo be clear, here, 0 denotes the zero tangent vector at x, that is, the equivalence\nclass of smooth curves on M that pass through x at t = 0 with zero velocity, as\njudged through any chart around x. Also, the differential DRx(0) makes sense as\nRx : TxM \u2192 Mis a smooth map, and we identify the tangent spaces of T xM\n(a linear space) with itself (using Theorem 8.35), so that T0(TxM)\u2014the domain\nof DRx(0)\u2014is identified with TxM.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2248, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b8312523-7e40-4569-8df0-c5e54eea0537": {"__data__": {"id_": "b8312523-7e40-4569-8df0-c5e54eea0537", "embedding": null, "metadata": {"page_label": "200", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af980508-4ddc-4e09-abe7-31c2d2f42d6c", "node_type": "4", "metadata": {"page_label": "200", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "4f125ba806b582f0125cf659ae8c2f1c69ec7477957df771f65369de4fbd518e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n200 General manifolds\n8.8 Coordinate vector fields as local frames\nLet (U, \u03c6) be a chart on a d-dimensional manifold M. Here and in many places,\nwe use that U itself is a manifold; specifically, an open submanifold of M: see\nDefinition 8.24. Consider the following vector fields onU, called coordinate vector\nfields:\nWi(x) =\n\u0002\nt 7\u2192 \u03c6\u22121(\u03c6(x) + tei)\n\u0003\n, i = 1, . . . , d, (8.15)\nwhere e1, . . . , ed are the canonical basis vectors for Rd (that is, the columns of\nthe identity matrix of size d). The defining property of these vector fields is\nthat, when pushed through \u03b8\u03c6\nx (8.5), they correspond to the constant coordinate\nvector fields of Rd:\n\u03b8\u03c6\nx (Wi(x)) = d\ndt\u03c6(\u03c6\u22121(\u03c6(x) + tei))\n\f\f\f\f\nt=0\n= ei. (8.16)\nAs a corollary, we obtain a generalization of Proposition 3.69: local frames exist\naround any point on a manifold (see Definition 3.68).\nProposition 8.51. Coordinate vector fields (8.15) are smooth on U, that is,\nW1, . . . , Wd belong to X(U). Furthermore, they form a local frame, that is, for\nall x \u2208 U, the tangent vectors W1(x), . . . , Wd(x) are linearly independent.\nProof. Smoothness follows from (8.16) and Proposition 8.46. Now consider the\nlinear structure on T xM defined by (8.6): W1(x), . . . , Wd(x) are linearly inde-\npendent if and only if they are so after being pushed through \u03b8\u03c6\nx , which is clearly\nthe case owing to (8.16).\nTo interpret the corollary below, use the fact that a vector field is smooth on\nM if and only if it is smooth when restricted to each chart domain U.\nCorollary 8.52. Given a vector field V on M and a chart (U, \u03c6), there exist\nunique functions g1, . . . , gd : U \u2192R such that V |U = g1W1 + \u00b7 \u00b7\u00b7+ gdWd. These\nfunctions are smooth if and only if V |U is smooth.\nProof. That functions gi : U \u2192R such that V |U = P\ni giWi exist and are unique\nfollows from linear independence of W1(x), . . . , Wd(x). The smoothness equiva-\nlence follows from Proposition 8.46 and\n\u03b8\u03c6\nx (V (x)) =\ndX\ni=1\ngi(x)\u03b8\u03c6\nx (Wi(x)) = (g1(x), . . . , gd(x))\u22a4, (8.17)\nwhere we used \u03b8\u03c6\nx (Wi(x)) = ei by (8.16).\nExercise 8.53. Show that for all V \u2208 X(M) and f \u2208 F(M) the function V fis\nsmooth on M (this is one direction of Proposition 8.47).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2413, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "05c83dd7-7006-48cb-831a-bcd8c17460ee": {"__data__": {"id_": "05c83dd7-7006-48cb-831a-bcd8c17460ee", "embedding": null, "metadata": {"page_label": "201", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0788d96a-90d1-40e3-9550-8e7ee829a559", "node_type": "4", "metadata": {"page_label": "201", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "fe446228c93c6e172676fae99ceaf89af16b734ec87375df605e747ad25cfa94", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n8.9 Riemannian metrics and gradients 201\n8.9 Riemannian metrics and gradients\nSince tangent spaces are linear spaces, we can define inner products on them. The\nfollowing definitions already appeared in the context of embedded submanifolds\nin Sections 3.7 and 3.8: they extend verbatim to the general case.\nDefinition 8.54. An inner product on TxM is a bilinear, symmetric, positive\ndefinite function \u27e8\u00b7, \u00b7\u27e9x : TxM \u00d7TxM \u2192R. It induces a norm for tangent vec-\ntors: \u2225u\u2225x =\np\n\u27e8u, u\u27e9x. A metric on M is a choice of inner product \u27e8\u00b7, \u00b7\u27e9x for\neach x \u2208 M.\nDefinition 8.55. A metric \u27e8\u00b7, \u00b7\u27e9x on M is a Riemannian metric if it varies\nsmoothly with x, in the sense that for all smooth vector fields V, Won M the\nfunction x 7\u2192 \u27e8V (x), W(x)\u27e9x is smooth from M to R.\nDefinition 8.56. A Riemannian manifold is a manifold with a Riemannian\nmetric.\nDefinition 8.57. Let f : M \u2192R be smooth on a Riemannian manifold M. The\nRiemannian gradient of f is the vector field gradf on M uniquely defined by the\nfollowing identities:\n\u2200(x, v) \u2208 TM, Df(x)[v] = \u27e8v, gradf(x)\u27e9x , (8.18)\nwhere Df(x) is as in Proposition 8.38 and \u27e8\u00b7, \u00b7\u27e9x is the Riemannian metric.\nThe gradient of a smooth function is a smooth vector field: the proof of Propo-\nsition 3.70 extends as is, using local frames provided by Proposition 8.51 for\nexample.\nProposition 8.58. For f \u2208 F(M), the gradient gradf is smooth.\nProposition 3.59 also holds true in the general case, with the same proof. We\nrestate the claim here. See also Exercise 10.73.\nProposition 8.59. Let f : M \u2192R be a smooth function on a Riemannian\nmanifold M equipped with a retraction R. Then, for all x \u2208 M,\ngradf(x) = grad(f \u25e6 Rx)(0), (8.19)\nwhere f \u25e6 Rx : TxM \u2192R is defined on a Euclidean space ( TxM with the inner\nproduct \u27e8\u00b7, \u00b7\u27e9x), hence its gradient is a \u201cclassical\u201d gradient.\nLikewise, Example 3.57 and Exercise 3.67 regarding Riemannian product man-\nifolds generalize verbatim for product manifolds as defined in Exercise 8.31.\n8.10 Lie brackets as vector fields\nRecall Definition 5.5 where we introduced the notion of Lie bracket of smooth\nvector fields U, V\u2208 X(M): for all f \u2208 F(U) with U open in M, the Lie bracket", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2392, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "36137c6a-a6eb-43e1-b8dd-f26f08699ec2": {"__data__": {"id_": "36137c6a-a6eb-43e1-b8dd-f26f08699ec2", "embedding": null, "metadata": {"page_label": "202", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da33cebe-16b2-4be5-bdc3-515357f7ff68", "node_type": "4", "metadata": {"page_label": "202", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "5c5a03673c99ad69a54c23fd4841ef61262641304c0156e5522d8f6e964d23b8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n202 General manifolds\n[U, V] acts on f and produces a smooth function on U defined by\n[U, V]f = U(V f) \u2212 V (Uf ). (8.20)\nWe now extend Proposition 5.10 to show that [ U, V] acts on F(M) in the exact\nsame way that a specific smooth vector field does, which allows us to think of\n[U, V] itself as being that smooth vector field. To this end, we first show a special\nproperty of coordinate vector fields.\nProposition 8.60. Lie brackets of coordinate vector fields (8.15) vanish iden-\ntically, that is,\n[Wi, Wj]f = 0\nfor all 1 \u2264 i, j\u2264 d and all f \u2208 F(U).\nProof. Writing f in coordinates as \u02dcf = f \u25e6 \u03c6\u22121 (smooth from \u03c6(U) open in Rd\nto R by Definition 8.5), we find using Proposition 8.38:\n(Wif)(x) = Df(x)[Wi(x)]\n= d\ndtf\n\u0000\n\u03c6\u22121 (\u03c6(x) + tei)\n\u0001\f\f\f\f\nt=0\n= d\ndt\n\u02dcf(\u03c6(x) + tei)\n\f\f\f\f\nt=0\n= D \u02dcf(\u03c6(x))[ei]\n= \u27e8grad \u02dcf(\u03c6(x)), ei\u27e9, (8.21)\nwhere we use the canonical inner product \u27e8\u00b7, \u00b7\u27e9 on Rd to define the Euclidean\ngradient of \u02dcf. Using this result twice, we obtain\n(Wj(Wif))(x) = D\n\u0000\n(Wif) \u25e6 \u03c6\u22121\u0001\n(\u03c6(x))[ej]\n= D\n\u0010\n\u27e8grad \u02dcf, ei\u27e9\n\u0011\n(\u03c6(x))[ej]\n=\nD\nHess \u02dcf(\u03c6(x))[ej], ei\nE\n.\nSince the Euclidean Hessian Hess \u02dcf is self-adjoint, we find that\n(Wi(Wjf))(x) = (Wj(Wif))(x)\nhence ([Wi, Wj]f)(x) = 0 for all x \u2208 Uand for all i, j.\nProposition 8.61. Let U, V be two smooth vector fields on a manifold M.\nThere exists a unique smooth vector field W on M such that [U, V]f = W ffor\nall f \u2208 F(M). We identify [U, V] with that smooth vector field.\nProof. We first show the claim on a chart domain. Let ( U, \u03c6) be a chart of\nM, and let W1, . . . , Wd be the corresponding coordinate vector fields (8.15). By", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1837, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7c007a53-d6d2-4dec-8423-a7f08c441b1f": {"__data__": {"id_": "7c007a53-d6d2-4dec-8423-a7f08c441b1f", "embedding": null, "metadata": {"page_label": "203", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85eae4b5-95a7-4951-afe4-2fdf2e96ff44", "node_type": "4", "metadata": {"page_label": "203", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "5d35d44ba52d4303a455d08958a14d23bd0aec35dc04e6350bd260fcb6351d90", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n8.11 Riemannian connections and Hessians 203\nCorollary 8.52, any two vector fields U, V\u2208 X(M) can be expressed on U as\nU|U =\ndX\ni=1\ngiWi, V |U =\ndX\nj=1\nhjWj,\nfor a unique set of smooth functions gi, hj \u2208 F(U). For all f \u2208 F(U),\nV f=\ndX\nj=1\nhjWjf.\nUsing linearity and Leibniz\u2019 rule (Exercise 5.11),\nU(V f) =\nX\ni,j\ngiWi(hjWjf) =\nX\ni,j\ngi(Wihj)(Wjf) + gihjWi(Wjf).\nWith similar considerations for V (Uf ), namely,\nV (Uf ) =\nX\ni,j\nhjWj(giWif) =\nX\ni,j\nhj(Wjgi)(Wif) + hjgiWj(Wif),\nwe find\n[U, V]f = U(V f) \u2212 V (Uf )\n=\nX\ni,j\ngi(Wihj)(Wjf) \u2212 hj(Wjgi)(Wif) +\nX\ni,j\ngihj[Wi, Wj]f.\nSince [Wi, Wj]f = 0 by Proposition 8.60, it follows that, on the domain U, there\nis a unique smooth vector field, specifically,\nX\ni,j\ngi(Wihj)Wj \u2212 hj(Wjgi)Wi, (8.22)\nwhich acts on F(U) in the exact same way as does [ U, V]. This construction\ncan be repeated on a set of charts whose domains cover M. By uniqueness, the\nconstructions on overlapping chart domains are compatible. Hence, this defines\na smooth vector field on all of M. We identify it with [ U, V].\n8.11 Riemannian connections and Hessians\nThe notion of connection applies in the general case. For convenience we repeat\nDefinition 5.20 here. (Definition 5.1 also extends as is.)\nDefinition 8.62. An (affine) connection on M is an operator\n\u2207: X(M) \u00d7 X(M) \u2192 X(M): ( U, V) 7\u2192 \u2207U V\nwhich has three properties for all U, V, W\u2208 X(M), f, g\u2208 F(M) and a, b\u2208 R:\n1. F(M)-linearity in U: \u2207fU +gW V = f\u2207U V + g\u2207W V ;\n2. R-linearity in V : \u2207U (aV + bW) = a\u2207U V + b\u2207U W; and", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1752, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "adaf78a4-f6c7-4ac7-b1e7-33208eeab63a": {"__data__": {"id_": "adaf78a4-f6c7-4ac7-b1e7-33208eeab63a", "embedding": null, "metadata": {"page_label": "204", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eb03315b-7300-4936-8769-2f90151fc43d", "node_type": "4", "metadata": {"page_label": "204", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "779f9f5da32c3956da1ac944089c19abd1b486304d903be0c47ac7485e957f5e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n204 General manifolds\n3. Leibniz rule: \u2207U (fV ) = (Uf )V + f\u2207U V .\nThe field \u2207U V is the covariant derivative of V along U with respect to \u2207.\nLikewise, Theorem 5.6 regarding the existence and uniqueness of a Rieman-\nnian connection extends without difficulty. We use Proposition 8.61 (stating Lie\nbrackets are vector fields) to state the symmetry condition in a more standard\nway.\nTheorem 8.63. On a Riemannian manifold M, there exists a unique connec-\ntion \u2207 which satisfies two additional properties for all U, V, W\u2208 X(M):\n4. Symmetry: [U, V] = \u2207U V \u2212 \u2207V U; and\n5. Compatibility with the metric: U\u27e8V, W\u27e9 = \u27e8\u2207U V , W\u27e9 + \u27e8V, \u2207U W\u27e9.\nThis connection is called the Levi-Civita or Riemannian connection.\nAs we showed in Proposition 5.21 in the embedded case, connections are point-\nwise operators in U. The proof from the embedded case extends to the general\ncase with two changes: first, we now use the more general proof of existence of\nlocal frames provided by Proposition 8.51; second, we must reaffirm the technical\nLemma 5.27 which allows us to make sense of \u2207 when applied to locally defined\nsmooth vector fields (such as coordinate vector fields for example).\nProposition 8.64. For any connection \u2207 and smooth vector fields U, V on a\nmanifold M, the vector field \u2207U V at x depends on U only through U(x). Thus,\nwe can write \u2207uV to mean (\u2207U V )(x) for any U \u2208 X(M) such that U(x) = u,\nwithout ambiguity.\nThese observations allow us to extend Definition 5.14 for Riemannian Hessians\nto general manifolds.\nDefinition 8.65. Let M be a Riemannian manifold with its Riemannian con-\nnection \u2207. The Riemannian Hessian of f \u2208 F(M) at x \u2208 Mis the linear map\nHessf(x): T xM \u2192TxM defined as follows:\nHessf(x)[u] = \u2207ugradf.\nEquivalently, Hessf maps X(M) to X(M) as Hessf[U] = \u2207U gradf.\nThe proof that the Riemannian Hessian is self-adjoint, given for embedded\nsubmanifolds in Proposition 5.15, extends verbatim.\nProposition 8.66. The Riemannian Hessian is self-adjoint with respect to the\nRiemannian metric. That is, for all x \u2208 Mand u, v\u2208 TxM,\n\u27e8Hessf(x)[u], v\u27e9x = \u27e8u, Hessf(x)[v]\u27e9x .\nLikewise, considerations for connections on product manifolds from Exer-\ncises 5.4 and 5.13 also extend to the general case.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2458, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d0767f80-1684-44bc-8d62-6b746972f3bd": {"__data__": {"id_": "d0767f80-1684-44bc-8d62-6b746972f3bd", "embedding": null, "metadata": {"page_label": "205", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36ecb6e4-ae9a-4767-902f-b4d377d79ea8", "node_type": "4", "metadata": {"page_label": "205", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "073801b11583fa8207c90fb46694ef1fb05e2aa67fec6deb9e1ce301886685c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n8.12 Covariant derivatives and geodesics 205\n8.12 Covariant derivatives and geodesics\nRecall Definition 5.28: given a smooth curve c: I \u2192 Mon a manifold M, the\nmap Z : I \u2192 TM is a smooth vector field on c if Z(t) is in T c(t)M for all t \u2208 I\nand Z is smooth as a map from I (open in R) to TM. The set of smooth vector\nfields on c is denoted by X(c).\nTheorem 5.29, both a definition of covariant derivatives and a statement of\ntheir existence and uniqueness, extends to general manifolds as is. So does its\nproof, provided we use local frames on general manifolds (Proposition 8.51) and\nwe reaffirm the notation (5.14) justified in the embedded case.\nTheorem 8.67. Let c: I \u2192 Mbe a smooth curve on a manifold equipped with\na connection \u2207. There exists a unique operator D\ndt : X(c) \u2192 X(c) which satisfies\nthe following properties for all Y, Z\u2208 X(c), U \u2208 X(M), g\u2208 F(I), and a, b\u2208 R:\n1. R-linearity: D\ndt (aY + bZ) = a D\ndt Y + b D\ndt Z;\n2. Leibniz rule: D\ndt (gZ) = g\u2032Z + g D\ndt Z;\n3. Chain rule:\n\u0000D\ndt (U \u25e6 c)\n\u0001\n(t) = \u2207c\u2032(t)U for all t \u2208 I.\nWe call D\ndt the induced covariant derivative. If moreover M is a Riemannian\nmanifold and \u2207 is compatible with its metric \u27e8\u00b7, \u00b7\u27e9 (e.g., if \u2207 is the Riemannian\nconnection), then the induced covariant derivative also satisfies:\n4. Product rule: d\ndt \u27e8Y, Z\u27e9 =\n\nD\ndt Y , Z\n\u000b\n+\n\nY, D\ndt Z\n\u000b\n,\nwhere \u27e8Y, Z\u27e9 \u2208F(I) is defined by \u27e8Y, Z\u27e9(t) = \u27e8Y (t), Z(t)\u27e9c(t).\nRecall the notion of velocity c\u2032 of a smooth curve c stated in Definition 8.50.\nClearly, c\u2032 is a smooth vector field along c, that is, c\u2032 \u2208 X(c). Then, using the in-\nduced covariant derivative D\ndt , we may define acceleration along a curve similarly\nto Definition 5.36, and geodesics as in Definition 5.38.\nDefinition 8.68. Let c: I \u2192 Mbe a smooth curve. The acceleration of c is the\nsmooth vector field c\u2032\u2032 \u2208 X(c) defined by:\nc\u2032\u2032 = D\ndtc\u2032.\nA geodesic is a smooth curve c: I \u2192 Msuch that c\u2032\u2032(t) = 0 for all t \u2208 I.\nExercises 5.34 and 5.39 regarding covariant derivatives on product manifolds\nextend as is, as does Exercise 5.35 for reparameterizations.\n8.13 Taylor expansions and second-order retractions\nUsing the general tools constructed thus far, the reasoning that led to second-\norder Taylor expansions for embedded submanifolds and which culminated in\neq. (5.26) extends to a general Riemannian manifold M. Hence, we can state in", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2585, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a7d02906-87fb-4945-a22c-de2bdea311db": {"__data__": {"id_": "a7d02906-87fb-4945-a22c-de2bdea311db", "embedding": null, "metadata": {"page_label": "206", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "875e9265-07d9-4691-9e67-4827c9f9054d", "node_type": "4", "metadata": {"page_label": "206", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "207e6ebf06aefee2ee703e50866ffd321f5de0c9282d351a1a14e8c25d55374d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n206 General manifolds\ngeneral that, for f \u2208 F(M) and any smooth curve c on M such that c(0) = x\nand c\u2032(0) = v,\nf(c(t)) = f(x) + t \u27e8gradf(x), v\u27e9x + t2\n2 \u27e8Hessf(x)[v], v\u27e9x\n+ t2\n2 \u27e8gradf(x), c\u2032\u2032(0)\u27e9x + O(t3). (8.23)\nDefinition 5.42 extends as is to the general case.\nDefinition 8.69. A second-order retraction R on a Riemannian manifold M is\na retraction such that, for all x \u2208 Mand all v \u2208 TxM, the curve c(t) = Rx(tv)\nhas zero acceleration at t = 0, that is, c\u2032\u2032(0) = 0.\nIn turn, this allows us to extend Propositions 5.44 and 5.45 to the general case\nwith the same proofs, verbatim.\nProposition 8.70. Consider a Riemannian manifold M equipped with any re-\ntraction R, and a smooth function f : M \u2192R. If x is a critical point of f (that\nis, if gradf(x) = 0), then\nf(Rx(s)) = f(x) + 1\n2 \u27e8Hessf(x)[s], s\u27e9x + O(\u2225s\u22253\nx). (8.24)\nIf R is a second-order retraction, then for any point x \u2208 Mwe have\nf(Rx(s)) = f(x) + \u27e8gradf(x), s\u27e9x + 1\n2 \u27e8Hessf(x)[s], s\u27e9x + O(\u2225s\u22253\nx). (8.25)\nProposition 8.71. If the retraction is second order or if gradf(x) = 0, then\nHessf(x) = Hess(f \u25e6 Rx)(0),\nwhere the right-hand side is the Hessian of f \u25e6 Rx : TxM \u2192R at 0 \u2208 TxM.\n8.14 Submanifolds embedded in manifolds\nIn Chapter 3, we defined our first class of smooth sets, which we called embedded\nsubmanifolds of linear spaces. In Section 8.3, we showed that embedded subman-\nifolds of linear spaces are manifolds. Now, we define the concept of embedded\nsubmanifold of a manifold: this includes embedded submanifolds of linear spaces\nas a special case. This will serve us well in Chapter 9.\nGiven a subset M of a manifold M, there may exist many smooth structures\nfor M. These may or may not interact nicely with the smooth structure of M.\nLet us make this precise.\nConsider the inclusion map i: M \u2192M: it maps points of M to themselves in\nM, that is, i(x) = x. Depending on the smooth structure we choose for M, this\nmap may or may not be smooth. If it is, then we can differentiate it and D i(x)\nis a linear map from T xM to TxM. If that map is injective (for all x), we call\nM a submanifold of M.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2325, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b74ff94e-a03a-4e9a-b864-74f6c41af5e3": {"__data__": {"id_": "b74ff94e-a03a-4e9a-b864-74f6c41af5e3", "embedding": null, "metadata": {"page_label": "207", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f4236a4f-9f76-4230-84a1-d483eb3591c8", "node_type": "4", "metadata": {"page_label": "207", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "1e46d3c8854cc3cbbfc8029fd8cdfe1b7c63de3a61e175f11000fb44d8092bdb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n8.14 Submanifolds embedded in manifolds 207\nBelow, notice how, in order to define whether or not M is a submanifold of\nM, we first need M to be a manifold in its own right.\nDefinition 8.72. Consider two manifolds, M and M, such that M (as a set)\nis included in M. If the inclusion map i: M \u2192M is smooth and Di(x) has rank\nequal to dim M for all x \u2208 M, we say M is an (immersed) submanifold of M.\nUnder the rank condition, dim M \u2264dim M and the kernel of D i(x) is trivial.\nThis is just as well, because otherwise there exists a smooth curve c: I \u2192 M\npassing through c(0) = x with nonzero velocity c\u2032(0), yet the \u2018same\u2019 curve \u00afc =\ni\u25e6c: I \u2192 M on M (smooth by composition) passes through x with zero velocity\n\u00afc\u2032(0) = Di(x)[c\u2032(0)]. The definition excludes such peculiarities.\nAmong the submanifold structures of M (if any), there may exist at most\none such that the atlas topology on M coincides with the subspace topology\ninduced by M [Lee12, Thm. 5.31]. When M admits such a smooth structure, we\ncall M (with that structure) an embedded submanifold of M. (The \u2018figure-eight\u2019\nexample shows this is not always the case [Lee12, Fig. 4.3].)\nDefinition 8.73. If M is a submanifold of M and its atlas topology coincides\nwith the subspace topology of M \u2286M (that is, every open set of M is the\nintersection of some open set of M with M), then M is called an embedded\nsubmanifold of M, while M is called the ambient or embedding space.\nTheorem 8.74. A subset M of a manifold M admits at most one smooth\nstructure that makes M an embedded submanifold of M.\nHence, \u22c6it makes sense to say that a subset of a manifold is or is not an em-\nbedded submanifold, where in the affirmative we implicitly mean to endow M\nwith that (unique) smooth structure.\nThe next result gives a complete characterization of embedded submanifolds.\nIt reduces to Definition 3.10 when M is a linear space E.\nTheorem 8.75. Let M be a manifold. A non-empty subset M of M is an\nembedded submanifold of M if and only if either of the following holds:\n1. M is an open subset of M. Then, dim M = dim M and we also call this an\nopen submanifold as in Definition 8.24; or\n2. For a fixed integer k \u2265 1 and for each x \u2208 M, there exists a neighborhood U\nof x in M and a smooth function h: U \u2192Rk such that\nh\u22121(0) = M \u2229U and rank Dh(x) = k.\nThen, dim M = dim M \u2212k and h is called a local defining function.\nThe tangent spaces of M are linear subspaces of those of M:\nTxM = ker Dh(x) \u2286 TxM, (8.26)\nwhere h is any local defining function forM around x. Formally, the identification\nis done through D i(x): T xM \u2192TxM.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2820, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a12c3bdc-da7b-43db-9048-2c933fd39c60": {"__data__": {"id_": "a12c3bdc-da7b-43db-9048-2c933fd39c60", "embedding": null, "metadata": {"page_label": "208", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bf6d08df-33a8-44c7-acdc-c852b72d8a43", "node_type": "4", "metadata": {"page_label": "208", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "f4008efc97be835c37a2aa59905ccbb92bd7184fa45b4ba4365c666245fd1ff7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n208 General manifolds\nIn Theorem 8.75, there is nothing special about Rk: we could just as well\nconsider local defining maps into an arbitrary manifold of dimension k, as this\nis locally equivalent to Rk through a chart. In particular, it often happens that\nan embedded submanifold can be defined with a single defining map, motivating\nthe next corollary.\nCorollary 8.76. Let h: M \u2192 Nbe a smooth map and consider its non-empty\nlevel set M = h\u22121(\u03b1). If Dh(x) has rank equal to dim N for all x \u2208 M, then M\nis closed in M, it is an embedded submanifold of M with dimension dim M =\ndim M \u2212dim N, and TxM = ker Dh(x).\nAbove, the set h\u22121(\u03b1) is closed since it is the pre-image of the singleton {\u03b1}\nthrough the continuous map h, and a singleton is closed in atlas topology since it\nmaps to a singleton through a chart. An embedded submanifold which is closed\nin the embedding space is called properly embedded [Lee12, Prop. 5.5].\nIf the differential of h is not surjective at all points of M, a version of Corol-\nlary 8.76 still holds provided the rank of the differential is constant in a neigh-\nborhood of M. Crucially, it is not sufficient for this condition to hold just on M:\nsee Section 3.10.\nProposition 8.77. Let M = h\u22121(\u03b1) be a non-empty level set of the smooth map\nh: M \u2192 N. If rank Dh(x) = r for all x in a neighborhood of M in M, then M is\nclosed, it is an embedded submanifold of M with dimension dim M = dim M\u2212r,\nand TxM = ker Dh(x).\nIn Definition 3.30, we defined smooth maps to and from embedded subman-\nifolds of linear spaces as those maps which admit a smooth extension to and\nfrom the embedding spaces. Now that we understand embedded submanifolds\nas manifolds, we must verify that our early definition of smooth map agrees\nwith the general notion in Definition 8.5. That is indeed true: Propositions 8.79\nand 8.80 below assert as much for the general case of embedded submanifolds of\nmanifolds. To prove them, we introduce a powerful technical result first.\nLemma 8.78. If M is an embedded submanifold of M, there exists a neighbor-\nhood U of M in M and a smooth map r: U \u2192 Msuch that r(x) = x for all\nx \u2208 M.\nProof sketch. Endow M with a Riemannian metric: this is always doable [Lee18,\nProp. 2.4]. Since M is embedded in M, it has a tubular neighborhood U [Lee18,\nThm. 5.25]. It is straightforward to construct r from the properties of tubular\nneighborhoods. Note: r is a (topological) retraction. This is different from (but\nrelated to) our retractions.\nProposition 8.79. Let M be an embedded submanifold of M and let N be a\nmanifold.\n1. If \u00afF : M \u2192 Nis smooth (at x \u2208 M), then F = \u00afF|M is smooth (at x).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2878, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "268fe9f8-6b42-4fac-9597-14fb651fd7a9": {"__data__": {"id_": "268fe9f8-6b42-4fac-9597-14fb651fd7a9", "embedding": null, "metadata": {"page_label": "209", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9ed703b9-be5d-4830-9a26-4c3f704bae39", "node_type": "4", "metadata": {"page_label": "209", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "6822962b6a181cb2e3d76c60394ff760112fd1bd1f1e0e9bb326a032478f63ee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n8.14 Submanifolds embedded in manifolds 209\n2. There exists a neighborhood U of M in M such that any map F : M \u2192 N\ncan be extended to a map \u00afF : U \u2192 Nwith the property that \u00afF is smooth if F\nis smooth, and \u00afF is smooth at x \u2208 Mif F is smooth at x.\nProof. The first part holds because the inclusion map i: M \u2192M is smooth for\nsubmanifolds hence \u00afF|M = \u00afF \u25e6 i inherits the smoothness of \u00afF by composition.\nFor the second part, summon the map r: U \u2192 Mprovided by Lemma 8.78.\nDefine \u00afF = F \u25e6 r; note that \u00afF|M = F.\nAs a side note, a map defined on any subset of a manifold is said to be smooth if\nit can be smoothly extended to a neighborhood of its domain. This is compatible\nwith the notion of smooth maps on embedded submanifolds.\nProposition 8.80. Let M be an embedded submanifold of M and let N be a\nmanifold. A map F : N \u2192 Mis smooth (at x) if and only if \u00afF : N \u2192M, defined\nby \u00afF(y) = F(y), is smooth (at x).\nProof. Smoothness of F implies smoothness of \u00afF since \u00afF = i\u25e6F, where i: M \u2192\nM is the inclusion map. The other way around, summon the map r: U \u2192 M\nprovided by Lemma 8.78. Through charts, it is easy to confirm that we may\nrestrict the codomain of \u00afF to U without affecting its smoothness. Since F = r\u25e6 \u00afF,\nit follows that smoothness of \u00afF implies that of F. See also [Lee12, Cor. 5.30].\nAs we discovered in Chapters 3 and 5, geometric tools for Riemannian sub-\nmanifolds of Euclidean spaces are related to their counterparts in that Euclidean\nspace in a straightforward way. This is true more generally for Riemannian sub-\nmanifolds of manifolds, and the proofs we have considered extend to the general\ncase with little friction. We now summarize these results.\nAssume M is a Riemannian manifold and M is embedded in M. We know\nfrom eq. (8.26) that T xM is a linear subspace of T xM. Equip the submanifold\nM with a Riemannian metric by restricting the metric\u27e8\u00b7, \u00b7\u27e9x of M to the tangent\nspaces of M. This makes M a Riemannian submanifold of M. Assume \u22c6these\nstructures for the remainder of the section.\nLet Proj x denote the linear map which projects vectors from T xM to TxM\northogonally with respect to \u27e8\u00b7, \u00b7\u27e9x. This object features abundantly in the for-\nmulas below.\nConsider a smooth function f : M \u2192R and any smooth extension \u00aff : U \u2192R\ndefined on a neighborhood U of M in M. Then, for all x \u2208 M,\ngradf(x) = Projx\n\u0000\ngrad \u00aff(x)\n\u0001\n. (8.27)\nFor any two smooth vector fields U, V\u2208 X(M) and corresponding smooth ex-\ntensions \u00afU, \u00afV \u2208 X(U), the Riemannian connection \u2207 on M is related to the\nRiemannian connection \u00af\u2207 on M through the identity (valid along M):\n\u2207U V = Proj( \u00af\u2207\u00afU \u00afV ). (8.28)\nOn a technical note: \u00afU, \u00afV are not necessarily defined on all of M. We interpret", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2938, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b08f0a1a-451f-41f0-b57f-173fe7e32276": {"__data__": {"id_": "b08f0a1a-451f-41f0-b57f-173fe7e32276", "embedding": null, "metadata": {"page_label": "210", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3415773f-3081-4401-87f0-285c4ed96389", "node_type": "4", "metadata": {"page_label": "210", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "663a6c5db22ccbc79fcc50732c149677eccb27c218a8d41dd19e8520b84ff800", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n210 General manifolds\n\u00af\u2207\u00afU \u00afV in the usual way, using the fact that ( \u00af\u2207\u00afU \u00afV )(x) depends on \u00afU, \u00afV only\nlocally around x. See also [Lee18, Thm. 8.2]. In pointwise notation, we have for\nall u \u2208 TxM:\n\u2207uV = Projx( \u00af\u2207u \u00afV ). (8.29)\nAs a result, the Hessian of the function f above is related to the gradient and\nHessian of \u00aff through these relations: let G(x) = gradf(x) be the gradient vector\nfield of f on M, and let \u00afG be a smooth extension of G to a neighborhood of M\nin M. Then, for all u \u2208 TxM \u2286TxM,\nHessf(x)[u] = \u2207ugradf = Projx\n\u0000\u00af\u2207u \u00afG\n\u0001\n. (8.30)\nA similarly simple expression is valid for covariant derivatives of vector fields\nalong curves, in analogy to (5.18):\nD\ndtZ(t) = Projc(t)\n\u0012 \u00afD\ndtZ(t)\n\u0013\n, (8.31)\nwhere c is a smooth curve on M (hence also on M), Z is a smooth vector field on\nc (which can be understood both in M and in M), \u00afD\ndt is the covariant derivative\nfor vector fields on c in M, and D\ndt is the covariant derivative for vector fields\non c in M. From this expression we also recover a convenient formula for the\nacceleration c\u2032\u2032 = D\ndt c\u2032 of a curve c on M in terms of its acceleration \u00a8c = \u00afD\ndt \u02d9c in\nthe embedding space M, akin to (5.23):\nc\u2032\u2032(t) = Projc(t)(\u00a8c(t)). (8.32)\nMoreover, the objects and results presented in Section 5.11 extend to the general\ncase of Riemannian submanifolds of Riemannian manifolds. In particular, the\nsecond fundamental form II and the Weingarten map W are defined in the same\nway and lead to the same formulas for the Hessian and for the decomposition of\n\u00af\u2207 and \u00afD\ndt in tangent and normal parts.\n8.15 Notes and references\nMain references for this chapter are the books by Lee [Lee12, Lee18], Brickell\nand Clark [BC70], O\u2019Neill [O\u2019N83], and Absil et al. [AMS08].\nBrickell and Clark define manifolds to be what we call manifolds*. As a result,\ntopological assumptions are always stated explicitly, which is instructive to track\ntheir importance in various aspects of the theory. O\u2019Neill defines a manifold\nto be a Hausdorff topological space equipped with a maximal atlas, without\nrequiring second-countability (though see pp21\u201322 of that reference). Lee defines\ntopological manifolds first\u2014imposing both Hausdorff and second-countability\u2014\nand defines smooth manifolds as an additional layer of structure on those spaces,\nrequiring the atlas topology to match the existing topology. We use the same\ndefinition as Absil et al.: this is compatible with Lee\u2019s definitions.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2684, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "63bd1377-37bd-4b38-bd64-7e1f2a996412": {"__data__": {"id_": "63bd1377-37bd-4b38-bd64-7e1f2a996412", "embedding": null, "metadata": {"page_label": "211", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9b7f85d6-71a8-4224-8d8c-291b2b2abb47", "node_type": "4", "metadata": {"page_label": "211", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "6d865e757a98042a0f193827e52793d9be6ff7ce07406b006240a5f5027b9a41", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n8.15 Notes and references 211\nAll of these references also lay out basics of topology. The relevance of the\ntopological conditions imposed in Section 8.2 for optimization is spelled out\nin [AMS08, \u00a7 3.1.2].\nA closed manifold is a compact manifold (Definition 8.27) without bound-\nary [Lee12, p27]. As we do not discuss manifolds with boundary, compact and\nclosed manifolds coincide in our treatment, but the latter terminology may be\nconfusing as the manifold itself is always closed with respect to its own topology\n(by Definition 8.15).\nWe defined tangent vectors as equivalence classes of curves, which is one of\nthe standard approaches. Another standard definition of tangent vectors, favored\nnotably by Lee and O\u2019Neill, is through the notion of derivation. These definitions\nare equivalent. A (brief) discussion of the link between these two definitions\nappears in [Lee12, p72].\nEmbedded submanifolds are called regular submanifolds by Brickell and Clark,\nand simply submanifolds by O\u2019Neill. Furthermore, we mean Riemannian subman-\nifolds to be embedded (as does O\u2019Neill), whereas Lee allows them to be merely\nimmersed, pointing out when it is necessary for them to be embedded [Lee18,\np15].\nTheorem 8.75 for embedded submanifolds follows [Lee12, Prop. 4.1 and 5.16].\nThe ensuing characterization of tangent spaces stated in (8.26) matches [Lee12,\nProp. 5.38]. Corollary 8.76 for embedded submanifolds defined by a single defin-\ning map appears as [Lee12, Cor. 5.14]. The related Proposition 8.77 is a conse-\nquence of the constant-rank level set theorem [Lee12, Thm. 5.12].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1837, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "74d769c4-97ed-438f-9bb5-95a9bdac71b6": {"__data__": {"id_": "74d769c4-97ed-438f-9bb5-95a9bdac71b6", "embedding": null, "metadata": {"page_label": "212", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "30c6c8ed-deb6-455b-aee5-d17a13d86c9a", "node_type": "4", "metadata": {"page_label": "212", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "2ab667cf1a1f43165d5be5eff812c99750834a3ce81d3bd99671bfc87bfd1090", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n9 Quotient manifolds\nThe Grassmannian Gr(n, p) is the set of linear subspaces of dimension p in Rn.\n(Many authors write Gr(p, n).) Perhaps the best-known example of an optimiza-\ntion problem over Gr(n, p) is principal component analysis (PCA) (Section 2.4).\nGiven k points y1, . . . , yk \u2208 Rn, the goal is to find a linear subspace L \u2208 Gr(n, p)\nwhich fits the data as well as possible, in the sense that it solves\nmin\nL\u2208Gr(n,p)\nkX\ni=1\ndist(L, yi)2, (9.1)\nwhere dist(L, y) is the Euclidean distance between y and the point in L closest\nto y. This particular formulation of the problem admits an explicit solution\ninvolving the SVD of the data matrix M = [ y1, . . . , yk]. This is not the case\nfor other cost functions, which may be more accommodating of outliers in the\ndata, or more amenable to the inclusion of priors. For these, we may need more\ngeneral optimization algorithms to address (9.1). Thus we ask: how can one solve\noptimization problems over Gr(n, p)?\nAny iterative algorithm to minimize a function f : Gr(n, p) \u2192 R generates\na sequence of subspaces L0, L1, L2, . . .The first point of order is to choose how\nthese subspaces are to be represented in memory. A reasonable idea is to represent\nL \u2208 Gr(n, p) with a matrix X \u2208 Rn\u00d7p whose columns form a basis for L. For\neach L, many matrices X fit this requirement. For numerical reasons, it is often\nbeneficial to use orthonormal bases. Thus, we decide to representL with a matrix\nX in St(n, p), that is, L = span(X) and X\u22a4X = Ip.\nEven working with orthonormal bases to represent subspaces, there are still\nmany possible choices. To be definite, we define an equivalence relation \u223c over\nSt(n, p). Two matrices X, Y \u2208 St(n, p) are deemed equivalent if their columns\nspan the same subspace:\nX \u223c Y \u21d0 \u21d2span(X) = span(Y ) \u21d0 \u21d2X = Y Qfor some Q \u2208 O(p),\nwhere O(p) is the orthogonal group: the set of orthogonal matrices of size p \u00d7 p.\nFormally, this allows us to identify subspaces with equivalence classes : if L =\nspan(X), we identify L with\n[X] = {Y \u2208 St(n, p) : Y \u223c X} = {XQ : Q \u2208 O(p)}.\nThis identification establishes a one-to-one correspondence between Gr(n, p) and", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2385, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5c5e9a30-132b-49b5-902d-8d420185de62": {"__data__": {"id_": "5c5e9a30-132b-49b5-902d-8d420185de62", "embedding": null, "metadata": {"page_label": "213", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d57d3330-42e3-48ba-82b7-3cfcc8951174", "node_type": "4", "metadata": {"page_label": "213", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "bb657a3658fc32527fba11e33989b28e1c97e48851c86bd5097faa43d97a60f0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n9 Quotient manifolds 213\nthe set of equivalence classes, called the quotient set :\nSt(n, p)/\u223c = {[X] : X \u2208 St(n, p)}. (9.2)\nIt is also common to denote this quotient set by St( n, p)/O(p), to highlight the\nspecial role of the orthogonal group in the equivalence relation: we discuss this\nmore below.\nGiven X \u2208 St(n, p) such that L = span( X), the distance function in (9.1)\nadmits an explicit expression: XX \u22a4 is the matrix which represents orthogonal\nprojection from Rn to L, so that, in the Euclidean norm \u2225 \u00b7 \u2225,\ndist(L, y)2 = \u2225y \u2212 XX \u22a4y\u22252 = \u2225y\u22252 \u2212 \u2225X\u22a4y\u22252.\nHence, with A = MM \u22a4, \u2225 \u00b7 \u2225denoting the Frobenius norm for matrices and\n\u00aff(X) =\nkX\ni=1\n\u2225X\u22a4yi\u22252 = \u2225X\u22a4M\u22252 = Tr(X\u22a4AX), (9.3)\nwe may rewrite (9.1) equivalently as\nmax\n[X]\u2208St(n,p)/\u223c\nf([X]), with f([X]) = \u00aff(X). (9.4)\nCrucially, f : St(n, p)/ \u223c \u2192R is well defined on the quotient set since \u00aff(X) =\n\u00aff(Y ) whenever X \u223c Y : we say \u00aff is invariant under \u223c.\nOn the one hand, problem (9.4) is closely related to\nmax\nX\u2208St(n,p)\n\u00aff(X), (9.5)\nwhich we know how to handle using our optimization tools for embedded sub-\nmanifolds, generating a sequence of matrices X0, X1, . . .in St(n, p).\nOn the other hand, a practical implementation of a (yet to be determined)\noptimization algorithm on St(n, p)/\u223c, which generates a sequence of equivalence\nclasses [X0], [X1], . . ., would also actually generate matrices X0, X1, . . .in Stiefel\nto represent these equivalence classes. One wonders then: in practical terms,\nwhat distinguishes an algorithm on St( n, p)/\u223c from one on St( n, p)?\nThe key consideration is preservation of invariance. To illustrate this notion,\nlet us consider how gradient descent proceeds to minimize \u00aff on St( n, p) as a\nRiemannian submanifold of Rn\u00d7p with the usual Euclidean metric. Using the\nprojector to the tangent spaces of Stiefel, Proj St\nX (7.27), the gradient is given by\n1\n2grad \u00aff(X) = ProjSt\nX (AX) (9.6)\n= (In \u2212 XX \u22a4)AX + X X\u22a4AX \u2212 X\u22a4AX\n2 = (In \u2212 XX \u22a4)AX.\n(Notice how the second term vanishes: we will see that this is not by accident.)\nAssuming constant step-size \u03b1 for simplicity, Riemannian gradient descent iter-\nates\nXk+1 = G(Xk), with G(X) = RX(\u2212\u03b1grad \u00aff(X)).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2398, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cd3068d0-db6f-4f83-acaf-d6123102dcf0": {"__data__": {"id_": "cd3068d0-db6f-4f83-acaf-d6123102dcf0", "embedding": null, "metadata": {"page_label": "214", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "43d1eaef-fe3a-44ff-8167-656c98ce10ae", "node_type": "4", "metadata": {"page_label": "214", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "3a240bcd3ccb9fc2f372d30315534851ab6cebc072784becc7585a6121438d02", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n214 Quotient manifolds\nWhen is it legitimate to think of this sequence of iterates as corresponding to a\nsequence on the quotient set?Exactly when the equivalence class of Xk+1 depends\nonly on the equivalence class of Xk, and not on Xk itself. Indeed, only then can\nwe claim that the algorithm iterates from [ Xk] to [Xk+1].\nTo assess the latter, we must determine how [ Xk+1] changes if Xk is replaced\nby another representative of the same equivalence class, that is, if Xk is replaced\nby XkQ for some orthogonal Q. A first observation is that\n\u2200X \u2208 St(n, p), Q\u2208 O(p), grad \u00aff(XQ) = grad \u00aff(X) \u00b7 Q.\nHence, if the retraction has the property 1 that\n\u2200(X, V) \u2208 TSt(n, p), Q\u2208 O(p), [RXQ(V Q)] = [RX(V )], (9.7)\nthen it follows that, for all Q \u2208 O(p),\n[G(XQ)] = [RXQ(\u2212\u03b1grad \u00aff(XQ))] = [RX(\u2212\u03b1grad \u00aff(X))] = [G(X)].\nThus, under that condition, [ Xk+1] is indeed a function of [ Xk]:\nX \u223c Y =\u21d2 G(X) \u223c G(Y ). (9.8)\nWe already know retractions which satisfy property (9.7). For example, the\npolar retraction (7.24) can be written as\nRpol\nX (V ) = (X + V )(Ip + V \u22a4V )\u22121/2,\nso that\nRpol\nXQ(V Q) = (X + V )Q \u00b7\n\u0010\nQ\u22a4\u0002\nIp + V \u22a4V\n\u0003\nQ\n\u0011\u22121/2\n= Rpol\nX (V ) \u00b7 Q. (9.9)\nAlso, the QR retraction (7.22) is such that R QR\nX (V ) is a matrix whose columns\nform an orthonormal basis for span( X + V ). As a result, R QR\nXQ(V Q) is a matrix\nwhose columns form a basis for span(( X + V )Q), which of course is the same\nsubspace (it does not, however, satisfy the stronger property that R XQ(V Q) =\nRX(V ) \u00b7 Q as the polar one did).\nThese considerations allow us to conclude that Riemannian gradient descent\nfor \u00aff on St(n, p) with either of these retractions induces a well-defined sequence\non the quotient set St( n, p)/\u223c, defined by\n[Xk+1] = F([Xk]), with F([X]) = [G(X)].\nAt this point, a few questions come naturally:\n1. Is the sequence defined by [ Xk+1] = F([Xk]) itself a \u201cgradient descent\u201d se-\nquence of sorts for the optimization problem (9.4) on the quotient set?\n2. Can we devise more sophisticated algorithms such as the trust-regions method\nto operate on the quotient set?\n3. Are other quotient sets similarly amenable to optimization?\n1 This property makes sense because if V is tangent to St( n, p) at X then V Qis tangent to\nSt(n, p) at XQ.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2485, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "db9e48c2-49d5-4737-b2e2-01b2abbf708f": {"__data__": {"id_": "db9e48c2-49d5-4737-b2e2-01b2abbf708f", "embedding": null, "metadata": {"page_label": "215", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "323954ca-6773-4339-8e96-c58e22f5b2c7", "node_type": "4", "metadata": {"page_label": "215", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "1564085bda6d9aa71c823ce3617e477e02bca9b1625ed3b083eae71609acefb8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n9 Quotient manifolds 215\nWe answer all questions in the affirmative. The crux of this chapter is to ar-\ngue that quotient sets such as St( n, p)/\u223c are themselves Riemannian manifolds\nin a natural way, called Riemannian quotient manifolds . This identification of\nGr(n, p) with St( n, p)/\u223c gives meaning to the claim that Gr( n, p) is a quotient\nmanifold; it is called the Grassmann manifold. All the tools and algorithms we\nhave developed for optimization on general manifolds apply in particular to quo-\ntient manifolds. The iterative method described above turns out to be a bona\nfide Riemannian gradient descent method in that geometry, and with more work\nwe can similarly describe second-order optimization algorithms.\nParts of this chapter focus on a particular class of Riemannian quotient man-\nifolds obtained through group actions on manifolds, as is the case for Gr( n, p)\nconstructed here. Particular attention is given to the practical representation of\npoints and tangent vectors for quotient manifolds, and to the computation of\nobjects such as gradients and Hessians.\nWhat do we stand to gain from the quotient approach? First, it should be clear\nthat nothing is lost: Riemannian quotient manifolds are Riemannian manifolds,\nhence all algorithms and accompanying theory apply. Second, optimization on\nthe quotient achieves a natural goal: if the cost function of an optimization\nproblem is insensitive to certain transformations, then it is reasonable to require\nan algorithm for that problem to be similarly unfazed.\nSometimes, this property leads to computational advantages. Even when it\ndoes not, the quotient perspective can yield better theoretical understanding.\nSpecifically, consider the local convergence rates we discussed for gradient de-\nscent (Theorem 4.20), Newton\u2019s method (Theorem 6.7) and trust regions (The-\norem 6.30): for all of these, the fast convergence guarantees hold provided the\nalgorithm converges to a critical point where the Hessian of the cost function is\npositive definite. It is easy to come up with counter-examples showing that the\ncondition is necessary in general. For example, with f(x) = x4 on the real line,\ngradient descent with (appropriate) constant step-size converges sublinearly, and\nNewton\u2019s method converges only linearly to zero.\nAs we show in Lemma 9.41, if the cost function on the total space (the set\nbefore we pass to the quotient) is invariant under the quotient, then its Hessian\ncannot possibly be positive definite at critical points. This is because the cost\nfunction is constant along the equivalence classes: directions tangent to these\nequivalence classes are necessarily in the kernel of the Hessian. Thus, the standard\nfast convergence results do not ever apply on the total space.\nYet, it often happens that we do see fast convergence on the total space empir-\nically. This is the case notably for problem (9.5) above, on the Stiefel manifold.\nWhy is that?\nAs transpires from the discussion above and as we detail further in this chapter,\nthe reason is that, under certain circumstances, optimization algorithms on the\ntotal space can be interpreted as matching algorithms on the quotient manifold.\nMoreover, the spurious directions tangent to equivalence classes are quotiented\nout in their own way, so that they do not appear in the kernel of the Hessian", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3595, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1ce5b567-8733-4a12-bef2-7c6319225df0": {"__data__": {"id_": "1ce5b567-8733-4a12-bef2-7c6319225df0", "embedding": null, "metadata": {"page_label": "216", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f8870fa7-fdf8-40b9-a806-1278060db34c", "node_type": "4", "metadata": {"page_label": "216", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "79a3801b856310e8cef32185ab9449a0a55ec1d913729d71396dc58808df9ad9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n216 Quotient manifolds\non the quotient manifold: that Hessian can be positive definite. In that scenario,\nthe quotient approach does not confer a computational advantage over the total\nspace approach (the two are algorithmically equivalent or close), but it does\nprovide the stronger theoretical perspective, aptly explaining why we do get fast\nlocal convergence. As for second-order methods, the quotient perspective can\ndeliver genuinely new and crisply motivated algorithms.\nThroughout the chapter, we use the Grassmann manifold as a running exam-\nple. For convenience, we collect our findings about it in Section 9.16. In that\nsection, we also show that the same geometry for Gr( n, p) can be realized as a\nRiemannian submanifold of a Euclidean space, so that we could have also dis-\ncussed this manifold in Chapter 7. The hope is that by then it will be clear\nthat the quotient perspective carries many conceptual (and aesthetic) advan-\ntages, and that it provides a firm grasp of symmetries beyond the Grassmann\nmanifold.\n9.1 A definition and a few facts\nLet \u223c be an equivalence relation on a manifold M with equivalence classes\n[x] = {y \u2208 M : x \u223c y},\nand let\nM = M/\u223c = {[x] : x \u2208 M} (9.10)\nbe the resulting quotient set. The canonical projection or natural projection links\nthe total space M to its quotient M:\n\u03c0 : M \u2192 M: x 7\u2192 \u03c0(x) = [x]. (9.11)\nThe quotient set M inherits a topology from M called the quotient topology,\nturning M into a quotient space. This topology is defined as follows:\nU \u2286 Mis open \u21d0 \u21d2\u03c0\u22121(U) is open in M.\nThis notably ensures that \u03c0, then called the quotient map, is continuous.\nSay we equip the quotient space M with a smooth structure as in Chapter 8\n(assuming this is possible). Then, it makes sense to ask whether \u03c0 is smooth\nand, accordingly, whether its differential at some point has full rank. These\nconsiderations enter into the definition of quotient manifold.\nDefinition 9.1. The quotient set M = M/\u223c equipped with a smooth structure is\na quotient manifold of M if the projection \u03c0 (9.11) is smooth and its differential\nD\u03c0(x): T xM \u2192T[x]M has rank dim M for all x \u2208 M.\nAs an exercise, one can show that the projective space RP n\u22121 with smooth\nstructure as in Example 8.11 is a quotient manifold of Rn\\{0} with the equiva-\nlence relation that deems two points to be equivalent if they belong to the same", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2602, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d33923c9-8f15-4537-a691-755c81fda63a": {"__data__": {"id_": "d33923c9-8f15-4537-a691-755c81fda63a", "embedding": null, "metadata": {"page_label": "217", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b68c90e4-25f7-415e-9e76-a667a48e40fc", "node_type": "4", "metadata": {"page_label": "217", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "5d96670ab75512bd6dd9159cc6ee23efd76750b86a846685aaea0842a6f4caf1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n9.1 A definition and a few facts 217\nline through the origin. However, this way of identifying quotient manifolds is\nimpractical, as it requires first to know that the quotient space is a manifold with\na certain atlas, then to check explicitly that \u03c0 has the required properties using\nthat particular smooth structure. In this chapter, we discuss more convenient\ntools.\nBy construction, \u03c0 is continuous with respect to the quotient space topology.\nWith a quotient manifold structure onM, \u03c0 is smooth and hence also continuous\nwith respect to the atlas topology. In fact, the atlas topology coincides with the\nquotient topology in that case. We have the following remarkable result [Lee12,\nThm. 4.31].\nTheorem 9.2. A quotient space M = M/\u223c admits at most one smooth struc-\nture that makes it a quotient manifold of M. When this is the case, the atlas\ntopology of M is the quotient topology.\nThis statement should be compared to Theorem 8.74 for embedded submani-\nfolds: a subset of a manifold admits at most one smooth structure that makes it\nan embedded submanifold. Thus, just as it made sense to say that a subset of a\nmanifold is or is not an embedded submanifold, so it makes sense to say that a\nquotient space of a manifold is or is not a quotient manifold.\nA direct consequence of Definition 9.1 and Corollary 8.76 is that equivalence\nclasses are embedded submanifolds of the total space. As we discuss this, it\nappears that we must sometimes distinguish between [ x] as a point of M and\n[x] as a subset of M. When in need, we adopt this convention: [ x] = \u03c0(x) is a\npoint of M, whereas [x] = \u03c0\u22121(\u03c0(x)) is a subset of M.\nProposition 9.3. Let M = M/\u223c be a quotient manifold. For any x \u2208 M, the\nequivalence class F = \u03c0\u22121(\u03c0(x)), also called a fiber, is closed in M and it is an\nembedded submanifold of M. Its tangent spaces are given by\nTyF = ker D\u03c0(y) \u2286 TyM. (9.12)\nIn particular, dim F = dim M \u2212dim M.\nProof. Apply Corollary 8.76 with \u03c0 : M \u2192 Mas the defining map, and F as\nthe level set {y \u2208 M : \u03c0(y) = [x]}.\nThus, when an equivalence relation yields a quotient manifold, that equiv-\nalence relation partitions the total space into closed, embedded submanifolds\ncalled fibers. In particular, notice that all fibers have the same dimension. This\nsometimes allows one to determine quickly that a given quotient space cannot\npossibly be a quotient manifold\u2014see Exercise 9.8. In the following example, we\nillustrate Proposition 9.3 through the spaces that featured in the introduction.\nExample 9.4. Consider the set M = St( n, p)/ \u223c as in (9.2). We have not\nyet argued that this is a quotient manifold: we do so in the next section. For\nnow, let us assume that M indeed is a quotient manifold. Then, given a point", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2973, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fd5f9550-4e51-4074-8d13-fa023d7a1120": {"__data__": {"id_": "fd5f9550-4e51-4074-8d13-fa023d7a1120", "embedding": null, "metadata": {"page_label": "218", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "81891401-d971-4807-b1f4-8214c74b3403", "node_type": "4", "metadata": {"page_label": "218", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "7e34586a1b32832c35fdd34e192f16be01f7b56c690e6e817e5de79468529ae2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n218 Quotient manifolds\nX \u2208 St(n, p), Proposition 9.3 tells us that the fiber\nF = {Y \u2208 St(n, p) : X \u223c Y } = {XQ : Q \u2208 O(p)}\nis an embedded submanifold of St(n, p). (We could also show this directly.)\nThe tangent space to F at X is a subspace of TXSt(n, p), corresponding to the\nkernel of the differential of \u03c0 at X (9.12). As D\u03c0(x) is an abstract object, it is\noften more convenient to approach TXF as follows: all tangent vectors in TXF\nare of the form \u00afc\u2032(0) for some smooth curve \u00afc: I \u2192 Fwith \u00afc(0) = X. Moreover,\nany such curve is necessarily of the form \u00afc(t) = XQ(t) with Q: I \u2192 O(p) a\nsmooth curve on the manifold O(p) with Q(0) = Ip. Thus, all tangent vectors in\nTXF are of the form XQ\u2032(0). Now we recall that the tangent space to O(p) at\nQ(0) = Ip is the set of skew-symmetric matrices of size p (7.32) to conclude that\nTXF = {X\u2126 : \u2126 + \u2126\u22a4 = 0} \u2282TXSt(n, p).\nWe can connect this to \u03c0: by design, c(t) \u225c \u03c0(\u00afc(t)) = [X] is a constant curve on\nSt(n, p)/\u223c. Since we are assuming M is a quotient manifold, \u03c0 is smooth too.\nThis allows us to use the chain rule, writing Q\u2032(0) = \u2126:\n0 = c\u2032(0) = D\u03c0(\u00afc(0))[\u00afc\u2032(0)] = D\u03c0(X)[X\u2126].\nThis confirms that any matrix of the form X\u2126 is in the kernel of D\u03c0(X).\nTheorem 9.2 tells us that a quotient space may be a quotient manifold in at\nmost one way. When it is, we sometimes want to have access to charts of the\nresulting smooth structure on the quotient manifold. The next result provides\nsuch charts. It constitutes one part of the rank theorem in differential geome-\ntry [Lee12, Thm. 4.12].\nProposition 9.5. Let M = M/\u223c be a quotient manifold with canonical pro-\njection \u03c0 and dim M = n + k, dim M = n. For all x \u2208 M, there exists a chart\n(U, \u00af\u03c6) of M around x and a chart (U, \u03c6) of M around \u03c0(x) = [ x] such that\n\u03c0(U) \u2286 Uand the coordinate representation of \u03c0,2\n\u02dc\u03c0 = \u03c6 \u25e6 \u03c0 \u25e6 \u00af\u03c6\u22121 : \u00af\u03c6(U) \u2286 Rn+k \u2192 \u03c6(U) \u2286 Rn, (9.13)\nis simply the function \u02dc\u03c0(z1, . . . , zn+k) = (z1, . . . , zn).\nIt is an exercise to check that \u03c0 is an open map, that is, it maps open sets\nof M to open sets of M [Lee12, Prop. 4.28]. We may thus replace U with \u03c0(U)\nin Proposition 9.5 when convenient. By Definition 9.1, we also know that \u03c0 is\nsurjective, and that its differentials D \u03c0(x) are surjective as well. (The latter\nmakes it a submersion.)\nGiven our focus on optimization, when facing quotient manifolds, we are natu-\nrally led to consider pairs of problems: one which consists in minimizingf : M \u2192\nR, and a companion problem which consists in minimizing \u00aff = f \u25e6 \u03c0 : M \u2192R.\n2 Note that U is not guaranteed to contain whole fibers, that is, \u03c0\u22121(\u03c0(U)) may not be\nincluded in U.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2850, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b63d0326-d32b-47a1-91dc-1bf341825276": {"__data__": {"id_": "b63d0326-d32b-47a1-91dc-1bf341825276", "embedding": null, "metadata": {"page_label": "219", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b4332993-7547-4bc4-a723-ca5fcb14d801", "node_type": "4", "metadata": {"page_label": "219", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "b6a72add03da8db148175a82e367d749574d703206681776b0804bf645cae48f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n9.2 Quotient manifolds through group actions 219\nThe properties of \u03c0 provide strong links between the salient points of both prob-\nlems. The following proposition states those links beyond the context of quo-\ntients. Notice that it does not require any Riemannian structures. The proof is\ndeferred to Section 9.17.\nProposition 9.6. Let M and M be two manifolds with a map \u03c0 : M \u2192 M.\nConsider a function f : M \u2192R and its lift \u00aff = f \u25e6 \u03c0 : M \u2192R. The two\noptimization problems miny\u2208M f(y) and minx\u2208M\n\u00aff(x) are related as follows:\n1. If \u03c0 is surjective, then x is a global minimizer of \u00aff if and only if \u03c0(x) is a\nglobal minimizer of f.\n2. If \u03c0 is continuous and open, then x is a local minimizer of \u00aff if and only if\n\u03c0(x) is a local minimizer of f.\n3. If \u03c0 is smooth and its differential at each point is surjective, then:\n(a) x is a first-order critical point of \u00aff if and only if \u03c0(x) is a first-order\ncritical point of f, and\n(b) x is a second-order critical point of \u00aff if and only if \u03c0(x) is a second-order\ncritical point of f.\nIf M is a quotient manifold of M with projection \u03c0, all of the above hold.\nExercise 9.7. Show that the projective space RPn\u22121 with the smooth structure\nof Example 8.11 is a quotient manifold of Rn\\{0} with the equivalence relation\nx \u223c y \u21d0 \u21d2x = \u03b1y for some \u03b1 \u2208 R.\nExercise 9.8. Consider the following equivalence relation over M = Rn\u00d7p, with\n1 \u2264 p < n: X \u223c Y if and only if Y = XQ for some Q \u2208 O(p). Argue that M/\u223c\nis not a quotient manifold. (Contrast with the introduction of this chapter, where\nM = St(n, p).)\nExercise 9.9. Let M be a quotient manifold of M with canonical projection \u03c0.\nShow that \u03c0 is an open map, that is, if U is open in M, then \u03c0(U) is open in\nM.\n9.2 Quotient manifolds through group actions\nThere exists an explicit characterization of which equivalence relations on a man-\nifold M yield quotient manifolds (see Section 9.17). Using this characterization,\nhowever, is not straightforward. Fortunately, there exists a special class of suit-\nable equivalence relations defined through group actions on manifolds that are\nboth simple to identify and important in practice. This covers our approach to\nthe Grassmann manifold in the introduction of this chapter (where the group is\nO(p)) and other examples we discuss below (see Exercise 9.20).\nWe start with a few definitions regarding groups, Lie groups and group actions.\nA set G equipped with an operation \u00b7: G \u00d7 G \u2192 G is a group if:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2693, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "56a1f123-95db-4e6a-8e04-03d606e0dc66": {"__data__": {"id_": "56a1f123-95db-4e6a-8e04-03d606e0dc66", "embedding": null, "metadata": {"page_label": "220", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5242e3f9-5a43-4f86-a065-dbbe58304f50", "node_type": "4", "metadata": {"page_label": "220", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "05c36e8a664df1e09d2f0c4566c2c3b6d679b948d68eecf6313bcacceb751106", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n220 Quotient manifolds\n1. The operation is associative: a \u00b7 (b \u00b7 c) = (a \u00b7 b) \u00b7 c for all a, b, c\u2208 G;\n2. There exists a unique element e \u2208 G (called the identity) such that e \u00b7 g =\ng \u00b7 e = g for all g \u2208 G; and\n3. For each g \u2208 G there is an element g\u22121 \u2208 G (called the inverse of g) such that\ng \u00b7 g\u22121 = g\u22121 \u00b7 g = e.\nIf the set G is further equipped with a smooth structure\u2014making it a manifold\nG\u2014and the group operation plays nicely with the smooth structure, then we call\nG a Lie group.\nDefinition 9.10. Let G be both a manifold and a group with operation \u00b7. If the\nproduct map\nprod: G \u00d7 G \u2192 G: (g, h) 7\u2192 prod(g, h) = g \u00b7 h\nand the inverse map\ninv: G \u2192 G: g 7\u2192 inv(g) = g\u22121\nare smooth, then G is a Lie group. Smoothness of prod is understood with respect\nto the product manifold structure on G \u00d7 G(see Exercise 8.31).\nExample 9.11. Some examples of Lie groups include O(n) (the orthogonal\ngroup), SO(n) (the rotation group) and GL(n) (the general linear group, which\nis the set of invertible matrices of size n \u00d7 n), with group operation given by\nthe matrix product, and smooth structure as embedded submanifolds of Rn\u00d7n.\nTheir identity is the identity matrix In. Another example is the group of trans-\nlations, Rn, whose group operation is vector addition. Its identity is the zero\nvector. Yet another common example is the special Euclidean group , SE(n),\nwhose elements are of the form (R, t) \u2208 SO(n) \u00d7 Rn, with group operation\n(R, t) \u00b7 (R\u2032, t\u2032) = ( RR\u2032, Rt\u2032 + t). The identity element is (In, 0). Equivalently,\nwe may represent (R, t) as the matrix [ R t\n0 1 ], in which case the group operation\nis the matrix product.\nElements of a group can sometimes be used to transform points of a manifold.\nFor example, X \u2208 St(n, p) can be transformed into another element of St( n, p)\nby right-multiplication with an orthogonal matrix Q \u2208 O(p). Under some condi-\ntions, these transformations are called group actions.\nDefinition 9.12. Given a Lie group G and a manifold M, a left group action\nis a map \u03b8: G \u00d7M \u2192M such that:\n1. For all x \u2208 M, \u03b8(e, x) = x (identity), and\n2. For all g, h\u2208 Gand x \u2208 M, \u03b8(g \u00b7 h, x) = \u03b8(g, \u03b8(h, x)) (compatibility).\nAs a consequence, for all g \u2208 G, the map x 7\u2192 \u03b8(g, x) is invertible on M, with\ninverse x 7\u2192 \u03b8(g\u22121, x). The group action is smooth if \u03b8 is smooth as a map on\nthe product manifold G \u00d7M to the manifold M. We then say the group G acts\nsmoothly on M.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2641, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d43ab2e7-4f6c-4fd7-af38-dcdf7785339b": {"__data__": {"id_": "d43ab2e7-4f6c-4fd7-af38-dcdf7785339b", "embedding": null, "metadata": {"page_label": "221", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18b72851-6d5e-48cc-b00e-1e442f2d9590", "node_type": "4", "metadata": {"page_label": "221", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "e8b30db946848a738a55426c6ead24b7e54ffb416effb87ea76aaff3b1d4cc37", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n9.2 Quotient manifolds through group actions 221\nSimilarly, a right group action is a map \u03b8: M\u00d7G \u2192M such that \u03b8(x, e) = x\nand \u03b8(x, g\u00b7 h) = \u03b8(\u03b8(x, g), h), for all g, h\u2208 Gand x \u2208 M, and this action is\nsmooth if \u03b8 is smooth as a map between manifolds.\nA group action induces an equivalence relation, as follows.\nDefinition 9.13. The orbit of x \u2208 M through the left action \u03b8 of G is the set\nGx \u225c {\u03b8(g, x) : g \u2208 G}. This induces an equivalence relation \u223c on M:\nx \u223c y \u21d0 \u21d2y = \u03b8(g, x) for some g \u2208 G,\nthat is, two points of M are equivalent if they belong to the same orbit. As such,\norbits and equivalence classes coincide. We denote the quotient space M/\u223c as\nM/G (also called the orbit space), where the specific group action is indicated\nby context. The definition is similar for right actions.\nSome authors write G\\M or M/G to distinguish between left and right action\nquotients. We always write M/G.\nExample 9.14. The map \u03b8(X, Q) = XQ defined on St(n, p)\u00d7O(p) is a smooth,\nright group action. Its orbits are the equivalence classes we have considered thus\nfar, namely, [X] = {XQ : Q \u2208 O(p)}. Thus, St(n, p)/O(p) is one-to-one with\nthe Grassmann manifold Gr(n, p).\nWe have already discussed that not all equivalence relations on manifolds\nlead to quotient manifolds. Unfortunately, neither do all smooth group actions:\nfurther properties are required. Specifically, it is sufficient for the actions also to\nbe free and proper.\nDefinition 9.15. A group action \u03b8 is free if, for all x, acting on x with any\ngroup element which is not the identity results in a point different from x. For\ninstance, a left action is free if, for all x \u2208 M, \u03b8(g, x) = x =\u21d2 g = e.\nIf the action is not free, then different orbits could have different dimensions,\nwhich is impossible for a quotient manifold.\nFor the following definition, recall Definition 8.26 for compact sets.\nDefinition 9.16. A left group action \u03b8 is proper if\n\u03d1: G \u00d7M \u2192M \u00d7M: (g, x) 7\u2192 \u03d1(g, x) = (\u03b8(g, x), x)\nis a proper map, that is, all compact subsets of M\u00d7 M map to compact subsets\nof G \u00d7M through \u03d1\u22121. The definition is similar for right actions.\nThe reason we require the action to be proper is topological: if the action is\nsmooth and proper, then the quotient topology is Hausdorff [Lee12, Prop. 21.4].\nOn the other hand, there exist smooth, free actions that are not proper and\nfor which the quotient topology ends up not being Hausdorff [Lee12, Ex. 21.3,\nPb. 21-5].\nChecking whether an action is free is often straightforward. Checking for", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2751, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2ee0239a-ed12-43f0-b9a1-d9069665172a": {"__data__": {"id_": "2ee0239a-ed12-43f0-b9a1-d9069665172a", "embedding": null, "metadata": {"page_label": "222", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63c1c3f1-e428-445e-b41a-786e757dc764", "node_type": "4", "metadata": {"page_label": "222", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "368dca8319ba8ffe6557fbafa7c7e471d72d5547245b48a32ee40ce1c9448d2f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n222 Quotient manifolds\nproperness, on the other hand, can be more delicate. Fortunately, if the group G\nis compact (which is the case for SO( n) and O(n)), then every smooth action is\nproper [Lee12, Cor. 21.6].\nProposition 9.17. Every smooth action by a compact Lie group is proper.\nIf the group is not compact, see Section 9.17 for some pointers, specifically\nProposition 9.60.\nWe can now state the main theorem of this section. This is our tool of choice\nto identify quotient manifolds [Lee12, Thm. 21.10].\nTheorem 9.18. If the Lie group G acts smoothly, freely and properly on the\nsmooth manifold M, then the quotient space M/G is a quotient manifold of\ndimension dim M \u2212dim G; orbits (that is, fibers) have dimension dim G.\nExample 9.19. Continuing our running example, we now check that the Grass-\nmann manifold, seen as the quotient space St(n, p)/O(p), is indeed a quotient\nmanifold. We already checked that the action \u03b8(X, Q) = XQ is smooth. By\nProposition 9.17, it is proper since O(p) is compact. It is also free since XQ =\nX implies Q = Ip (by left-multiplying with X\u22a4). Thus, Theorem 9.18 implies\nGr(n, p), identified with St(n, p)/O(p), is a quotient manifold. More explicitly:\nthe theorem tells us there exists a unique smooth structure which turns Gr(n, p)\ninto a manifold such that\n\u03c0 : St(n, p) \u2192 Gr(n, p): X 7\u2192 \u03c0(X) \u225c [X] = {XQ : Q \u2208 O(p)}\nhas the properties laid out in Definition 9.1. Additionally, we know that\ndim Gr(n, p) = dim St(n, p) \u2212 dim O(p) = p(n \u2212 p).\nBy Proposition 9.3, the fibers are closed, embedded submanifolds of St(n, p) with\ndimension p(p\u22121)\n2 : this is compatible with our work in Example 9.4 where we\nshowed the tangent space to a fiber at X is {X\u2126 : \u2126 \u2208 Skew(p)}.\nIn contrast, one can check that the group action underlying Exercise 9.8 is\nsmooth and proper, but it is not free: not all orbits have the same dimension,\nhence the quotient space is not a quotient manifold.\nExercise 9.20. In each item below, a Lie group G (recall Example 9.11) acts\non a manifold M through some action \u03b8 (the first one is a right action, the\nother two are left actions). Check that these are indeed group actions and that\nthe quotient spaces M/G are quotient manifolds. (Recall that Rm\u00d7n\nk is the set of\nmatrices of size m \u00d7 n and rank k.)\n1. M = SO(n)k, G = SO(n), \u03b8((R1, . . . , Rk), Q) = (R1Q, . . . , RkQ).\n2. M = Rm\u00d7r\nr \u00d7 Rn\u00d7r\nr , G = GL(r), \u03b8(J, (L, R)) = (LJ\u22121, RJ\u22a4).\n3. M = Rd\u00d7n\nd , G = SE(d), \u03b8((R, t), X) = RX + t1\u22a4.\nDescribe the equivalence classes and the significance of the quotient manifolds.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2789, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d6c89e94-dd91-4d8f-b625-7e4c0a078be3": {"__data__": {"id_": "d6c89e94-dd91-4d8f-b625-7e4c0a078be3", "embedding": null, "metadata": {"page_label": "223", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b442e95e-8120-494a-9aec-d4bd544fd529", "node_type": "4", "metadata": {"page_label": "223", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "77cc24b6e92ea83993b03874c696425a03ddd03a918f7f6ea9ea11f06ca4f89e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n9.3 Smooth maps to and from quotient manifolds 223\nM\nM\nNF\n\u03c0\n\u00afF = F \u25e6 \u03c0\nM\nMN\n\u00afF\nF = \u03c0 \u25e6 \u00afF\n\u03c0\nFigure 9.1 Commutative diagrams for Theorem 9.21 (left) and Proposition 9.23 (right)\nabout lifting a map F on or to a quotient manifold.\n9.3 Smooth maps to and from quotient manifolds\nSmooth maps on a quotient manifold M/\u223c can be understood entirely through\nsmooth maps on the corresponding total space M. See Figure 9.1.\nTheorem 9.21. Given a quotient manifold M = M/\u223c with canonical projection\n\u03c0 and any manifold N, a map F : M \u2192 Nis smooth if and only if \u00afF = F \u25e6\n\u03c0 : M \u2192 Nis smooth.\nOne direction is clear: if F is smooth on the quotient manifold, then \u00afF = F \u25e6\u03c0\nis smooth on the total space by composition: we call \u00afF the lift of F. Consider the\nother direction: if \u00afF on M is invariant under \u223c, then it is of the form \u00afF = F \u25e6\u03c0\nfor some map F on the quotient and we say \u00afF descends to the quotient. To\nargue that F is smooth if \u00afF is smooth we introduce the notion of local section: a\nmap S which smoothly selects a representative of each equivalence class in some\nneighborhood. One can establish their existence using the special charts afforded\nby Proposition 9.5\u2014see [Lee12, Thm. 4.26].\nProposition 9.22. For any x \u2208 M there exists a neighborhood U of [x] on the\nquotient manifold M = M/\u223c and a smooth map S : U \u2192M (called a local\nsection) such that \u03c0 \u25e6 S is the identity map on U and S([x]) = x.\nProof of Theorem 9.21. If F is smooth, then \u00afF is smooth by composition. The\nother way around, if \u00afF is smooth, let us show that F is smooth at an arbitrary\n[x]. Use Proposition 9.22 to pick a local section S defined on a neighborhood U\nof [x]. Since \u00afF = F \u25e6\u03c0, we find that F|U = \u00afF \u25e6S: this is smooth by composition.\nThus, F is smooth in some neighborhood around any point [ x], that is, F is\nsmooth.\nWe note another result, this one about maps into quotient manifolds.\nProposition 9.23. Let \u00afF : N \u2192M be a map from one manifold into another,\nand let M = M/\u223c be a quotient manifold of M with projection \u03c0. If \u00afF is smooth,\nthen F = \u03c0 \u25e6 \u00afF : N \u2192 Mis smooth. The other way around, if F : N \u2192 Mis\nsmooth, then for all [x] \u2208 Mthere exists a neighborhood U of [x] such that\nF|F\u22121(U) = \u03c0 \u25e6 \u00afF for some smooth map \u00afF : F\u22121(U) \u2192 M.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2481, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5db3f0a8-2896-48b9-9571-546c0bdf9383": {"__data__": {"id_": "5db3f0a8-2896-48b9-9571-546c0bdf9383", "embedding": null, "metadata": {"page_label": "224", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6fa11236-ec5a-4812-b768-8cef0a8568f2", "node_type": "4", "metadata": {"page_label": "224", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "993a7b62a6acaa68f989f4b90495114c071f784ec763341bfdcd36dd6e6fc641", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n224 Quotient manifolds\nProof. The first part is through composition of smooth maps. For the second\npart, consider \u00afF = S \u25e6 F with a local section S defined on U, as provided by\nProposition 9.22: the domain F\u22121(U) is open in N since F is continuous, \u00afF is\nsmooth by composition, and it is indeed the case that \u03c0 \u25e6 \u00afF is equal to F on\nF\u22121(U) since \u03c0 \u25e6 S is the identity on U.\nThe first part states that a smooth map into the total space yields a smooth\nmap into the quotient manifold after composition with \u03c0. In particular, if \u00afc: I \u2192\nM is a smooth curve on the total space, then c = \u03c0 \u25e6 \u00afc is a smooth curve on the\nquotient manifold.\nThe second part offers a partial converse. For example, if c: I \u2192 Mis a\nsmooth curve on the quotient, then for any t0 \u2208 I there exists an interval J \u2286 I\naround t0 and a smooth curve \u00afc: J \u2192 M such that c(t) = \u03c0(\u00afc(t)).\n9.4 Tangent, vertical and horizontal spaces\nTangent vectors to a quotient manifold M = M/\u223c are rather abstract objects:\na point [ x] \u2208 Mis an equivalence class for \u223c, and a tangent vector \u03be \u2208 T[x]M\nis an equivalence class of smooth curves on M passing through [ x] as defined\nin Section 8.4. Fortunately, we can put the total space M to good use to select\nconcrete representations of tangent vectors.\nIn all that follows, it is helpful to think of the case where M is itself an\nembedded submanifold of a linear spaceE (in our running example,M = St(n, p)\nis embedded in Rn\u00d7p). Then, tangent vectors to M can be represented easily as\nmatrices, as we did in early chapters.\nAccordingly, our goal is to establish one-to-one correspondences between cer-\ntain tangent vectors of M and tangent vectors of M. Owing to Definition 9.1, a\ntool of choice for this task is D \u03c0(x): it maps TxM onto T[x]M. This map, how-\never, is not one-to-one. To resolve this issue, we proceed to restrict its domain\nConsider a point x \u2208 M and its fiber F. We know from Proposition 9.3 that\nTxF is a subspace of TxM and that it coincides with the kernel of D\u03c0(x). We call\nit the vertical space Vx. In some sense, vertical directions are the \u201cuninteresting\u201d\ndirections of T xM from the standpoint of the quotient manifold. If M is a\nRiemannian manifold, we have access to an inner product \u27e8\u00b7, \u00b7\u27e9x on TxM. This\nnaturally suggests that we also consider the orthogonal complement of V x.\nDefinition 9.24. For a quotient manifold M = M/\u223c, the vertical space at\nx \u2208 M is the subspace\nVx = TxF = ker D\u03c0(x),\nwhere F = {y \u2208 M : y \u223c x} is the fiber of x. If M is Riemannian, we call the\northogonal complement of Vx the horizontal space at x:\nHx = (Vx)\u22a5 =\n\b\nu \u2208 TxM : \u27e8u, v\u27e9x = 0 for all v \u2208 Vx\n\t\n.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2871, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "70528964-8697-4085-8d1e-f1638b055679": {"__data__": {"id_": "70528964-8697-4085-8d1e-f1638b055679", "embedding": null, "metadata": {"page_label": "225", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6fe96df6-7687-421a-952d-2798e7c01060", "node_type": "4", "metadata": {"page_label": "225", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "7654972b4231bc7f7771ba84b70ef8a19ffdcbb2a73ce7654e18cc5d59da4560", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n9.4 Tangent, vertical and horizontal spaces 225\nThen, TxM = Vx \u2295 Hx is a direct sum of linear spaces.\nSince ker D\u03c0(x) = Vx, the restricted linear map\nD\u03c0(x)|Hx : Hx \u2192 T[x]M (9.14)\nis bijective. Via this map, we may use (concrete) horizontal vectors at x to\nrepresent (abstract) tangent vectors at [x] unambiguously. The former are called\nhorizontal lifts of the latter.\nDefinition 9.25. Consider a point x \u2208 M and a tangent vector \u03be \u2208 T[x]M.\nThe horizontal lift of \u03be at x is the (unique) horizontal vector u \u2208 Hx such that\nD\u03c0(x)[u] = \u03be. We write\nu = (D\u03c0(x)|Hx)\u22121[\u03be] = liftx(\u03be). (9.15)\nThe following compositions are often useful:\nD\u03c0(x) \u25e6 liftx = Id and liftx \u25e6D\u03c0(x) = ProjH\nx , (9.16)\nwhere ProjH\nx is the orthogonal projector from TxM to Hx.\nConveniently, this definition also allows us to understand smooth curves that\nrepresent \u03be on the quotient manifold. Indeed, since the horizontal lift u of \u03be at x\nis a tangent vector to M at x, there exists a smooth curve \u00afc: I \u2192 M such that\n\u00afc(0) = x and \u00afc\u2032(0) = u. Push this curve to the quotient manifold as follows:\nc = \u03c0 \u25e6 \u00afc: I \u2192 M.\nThis is a curve on M, smooth by composition. Moreover, c(0) = [x] and, by the\nchain rule,\nc\u2032(0) = D\u03c0(\u00afc(0))[\u00afc\u2032(0)] = D\u03c0(x)[u] = \u03be. (9.17)\nIn other words: c = \u03c0 \u25e6 \u00afc is a smooth curve on the quotient space which passes\nthrough [x] with velocity \u03be.\nOf course, the horizontal lift depends on the point at which the vector is\nlifted, but there is no ambiguity as to which abstract tangent vector it represents.\nSpecifically, for a tangent vector\u03be \u2208 T[x]M, if x \u223c y, we may consider horizontal\nlifts ux \u2208 Hx and uy \u2208 Hy. While ux and uy are generally different objects, they\nrepresent the same tangent vector of M:\nD\u03c0(x)[ux] = \u03be = D\u03c0(y)[uy]. (9.18)\nThe following example illustrates the concept of vertical and horizontal spaces\nfor St(n, p)/O(p) and shows how horizontal lifts of a same vector at two different\npoints of a fiber are related.\nExample 9.26. In Example 9.19 we determined the tangent spaces of fibers of\nM = St(n, p)/O(p). That reveals the vertical spaces:\nVX = {X\u2126 : \u2126 \u2208 Skew(p)}.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2331, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "788a234a-c975-4c6a-bc7c-c7cea6b490b8": {"__data__": {"id_": "788a234a-c975-4c6a-bc7c-c7cea6b490b8", "embedding": null, "metadata": {"page_label": "226", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6f303d2f-a1d2-4424-bbe1-fb9c7d0c13a8", "node_type": "4", "metadata": {"page_label": "226", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "6dfe51ba88791b9edffa79cb15ba561255f78df3c07a957d7a218353117fb9e0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n226 Quotient manifolds\nWith the usual Riemannian metric on St(n, p), namely, \u27e8U, V\u27e9X = Tr(U\u22a4V ), we\ncan determine the horizontal spaces:\nHX = {U \u2208 TXSt(n, p) : \u27e8U, X\u2126\u27e9X = 0 for all \u2126 \u2208 Skew(p)}.\nIn this definition, we conclude that X\u22a4U must be symmetric. Yet, from (7.17) we\nalso know that U \u2208 Rn\u00d7p is in TXSt(n, p) exactly when X\u22a4U is skew-symmetric.\nHence, we deduce that\nHX = {U \u2208 Rn\u00d7p : X\u22a4U = 0}.\nFor a given \u03be \u2208 T[X]M, say UX is its horizontal lift at X. Consider another\npoint in [X], namely, Y = XQ for some Q \u2208 O(p). What is the horizontal lift of\n\u03be at Y ? To determine this, as a first step, we select a smooth curve \u00afc on St(n, p)\nsuch that \u00afc(0) = X and \u00afc\u2032(0) = UX. From eq. (9.17) we know that\n\u03be = (\u03c0 \u25e6 \u00afc)\u2032(0).\nNow, consider another smooth curve on St(n, p): \u02dcc(t) = \u00afc(t)Q. Clearly, \u02dcc(0) =\nXQ = Y and \u02dcc\u2032(0) = \u00afc\u2032(0)Q = UXQ. Since by construction \u03c0 \u25e6 \u00afc and \u03c0 \u25e6 \u02dcc are\nthe same curve on M, we may conclude that\n\u03be = (\u03c0 \u25e6 \u00afc)\u2032(0) = (\u03c0 \u25e6 \u02dcc)\u2032(0) = D\u03c0(\u02dcc(0))[\u02dcc\u2032(0)] = D\u03c0(Y )[UXQ].\nCrucially, UXQ is a horizontal vector at Y since Y \u22a4UXQ = Q\u22a4X\u22a4UXQ = 0\nowing to X\u22a4UX = 0. Uniqueness of horizontal lifts then tells us that UY = UXQ\nis the horizontal lift of \u03be at Y , that is,\nliftXQ(\u03be) = liftX(\u03be) \u00b7 Q. (9.19)\nThat formula proves useful later on.\n9.5 Vector fields\nA vector field on a quotient manifold M is defined in the usual way as a suit-\nable map from M to the tangent bundle T M. In light of the above discussion\nregarding horizontal lifts of vectors, it is natural to relate vector fields on M\nto horizontal vector fields on M, that is, vector fields on the total space whose\ntangent vectors are horizontal.\nSpecifically, if V is a vector field on M = M/\u223c, then\n\u00afV (x) = liftx(V ([x])) (9.20)\nuniquely defines a vector field \u00afV on M called the horizontal lift of V . We also\nwrite more compactly\n\u00afV = lift(V ). (9.21)\nConveniently, a vector field is smooth exactly if its horizontal lift is smooth\n(Figure 9.2).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2186, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "30d1e881-140e-4203-a4f6-a671e641eafd": {"__data__": {"id_": "30d1e881-140e-4203-a4f6-a671e641eafd", "embedding": null, "metadata": {"page_label": "227", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38a8820d-d0ff-46fc-a09e-5a963749ff0f", "node_type": "4", "metadata": {"page_label": "227", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "65bc5dc86539cdd1ce8c07a51660b1dedb9a714f5a88b83dda222941bf487fe9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n9.5 Vector fields 227\nM\nM\nTM\nTM\nV\n\u03c0\n\u00afV\nD\u03c0\nFigure 9.2 Commutative diagram for Theorem 9.27 about lifted vector fields.\nTheorem 9.27. A vector field V on a quotient manifold M = M/ \u223c with\ncanonical projection \u03c0 is related to its horizontal lift \u00afV by:\nV \u25e6 \u03c0 = D\u03c0 \u25e6 \u00afV . (9.22)\nMoreover, V is smooth on M if and only if \u00afV is smooth on M.\nThe \u201cif\u201d direction of this proposition is fairly direct. To establish the \u201conly\nif\u201d part, we need one additional technical result first. From Proposition 9.5,\nrecall that for all x\u2032 \u2208 M there exists a chart ( U, \u00af\u03c6) of M around x\u2032 and a\nchart ( U, \u03c6) of M around \u03c0(x\u2032) = [ x\u2032] such that \u02dc\u03c0 = \u03c6 \u25e6 \u03c0 \u25e6 \u00af\u03c6\u22121 is simply\n\u02dc\u03c0(z1, . . . , zn+k) = (z1, . . . , zn), with dim M = n + k and dim M = n. Recall also\nthe definition of coordinate vector fields given by equation (8.15).\nProposition 9.28. The coordinate vector fields \u00afW1, . . . ,\u00afWn+k for the chart \u00af\u03c6\nhave the property that (with e1, . . . , en the canonical basis vectors of Rn):\nD\u03c0(x)\n\u0002 \u00afWi(x)\n\u0003\n=\n(\n(D\u03c6(\u03c0(x)))\u22121[ei] if i \u2208 {1, . . . n},\n0 if i \u2208 {n + 1, . . . , n+ k}.\nIn particular, \u00afWn+1, . . . ,\u00afWn+k are vertical.\nProof. Each coordinate vector field \u00afWi is defined for z \u2208 \u00af\u03c6(U) by (8.15):\n\u00afWi( \u00af\u03c6\u22121(z)) = D \u00af\u03c6\u22121(z)[\u00afei], (9.23)\nwhere \u00afei is the ith canonical basis vector of Rn+k. Differentiate \u02dc\u03c0 at z along the\ndirection \u02d9z \u2208 Rn+k: using the simple expression of \u02dc\u03c0 on one side, and the chain\nrule for \u02dc\u03c0 = \u03c6 \u25e6 \u03c0 \u25e6 \u00af\u03c6\u22121 on the other side, we get\n( \u02d9z1, . . . ,\u02d9zn) = D\u02dc\u03c0(z)[ \u02d9z]\n= D\u03c6(\u03c0( \u00af\u03c6\u22121(z)))\n\u0002\nD\u03c0( \u00af\u03c6\u22121(z))\n\u0002\nD \u00af\u03c6\u22121(z)[ \u02d9z]\n\u0003\u0003\n.\nIntroducing the notation x = \u00af\u03c6\u22121(z), the expression simplifies to:\n( \u02d9z1, . . . ,\u02d9zn) = D\u03c6(\u03c0(x))\n\u0002\nD\u03c0(x)\n\u0002\nD \u00af\u03c6\u22121(z)[ \u02d9z]\n\u0003\u0003\n. (9.24)\nIn particular, for \u02d9z = \u00afei we recognize the coordinate vector fields as in (9.23) so", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2010, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "32f086bb-6413-47fb-ace0-395e4a85461e": {"__data__": {"id_": "32f086bb-6413-47fb-ace0-395e4a85461e", "embedding": null, "metadata": {"page_label": "228", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6eea69db-d2af-4e66-8e7a-8be1d149e02a", "node_type": "4", "metadata": {"page_label": "228", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "5f8f7665ac58721e9594405e3dfbb5a83b4ba38a152e7a2bf958ad1aadcccf3a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n228 Quotient manifolds\nthat\nD\u03c6(\u03c0(x))\n\u0002\nD\u03c0(x)\n\u0002 \u00afWi(x)\n\u0003\u0003\n=\n(\nei if i \u2208 {1, . . . , n},\n0 if i \u2208 {n + 1, . . . , n+ k}.\nTo conclude, note that D\u03c6(\u03c0(x)) is invertible since \u03c6 is a chart.\nWe can now give a proof for Theorem 9.27 characterizing smoothness of vector\nfields on quotient manifolds.\nProof of Theorem 9.27. Equation (9.22) follows from the definition of \u00afV (9.21)\nand from the properties of horizontal lifts (9.16). Using Theorem 9.21 then equa-\ntion (9.22), we find the following equivalences:\nV is smooth \u21d0 \u21d2V \u25e6 \u03c0 is smooth \u21d0 \u21d2D\u03c0 \u25e6 \u00afV is smooth.\nSince D\u03c0 is smooth by Proposition 8.44, if \u00afV is smooth, then V is smooth by\ncomposition.\nThe other way around, if V is smooth, then D \u03c0 \u25e6 \u00afV is smooth. We want to\ndeduce that \u00afV is smooth. To this end, for any x\u2032 \u2208 M, summon the coordinate\nvector fields \u00afW1, . . . ,\u00afWn+k afforded by Proposition 9.28 and defined on some\nneighborhood U of x\u2032. By Corollary 8.52, there exist unique functions gi : U \u2192R\nsuch that, on the domain U,\n\u00afV (x) =\nn+kX\ni=1\ngi(x) \u00afWi(x), (9.25)\nand \u00afV is smooth on U if (and only if) these functions are smooth.\nWe first show g1, . . . , gn are smooth. Since D \u03c0 \u25e6 \u00afV is smooth,\nx 7\u2192\nn+kX\ni=1\ngi(x)D\u03c0(x)[ \u00afWi(x)]\nis smooth. Using properties of D \u03c0 \u25e6 \u00afWi specified by Proposition 9.28, we further\nfind that\nx 7\u2192\nnX\ni=1\ngi(x) (D\u03c6(\u03c0(x)))\u22121 [ei] = (D\u03c6(\u03c0(x)))\u22121 [(g1(x), . . . , gn(x))]\nis smooth, where ei is the ith canonical basis vector of Rn. Since D \u03c6(\u03c0(x)) is\nsmooth as a function of x (because it is part of a chart for T M, see Theo-\nrem 8.43), it follows that g1, . . . , gn are smooth.\nIt only remains to show gn+1, . . . , gn+k are also smooth. To this end, we es-\ntablish linear equations relating the coordinate functions. With j \u2208 {1, . . . , k},\nwe get k equations by taking an inner product of (9.25) against \u00afWn+j. Since \u00afV", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2074, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "54b4bdbf-27c2-41be-9054-ae9fe1f3be4a": {"__data__": {"id_": "54b4bdbf-27c2-41be-9054-ae9fe1f3be4a", "embedding": null, "metadata": {"page_label": "229", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18488222-f311-4bb6-8cde-a63c5bb357c8", "node_type": "4", "metadata": {"page_label": "229", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "14cd8195fa83453b5de8001ae29925fed445106184b6960aaa75619aae203f7e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n9.5 Vector fields 229\nis horizontal and each \u00afWn+j is vertical, we find:\n\uf8ee\n\uf8ef\uf8f0\n0\n...\n0\n\uf8f9\n\uf8fa\uf8fb =\n\uf8ee\n\uf8ef\uf8f0\n\n \u00afW1, \u00afWn+1\n\u000b\n\u00b7 \u00b7\u00b7\n\n \u00afWn, \u00afWn+1\n\u000b\n... ...\n \u00afW1, \u00afWn+k\n\u000b\n\u00b7 \u00b7\u00b7\n\n \u00afWn, \u00afWn+k\n\u000b\n\uf8f9\n\uf8fa\uf8fb\n\uf8ee\n\uf8ef\uf8f0\ng1\n...\ngn\n\uf8f9\n\uf8fa\uf8fb\n+\n\uf8ee\n\uf8ef\uf8f0\n\n \u00afWn+1, \u00afWn+1\n\u000b\n\u00b7 \u00b7\u00b7\n\n \u00afWn+k, \u00afWn+1\n\u000b\n...\n...\n \u00afWn+1, \u00afWn+k\n\u000b\n\u00b7 \u00b7\u00b7\n\n \u00afWn+k, \u00afWn+k\n\u000b\n\uf8f9\n\uf8fa\uf8fb\n\uf8ee\n\uf8ef\uf8f0\ngn+1\n...\ngn+k\n\uf8f9\n\uf8fa\uf8fb.\nSince (a) the functions\n\n \u00afWi, \u00afWj\n\u000b\nare smooth for all i and j (by definition of\nRiemannian metrics and smoothness of coordinate vector fields), (b) the coor-\ndinate functions g1, . . . , gn are smooth, and (c) the k \u00d7 k coefficient matrix is\npositive definite (by linear independence of the coordinate vector fields) and thus\nsmoothly invertible, we conclude that gn+1, . . . , gn+k are indeed smooth. This\nconfirms that \u00afV is smooth at an arbitrary x\u2032, hence \u00afV is smooth.\nIn light of the above result, actions (recall (8.14)) of smooth vector fields on\nsmooth functions on the quotient manifold are easily understood in the total\nspace.\nProposition 9.29. For a quotient manifold M = M/\u223c with canonical pro-\njection \u03c0, consider a smooth vector field V \u2208 X(M) and a smooth function\nf \u2208 F(M) together with their lifts \u00afV \u2208 X(M) and \u00aff \u2208 F(M). Then, the lift of\nV fis \u00afV \u00aff, that is,\n(V f) \u25e6 \u03c0 = \u00afV \u00aff. (9.26)\nIn words: we may lift then act, or act then lift.\nProof. By definition of the action of a smooth vector field on a smooth func-\ntion (8.14), for all [ x] \u2208 M,\n(V f)([x]) = Df([x])[V ([x])].\nOn the other hand, by the chain rule on \u00aff = f \u25e6 \u03c0 and (9.22),\n( \u00afV \u00aff)(x) = D \u00aff(x)[ \u00afV (x)] = Df([x])[D\u03c0(x)[ \u00afV (x)]] = Df([x])[V ([x])].\nHence, ( \u00afV \u00aff)(x) = (V f)(\u03c0(x)) for all x \u2208 M.\nAnother useful consequence of Theorem 9.27 is that we can construct local\nframes (Definition 3.68) for M that separate into horizontal and vertical parts.\nWe use this and the next proposition to argue smoothness of certain retractions\nin Theorem 9.33 below. Recall the definition of orthonormal local frame given in\nExercise 3.72.\nProposition 9.30. Let M = M/\u223c be a quotient manifold with canonical pro-\njection \u03c0 and dim M = n, dim M = n + k. For every x \u2208 M, there exists an", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2344, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d1528309-abe3-4d29-bbb9-b0a569aab059": {"__data__": {"id_": "d1528309-abe3-4d29-bbb9-b0a569aab059", "embedding": null, "metadata": {"page_label": "230", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b3a481e2-5388-4dfe-b0e2-9118b4a8abc7", "node_type": "4", "metadata": {"page_label": "230", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "e9bbd7726b044d21b08336f326e2295b5540e1ddf69b4906f9679a948f462d83", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n230 Quotient manifolds\northonormal local frame \u02c6W1, . . . ,\u02c6Wn+k of M smoothly defined on a neighborhood\nU of x in M such that\n1. \u02c6W1, . . . ,\u02c6Wn are horizontal vector fields, and\n2. \u02c6Wn+1, . . . ,\u02c6Wn+k are vertical vector fields.\nAlso, Wi = D\u03c0[ \u02c6Wi] for i = 1, . . . , nform a local frame for M on \u03c0(U).\nProof. By Proposition 8.51, the coordinate vector fields \u00afW1, . . . ,\u00afWn+k provided\nby Proposition 9.28 already form a local frame. Moreover, \u00afWn+1, . . . ,\u00afWn+k\nare already vertical. However, \u00afW1, . . . ,\u00afWn may not be horizontal. To build\n\u02c6W1, . . . ,\u02c6Wn+k, apply Gram\u2013Schmidt orthogonalization to \u00afW1, . . . ,\u00afWn+k in re-\nverse order: it is an exercise to verify that this achieves the desired result and\npreserves smoothness.\nSee Section 9.7 for a comment regarding orthonormality of the local frame\nW1, . . . , Wn constructed in Proposition 9.30.\nWe can use this last proposition to show that the lift map is smooth. To make\nsense of this statement, we resort to local sections (Proposition 9.22).\nProposition 9.31. For every [x\u2032] on a quotient manifold M = M/\u223c, there\nexists a local section S : U \u2192M on a neighborhood U of [x\u2032] such that\n\u2113: TU \u2192TM: ([x], \u03be) 7\u2192 \u2113([x], \u03be) = liftS([x]) (\u03be)\nis smooth.\nProof. Using Proposition 9.30, select a local frame \u02c6W1, . . . ,\u02c6Wn+k on a neigh-\nborhood U of x\u2032 in M. This also yields a corresponding local frame W1, . . . , Wn\non U = \u03c0(U) (a neighborhood of [ x\u2032]) defined by Wi = D\u03c0[ \u02c6Wi]. By construc-\ntion, \u02c6W1, . . . ,\u02c6Wn are horizontal and \u02c6Wn+1, . . . ,\u02c6Wn+k are vertical. Select a local\nsection S : U \u2192M such that S([x\u2032]) = x\u2032 (if this requires reducing the domain\nU, do so and reduce U as well to preserve the relation U = \u03c0(U)). Notice that\nthese domains are still neighborhoods of [ x\u2032] and x\u2032, respectively, and the local\nframes are well defined on them. Further select a chart ( U, \u03c6) of M around [x\u2032]\nand a chart ( U, \u00af\u03c6) of M around x\u2032 such that S(U) \u2286 U; again, reduce domains\nif necessary.\nUse the local frame W1, . . . , Wn to build a chart of T U as follows:\n([x], \u03be) 7\u2192 (\u03c6([x]), a1, . . . , an),\nwhere a1, . . . , an are defined through \u03be = a1W1([x]) +\u00b7 \u00b7\u00b7+ anWn([x]). That this\nis a chart follows from the fact that basic charts of the tangent bundle are defined\nusing coordinate vector fields (see Theorem 8.43), and changing coordinates be-\ntween these and any local frame is a diffeomorphism. Likewise, build a chart of\nTM on the domain T U as\n(x, u) 7\u2192 ( \u00af\u03c6(x), a1, . . . , an+k),\nwhere ais are uniquely defined by u = a1 \u02c6W1(x) + \u00b7 \u00b7\u00b7+ an+k \u02c6Wn+k(x).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2780, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e4644f69-f392-4cd4-97ba-d2a254a59b23": {"__data__": {"id_": "e4644f69-f392-4cd4-97ba-d2a254a59b23", "embedding": null, "metadata": {"page_label": "231", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "290412b5-4303-4786-81cf-b04b75e5de1e", "node_type": "4", "metadata": {"page_label": "231", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "90ee4c72c8ccd6cce20b0a33233b879b2e7de707ba508116c615e9477ab8040d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n9.6 Retractions 231\nWe can write \u2113 through these charts as \u02dc\u2113: R2n \u2192 R2(n+k). Since\nliftS([x])(\u03be) = liftS([x]) (a1W1([x]) + \u00b7 \u00b7\u00b7+ anWn([x]))\n= a1 \u02c6W1(S([x])) + \u00b7 \u00b7\u00b7+ an \u02c6Wn(S([x])),\nthe coordinates of the vector part of \u2113([x], \u03be) are obtained simply by appending\nk zeros, so that\n\u02dc\u2113(z1, . . . , zn, a1, . . . , an) =\n\u0000\n\u00af\u03c6(S(\u03c6\u22121(z))), a1, . . . , an, 0, . . . ,0\n\u0001\n.\nThis is a smooth function, hence \u2113 is smooth.\nExercise 9.32. Work out the details for the proof of Proposition 9.30.\n9.6 Retractions\nGiven a retraction R on the total space M, we may try to define a retraction R\non the quotient manifold M = M/\u223c as follows:\nR[x](\u03be) =\n\u0002\nRx(liftx(\u03be))\n\u0003\n. (9.27)\nIf this is well defined, that is, if the right-hand side does not depend on the choice\nof lifting point x \u2208 [x], this is indeed a retraction.\nTheorem 9.33. If the retraction R on the total space M satisfies\nx \u223c y =\u21d2 Rx(liftx(\u03be)) \u223c Ry(lifty(\u03be)) (9.28)\nfor all x, y\u2208 M and \u03be \u2208 T[x]M, then (9.27) defines a retraction R on M.\nProof. Since liftx(0) = 0, it holds that R [x](0) = [x]. Assuming for now that R\nis indeed smooth, by the chain rule, for all \u03be \u2208 T[x]M we have\nDR[x](0)[\u03be] = D\u03c0(x)\n\u0002\nDRx(0)[Dliftx(0)[\u03be]]\n\u0003\n= \u03be,\nwhere we used Dliftx(0) = liftx since it is a linear map, D Rx(0) is identity since\nR is a retraction, and D\u03c0(x) \u25e6liftx is identity. This confirms that DR[x](0) is the\nidentity map on T[x]M.\nTo verify smoothness, invoke Proposition 9.31 to select a local section S : U \u2192\nM; then, R [x](\u03be) = \u03c0\n\u0000\nRS([x])\n\u0000\nliftS([x])(\u03be)\n\u0001\u0001\nis smooth on T U since \u03c0, S and R\nare smooth, and so is the map ([ x], \u03be) 7\u2192 liftS([x])(\u03be). To conclude, repeat this\nargument on a collection of domains U to cover all of T M.\nExample 9.34. As we can guess from the introduction of this chapter, both\nthe QR retraction and the polar retraction on St(n, p) satisfy the condition in\nTheorem 9.33. Indeed, from (9.19) we know that, for all Q \u2208 O(p),\nXQ + liftXQ(\u03be) = (X + liftX(\u03be)) Q.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2186, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "af0daeb8-655d-47a9-8b19-29719cf893f9": {"__data__": {"id_": "af0daeb8-655d-47a9-8b19-29719cf893f9", "embedding": null, "metadata": {"page_label": "232", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b54a0ff-24c9-415d-ac1e-eef923aacb8a", "node_type": "4", "metadata": {"page_label": "232", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "d86922966bc63a05e405eee3bed9b5b8970923034bd9405025944322f0cd0a85", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n232 Quotient manifolds\nAs a result, these are valid retractions on the quotient manifold St(n, p)/O(p):\nRQR\n[X](\u03be) = [qfactor(X + liftX(\u03be))] , and (9.29)\nRpol\n[X](\u03be) = [pfactor(X + liftX(\u03be))] , (9.30)\nwhere qfactor extracts the Q-factor of a QR decomposition of a matrix in Rn\u00d7p,\nand pfactor extracts its polar factor: see (7.22) and (7.24).\n9.7 Riemannian quotient manifolds\nA quotient manifold M = M/\u223c is a manifold in its own right. As such, we may\nendow it with a Riemannian metric of our choosing. As is the case for Riemannian\nsubmanifolds\u2014which inherit their metric from the embedding space\u2014endowing\nthe quotient manifold with a metric inherited from the total space leads to\nnice formulas for objects such as gradients, connections, Hessians and covariant\nderivatives.\nWhat does it take for the Riemannian metric of M to induce a Riemannian\nmetric on the quotient manifold M? A natural idea is to try to work with\nhorizontal lifts. Specifically, consider\u03be, \u03b6\u2208 T[x]M. It is tempting to (tentatively)\ndefine an inner product \u27e8\u00b7, \u00b7\u27e9[x] on T[x]M by\n\u27e8\u03be, \u03b6\u27e9[x] = \u27e8liftx(\u03be), liftx(\u03b6)\u27e9x , (9.31)\nwhere \u27e8\u00b7, \u00b7\u27e9x is the Riemannian metric on T xM. For this to make sense, the\ndefinition of \u27e8\u03be, \u03b6\u27e9[x] must not depend on our choice of x: the point at which\ntangent vectors are lifted. That is, for all \u03be, \u03b6\u2208 T[x]M, we must have\nx \u223c y =\u21d2 \u27e8liftx(\u03be), liftx(\u03b6)\u27e9x = \u27e8lifty(\u03be), lifty(\u03b6)\u27e9y . (9.32)\nIf this condition holds for all [x], we may ask whether (9.31) defines aRiemannian\nmetric on M. The answer is yes. Indeed, recall from Definition 8.55 that a metric\nis Riemannian if for every pair of smooth vector fields U, V\u2208 X(M) the function\nf([x]) = \u27e8U([x]), V([x])\u27e9[x]\nis smooth on M. To see that this is the case, consider the horizontal lifts \u00afU, \u00afV\nof U, V, and the function \u00aff =\n\n\u00afU, \u00afV\n\u000b\non M:\n\u00aff(x) =\n\n\u00afU(x), \u00afV (x)\n\u000b\nx = \u27e8liftx(U([x])), liftx(V ([x]))\u27e9x\n(9.31)\n= \u27e8U([x]), V([x])\u27e9[x] = f([x]).\nThe function \u00aff is smooth since \u00afU and \u00afV are smooth by Theorem 9.27. Further-\nmore, \u00aff = f \u25e6 \u03c0, which shows f is smooth by Theorem 9.21. Thus, if (9.31) is\nwell defined, then it is a Riemannian metric, and lifting commutes with taking\ninner products, that is,\n\u2200U, V\u2208 X(M), \u27e8U, V\u27e9 \u25e6\u03c0 = \u27e8lift(U), lift(V )\u27e9. (9.33)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2462, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "09c8a28e-6718-4027-b375-23c4ec5fe313": {"__data__": {"id_": "09c8a28e-6718-4027-b375-23c4ec5fe313", "embedding": null, "metadata": {"page_label": "233", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8570e97b-6fd5-488b-a397-6cc2e940e33a", "node_type": "4", "metadata": {"page_label": "233", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "ea2fc12c46501b153241707412755c1a10d80aad5746177d1f234d0cd3bdc75d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n9.7 Riemannian quotient manifolds 233\nThe above discussion supports the following result, which doubles as a definition\nof Riemannian quotient manifold (coined in [AMS08, p53]).\nTheorem 9.35. If the Riemannian metric on M satisfies (9.32), then (9.31)\ndefines a Riemannian metric on the quotient manifold M = M/\u223c. With this\nmetric, M is called a Riemannian quotient manifold of M.\nFor a Riemannian quotient manifold, D\u03c0(x)|Hx and its inverse liftx are isome-\ntries for all x \u2208 M; the canonical projection \u03c0 is then called a Riemannian\nsubmersion [O\u2019N83, Def. 7.44]. One particular consequence is that the vector\nfields Wi in Proposition 9.30 are orthonormal.\nExample 9.36. With the usual trace inner product on St(n, p) to make it a\nRiemannian submanifold of Rn\u00d7p, we consider the following tentative metric for\nSt(n, p)/O(p):\n\u27e8\u03be, \u03b6\u27e9[X] = \u27e8U, V\u27e9X = Tr(U\u22a4V ),\nwhere U = liftX(\u03be) and V = liftX(\u03b6). Using the relationship (9.19) between lifts\nat different points of a fiber, we find that, for all Q \u2208 O(p),\n\u27e8liftXQ(\u03be), liftXQ(\u03b6)\u27e9XQ = \u27e8UQ, V Q\u27e9XQ = Tr((UQ)\u22a4(V Q))\n= Tr(U\u22a4V ) = \u27e8U, V\u27e9X = \u27e8liftX(\u03be), liftX(\u03b6)\u27e9X .\nThis confirms that the tentative metric is invariant under the choice of lifting\npoint: condition (9.32) is fulfilled, thus Theorem 9.35 tells us this is a Rieman-\nnian metric on the quotient manifold, turning it into a Riemannian quotient\nmanifold.\nFor the special case of a quotient manifold defined through a Lie group action,\ncondition (9.32) holds if the group action plays nicely with the metric.\nDefinition 9.37. A smooth left group action \u03b8 of a Lie group G on a Riemannian\nmanifold M is isometric if for all g \u2208 Gthe map F : M \u2192M defined by\nF(x) = \u03b8(g, x) is isometric, in the sense that\n\u2200x \u2208 M, \u2200u, v\u2208 TxM, \u27e8DF(x)[u], DF(x)[v]\u27e9F(x) = \u27e8u, v\u27e9x .\nThe definition is similar for right actions.\nTheorem 9.38. If the Lie group G acts smoothly, freely, properly and isomet-\nrically on the Riemannian manifold M, then the quotient space M/G is a Rie-\nmannian quotient manifold with the Riemannian metric defined in (9.31).\nProof. In light of Theorem 9.18, it remains to verify condition (9.32). Fix an\narbitrary g \u2208 G. Consider the map F : M \u2192M defined by F(x) = \u03b8(g, x)\n(assuming without loss of generality that we are dealing with a left action). Fix\nan arbitrary point x \u2208 M. Let c: I \u2192 M be an arbitrary smooth curve on M", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2591, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "45a5cb18-ac79-450e-b445-f43e9d00069f": {"__data__": {"id_": "45a5cb18-ac79-450e-b445-f43e9d00069f", "embedding": null, "metadata": {"page_label": "234", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8f8655f5-0f66-4219-888b-62f33291abd6", "node_type": "4", "metadata": {"page_label": "234", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "b334d707f99fc6e2a3b07ceed9b410b3befd1afc33cdd3e58b9b33a697d1f740", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n234 Quotient manifolds\nsatisfying c(0) = x, and let u \u225c c\u2032(0). It holds that \u03c0 \u25e6c = \u03c0 \u25e6F \u25e6c. In particular,\ntheir derivatives at t = 0 are equal; by the chain rule, this translates into:\nD\u03c0(x)[u] = D\u03c0(F(x))[DF(x)[u]].\nThis holds for all u \u2208 TxM. Apply lift F(x) to both sides on the left to deduce\nthat\nliftF(x) \u25e6D\u03c0(x) = ProjH\nF(x) \u25e6 DF(x). (9.34)\nFrom this expression, it is clear that D F(x) transforms vertical vectors at x\ninto vertical vectors at F(x). Moreover, since D F(x) is a linear isometry and\nsince Vx and VF(x) have the same dimension, it follows that D F(x) maps any\northonormal basis of Vx to an orthonormal basis of VF(x). Consequently, DF(x)\nalso transforms horizontal vectors atx into horizontal vectors atF(x). Therefore,\nliftF(x) \u25e6D\u03c0(x)|Hx = DF(x)|Hx, (9.35)\nwhere D F(x)|Hx : Hx \u2192 HF(x) is a linear isometry. It is now clear that con-\ndition (9.32) holds: given y \u223c x, pick g such that y = F(x) and use \u03be =\nD\u03c0(x)[u], \u03b6= D\u03c0(x)[v] for some horizontal vectors u, v\u2208 Hx.\nWith this last result, we can revisit Example 9.36. For any Q \u2208 O(p), consider\nthe map F : St(n, p) \u2192 St(n, p) defined by F(X) = XQ\u2014this captures the group\naction. Of course, D F(X)[U] = UQ. Thus,\n\u2200X \u2208 St(n, p), \u2200U, V\u2208 TXSt(n, p),\n\u27e8DF(X)[U], DF(X)[V ]\u27e9F(X) = \u27e8UQ, V Q\u27e9XQ = \u27e8U, V\u27e9X .\nThis confirms that F is an isometry: apply Theorem 9.38 to conclude.\n9.8 Gradients\nThe gradient of a smooth function on a quotient manifold equipped with a Rie-\nmannian metric is defined in the same way as for any manifold, see Defini-\ntion 8.57. Being a smooth vector field on the quotient manifold, the gradient is\nan abstract object. Prompted by the discussions of the past few sections, we aim\nto represent the gradient via a horizontal lift. In the important special case of\na Riemannian quotient manifold as defined through Theorem 9.35, this can be\ndone rather easily, as we now show.\nConsider f : M \u2192R and its lift \u00aff = f \u25e6 \u03c0. On the one hand, the gradient of\nf with respect to the metric on M satisfies:\n\u2200([x], \u03be) \u2208 TM, Df([x])[\u03be] = \u27e8gradf([x]), \u03be\u27e9[x] .\nOn the other hand, the gradient of the lifted function \u00aff with respect to the\nmetric on the total space M obeys:\n\u2200(x, u) \u2208 TM, D \u00aff(x)[u] =\n\ngrad \u00aff(x), u\n\u000b\nx .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2453, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1c2bb72d-d2b3-42d3-814a-27b4db6fb679": {"__data__": {"id_": "1c2bb72d-d2b3-42d3-814a-27b4db6fb679", "embedding": null, "metadata": {"page_label": "235", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0f800b55-5208-4311-9cd0-a8bc59e6f84c", "node_type": "4", "metadata": {"page_label": "235", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "0f9cd053c1f09f7c3165f12c4344a75afbfb153092c946039a73c6325c8967ba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n9.8 Gradients 235\nFix a point x \u2208 M. Starting with the latter, using the chain rule on \u00aff = f \u25e6 \u03c0,\nand concluding with the former we find:\n\u2200u \u2208 TxM,\n\ngrad \u00aff(x), u\n\u000b\nx = D \u00aff(x)[u]\n= Df(\u03c0(x))[D\u03c0(x)[u]]\n= \u27e8gradf([x]), D\u03c0(x)[u]\u27e9[x] .\nThis holds for all tangent vectors. Thus, for horizontal vectors in particular, using\nthe definition of metric on a Riemannian quotient manifold given by (9.31) and\nthe relations (9.16), we get\n\u2200u \u2208 Hx,\n\ngrad \u00aff(x), u\n\u000b\nx = \u27e8gradf([x]), D\u03c0(x)[u]\u27e9[x]\n= \u27e8liftx(gradf([x])), u\u27e9x .\nThis tells us that the horizontal part of grad \u00aff(x) is equal to the lift of gradf([x])\nat x. What about the vertical part? That one is necessarily zero, owing to the\nfact that \u00aff is constant along fibers. Indeed,\n\u2200v \u2208 Vx,\n\ngrad \u00aff(x), v\n\u000b\nx = D \u00aff(x)[v] = Df([x])[D\u03c0(x)[v]] = 0, (9.36)\nsince D\u03c0(x)[v] = 0 for v \u2208 Vx. This leads to a simple conclusion.\nProposition 9.39. The Riemannian gradient of f on a Riemannian quotient\nmanifold is related to the Riemannian gradient of the lifted function \u00aff = f \u25e6 \u03c0\non the total space via\nliftx(gradf([x])) = grad \u00aff(x), (9.37)\nfor all x \u2208 M.\nIn words: to compute the horizontal lift of the gradient of a smooth function\nf on a Riemannian quotient manifold, we only need to compute the gradient of\nthe lifted function, \u00aff = f \u25e6 \u03c0. In other words: taking gradients commutes with\nlifting. Compare with (8.27) for Riemannian submanifolds.\nExample 9.40. In the introduction of this chapter, we considered the cost func-\ntion \u00aff (9.3) defined on St(n, p):\n\u00aff(X) = Tr(X\u22a4AX).\nThis function has the invariance \u00aff(XQ) = \u00aff(X) for all Q \u2208 O(p). Thus,\nthere is a well-defined smooth function f on the Riemannian quotient mani-\nfold St(n, p)/O(p) related to \u00aff by \u00aff = f \u25e6 \u03c0. Remembering the expression (9.6)\nfor the gradient of \u00aff with respect to the usual Riemannian metric on St(n, p),\nand applying Proposition 9.39 to relate it to the gradient of f on the Riemannian\nquotient manifold, we find:\nliftX(gradf([X])) = grad \u00aff(X) = 2(In \u2212 XX \u22a4)AX. (9.38)\nNotice how the gradient of \u00aff is necessarily horizontal: comparing with the explicit", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2335, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c55de6f5-1c5f-4e94-a720-b4f0033fa9a2": {"__data__": {"id_": "c55de6f5-1c5f-4e94-a720-b4f0033fa9a2", "embedding": null, "metadata": {"page_label": "236", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fafd2ba6-5df4-48b3-a6ac-12ecd07c9f37", "node_type": "4", "metadata": {"page_label": "236", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "64299c7baa116d1c743f89c1674d2d4f82dc04f296649f490a8f8192953ed1b3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n236 Quotient manifolds\ndescription of the horizontal and vertical spaces given in Example 9.26, we see\nnow why one of the terms in (9.6) had to cancel.\n9.9 A word about Riemannian gradient descent\nConsider a smooth cost function f on a quotient manifold M = M/\u223c endowed\nwith a Riemannian metric and a retraction R. Given an initial guess [ x0] \u2208 M,\nRGD on f iterates\n[xk+1] = R[xk](\u2212\u03b1kgradf([xk])) (9.39)\nwith step-sizes \u03b1k determined in some way. How can we run this abstract algo-\nrithm numerically, in practice?\nThe first step is to decide how to store the iterates [x0], [x1], [x2], . . .in memory.\nAn obvious choice is to store x0, x1, x2, . . .themselves. These are points of M:\nif the latter is an embedded submanifold of a Euclidean space for example, this\nshould be straightforward.\nWith access to xk as a representative of [xk], we turn to computing gradf([xk]).\nIn the spirit of Section 9.4, we consider its horizontal lift at xk. This is a tangent\nvector to M at xk: it should be straightforward to store in memory as well. If\nM is a Riemannian quotient manifold of M, then Proposition 9.39 conveniently\ntells us that\nliftxk (gradf([xk])) = grad \u00aff(xk), (9.40)\nwhere \u00aff = f \u25e6\u03c0 and \u03c0 : M \u2192 Mis the canonical projection. Thus, by computing\ngrad \u00aff(xk), we get a hold of grad f([xk]).\nWith these ingredients in memory, it remains to discuss how we can compute\nxk+1. Let us assume that R is related to a retraction R on M through (9.27).\nThen, proceeding from (9.39) we deduce that\n[xk+1] = R[xk](\u2212\u03b1kgradf([xk]))\n=\n\u0002\nRxk (\u2212\u03b1k liftxk (gradf([xk])))\n\u0003\n=\n\u0002\nRxk (\u2212\u03b1kgrad \u00aff(xk))\n\u0003\n. (9.41)\nThus, if M is a Riemannian quotient manifold of M and if R and R are related\nvia (9.27), then numerically iterating\nxk+1 = Rxk (\u2212\u03b1kgrad \u00aff(xk)) (9.42)\non M is equivalent to running the abstract iteration (9.39) on M. Interestingly,\niteration (9.42) is nothing but RGD on \u00aff. In this sense, and under the stated\nassumptions, RGD on M and on M are identical.\nAs a technical point, note that for (9.42) to be a proper instantiation of (9.39),\nwe must make sure that the chosen step-size \u03b1k depends on xk only through the\nequivalence class [xk]. Under the same assumptions as above, this is indeed the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2449, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8317ff91-8560-45bf-a7a4-58ad987d6844": {"__data__": {"id_": "8317ff91-8560-45bf-a7a4-58ad987d6844", "embedding": null, "metadata": {"page_label": "237", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2808b57a-9ed3-44a0-b321-a62572cda607", "node_type": "4", "metadata": {"page_label": "237", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "4a290893a4a4f660b7b609bbad5f794b4b845b10e912356abb297d3206e91196", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n9.9 A word about Riemannian gradient descent 237\ncase so long as \u03b1k is determined based on the line-search function (and possibly\nother invariant quantities)\u2014this covers typical line-search methods. Explicitly,\nthe line-search functions for \u00aff at x and for f at [x] are the same:\n\u2200t, \u00aff\n\u0000\nRx(\u2212tgrad \u00aff(x))\n\u0001\n= f\n\u0000\nR[x](\u2212tgradf([x]))\n\u0001\n.\nThough running RGD on M or on M may be the same, we still reap a theo-\nretical benefit from the quotient perspective. We discussed the local convergence\nbehavior of RGD in Section 4.6, noting that we may expect linear convergence\nto a local minimizer if the Hessian of the cost function at that point is positive\ndefinite. Crucially, the cost function \u00aff on the total space M cannot admit such\ncritical points because of its invariance under \u223c.\nLemma 9.41. Let M be a Riemannian manifold and let f : M \u2192R be smooth\non a quotient manifold M = M/\u223c with canonical projection \u03c0. If x \u2208 M is a\ncritical point for \u00aff = f \u25e6 \u03c0, then the vertical space Vx is included in the kernel\nof Hess \u00aff(x). In particular, if dim M < dim M then Hess \u00aff(x) is not positive\ndefinite.\nProof. Pick an arbitrary vertical vector v \u2208 Vx. Since the fiber of x is an em-\nbedded submanifold of M with tangent space V x at x, we can pick a smooth\ncurve \u00afc on the fiber of x such that \u00afc(0) = x and \u00afc\u2032(0) = v. With \u00af\u2207 and \u00afD\ndt\ndenoting the Riemannian connection and induced covariant derivative onM, we\nhave identities as in (5.17):\nHess \u00aff(x)[v] = \u00af\u2207vgrad \u00aff =\n\u00afD\ndtgrad \u00aff(\u00afc(t))\n\f\f\f\f\nt=0\n.\nBy Proposition 9.39 , the fact that x is a critical point for \u00aff implies that [ x] is\na critical point for f. Still using that same proposition, we also see that, for all\nt in the domain of \u00afc,\ngrad \u00aff(\u00afc(t)) = lift\u00afc(t)(gradf([\u00afc(t)])) = lift\u00afc(t)(gradf([x])) = 0.\nIt follows that Hess \u00aff(x)[v] = 0.\nThus, the standard theory does not predict fast local convergence for RGD on\n\u00aff.\nThe good news is: the trivial eigenvalues of the Hessian of \u00aff associated to\nvertical directions do not appear in the spectrum of the Hessian of f on the\nquotient manifold (see Exercise 9.46). Thus, if the version of RGD we actually\nrun on M is equivalent to RGD onM, we may apply the local convergence results\nof Section 4.6 to f rather than to \u00aff. In many instances, the local minimizers of\nf do have the property that the Hessian there is positive definite, in which case\nwe can claim (and indeed observe) fast local convergence.\nAs a closing remark: bear in mind that, in full generality, given a sequence\nx0, x1, x2, . . .on M, it may happen that the sequence of equivalence classes\n[x0], [x1], [x2], . . .converges to a limit point in M, while x0, x1, x2, . . .itself does\nnot converge in M.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2937, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "81dd0e12-d9a7-4eb2-8255-fdade662ae54": {"__data__": {"id_": "81dd0e12-d9a7-4eb2-8255-fdade662ae54", "embedding": null, "metadata": {"page_label": "238", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb55b00e-11c1-46b0-b030-43539644fae3", "node_type": "4", "metadata": {"page_label": "238", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "6e3f9b217b80fef59e8b1bb4fe3c94704b8e58a9fd1f1d6e96eee284b811a560", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n238 Quotient manifolds\n9.10 Connections\nLet M be a Riemannian manifold. Recall from Theorem 8.63 thatM is equipped\nwith a uniquely defined Riemannian connection, here denoted by \u00af\u2207. Likewise,\na Riemannian quotient manifold M = M/\u223c is equipped with its own uniquely\ndefined Riemannian connection, \u2207. Conveniently, due to the strong link between\nthe Riemannian metric on M and that on M, their Riemannian connections are\nalso tightly related. The main object of this section is to establish the formula:\nlift(\u2207U V ) = ProjH\u0000\u00af\u2207\u00afU \u00afV\n\u0001\n, (9.43)\nwhere lift: X(M) \u2192 X(M) extracts the horizontal lift of a vector field as\nin (9.21), \u00afU = lift(U), \u00afV = lift(V ), and Proj H : X(M) \u2192 X(M) orthogonally\nprojects each tangent vector of a vector field to the horizontal space at its base.\nThe proof of this statement is based on the Koszul formula (5.11), which we\nfirst encountered in the proof of Theorem 5.6. Recall that this formula completely\ncharacterizes the Riemannian connection in terms of the Riemannian metric and\nLie brackets: for all U, V, W\u2208 X(M),\n2 \u27e8\u2207U V , W\u27e9 = U\u27e8V, W\u27e9 + V \u27e8W, U\u27e9 \u2212W\u27e8U, V\u27e9\n\u2212 \u27e8U, [V, W]\u27e9 + \u27e8V, [W, U]\u27e9 + \u27e8W,[U, V]\u27e9.\nTo make progress, we must first understand how Lie brackets on the quotient\nmanifold are related to Lie brackets of horizontal lifts.\nProposition 9.42. For any two smooth vector fields U, V\u2208 X(M) and their\nhorizontal lifts \u00afU, \u00afV \u2208 X(M),\nlift([U, V]) = ProjH([ \u00afU, \u00afV ]). (9.44)\nProof. From (9.26) and (9.33), recall that for all U, V\u2208 X(M) and f \u2208 F(M)\nand their lifts \u00afU = lift(U), \u00afV = lift(V ) and \u00aff = f \u25e6 \u03c0:\n(V f) \u25e6 \u03c0 = \u00afV \u00aff, \u27e8U, V\u27e9 \u25e6\u03c0 =\n\n\u00afU, \u00afV\n\u000b\n. (9.45)\nThen, by definition of Lie brackets,\nlift([U, V]) \u00aff = ([U, V]f) \u25e6 \u03c0\n= (UV f) \u25e6 \u03c0 \u2212 (V Uf) \u25e6 \u03c0\n= \u00afU \u00afV \u00aff \u2212 \u00afV \u00afU \u00aff\n= [ \u00afU, \u00afV ] \u00aff\n= ProjH([ \u00afU, \u00afV ]) \u00aff,\nwhere the last equality holds because \u00aff is constant along vertical directions. The\nfact that this holds for all lifted functions \u00aff allows to conclude. Slightly more\nexplicitly, using \u00afV \u00aff =\n\n\u00afV ,grad \u00aff\n\u000b\ntwice, the above can be reformulated as:\n\u27e8lift([U, V]), grad \u00aff\u27e9 = \u27e8ProjH([ \u00afU, \u00afV ]), grad \u00aff\u27e9.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2319, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8e1785f2-adee-4e84-b242-0f45515b9da5": {"__data__": {"id_": "8e1785f2-adee-4e84-b242-0f45515b9da5", "embedding": null, "metadata": {"page_label": "239", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36b27665-c63e-49da-b97a-420e5399eb89", "node_type": "4", "metadata": {"page_label": "239", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "d1b248043e3c57eba19c9b8dffa8f28c71dfbeb0e8fd2c6244c2b6c517c877c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n9.11 Hessians 239\nThen, consider for each point x \u2208 M a collection of dim M functions f whose\ngradients at [x] form a basis for the tangent space T [x]M: the gradients of their\nlifts form a basis for the horizontal space Hx, which forces the horizontal parts of\nlift([U, V]) and Proj H([ \u00afU, \u00afV ]) to coincide. Since both fields are horizontal, they\nare equal.\nLet \u00afU, \u00afV ,\u00afW denote the horizontal lifts of U, Vand W. Using identities (9.44)\nand (9.45) several times we find that:\n(U \u27e8V, W\u27e9) \u25e6 \u03c0 = \u00afU(\u27e8V, W\u27e9 \u25e6\u03c0) = \u00afU\n\n\u00afV ,\u00afW\n\u000b\n, and\n\u27e8U, [V, W]\u27e9 \u25e6\u03c0 =\n\n\u00afU, lift([V, W])\n\u000b\n=\n\n\u00afU, [ \u00afV ,\u00afW]\n\u000b\n, (9.46)\nwhere in the last equality we used that \u00afU is horizontal. With these identities in\nhand, compare the Koszul formulas for both \u2207 and \u00af\u2207: this justifies the second\nequality in\n\nProjH( \u00af\u2207\u00afU \u00afV ), \u00afW\n\u000b\n=\n\n\u00af\u2207\u00afU \u00afV ,\u00afW\n\u000b\n= \u27e8\u2207U V , W\u27e9 \u25e6\u03c0 =\n\nlift(\u2207U V ), \u00afW\n\u000b\n,\nwhile the first equality holds owing to horizontality of \u00afW. Once more, since this\nholds for all lifted horizontal fields \u00afW, we see that (9.43) holds, as announced.\nThis discussion warrants the following theorem.\nTheorem 9.43. Let M be a Riemannian quotient manifold of M. The Rie-\nmannian connections \u2207 on M and \u00af\u2207 on M are related by\nlift(\u2207U V ) = ProjH\u0000\u00af\u2207\u00afU \u00afV\n\u0001\nfor all U, V\u2208 X(M), with \u00afU = lift(U) and \u00afV = lift(V ).\nCompare this result to (8.28) for Riemannian submanifolds.\nExercise 9.44. Show that\n\u00af\u2207\u00afU \u00afV = lift(\u2207U V ) + 1\n2ProjV([ \u00afU, \u00afV ]),\nwhere ProjV = Id \u2212ProjH is the orthogonal projector to vertical spaces. Argue\nfurthermore that ProjV([ \u00afU, \u00afV ]) at x depends only on \u00afU(x) and \u00afV (x).\n9.11 Hessians\nFor a smooth function f on a Riemannian quotient manifold M = M/\u223c, the\nHessian of f is defined at ([ x], \u03be) \u2208 TM by\nHessf([x])[\u03be] = \u2207\u03begradf. (9.47)\nFor any vector field V \u2208 X(M), Theorem 9.43 tells us that\nliftx(\u2207\u03beV ) = ProjH\nx ( \u00af\u2207u \u00afV ), (9.48)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2076, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "81ec5d99-30b9-4f78-ba96-0cc531fed29b": {"__data__": {"id_": "81ec5d99-30b9-4f78-ba96-0cc531fed29b", "embedding": null, "metadata": {"page_label": "240", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d3973197-d432-4b86-889d-0ab950f29f57", "node_type": "4", "metadata": {"page_label": "240", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "04df6f3502c057f2a1c0f6f3f7abc4ba8872629ea7a12c261d9bd9ba115ad05c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n240 Quotient manifolds\nwhere u = liftx(\u03be) and \u00afV = lift(V ). Recall from Proposition 9.39 that\nlift(gradf) = grad \u00aff, (9.49)\nwith \u00aff = f \u25e6 \u03c0. Combining, we find that\nliftx(Hessf([x])[\u03be]) = ProjH\nx\n\u0000\u00af\u2207ugrad \u00aff\n\u0001\n. (9.50)\nFinally, since \u00af\u2207ugrad \u00aff = Hess \u00aff(x)[u], we get the following result.\nProposition 9.45. The Riemannian Hessian of f on a Riemannian quotient\nmanifold is related to the Riemannian Hessian of the lifted function \u00aff = f \u25e6 \u03c0\non the total space as\nliftx(Hessf([x])[\u03be]) = ProjH\nx\n\u0000\nHess \u00aff(x)[u]\n\u0001\n, (9.51)\nfor all x \u2208 M and \u03be \u2208 T[x]M, with u = liftx(\u03be).\nSee Example 9.49 for an illustration.\nWe already knew from Proposition 9.6 that second-order criticality is pre-\nserved between M and M for f and \u00aff. This has implications for how their\nHessians relate at critical points. It is an exercise to make this more precise for\nthe Riemannian structures chosen here.\nExercise 9.46. Let f : M \u2192R be smooth on a Riemannian quotient manifold\nM = M/\u223c with canonical projection \u03c0, and let \u00aff = f \u25e6\u03c0. We know that x \u2208 M\nis a first-order critical point for \u00aff if and only if [x] is a first-order critical point\nfor f. (See Proposition 9.6 or 9.39.)\nShow that if x is critical then the eigenvalues of Hess \u00aff(x) are exactly the eigen-\nvalues of Hessf([x]) together with a set of dim M\u2212dim M additional eigenvalues\nequal to zero. (Hint: use Lemma 9.41.)\n9.12 A word about Riemannian Newton\u2019s method\nWe considered Newton\u2019s method on a general Riemannian manifold in Sec-\ntion 6.2. Applied to the minimization of a function f on a quotient manifold\nM = M/\u223c with retraction R, the update equation is\n[xk+1] = R[xk](\u03bek), (9.52)\nwhere \u03bek \u2208 T[xk]M is the solution of the linear equation\nHessf([xk])[\u03bek] = \u2212gradf([xk]), (9.53)\nwhich we assume to be unique. In the spirit of Section 9.9, we now discuss how\nto run (9.52) in practice.\nLet us assume that M is a Riemannian quotient manifold with canonical\nprojection \u03c0. We lift both sides of (9.53) to the horizontal space at xk. With", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2236, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9feb501b-5e5d-4639-b0fd-3d14c432995a": {"__data__": {"id_": "9feb501b-5e5d-4639-b0fd-3d14c432995a", "embedding": null, "metadata": {"page_label": "241", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b9091e5-a412-47ce-b495-247a75c10ef5", "node_type": "4", "metadata": {"page_label": "241", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "4571b6bdaf746aadf1c6140f251c1911d1762270663326d22e890f565527fa64", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n9.12 A word about Riemannian Newton\u2019s method 241\nsk = lift xk (\u03bek), Propositions 9.39 and 9.45 tell us that the linear equation is\nequivalent to\nProjH\nxk\n\u0000\nHess \u00aff(xk)[sk]\n\u0001\n= \u2212grad \u00aff(xk), (9.54)\nwhere \u00aff = f \u25e6 \u03c0. This system is to be solved for sk in the horizontal space H xk .\nAlthough Proj H\nxk and Hess \u00aff(xk) are both symmetric maps on T xk M, their\ncomposition is not necessarily symmetric. This is easily resolved: since sk is\nhorizontal, we may also rewrite the above as\nProjH\nxk\n\u0000\nHess \u00aff(xk)\n\u0002\nProjH\nxk (sk)\n\u0003\u0001\n= \u2212grad \u00aff(xk). (9.55)\nThis is a linear system with symmetric linear map\nProjH\nxk \u25e6 Hess \u00aff(xk) \u25e6 ProjH\nxk . (9.56)\nBy construction, if (9.53) has a unique solution \u03bek, then (9.55) has a unique\nhorizontal solution sk = liftxk (\u03bek). (If we solve (9.55) in the whole tangent space\nTxk M, then all solutions are of the form sk + v with v \u2208 Vxk arbitrary, and sk\nis the solution of minimal norm.)\nIf the retraction R is related to a retractionR on M via (9.27), then continuing\nfrom (9.52) we see that\n[xk+1] = R[xk](\u03bek) =\n\u0002\nRxk (liftxk (\u03bek))\n\u0003\n=\n\u0002\nRxk (sk)\n\u0003\n. (9.57)\nIn summary, to run Newton\u2019s method on f in practice, we may iterate\nxk+1 = Rxk (sk) (9.58)\nwith sk \u2208 Hxk the horizontal solution of (9.55) (unique if and only if the solu-\ntion of (9.53) is unique). It is an exercise to check that the conjugate gradients\nalgorithm (CG) from Section 6.3 is well attuned to the computation of sk.\nIn contrast, Newton\u2019s method on M also iterates (9.58) but with sk a solution\nof the following linear system over T xk M (if one exists):\nHess \u00aff(xk)[sk] = \u2212grad \u00aff(xk). (9.59)\nSuch a solution may not be horizontal (and its horizontal part may not be a\nsolution), or it may not be unique. Thus, running Newton\u2019s method in the total\nspace is not equivalent to running it on the quotient manifold. What is more,\nif xk converges to a critical point (which is desirable), Lemma 9.41 tells us that\nHess \u00aff(xk) converges to a singular map. Thus, we must expect difficulties in\nsolving the linear system on the total space. (However, see Exercise 9.48 for a\nnumerical twist.)\nThe reasoning above extends to see how to run the Riemannian trust-region\nmethod on Riemannian quotient manifolds as well.\nExercise 9.47. In light of eq. (9.55), consider the linear system Hs = b with\nH = ProjH\nx \u25e6 Hess \u00aff(x) \u25e6 ProjH\nx and b = \u2212grad \u00aff(x)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2599, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9b5d7e8b-2fb7-4b57-9c99-ac8b305a1a6a": {"__data__": {"id_": "9b5d7e8b-2fb7-4b57-9c99-ac8b305a1a6a", "embedding": null, "metadata": {"page_label": "242", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b9f36262-f69e-4e01-959e-ed420251cbcf", "node_type": "4", "metadata": {"page_label": "242", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "965033f7e03739ab41e94bad839c8a8ac0b5b12f786b8afd95dba97dc6aa3a72", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n242 Quotient manifolds\ndefined at some point x \u2208 M. Show that the eigenvalues of H (a self-adjoint map\non TxM) are those of Hessf([x]) together with an additional dim M \u2212dim M\ntrivial eigenvalues. In contrast with Lemma 9.41, show this even if x is not a\ncritical point. Conclude that H is positive definite on Hx exactly if Hessf([x]) is\npositive definite. In that scenario, discuss how the iterates of CG (Algorithm 6.2)\nbehave when applied to the system Hs = b, especially minding horizontality. How\nmany iterations need to be run at most?\nExercise 9.48. Continuing from Exercise 9.47, assume [\u02dcx] is a strict second-\norder critical point: gradf([\u02dcx]) = 0 and Hessf([\u02dcx]) \u227b 0. Newton\u2019s method on\nf converges to [\u02dcx] if it ever gets close enough. What happens if we ignore the\nquotient structure and optimize \u00aff = f \u25e6 \u03c0 instead?\nConsider the Newton system for \u00aff at a point x near \u02dcx on the total space,\nignoring the quotient structure:\nHess \u00aff(x)[s] = \u2212grad \u00aff(x), s \u2208 TxM. (9.60)\nFrom Lemma 9.41, we know that Hess \u00aff(\u02dcx) has a kernel, hence for x close to\n\u02dcx we expect this system to be ill conditioned. And indeed, solving (9.60) exactly\nwith a standard algorithm (e.g., Matlab\u2019s backslash operator after representing\nthe Hessian and the gradient in matrix and vector form) can lead to catastrophic\nfailure when x is close to \u02dcx.\nHowever, it is much more common to (try to) solve (9.60) with CG. A typical\nobservation then would be that roughly dim M iterations of CG on (9.60) are\nfairly well behaved. The next iteration would break CG. Since the iterates of\nCG are increasingly better approximate solutions of (9.60), if that happens, it\nis reasonable to return the best solution reached so far: that is what Matlab\u2019s\nimplementation of CG does ( pcg). As it turns out, that (approximate) solution\nis numerically close to the solution one would compute if working on the quotient\nmanifold (as in Exercise 9.47).\nExplain this observation. (Hint: compare the Krylov space implicitly generated\nby CG on (9.60) to the Krylov space that CG would generate for Newton\u2019s method\non the quotient manifold, as per Exercise 9.47. It is helpful to use local frames\nas in Proposition 9.30 together with Lemma 9.41 at \u02dcx.)\nExercise 9.48 gives some explanation as to why, empirically, running the trust-\nregion method with truncated CG as subproblem solver in the total space (ig-\nnoring the quotient) or on the quotient manifold (with matching retractions)\noften yields strikingly similar results, even though the superlinear convergence\nguarantees (Section 6.6) break in the total space due to Hessian singularity.\nThe use of CG (or another Krylov space-based solver) is important here: the\nsolution of the Newton system in the total space at a point which is close to a\nstrict (in the quotient) second-order critical point is not, in general, close to a\nlift of the Newton step in the quotient space; but numerically the CG algorithm\nfinds an approximate solution to that linear system which happens to mimic the\nquotient approach.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3283, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "82f68cc3-5c41-4375-8edf-7c00ee4c603b": {"__data__": {"id_": "82f68cc3-5c41-4375-8edf-7c00ee4c603b", "embedding": null, "metadata": {"page_label": "243", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e8c54f02-c166-4c95-a286-4dbf81c1f4fc", "node_type": "4", "metadata": {"page_label": "243", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "b8991ca00c92eedf0ecfcf8a1793a9f6e5f8f6d777d503cad6885da839c6fc55", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n9.13 Total space embedded in a linear space 243\nThus, the quotient formalism provides us with a basis to understand why run-\nning particular second-order optimization algorithms on the total space behaves\nfar better than one might reasonably expect if ignoring the quotient. Moreover,\nit provides us with a clean alternative that resolves those numerical issues al-\ntogether: running the second-order methods on the quotient manifold, through\nhorizontal lifts.\n9.13 Total space embedded in a linear space\nFor all quotient manifolds described in Exercise 9.20, the total space M is an\nembedded submanifold of a linear space E (a space of matrices). It is often conve-\nnient to make M into a Riemannian submanifold of E, then to make M = M/\u223c\ninto a Riemannian quotient manifold of M (when possible). In this scenario, the\ngeometric tools for M can be described directly in terms of objects in E.\nConsider a smooth function \u00af\u00aff : E \u2192R (possibly only defined on a neighbor-\nhood of M). Its restriction, \u00aff = \u00af\u00aff|M, is smooth too. Since M is a Riemannian\nsubmanifold of E, we know from (3.39) that\ngrad \u00aff(x) = Projx\n\u0010\ngrad \u00af\u00aff(x)\n\u0011\n, (9.61)\nwhere Projx is the orthogonal projector from E to TxM.\nIf furthermore \u00aff is invariant under \u223c so that \u00aff = f \u25e6 \u03c0 for some smooth\nfunction f on the Riemannian quotient manifold M, then Proposition 9.39 tells\nus that\nliftx(gradf([x])) = Projx\n\u0010\ngrad \u00af\u00aff(x)\n\u0011\n. (9.62)\nThis notably shows that the right-hand side is a horizontal vector, even though\nwe have only asked for \u00aff to be invariant: there is no such requirement for all of\n\u00af\u00aff, as the equivalence relation \u223c is not even formally defined outside of M. We\nexploit this observation to write also:\nliftx(gradf([x])) = ProjH\nx\n\u0010\ngrad \u00af\u00aff(x)\n\u0011\n, (9.63)\nwhere ProjH\nx is the orthogonal projector from E to the horizontal space H x. To\nsee this, note that Proj H\nx (u) = u for all u \u2208 Hx, and ProjH\nx \u25e6Projx = ProjH\nx since\nHx \u2286 TxM, then apply to (9.62).\nSimilarly, we can express the Hessian of f in terms of \u00af\u00aff. Indeed, Proposi-\ntion 9.45 states\nliftx(Hessf([x])[\u03be]) = ProjH\nx\n\u0000\nHess \u00aff(x)[u]\n\u0001\n(9.64)\nwith u = liftx(\u03be). Moreover, we know from connections on Riemannian subman-\nifolds (5.4) that\nHess \u00aff(x)[u] = \u00af\u2207ugrad \u00aff(x) = Projx\n\u0010\nD \u00af\u00afG(x)[u]\n\u0011\n, (9.65)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2505, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "67d6b035-9d28-49d9-86f8-5556137a414d": {"__data__": {"id_": "67d6b035-9d28-49d9-86f8-5556137a414d", "embedding": null, "metadata": {"page_label": "244", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fc1a7c7c-a819-4bba-84ca-39fe836c2490", "node_type": "4", "metadata": {"page_label": "244", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "eecc8cca3a78e4d374fe1c4e856abb3a5c071cd706367aa0a20a0a346e9f0049", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n244 Quotient manifolds\nwhere \u00af\u00afG is a smooth extension of grad \u00aff to a neighborhood of M in E. Note that\nto pick the extension \u00af\u00afG we are free to entertain expressions for Projx\n\u0010\ngrad \u00af\u00aff(x)\n\u0011\nor Proj H\nx\n\u0010\ngrad \u00af\u00aff(x)\n\u0011\nas they are both equal to grad \u00aff(x) but one may lead to\nmore convenient intermediate expressions than the other. Then,\nliftx\n\u0000\nHessf([x])[\u03be]\n\u0001\n= ProjH\nx\n\u0010\nD \u00af\u00afG(x)[u]\n\u0011\n. (9.66)\nThese formulas are best illustrated through an example.\nExample 9.49. Let us go a few steps further in Example 9.40. Consider the\nGrassmann manifold Gr(n, p) = St(n, p)/O(p) as a Riemannian quotient mani-\nfold of St(n, p), itself a Riemannian submanifold of Rn\u00d7p equipped with the usual\ntrace inner product. The cost function \u00af\u00aff(X) = 1\n2 Tr(X\u22a4AX) is smooth on Rn\u00d7p,\nhence its restriction \u00aff = \u00af\u00aff|St(n,p) is smooth too. Since \u00aff is invariant under O(p),\nwe further find that f is smooth on Gr(n, p), with f([X]) = \u00aff(X). The Euclidean\nderivatives of \u00af\u00aff are:\ngrad \u00af\u00aff(X) = AX and Hess \u00af\u00aff(X)[U] = AU.\nThe horizontal spaces are given by HX = {U \u2208 Rn\u00d7p : X\u22a4U = 0 }, and the\ncorresponding orthogonal projectors are\nProjH\nX(Z) = (In \u2212 XX \u22a4)Z = Z \u2212 X(X\u22a4Z). (9.67)\nChoosing to work with (9.63) (rather than (9.62)) because ProjH\nX is somewhat\nsimpler than ProjX (the projector to tangent spaces of the Stiefel manifold), we\ndeduce that the lifted gradient of f is:\nliftX(gradf([X])) = (In \u2212 XX \u22a4)grad \u00af\u00aff(X)\n= AX \u2212 X(X\u22a4AX). (9.68)\nAn obvious smooth extension to all of Rn\u00d7p is simply given by\n\u00af\u00afG(X) = (In \u2212 XX \u22a4)grad \u00af\u00aff(X),\nwith directional derivatives\nD \u00af\u00afG(X)[U] = (In \u2212 XX \u22a4)Hess \u00af\u00aff(X)[U] \u2212 (UX \u22a4+ XU \u22a4)grad \u00af\u00aff(X).\nThen, we get the lifted Hessian via (9.66). For any U = lift X(\u03be), since U is\nhorizontal, we get after some simplifications:\nliftX\n\u0000\nHessf([X])[\u03be]\n\u0001\n= (In \u2212 XX \u22a4)Hess \u00af\u00aff(X)[U] \u2212 UX \u22a4grad \u00af\u00aff(X)\n= AU \u2212 X(X\u22a4AU) \u2212 U(X\u22a4AX). (9.69)\nWe can also compute with the Hessian in a quadratic form:\n\u27e8\u03be, Hessf([X])[\u03be]\u27e9[X] = \u27e8U, AU\u2212 X(X\u22a4AU) \u2212 U(X\u22a4AX)\u27e9X\n= \u27e8U, AU\u2212 U(X\u22a4AX)\u27e9,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2233, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "882fe263-a4aa-48b5-84d8-951312774a0a": {"__data__": {"id_": "882fe263-a4aa-48b5-84d8-951312774a0a", "embedding": null, "metadata": {"page_label": "245", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a2cbf396-ef19-43c7-a15c-86baf1f001a0", "node_type": "4", "metadata": {"page_label": "245", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "7ec794690d15a04f7e945edfe8e710ec407a288bfefae409b8bdeba976542ac1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n9.13 Total space embedded in a linear space 245\nwhere \u27e8U, V\u27e9 = Tr(U\u22a4V ) is the usual trace (Frobenius) inner product.\nNotice how intermediate formulas in (9.68) and (9.69) provide convenient\nexpressions directly in terms of the Euclidean gradient and Hessian of \u00af\u00aff. In\nManopt, these formulas are implemented in grassmannfactory as egrad2rgrad\nand ehess2rhess.\nExample 9.50. In the previous example, projections to horizontal spaces are\nmore convenient than projections to tangent spaces of the total space. This is not\nalways the case. For example, let E = Rd\u00d7n be the embedding space for\nM = {X \u2208 Rd\u00d7n : det(XX \u22a4) \u0338= 0 and X1 = 0},\nthat is, rank- d matrices whose columns sum to zero, and let M = M/O(d)\nbe defined by the equivalence classes [X] = {QX : Q \u2208 O(d)}. Equivalence\nclasses are one-to-one with non-degenerate clouds of n labeled points in Rd up\nto rigid motion. Make M into a Riemannian quotient manifold of M, itself a\nRiemannian submanifold of E.\nIn this case, M is an open subset of an affine subspace of E. Consequently, the\ntangent spaces TXM are all the same: ProjX is independent of X; let us denote\nit with Proj. It is more convenient then to use (9.62) for the gradient:\nliftX(gradf([X])) = Proj\n\u0010\ngrad \u00af\u00aff(X)\n\u0011\n.\nThe right-hand side offers a suitable smooth extension \u00af\u00afG of grad \u00aff. It is easy\nto differentiate it since Proj is constant: D \u00af\u00afG(X)[U] = Proj\n\u0010\nHess \u00af\u00aff(X)[U]\n\u0011\n. We\nconclude via (9.66) that\nliftX\n\u0000\nHessf([X])[\u03be]\n\u0001\n= ProjH\nX\n\u0010\nHess \u00af\u00aff(X)[U]\n\u0011\n,\nwhere U = liftX(\u03be) and we used ProjH\nX \u25e6 Proj = ProjH\nX.\nExercise 9.51. Let A \u2208 Rn\u00d7n be a symmetric matrix. A subspace V is sta-\nble (or invariant) under A if v \u2208 V =\u21d2 Av \u2208 V . Show that any such\nsubspace admits an orthonormal basis composed of eigenvectors of A (and vice\nversa). Based on Example 9.49, establish the following facts about the problem\nmin[X]\u2208Gr(n,p)\n1\n2 Tr(X\u22a4AX) (called Rayleigh quotient optimization):\n1. The critical points are the subspaces of dimension p stable under A.\n2. The global minimizers are the subspaces spanned by p orthonormal eigenvec-\ntors associated to p smallest eigenvalues of A (counting multiplicities).\n3. The second-order critical points are the global minimizers.\nIn particular, all local minimizers are second-order critical hence they are global\nminimizers. This is a well-known fact sometimes referred to as the hidden con-\nvexity of eigenvalue computation.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2647, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d7d175f3-0b44-47ae-930b-be40fe71822f": {"__data__": {"id_": "d7d175f3-0b44-47ae-930b-be40fe71822f", "embedding": null, "metadata": {"page_label": "246", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd6af929-dff0-4a99-b9e0-0e460c08b7e4", "node_type": "4", "metadata": {"page_label": "246", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "aa338aac848bfb2c5e1f09aa932f991ecbd73ea7977553c94c4b4d73c571b88a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n246 Quotient manifolds\n9.14 Horizontal curves and covariant derivatives\nOur primary goal in this section is to understand the covariant derivative of\nsmooth vector fields along smooth curves on a Riemannian quotient manifold\nM = M/\u223c, following their discussion for general manifolds in Section 8.12. In\nso doing, we aim to relate the (uniquely defined) covariant derivative D\ndt along a\ncurve c on M to that of M along a related curve \u00afc, denoted by \u00afD\ndt .\nWe can push any curve \u00afc from M to a smooth curve c on M, defined through\nc = \u03c0 \u25e6 \u00afc. By the chain rule, their velocities are related as:\nc\u2032(t) = D\u03c0(\u00afc(t))[\u00afc\u2032(t)]. (9.70)\nSince lift\u00afc(t) \u25e6D\u03c0(\u00afc(t)) is the orthogonal projector to the horizontal space H \u00afc(t),\nwe can also write\nlift\u00afc(t)(c\u2032(t)) = ProjH\n\u00afc(t)(\u00afc\u2032(t)). (9.71)\nIn particular, \u2225c\u2032(t)\u2225c(t) = \u2225ProjH\n\u00afc(t)\u00afc\u2032(t)\u2225\u00afc(t) \u2264 \u2225\u00afc\u2032(t)\u2225\u00afc(t): speed can only de-\ncrease in going to the quotient.\nExpression (9.71) simplifies in a way that proves particularly useful for our\npurpose if the velocity of \u00afc is everywhere horizontal.\nDefinition 9.52. A smooth curve \u00afc: I \u2192 M is a horizontal curve if \u00afc\u2032(t) is a\nhorizontal vector for all t, that is, if \u00afc\u2032(t) \u2208 H\u00afc(t) for all t.\nFor a smooth vector field Z \u2208 X(c), we define its horizontal lift \u00afZ by \u00afZ(t) =\nlift\u00afc(t)(Z(t)), and we write \u00afZ = lift(Z) for short. We use similar definitions for\nProjH acting on vector fields of X(\u00afc). It is an exercise to show that smoothness\nis preserved.\nTheorem 9.53. Given a horizontal curve \u00afc: I \u2192 M and the corresponding\nsmooth curve c = \u03c0 \u25e6 \u00afc on the Riemannian quotient manifold M = M/\u223c, the\ncovariant derivative of a vector field Z \u2208 X(c) is given by\nD\ndtZ = D\u03c0(\u00afc)\n\u0014 \u00afD\ndt\n\u00afZ\n\u0015\n, (9.72)\nwhere \u00afZ = lift(Z) is the horizontal lift of Z to the curve \u00afc.\nProof. First of all, this is well defined. Indeed, for anyZ \u2208 X(c), the lift \u00afZ \u2208 X(\u00afc)\nis uniquely defined, its covariant derivative \u00afD\ndt\n\u00afZ is indeed in X(\u00afc), and pushing it\nthrough D\u03c0(\u00afc) produces a specific smooth vector field in X(c). We need to prove\nthat this vector field happens to be D\ndt Z. To this end, we contemplate the three\ndefining properties of D\ndt in Theorem 8.67.\nThe first property, R-linearity in Z, follows easily from linearity of lift, \u00afD\ndt and\nD\u03c0(\u00afc). The second property, the Leibniz rule, does too for similar reasons. More\nwork is needed to verify the chain rule: we must show that, for all U \u2208 X(M),", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2644, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d9717651-a579-4502-ab35-d82ed167cfdf": {"__data__": {"id_": "d9717651-a579-4502-ab35-d82ed167cfdf", "embedding": null, "metadata": {"page_label": "247", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c61eae0f-d1bb-4290-829f-c7072f80d08c", "node_type": "4", "metadata": {"page_label": "247", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "e0411d63f2b39c4577a483dfd060e55d9038f20272be6d8e5225aa62333bf076", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n9.15 Acceleration, geodesics and second-order retractions 247\nthe proposed formula (9.72) satisfies\nD\ndt(U \u25e6 c) = \u2207c\u2032U.\nSince lift(U \u25e6 c) = \u00afU \u25e6 \u00afc, the right-hand side of (9.72) yields\nD\u03c0(\u00afc)\n\u0014 \u00afD\ndt lift(U \u25e6 c)\n\u0015\n= D\u03c0(\u00afc)\n\u0014 \u00afD\ndt\n\u0000\u00afU \u25e6 \u00afc\n\u0001\u0015\n= D\u03c0(\u00afc)\n\u0002\u00af\u2207\u00afc\u2032 \u00afU\n\u0003\n, (9.73)\nwhere we used the chain rule for \u00afD\ndt . Importantly, we now use that \u00afc\u2032 is horizon-\ntal to invoke the pointwise formula for \u2207 (9.48). More specifically, using that\nvertical vectors are in the kernel of D \u03c0(\u00afc) and \u00afc\u2032 = lift(c\u2032) as in (9.71) owing to\nhorizontality, we have\nD\u03c0(\u00afc)\n\u0002\u00af\u2207\u00afc\u2032 \u00afU\n\u0003\n= D\u03c0(\u00afc)\n\u0002\nProjH\u0000\u00af\u2207\u00afc\u2032 \u00afU\n\u0001\u0003\n= \u2207c\u2032U.\nThis concludes the proof.\nUnder the same assumptions, the formula in Theorem 9.53 can be stated\nequivalently as:\nlift\n\u0012 D\ndtZ\n\u0013\n= ProjH\n\u0012 \u00afD\ndt\n\u00afZ\n\u0013\n, (9.74)\nwhich is more informative regarding numerical representation. See Section 9.17\nfor a comment about the horizontality assumption in Theorem 9.53.\nExercise 9.54. Consider a smooth curve \u00afc: I \u2192 M and its projection to M:\nc = \u03c0 \u25e6 \u00afc. With Z a vector field along c, show that Z is smooth if and only\nif \u00afZ = lift(Z) is smooth along \u00afc. Furthermore, show that if \u00afZ is a (not neces-\nsarily horizontal) smooth vector field along \u00afc, then ProjH( \u00afZ) is a (necessarily\nhorizontal) smooth vector field along \u00afc.\n9.15 Acceleration, geodesics and second-order retractions\nThe acceleration c\u2032\u2032 of a smooth curve c on a quotient manifold M is defined\u2014\nas it is in the general case\u2014as the covariant derivative of its velocity. Owing to\nTheorem 9.53, if \u00afc is a horizontal curve related to c by c = \u03c0 \u25e6 \u00afc, and if M is a\nRiemannian quotient manifold of M, then\nc\u2032\u2032(t) = D\ndtc\u2032(t) = D\u03c0(\u00afc(t))\n\u0014 \u00afD\ndt\u00afc\u2032(t)\n\u0015\n= D\u03c0(\u00afc(t))[\u00afc\u2032\u2032(t)] , (9.75)\nwhich we can also write as\nlift\u00afc(t)(c\u2032\u2032(t)) = ProjH\n\u00afc(t)(\u00afc\u2032\u2032(t)) . (9.76)\nIn particular, \u2225c\u2032\u2032(t)\u2225c(t) = \u2225ProjH\n\u00afc(t)\u00afc\u2032\u2032(t)\u2225\u00afc(t) \u2264 \u2225\u00afc\u2032\u2032(t)\u2225\u00afc(t): acceleration can only\ndecrease in going to the quotient.\nRecall that geodesics are curves with zero acceleration. A direct consequence", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2230, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "93e40bf5-ed62-44ed-9369-776cc871e02a": {"__data__": {"id_": "93e40bf5-ed62-44ed-9369-776cc871e02a", "embedding": null, "metadata": {"page_label": "248", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7349a39a-cfd7-4c23-bf17-ed270c770ea0", "node_type": "4", "metadata": {"page_label": "248", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "47ce53e490279b0d1c7a122f38b859b793b761f4d14dec7ba0e960afc0d1bb56", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e921ee1f-6a78-4c7f-84b3-4d10f68e48a3", "node_type": "1", "metadata": {}, "hash": "a03723f988663c382b7472145f64f7fa1a8fdd4cf418e013ede59586ddf77f49", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n248 Quotient manifolds\nof (9.75) is that horizontal geodesics (that is, horizontal curves which are also\ngeodesics) on the total space descend to geodesics on the quotient manifold.\nCorollary 9.55. Let M be a Riemannian quotient manifold of M, with canon-\nical projection \u03c0. If \u00afc: I \u2192 M is a horizontal geodesic on M, then c = \u03c0 \u25e6 \u00afc is\na geodesic on M.\nIn this last corollary, one can show that it is sufficient to have \u00afc be a geodesic\nwith horizontal velocity at any single time (e.g., \u00af c\u2032(0) horizontal) as then it is\nnecessarily horizontal at all times. Anticipating the definition of completeness\nof a manifold (Section 10.1), this notably implies that M is complete if M is\ncomplete [GHL04, Prop. 2.109]. A local converse to Corollary 9.55 also holds. The\nproof (omitted) relies on standard results about ordinary differential equations.\nSee Section 9.17 for a discussion.\nProposition 9.56. Let c: I \u2192 M/ \u223c be a smooth curve on a Riemannian\nquotient manifold such that c\u2032(t0) \u0338= 0, with I an open interval around t0.\n1. For any x0 such that c(t0) = [x0], there exists an open interval J \u2286 I around\nt0 and a unique, smooth curve \u00afc: J \u2192 M such that c|J = \u03c0 \u25e6 \u00afc (that is, \u00afc is\na local lift of c), \u00afc is horizontal, and \u00afc(t0) = x0.\n2. On J, the curve c is a geodesic if and only if \u00afc is a geodesic.\nExample 9.57. Consider the total space of full-rank matrices M = Rd\u00d7n\nd with\nd \u2264 n: an open submanifold of Rd\u00d7n. Consider also the quotient space M = M/\n\u223c with equivalence relation X \u223c Y \u21d0 \u21d2X\u22a4X = Y \u22a4Y , that is, two clouds of\nn labeled points in Rd are equivalent if they have the same Gram matrix. Equiv-\nalence classes are of the form [X] = {QX : Q \u2208 O(d)}, that is, two clouds are\nequivalent if they are the same up to rotation and reflection. Use Theorem 9.18\nto verify that M is a quotient manifold of M. Its points are in one-to-one corre-\nspondence with positive semidefinite matrices of size n and rank d. With the usual\nmetric \u27e8U, V\u27e9X = Tr(U\u22a4V ) on Rd\u00d7n\nd , we can further turn M into a Riemannian\nquotient manifold (use Theorem 9.38).\nGiven X, Y\u2208 Rd\u00d7n\nd , the straight line \u00afc(t) = (1\u2212t)X +tY is a geodesic on [0, 1]\nprovided it remains in Rd\u00d7n\nd . Assuming this is the case, we may further ask: what\ndoes it take for \u00afc to be horizontal? Since the fiber of X is the submanifold {QX :\nQ \u2208 O(d)}, we find that the vertical spaces are VX = {\u2126X : \u2126 \u2208 Skew(d)},\nhence the horizontal spaces are given by HX = {U \u2208 Rd\u00d7n : XU \u22a4 = UX \u22a4}.\nThus, \u00afc\u2032(t) = Y \u2212 X belongs to H\u00afc(t) exactly if the following is symmetric:\n\u00afc(t)\u00afc\u2032(t)\u22a4 = ((1 \u2212 t)X + tY )(Y \u2212 X)\u22a4\n= XY \u22a4\u2212 t(XY \u22a4+ Y X\u22a4) + tY Y\u22a4\u2212 (1 \u2212 t)XX \u22a4.\nThis holds for all t exactly if XY \u22a4 is symmetric.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2919, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e921ee1f-6a78-4c7f-84b3-4d10f68e48a3": {"__data__": {"id_": "e921ee1f-6a78-4c7f-84b3-4d10f68e48a3", "embedding": null, "metadata": {"page_label": "248", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7349a39a-cfd7-4c23-bf17-ed270c770ea0", "node_type": "4", "metadata": {"page_label": "248", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "47ce53e490279b0d1c7a122f38b859b793b761f4d14dec7ba0e960afc0d1bb56", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93e40bf5-ed62-44ed-9369-776cc871e02a", "node_type": "1", "metadata": {"page_label": "248", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "50fc99c4baf13dfa980af48cec75b9eae5060988eda0f763f04be0e91a04c785", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Assuming this is the case, we may further ask: what\ndoes it take for \u00afc to be horizontal? Since the fiber of X is the submanifold {QX :\nQ \u2208 O(d)}, we find that the vertical spaces are VX = {\u2126X : \u2126 \u2208 Skew(d)},\nhence the horizontal spaces are given by HX = {U \u2208 Rd\u00d7n : XU \u22a4 = UX \u22a4}.\nThus, \u00afc\u2032(t) = Y \u2212 X belongs to H\u00afc(t) exactly if the following is symmetric:\n\u00afc(t)\u00afc\u2032(t)\u22a4 = ((1 \u2212 t)X + tY )(Y \u2212 X)\u22a4\n= XY \u22a4\u2212 t(XY \u22a4+ Y X\u22a4) + tY Y\u22a4\u2212 (1 \u2212 t)XX \u22a4.\nThis holds for all t exactly if XY \u22a4 is symmetric. If so, \u00afc is a horizontal geodesic\nand Corollary 9.55 states c(t) = [(1 \u2212 t)X + tY ] is a geodesic on M.\nWhat is the significance of the condition XY \u22a4 = Y X\u22a4? Consider the Eu-\nclidean distance between QX and Y in the total space, where Q \u2208 O(d) remains", "mimetype": "text/plain", "start_char_idx": 2426, "end_char_idx": 3173, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1286d26d-c1fa-4703-819f-5c44ec569293": {"__data__": {"id_": "1286d26d-c1fa-4703-819f-5c44ec569293", "embedding": null, "metadata": {"page_label": "249", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5c49fe42-d58b-4c45-a243-4d8a016ac7d8", "node_type": "4", "metadata": {"page_label": "249", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "a948a5bce8c70faa97c2b86b4614803666a4d707c06a8c4accb03795e3bf1159", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n9.15 Acceleration, geodesics and second-order retractions 249\nunspecified for now:\n\u2225QX \u2212 Y \u22252 = \u2225X\u22252 + \u2225Y \u22252 \u2212 2 Tr(Q\u22a4Y X\u22a4).\nIt can be shown that this distance is minimized with respect to Q \u2208 O(d) if\nQ\u22a4Y X\u22a4 is symmetric and positive semidefinite. Specifically, if Y X\u22a4 = U\u03a3V \u22a4 is\nan SVD decomposition, then the minimum is attained by Q = UV \u22a4 (the polar\nfactor of Y X\u22a4). Replacing X by QX (which does not change its equivalence\nclass) \u201caligns\u201d X to Y in the sense that \u2225X \u2212 Y \u2225 is minimized.\nAssume X and Y are aligned as described, so that XY \u22a4 = Y X\u22a4 \u2ab0 0. Since X\nand Y have full rank, we see that\n\u00afc(t)\u00afc(t)\u22a4 = (1 \u2212 t)2XX \u22a4+ t(1 \u2212 t)(XY \u22a4+ Y X\u22a4) + t2Y Y\u22a4\nis positive definite for all t \u2208 [0, 1]. In other words: if X and Y are aligned, the\nstraight line \u00afc(t) connecting them indeed remains in Rd\u00d7n\nd for t \u2208 [0, 1] and it is\nhorizontal. Since \u00afc is a horizontal geodesic, c is a geodesic too.\nAnticipating concepts of length and distance from Section 10.1, we claim that\nthe length of \u00afc on [0, 1] is \u2225Y \u2212 X\u2225, and that c has the same length as \u00afc. Since\nno shorter curve connects the same end points, the Riemannian distance between\nthe equivalence classes [X] and [Y ] is nothing but the Euclidean distance between\ntheir best aligned representatives. See also Exercise 10.15.\nMassart and Absil discuss the geometry of M in detail [MA20]. The points\nof M (that is, the equivalence classes of M) are one-to-one with the positive\nsemidefinite matrices of size n and rank d. The latter form an embedded sub-\nmanifold of the symmetric matrices of size n. The Riemannian metric for that\nmanifold as constructed here on M is called the Bures\u2013Wasserstein metric when\nd = n (sometimes also when d \u2264 n by extension). It is different from the Rie-\nmannian submanifold metric for that same manifold [VAV09].\nAnother direct consequence of (9.75) is that second-order retractions (Defini-\ntion 8.69) on M which satisfy condition (9.28) and whose curves are horizontal\nyield second-order retractions on the quotient manifold.\nCorollary 9.58. Let R be a retraction on M such that R as defined by (9.27)\nis a retraction on the Riemannian quotient manifold M = M/\u223c. If R is second\norder and its retraction curves,\n\u00afc(t) = Rx(tu),\nare horizontal for every x and every u \u2208 Hx, then R is second order on M.\nProof. The retraction on the quotient manifold generates curves\nc(t) = R[x](t\u03be) =\n\u0002\nRx(tu)\n\u0003\nwith u = liftx(\u03be); hence, c = \u03c0 \u25e6\u00afc. Since \u00afc is horizontal, we may apply (9.75) and\nevaluate at t = 0. (This also makes it clear that \u00afc needs only be horizontal in a\nneighborhood of t = 0.)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2834, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dd3c5156-76e8-4167-ae14-6d1473cfc6ee": {"__data__": {"id_": "dd3c5156-76e8-4167-ae14-6d1473cfc6ee", "embedding": null, "metadata": {"page_label": "250", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f97c04f7-7741-4c83-bf74-c3fac0856c3d", "node_type": "4", "metadata": {"page_label": "250", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "57ced77e64bd0972ae8dd24cb06394c27bd5365acab95e039e44d47c262f0204", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n250 Quotient manifolds\nExample 9.59. Recall the polar retraction (7.24) on St(n, p):\nRX(U) = (X + U)\n\u0000\nIp + U\u22a4U\n\u0001\u22121/2\n. (9.77)\nThis is a second-order retraction. We already checked condition (9.28) for it, so\nthat it yields a retraction on the quotient manifold Gr(n, p) = St(n, p)/O(p). Fur-\nthermore, the retraction curves are horizontal. Indeed, for any U \u2208 HX (meaning\nX\u22a4U = 0), consider the curve\n\u00afc(t) = RX(tU) = (X + tU)\n\u0000\nIp + t2U\u22a4U\n\u0001\u22121/2\nand its velocity\n\u00afc\u2032(t) = U\n\u0000\nIp + t2U\u22a4U\n\u0001\u22121/2\n+ (X + tU) d\ndt\n\u0010\u0000\nIp + t2U\u22a4U\n\u0001\u22121/2\u0011\n.\nThis curve is horizontal if, for all t, the matrix\n\u00afc(t)\u22a4\u00afc\u2032(t) =\n\u0000\nIp + t2U\u22a4U\n\u0001\u22121/2\n(tU\u22a4U)\n\u0000\nIp + t2U\u22a4U\n\u0001\u22121/2\n+\n\u0000\nIp + t2U\u22a4U\n\u0001+1/2 d\ndt\n\u0010\u0000\nIp + t2U\u22a4U\n\u0001\u22121/2\u0011\nis zero. Replace U\u22a4U with its eigendecomposition V DV\u22a4, with V \u2208 O(p) and D\ndiagonal: the right-hand side is diagonal in the basis V , and it is a simple exercise\nto conclude that it is indeed identically zero. As a result, the polar retraction is\nhorizontal on St(n, p) and, when used as a retraction on the Grassmann manifold\nGr(n, p), it is second order as well.\n9.16 Grassmann manifold: summary*\nFor convenience, this section collects the various tools we have constructed\nthroughout the chapter to work on the Grassmann manifold Gr( n, p). Equiv-\nalent constructions appear early in [EAS98].\nWe view Gr(n, p) as a Riemannian quotient manifold of the Stiefel manifold\nSt(n, p), itself a Riemannian submanifold of Rn\u00d7p with the usual trace inner\nproduct \u27e8A, B\u27e9 = Tr(A\u22a4B). (See the end of this section for an embedded view-\npoint.) The orthogonal group O(p) acts on St(n, p) as (X, Q) 7\u2192 XQ, so that the\nprojection\n\u03c0 : St(n, p) \u2192 Gr(n, p): X 7\u2192 \u03c0(X) \u225c [X] = {XQ : Q \u2208 O(p)}\nis surjective and smooth from St( n, p) to Gr( n, p) = St(n, p)/O(p), and its dif-\nferentials D\u03c0(X) are surjective. The dimension of Gr( n, p) is\ndim Gr(n, p) = dim St(n, p) \u2212 dim O(p) = p(n \u2212 p).\nA point [ X] on Gr( n, p) is represented by a matrix X \u2208 St(n, p) (an arbitrary\nrepresentative of the equivalence class [ X]).\nFor an arbitrary manifold M, a map F : Gr(n, p) \u2192 Mis smooth if and only", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2321, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5a352a3e-c2cb-485f-b20e-623314290df8": {"__data__": {"id_": "5a352a3e-c2cb-485f-b20e-623314290df8", "embedding": null, "metadata": {"page_label": "251", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c6859032-32f1-4703-93e8-7b9e6c95453e", "node_type": "4", "metadata": {"page_label": "251", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "77ae92344919fa8b06df16e1b12687dc649f8d06883fcdf0043290a3350a7148", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n9.16 Grassmann manifold: summary* 251\nif F \u25e6 \u03c0 : St(n, p) \u2192 Mis smooth. Likewise, a map G: M \u2192Gr(n, p) is smooth\nat x \u2208 Mif and only if there exists a neighborhood U of x on M and a map\n\u00afG: U \u2192St(n, p) (smooth at x) such that G|U = \u03c0 \u25e6 \u00afG. As usual, G is smooth if\nit is so at all points.\nGiven X \u2208 St(n, p), the tangent space T XSt(n, p) splits in two components,\northogonal for the inner product \u27e8A, B\u27e9X = Tr(A\u22a4B):\nTXSt(n, p) = VX + HX, VX = {X\u2126 : \u2126 \u2208 Skew(p)},\nHX = {U \u2208 Rn\u00d7p : X\u22a4U = 0}.\nThe orthogonal projectors to VX and HX (both from Rn\u00d7p and from TXSt(n, p))\ntake the form\nProjV\nX(Z) = X skew(X\u22a4Z) = X X\u22a4Z \u2212 Z\u22a4X\n2 , (9.78)\nProjH\nX(Z) = (In \u2212 XX \u22a4)Z = Z \u2212 X(X\u22a4Z). (9.79)\nThe vertical space V X is the tangent space to the fiber \u03c0\u22121(\u03c0(X)). The hori-\nzontal space HX is one-to-one with the tangent space of Gr( n, p) at [X] via the\ndifferential of \u03c0; its inverse is the horizontal lift:\nD\u03c0(X)|HX : HX \u2192 T[X]Gr(n, p),\nliftX = (D\u03c0(X)|HX )\u22121 : T[X]Gr(n, p) \u2192 HX.\nIf [X] \u2208 Gr(n, p) is represented by X \u2208 St(n, p), then we represent a tangent\nvector \u03be \u2208 T[X]Gr(n, p) with the (unique) matrix U \u2208 HX such that D\u03c0(X)[U] =\n\u03be, or equivalently, such that U = liftX(\u03be). We could of course represent [X] with\na different matrix; for example, with XQ where Q \u2208 O(p) is arbitrary. The\nhorizontal lifts of \u03be at X and at XQ are related by\nliftXQ(\u03be) = liftX(\u03be)Q.\n(See Example 9.26.)\nSay F : Gr(n, p) \u2192 Mis smooth, so that \u00afF = F \u25e6\u03c0 : St(n, p) \u2192 Mis smooth.\nStill equivalently, there exists a smooth extension \u00af\u00afF of \u00afF to a neighborhood U\nof St(n, p) in Rn\u00d7p such that \u00afF = \u00af\u00afF|St(n,p). For all (X, U) \u2208 TSt(n, p), we have\nD \u00af\u00afF(X)[U] = D \u00afF(X)[U] = DF([X])[D\u03c0(X)[U]]. (9.80)\nStated the other way around, this means that if ([ X], \u03be) \u2208 TGr(n, p) is repre-\nsented by (X, U) with U = liftX(\u03be), then\nDF([X])[\u03be] = D \u00afF(X)[U] = D \u00af\u00afF(X)[U]. (9.81)\nNow, say G: M \u2192Gr(n, p) is smooth at x \u2208 M, that is, there exists a neighbor-\nhood U of x on M and a smooth map \u00afG: U \u2192St(n, p) such that G|U = \u03c0 \u25e6 \u00afG.\nThen, for all v \u2208 TxM we have\nDG(x)[v] = D\u03c0( \u00afG(x))[D \u00afG(x)[v]]. (9.82)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2312, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7800b59c-ca27-460a-91d7-d67358ef8b74": {"__data__": {"id_": "7800b59c-ca27-460a-91d7-d67358ef8b74", "embedding": null, "metadata": {"page_label": "252", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bae0c8b0-fd27-4c32-bc8e-d95641bc5c7d", "node_type": "4", "metadata": {"page_label": "252", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "62aaeee7f6434cb449ae096ed1645953587a5f932423446a1f14ad390d1f6da4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n252 Quotient manifolds\nAccordingly, the lift of DG(x)[v] \u2208 TG(x)Gr(n, p) to the horizontal space at \u00afG(x)\nis given by:\nlift \u00afG(x)(DG(x)[v]) = ProjH\n\u00afG(x)(D \u00afG(x)[v]). (9.83)\nWe turn Gr( n, p) into a Riemannian manifold as a Riemannian quotient by\npushing the metric from St( n, p) to Gr( n, p) through \u03c0. This turns D \u03c0(X)|HX\nand liftX into isometries. Explicitly, for all\u03be, \u03b6\u2208 T[X]Gr(n, p), we may choose an\narbitrary representative X of [X] and lift \u03be, \u03b6as U = liftX(\u03be) and V = liftX(\u03b6);\nthen:\n\u27e8\u03be, \u03b6\u27e9[X] = \u27e8U, V\u27e9X = Tr(U\u22a4V ).\nThis structure provides gradients. Explicitly, if f : Gr(n, p) \u2192 R is smooth, then\n\u00aff = f \u25e6 \u03c0 : St(n, p) \u2192 R is smooth and it has a smooth extension \u00af\u00aff to a neigh-\nborhood of St( n, p) in Rn\u00d7p. Their gradients are related as follows:\nliftX(gradf([X])) = grad \u00aff(X) = (In \u2212 XX \u22a4)grad \u00af\u00aff(X). (9.84)\nLet us turn to the Riemannian connection on Gr( n, p). Say W is a smooth\nvector field on Gr(n, p), meaning \u00afW = lift(W) is smooth on St(n, p). Recall that\nthe Riemannian connection \u00af\u2207 on St(n, p) is given by \u00af\u2207U \u00afW = ProjX(D \u00afW(X)[U])\nfor all U \u2208 TXSt(n, p), where Proj X is the orthogonal projector from Rn\u00d7p to\nTXSt(n, p) (and D \u00afW(X)[U] = D \u00af\u00afW(X)[U] using any smooth extension \u00af\u00afW of \u00afW\nto a neighborhood of St( n, p) in Rn\u00d7p). Then, we reason from (9.43) that the\nRiemannian connection \u2207 on Gr(n, p) satisfies:\nliftX(\u2207\u03beW) = ProjH\nX\n\u0000\u00af\u2207U \u00afW\n\u0001\n= ProjH\nX\n\u0000\nD \u00afW(X)[U]\n\u0001\n, (9.85)\nwhere U = liftX(\u03be).\nWe can specialize the above discussion to Hessians, with W = grad f and\n\u00afW = lift(gradf) = grad \u00aff, using \u00aff = f \u25e6 \u03c0. Then, with \u00af\u00aff a smooth extension of\n\u00aff as above and with U = liftX(\u03be), we have\nliftX(Hessf([X])[\u03be]) = ProjH\nX\n\u0000\nHess \u00aff(X)[U]\n\u0001\n(9.86)\n= (In \u2212 XX \u22a4)Hess \u00af\u00aff(X)[U] \u2212 UX \u22a4grad \u00af\u00aff(X).\n(To obtain the last expression, it is useful to refer to (7.29) for Hessians on\nSt(n, p), noting that the vertical part of grad \u00af\u00aff(X) is zero.) Treating the Hessian\nas a quadratic map yields\n\u27e8\u03be, Hessf([X])[\u03be]\u27e9[X] =\n\nU, Hess \u00aff(X)[U]\n\u000b\n= \u27e8U, Hess \u00af\u00aff(X)[U] \u2212 UX \u22a4grad \u00af\u00aff(X)\u27e9, (9.87)\nstill with U = liftX(\u03be).\nSeveral retractions on St(n, p) descend to well-defined retractions on Gr(n, p),", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2376, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "60770af3-e928-4ef7-a7a5-ce103f403bd6": {"__data__": {"id_": "60770af3-e928-4ef7-a7a5-ce103f403bd6", "embedding": null, "metadata": {"page_label": "253", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea4b7b0a-1ced-4d51-b85d-de7c2cd9d3d3", "node_type": "4", "metadata": {"page_label": "253", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "d8e7f7f58a243f24403ac940254ec1c4ee423448c93d61e9418ed9dadbf5fd34", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n9.16 Grassmann manifold: summary* 253\nincluding the QR-based (7.22) and the polar-based (7.24) retractions:\nRQR\n[X](\u03be) = [qfactor(X + liftX(\u03be))] , and (9.88)\nRpol\n[X](\u03be) = [pfactor(X + liftX(\u03be))] . (9.89)\nWhat is more, the polar retraction is second order. We build a transporter com-\npatible with that retraction in Example 10.67. Expressions for the geodesics of\nGr(n, p) (hence for its exponential map) and for parallel transport along geodesics\nappear in [EAS98, Thms 2.3, 2.4].\nAn embedded geometry for Gr(n, p).\nIt is instructive to know that the Riemannian quotient geometry detailed above\ncan also be realized as a Riemannian submanifold of a Euclidean space [MS85,\nAMT13, SI14], [BH15, Def. 2.3, \u00a7 4.2], [BZA20, LLY20]. Interestingly, working\nout efficient numerical tools to optimize on the Grassmannian with either per-\nspective involves essentially the same steps: below, we only show equivalence of\nthe geometries without numerical concerns.\nExplicitly, consider the set\nM = {P \u2208 Rn\u00d7n : P = P\u22a4, P2 = P and Tr(P) = p}. (9.90)\nEach matrix in M is an orthogonal projector to a subspace of dimension p in\nRn. It can be shown that M is a smooth embedded submanifold of Rn\u00d7n of\ndimension p(n \u2212 p): that is the same dimension as Gr( n, p). It is easy to check\nthat\n\u03c6: Gr(n, p) \u2192 M: [X] 7\u2192 \u03c6([X]) = XX \u22a4 (9.91)\nis well defined. It is also smooth. Indeed, the map \u03d5: St(n, p) \u2192 Rn\u00d7n defined\nby \u03d5(X) = XX \u22a4 is clearly smooth, and \u03d5 = \u03c6\u25e6\u03c0 where \u03c0 is the quotient map of\nGr(n, p). One can further check that \u03c6 is bijective, so that \u03c6\u22121 is well defined.\nEndow Rn\u00d7n with the trace inner product \u27e8A, B\u27e9Rn\u00d7n\n= Tr(A\u22a4B), and let M\nbe a Riemannian submanifold of Rn\u00d7n with that metric: we write \u27e8A, B\u27e9M\nP =\n\u27e8A, B\u27e9Rn\u00d7n\nfor the inner product on T P M. Consider two arbitrary tangent vec-\ntors \u03be, \u03b6\u2208 T[X]Gr(n, p) and their horizontal liftsU, V\u2208 HX. Compute D\u03d5(X)[U]\nin two different ways: directly as\nD\u03d5(X)[U] = UX \u22a4+ XU \u22a4,\nthen also via the chain rule as\nD\u03d5(X)[U] = D(\u03c6 \u25e6 \u03c0)(X)[U] = D\u03c6(\u03c0(X))[D\u03c0(X)[U]] = D\u03c6([X])[\u03be].\nThus,\nD\u03c6([X])[\u03be] = UX \u22a4+ XU \u22a4. (9.92)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2303, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "80ce37bb-468f-4285-b907-514dfa102301": {"__data__": {"id_": "80ce37bb-468f-4285-b907-514dfa102301", "embedding": null, "metadata": {"page_label": "254", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0eaff0a8-fbd9-4bf3-99bc-947e0effa6ef", "node_type": "4", "metadata": {"page_label": "254", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "d549d0d3e40884dd4a1f4b431b0156a836b5a5ace29473a3580d793ea0a5040f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n254 Quotient manifolds\nOwing to X\u22a4X = Ip and X\u22a4U = X\u22a4V = 0, it follows that\n\u27e8D\u03c6([X])[\u03be], D\u03c6([X])[\u03b6]\u27e9M\n\u03c6([X]) = \u27e8UX \u22a4+ XU \u22a4, V X\u22a4+ XV \u22a4\u27e9Rn\u00d7n\n= 2 \u27e8U, V\u27e9St(n,p)\nX\n= 2 \u27e8\u03be, \u03b6\u27e9Gr(n,p)\n[X] , (9.93)\nwhere \u27e8\u00b7, \u00b7\u27e9St(n,p) and \u27e8\u00b7, \u00b7\u27e9Gr(n,p) denote the Riemannian metrics on St( n, p) and\nGr(n, p), respectively.\nWe have found that D\u03c6([X]) is (up to a factor\n\u221a\n2) a linear isometry between\nT[X]Gr(n, p) and T\u03c6([X])M. The inverse function theorem then implies that \u03c6 is\na local diffeomorphism around each point. Since\u03c6 is also invertible, it follows that\n\u03c6 is a global diffeomorphism: Gr( n, p) and M have the same smooth geometry.\nMoreover, their Riemannian metrics are identical up to a factor\n\u221a\n2. Thus, the\nRiemannian geometries of Gr( n, p) and M are equivalent.\nSee [BZA20] for more about these and other representations of the Grassmann\nmanifold, including discussions about equivalences and numerical aspects.\n9.17 Notes and references\nThe main source for this chapter is the book by Absil et al. [AMS08, \u00a7 3 and\n\u00a7 5], which gives an original and concise treatment of quotient manifolds for Rie-\nmannian optimization, with an emphasis on generality and practicality. The main\nsources for differential geometric aspects and proofs are the differential geometry\nbooks by Brickell and Clark [BC70, \u00a7 6] and Lee [Lee12, \u00a7 4 and \u00a7 21]. O\u2019Neill pro-\nvides useful results regarding connections on Riemannian submersions [O\u2019N83,\npp212\u2013213], as do Gallot et al. [GHL04]. The recent book by Gallier and Quain-\ntance [GQ20] offers an in depth look at the geometry of Lie groups and manifolds\narising through group actions.\nHere are further references for results above which included a proof: Theo-\nrem 9.21 follows [Lee12, Thm. 4.29]; Theorem 9.27 follows [Lee18, Prop. 2.25];\nTheorem 9.33 follows [AMS08, Prop. 4.1.3]; Theorem 9.38 appears in [Lee18,\nCor. 2.29]; Proposition 9.42 appears in [Lee18, p146]; Theorem 9.43 appears\nin [O\u2019N83, Lem. 7.45]; and Corollary 9.55 appears in [O\u2019N83, Lem. 7.46]. More-\nover, Exercise 9.44 echoes [GHL04, Prop. 3.35], [dC92, Ex. 8.9] and [Lee18, p146].\nMany results hold generally for the case where M and M are smooth man-\nifolds (not necessarily related by an equivalence relation) and \u03c0 : M \u2192 Mis\na submersion (it is smooth and its differential at each point is surjective) or a\nRiemannian submersion (its differential at each point is an isometry once re-\nstricted to a horizontal space). Reference books often state results at this level\nof generality. In contrast, we also require \u03c0 to be surjective (it is a quotient\nmap). For example, while \u03c0 : R2\\{0} \u2192R2 (both with their usual Riemannian\nstructures) defined by \u03c0(x) = x is a Riemannian submersion, it is not a quotient", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2957, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a2ac7bbe-0ac4-496b-aafb-a6cc411194bd": {"__data__": {"id_": "a2ac7bbe-0ac4-496b-aafb-a6cc411194bd", "embedding": null, "metadata": {"page_label": "255", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a1164754-9806-42e9-a6a5-e73ee406388e", "node_type": "4", "metadata": {"page_label": "255", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "5cc958976f79402431b1b1176818a7202cb9336f6ab3ee39cd3a3da9a11cb6c5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n9.17 Notes and references 255\nmap because it is not surjective: we exclude such cases. Certain results that do\nnot hold for general submersions may hold for surjective submersions.\nAn advantage of Brickell and Clark\u2019s treatment is that they define smooth\nmanifolds without topological restrictions (recall Section 8.2). As a result, the\nrole of the two topological properties (Hausdorff and second countability) is\napparent throughout their developments. This proves helpful here, considering\nthe fact that certain quotient spaces fail to be quotient manifolds specifically\nbecause their quotient topologies fail to have these properties.\nFor a full characterization of when a quotient space is or is not a quotient\nmanifold, see [AMS08, Prop. 3.4.2]. In this chapter, we presented a necessary\ncondition (Proposition 9.3, which implies fibers of a quotient manifold must all be\nsubmanifolds of the same dimension), and a sufficient condition (Theorem 9.18,\nwhich states free, proper and smooth group actions yield quotient manifolds).\nFor a partial converse to the latter, see [Lee12, Pb. 21-5].\nIn Definition 9.24, we summon a Riemannian structure on the total space to\ndefine horizontal spaces. Technically, that structure is not required: one could\njust as well define H x to be any subspace of T xM such that the direct sum of\nVx and Hx coincides with TxM, and still obtain that the restriction of D\u03c0(x) to\nHx is a bijection to T [x]M. For practical purposes, it is then useful to arrange\nthe choice of H x to vary smoothly with x, leading to a horizontal distribution .\nHowever, this leaves a lot of freedom that we do not need. We opt for a more\ndirective (and quite common) definition of horizontal space, while noting that\nother authors use the terminology in a broader sense [AMS08, \u00a7 3.5.8].\nWhen the group acting on the total space is not compact, it may be delicate\nto determine whether its action is proper. The following characterization may\nhelp in this regard [Lee12, Prop. 21.5].\nProposition 9.60. Let G be a Lie group acting smoothly on a manifold M. The\nfollowing are equivalent:\n1. The action \u03b8 is proper.\n2. If x0, x1, x2, . . .is a sequence on M and g0, g1, g2, . . .is a sequence on G such\nthat both {xk}k=0,1,2,... and {\u03b8(gk, xk)}k=0,1,2,... converge, then a subsequence\nof {gk}k=0,1,2,... converges.\n3. For every compact K \u2286 M, the set GK = {g \u2208 G: \u03b8(g, K) \u2229 K \u0338= \u2205} is\ncompact.\nThe smoothness criterion for vector fields on quotient manifolds given in The-\norem 9.27 is an exercise in do Carmo\u2019s book [dC92, Ex. 8.9, p186] and is linked\nto the concept of \u03c0-related vector fields [Lee12, Pb. 8-18c]. The main difference\nfor the latter is that Theorem 9.27 states results with respect to the special\nhorizontal distribution we chose (emanating from the Riemannian metric on the\ntotal space), whereas results regarding \u03c0-related vector fields often focus on the\nhorizontal distribution tied to charts in normal form (Proposition 9.5).\nIn the proof of Theorem 9.53, one may wonder what goes wrong if \u00afc is not hor-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3294, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "08ab5769-c73c-4cd8-8a7e-15995922bad9": {"__data__": {"id_": "08ab5769-c73c-4cd8-8a7e-15995922bad9", "embedding": null, "metadata": {"page_label": "256", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b5216b80-5103-48f6-81af-14b94e50a4a9", "node_type": "4", "metadata": {"page_label": "256", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "d41d2bdff1ab92c9a22929d96552a69120800c14dfe66ac7bd7e73a3540748c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n256 Quotient manifolds\nizontal. In that case, to proceed from (9.73) we separate \u00afc\u2032(t) into its horizontal\nand vertical parts, and we use linearity:\nD\u03c0(\u00afc)\n\u0002\u00af\u2207\u00afc\u2032 \u00afU\n\u0003\n= D\u03c0(\u00afc)\n\u0002\u00af\u2207ProjH(\u00afc\u2032) \u00afU\n\u0003\n+ D\u03c0(\u00afc)\n\u0002\u00af\u2207ProjV(\u00afc\u2032) \u00afU\n\u0003\n= \u2207c\u2032U + D\u03c0(\u00afc)\n\u0002\u00af\u2207ProjV(\u00afc\u2032) \u00afU\n\u0003\n. (9.94)\nFor the horizontal part, the same argument as in the proof of Theorem 9.53 based\non (9.48) and (9.71) still applies, yielding the first term. The second term though,\ndoes not vanish in general. This is because, in general, \u00afU is not \u201cconstant\u201d along\nfibers: the lift of U([x]) at x need not be the \u201csame\u201d as its lift at y \u223c x. (To make\nsense of the quoted terms, see the notion of parallel vector fields in Section 10.3.)\nWe verify this on an example. Consider Gr( n, p) = St(n, p)/O(p). We know a\nhorizontally lifted vector field on St(n, p): take for example \u00afU(X) = grad \u00aff(X) =\nAX \u2212X(X\u22a4AX), where \u00aff(X) = 1\n2 Tr(X\u22a4AX). Furthermore, any vertical vector\nat X is of the form X\u2126 for some \u2126 \u2208 Skew(p). Then, using the formula for the\nconnection \u00af\u2207 on St(n, p) (5.4),\n\u00af\u2207X\u2126 \u00afU = ProjX\n\u0000\nAX\u2126 \u2212 X\u2126(X\u22a4AX) \u2212 X(\u2126\u22a4X\u22a4AX + X\u22a4AX\u2126)\n\u0001\n= (In \u2212 XX \u22a4)AX\u2126,\nwhere ProjX is the orthogonal projector to T XSt(n, p) (7.27). To our point, this\nvector is horizontal, and it can be nonzero, hence the vector D \u03c0(X)[ \u00af\u2207X\u2126 \u00afU] can\nbe nonzero.\nIt is useful to add a word about Proposition 9.56: this concerns the possibility\nof horizontally lifting curves c: I \u2192 Mfrom the quotient manifold to curves \u00afc\non the total space. That this can be done locally (meaning that we can obtain a\nhorizontal lift \u00afc defined on an open interval around anyt0 \u2208 I) is relatively direct,\ninvoking standard results from ordinary differential equations (ODE) [Lee12,\nThm. 9.12].\nThe argument goes like this: if c\u2032(t0) \u0338= 0, then there exists an interval J \u2286 I\naround t0 such that c(J) is an embedded submanifold of M. As a result, it is\npossible to extend the smooth vector field c\u2032 on c(J) to a smooth vector field V\ndefined on a neighborhood U of c(J). It satisfies V (c(t)) = c\u2032(t) for all t \u2208 J. Pick\nx0 \u2208 M such that c(t0) = [x0], and consider the ODE below whose unknown is\nthe curve \u03b3 on U:\n\u03b3\u2032(t) = V (\u03b3(t)), \u03b3 (t0) = [x0].\nClearly, \u03b3(t) = c(t) is a solution for t \u2208 J. Since solutions of ODEs are unique,\nwe deduce that \u03b3|J = c|J. Now we turn to constructing horizontal lifts. Consider\n\u00afV = lift(V ): this is a smooth vector field on U = \u03c0\u22121(U). We can again write\ndown an ODE:\n\u00afc\u2032(t) = \u00afV (\u00afc(t)), \u00afc(t0) = x0.\nThere exist an open interval J\u2032 around t0 and a solution \u00afc: J\u2032 \u2192 U, smooth and", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2779, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d2ea0a61-1d3d-4591-9f29-38d28fa41419": {"__data__": {"id_": "d2ea0a61-1d3d-4591-9f29-38d28fa41419", "embedding": null, "metadata": {"page_label": "257", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f92cccab-29d5-40ba-bf6f-f614dcebd768", "node_type": "4", "metadata": {"page_label": "257", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "5cc1e788304da6d668b043bebe51dd27504cd4428574b2ab35e309d3cf7e79ba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n9.17 Notes and references 257\nunique. Clearly, \u00afc is horizontal because \u00afc\u2032(t) is a horizontal vector by construc-\ntion. If we project \u00afc to the quotient, then we get a curve \u03b3 = \u03c0 \u25e6 \u00afc. Notice that\n\u03b3(t0) = [x0] and\n\u03b3\u2032(t) = D\u03c0(\u00afc(t))[\u00afc\u2032(t)] = D\u03c0(\u00afc(t))[ \u00afV (\u00afc(t))] = V ([\u00afc(t)]) = V (\u03b3(t)).\nThus, \u03b3 satisfies the first ODE we considered, and we conclude that \u03c0 \u25e6 \u00afc =\n\u03b3 = c|J\u2032. In words: \u00afc is a horizontal lift of c on the interval J\u2032. Moreover, the\nlifted curve depends smoothly on the choice of representative x0 \u2208 c(t0) because\nsolutions of smooth ODEs depend smoothly not only on time but also on initial\nconditions.\nThis argument and a few more elements form the basis of the proof of Propo-\nsition 9.56 presented by Gallot et al. [GHL04, Prop. 2.109], where the emphasis\nis on the case where c is a geodesic.\nIt is natural to ask: if c is defined on the interval I, can we not lift it to a\nhorizontal curve \u00afc also defined on all of I? The argument above is not sufficient\nto reach this stronger conclusion, in part because it only uses the fact that \u03c0 is\na Riemannian submersion: it does not use the fact that \u03c0 is surjective. Hence,\nas Gallot et al. point out, we might be in the case where \u03c0 : R2\\{0} \u2192R2 is\nthe map \u03c0(x) = x between the punctured plane and the plane, both equipped\nwith their usual Riemannian metrics. This is indeed a Riemannian submersion,\nbut it is not surjective (in particular, it is not a quotient map). It is clear that\na geodesic (a straight line) through the origin in R2 cannot be lifted entirely to\nR2\\{0}.\nThus, at the very least, we should require \u03c0 to be surjective (which it is in the\nsetting of quotient manifolds). Unfortunately, that is not sufficient. John M. Lee\nshares3 a counter-example with M = (\u22121, 1) \u00d7 R and M = S1 as Riemannian\nsubmanifolds of R2. Consider the map \u03c0(x, y) = (cos(2\u03c0x), sin(2\u03c0x)). This is in-\ndeed a surjective Riemannian submersion from M to M. Yet, consider the curve\nc: (\u22122, 2) \u2192 Mdefined by c(t) = (cos(2\u03c0t), sin(2\u03c0t)). Its unique horizontal lift\nsatisfying \u00afc(0) = (0, 0) is given by \u00afc(t) = (t, 0): evidently, \u00afc can only be extended\nup to (\u22121, 1), not up to ( \u22122, 2).\nFortunately, there exist several sufficient conditions. One can read the following\nabout that question in a classic book by Besse [Bes87, \u00a7 9.E]. Recall that if M\nis a Riemannian quotient manifold of M then the canonical projection \u03c0 is a\nsurjective Riemannian submersion. Completeness is defined in Section 10.1. If\nM is not connected, apply the claim to each complete connected component.\nProposition 9.61. Let \u03c0 : M \u2192 Mbe a surjective Riemannian submersion.\nIf M is connected and complete, then M is also connected and complete, all\nfibers are complete (but not necessarily connected), and for every smooth curve\nc: I \u2192 Mwith non-vanishing velocity and for every x0 \u2208 c(t0) there exists a\nunique horizontal lift \u00afc: I \u2192 M such that c = \u03c0 \u25e6 \u00afc and \u00afc(t0) = x0. Moreover, c\nis a geodesic if and only if \u00afc is a geodesic.\n3 math.stackexchange.com/questions/3524475", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3269, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "87062351-14ce-4d5c-91df-3f5bd151dfee": {"__data__": {"id_": "87062351-14ce-4d5c-91df-3f5bd151dfee", "embedding": null, "metadata": {"page_label": "258", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "96afeab7-28ca-494a-9bfa-b5aa49aed86d", "node_type": "4", "metadata": {"page_label": "258", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "cab2266d7cc5d70144fb817e74920541d470421ad85fa46cb23838b9550ff44f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n258 Quotient manifolds\nIf \u03c0 has this property, namely, that all curves with non-vanishing velocity\ncan be lifted horizontally on their whole domain and made to pass through\nany representative of the equivalence class at some initial point, we say it is\nEhresmann complete . In this same reference, it is also noted that when this\nproperty holds, then \u03c0 is a smooth fiber bundle. Furthermore, the property may\nhold without M being complete.\nHere is a special case of interest. If the quotient is obtained as per Theorem 9.18\nthrough a smooth, free and proper Lie group action on a smooth manifold, then\nit also forms a fiber bundle [Lee12, Pb. 21-6]\u2014in that case, the fiber bundle is\nalso called a principal G-bundle, and the standard fiber is the Lie group itself. It\ncan be shown that if (but not only if) the standard fiber is compact, then the\nfiber bundle is Ehresmann complete [Mic08, pp204\u2013206]. This is summarized as\nfollows.\nProposition 9.62. If G is a compact Lie group acting smoothly and freely on\nM, then M = M/G is a quotient manifold, and it has the property that any\nsmooth curve c: I \u2192 Mwith non-vanishing velocity can be lifted to a unique\nhorizontal curve \u00afc: I \u2192 M passing through any x0 \u2208 c(t0) at t0. Moreover, c is\na geodesic if and only if \u00afc is a geodesic.\nSee also [Mic08, Lem. 26.11] and [KN63, Prop. II.3.1, p69]. Thanks to P.-A.\nAbsil, John M. Lee, Mario Lezcano-Casado and Estelle Massart for discussions\non this topic.\nRegarding Exercise 9.51: That second-order critical points are global optimiz-\ners is shown in [SI14, Prop. 3.4, Prop. 4.1] under the assumption that there is\na gap between the pth and (p + 1)st smallest eigenvalues of A. With some care,\nthe eigengap assumption can be removed (this is part of the exercise). The other\nclaims in the exercise are standard. It is also possible to control the spectrum of\nthe Hessian at approximate critical points [LTW22].\nWe close with a proof of Proposition 9.6 which provides strong links between\nthe salient points of optimization problems related through a map \u03c0 (which may\nor may not be a quotient map).\nProof of Proposition 9.6. Let us verify each claim in turn:\n1. The range of real values attained by \u00aff is \u00aff(M) = f(\u03c0(M)), and \u03c0(M) = M\nsince \u03c0 is surjective. Thus, \u00aff(M) = f(M).\n2. If x is a local minimizer of \u00aff, there exists a neighborhood U of x on M such\nthat \u00aff(x) = inf \u00aff(U). Since \u03c0 is open, the set U = \u03c0(U) is open, and it\ncontains \u03c0(x). Thus, U is a neighborhood of \u03c0(x). Also, f(\u03c0(x)) = \u00aff(x) =\ninf \u00aff(U) = inf f(\u03c0(U)) = inf f(U), hence \u03c0(x) is a local minimizer of f.\nIf \u03c0(x) is a local minimizer of f, there exists a neighborhood U of \u03c0(x) on\nM such that f(\u03c0(x)) = inf f(U). Since \u03c0 is continuous, the set U = \u03c0\u22121(U)\nis open, and it contains x. Thus, U is a neighborhood of x. We also have\n\u00aff(U) = f(\u03c0(\u03c0\u22121(U))) \u2286 f(U) (with equality if \u03c0 is surjective), hence \u00aff(x) =", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3131, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9ca53843-413c-4d39-9747-4e3dfc8dcd08": {"__data__": {"id_": "9ca53843-413c-4d39-9747-4e3dfc8dcd08", "embedding": null, "metadata": {"page_label": "259", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37a87002-5d33-4cb1-89c6-0e3a9cdd3b3a", "node_type": "4", "metadata": {"page_label": "259", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "bf09bd845e94addbb46d40372bd131dff84c99c0a7dba9ebe8c01282843ca36c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n9.17 Notes and references 259\nf(\u03c0(x)) = inf f(U) \u2264 inf \u00aff(U). (In fact, equality holds because x is in U.)\nTherefore, x is a local minimizer of \u00aff.\n3. Recall Definitions 4.4 and 6.1 for first- and second-order critical points. Note\nthat these do not require any Riemannian structure. For the following, we\ntacitly assume that f is once or twice differentiable, as needed.\n(a) By the chain rule, D \u00aff(x) = D f(\u03c0(x)) \u25e6 D\u03c0(x). If D f(\u03c0(x)) = 0, then\nD \u00aff(x) = 0. The other way around, if D \u00aff(x) = 0, then Df(\u03c0(x)) is zero on\nthe image of D \u03c0(x). Since D\u03c0(x) is surjective, this implies D f(\u03c0(x)) = 0.\n(b) Assume \u03c0(x) is second-order critical for f on M. Pick an arbitrary\nsmooth curve \u00afc on M such that \u00afc(0) = x. Let c = \u03c0 \u25e6 \u00afc: this is a smooth\ncurve on M such that c(0) = \u03c0(x). Notice that \u00aff \u25e6 \u00afc = f \u25e6 \u03c0 \u25e6 \u00afc = f \u25e6 c. In\nparticular, ( \u00aff \u25e6\u00afc)\u2032(0) = (f \u25e6c)\u2032(0) and ( \u00aff \u25e6\u00afc)\u2032\u2032(0) = (f \u25e6c)\u2032\u2032(0). Since \u03c0(x) is\nsecond-order critical, we know that (f \u25e6c)\u2032(0) = 0 and (f \u25e6c)\u2032\u2032(0) \u2265 0. Thus,\n( \u00aff \u25e6 \u00afc)\u2032(0) = 0 and ( \u00aff \u25e6 \u00afc)\u2032\u2032(0) \u2265 0, which implies that x is second-order\ncritical for \u00aff.\nFor the other direction, it is convenient to use that M can always be\nendowed with a Riemannian structure [Lee12, Prop. 13.3] (it does not\nmatter which one). Assume x is second-order critical for \u00aff. For v \u2208 TxM\n(to be determined), we can always select a smooth curve \u00afc such that \u00afc(0) =\nx and \u00afc\u2032(0) = v. The curve c = \u03c0 \u25e6 \u00afc satisfies c(0) = \u03c0(x) and c\u2032(0) =\nD\u03c0(x)[v]. Pick an arbitrary w \u2208 T\u03c0(x)M. Since D \u03c0(x) is surjective, we\ncan now choose v such that D\u03c0(x)[v] = w. From above, we know that \u03c0(x)\nis (at least) first-order critical. Then, owing to the usual Taylor expansion\nof f \u25e6 c (5.25), we have\n\u27e8w, Hessf(\u03c0(x))[w]\u27e9\u03c0(x) = (f \u25e6 c)\u2032\u2032(0),\nwhere Hessf is the Riemannian Hessian of f with respect to the arbitrarily\nchosen metric. Sincew is arbitrary and (f\u25e6c)\u2032\u2032(0) = ( \u00aff\u25e6\u00afc)\u2032\u2032(0) \u2265 0 (because\nx is second-order critical for \u00aff), it follows that Hess f(\u03c0(x)) is positive\nsemidefinite, that is, \u03c0(x) is second-order critical for f (Proposition 6.3).\nFinally, if \u03c0 is the projection of a quotient manifold, then it is surjective and\nsmooth and its differential at each point is surjective by Definition 9.1. The map\n\u03c0 is also open: see Exercise 9.9.\nIn the spirit of Proposition 9.6, the authors of [LKB22a] study conditions on\na map \u03c0 which allow one to relate salient points of min f and min f \u25e6\u03c0 for all f.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2659, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b4299dd8-9899-4475-8ffa-f8e85d8467a4": {"__data__": {"id_": "b4299dd8-9899-4475-8ffa-f8e85d8467a4", "embedding": null, "metadata": {"page_label": "260", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4ec1ac3c-5335-442b-9c87-94a92d636327", "node_type": "4", "metadata": {"page_label": "260", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "e506e471cadee94de696e524b8e60e19c2fb31ed0ec798ba12226def8df7f562", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n10 Additional tools\nAt times, it is useful to resort to some of the more advanced tools Riemannian\ngeometry has to offer. We discuss some of these here in relation to optimization.\nThe background on differential geometry given in Chapters 3 and 5 is often\nsufficient. We omit classical proofs, pointing to standard texts instead.\nWe start with the notion of Riemannian distance, which allows us to turn\na (connected) Riemannian manifold into a metric space. It turns out that the\nassociated metric space topology coincides with the manifold topology, and that\nshortest paths between pairs of points are geodesics. From there, we discuss the\nRiemannian exponential map: this is a retraction whose curves are geodesics.\nThen, we also give a formal and not-so-standard treatment of the inverse of the\nexponential map and, more generally, of the inverse of retractions.\nMoving on, parallel transports allow us to move tangent vectors around, from\ntangent space to tangent space, isometrically. Combined with the exponential\nmap, this tool makes it possible to define a notion of Lipschitz continuity for the\ngradient and the Hessian of a cost function on a Riemannian manifold. This leads\nto a sharp understanding of the regularity assumptions we made in Chapters 4\nand 6 to control the worst-case behavior of optimization algorithms.\nWe follow up with the (also not-so-standard) notion of transporter\u2014a poor\nman\u2019s version of parallel transport. This is useful to design certain algorithms.\nIt also affords us a practical notion of finite difference approximation for the\nHessian, which makes it possible to use second-order optimization algorithms\nwithout computing the Hessian.\nIn closing, we discuss covariant differentiation of tensor fields of any order.\nAs a notable omission, we do not discuss curvature at all: see for exam-\nple [Lee18, Ch. 1, 7] for an introduction.\n10.1 Distance, geodesics and completeness\nTwo points of M belong to the same connected component if there exists a\ncontinuous curve on M joining them. We say M is connected if it has a single\nconnected component. A manifold has finitely many, or countably infinitely many\nconnected components, owing to second-countability of the atlas topology (see\nSection 8.2).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2493, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6f84e8d3-a6a7-49d0-b888-7f371bc51ca9": {"__data__": {"id_": "6f84e8d3-a6a7-49d0-b888-7f371bc51ca9", "embedding": null, "metadata": {"page_label": "261", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6dedce11-ddbd-4f1e-8b8a-93aa99d7c5e1", "node_type": "4", "metadata": {"page_label": "261", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "544e458d9c79cd93022d5809f9966e2ae11c4fadeebdf92c36a52267f5923999", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n10.1 Distance, geodesics and completeness 261\nDefinition 10.1. A distance on a set M is a function dist: M \u00d7 M \u2192R such\nthat, for all x, y, z\u2208 M,\n1. dist(x, y) = dist(y, x);\n2. dist(x, y) \u2265 0, and dist(x, y) = 0 if and only x = y; and\n3. dist(x, z) \u2264 dist(x, y) + dist(y, z).\nEquipped with a distance, M is a metric space.\nA natural topology on a metric space is the metric topology, defined such that\nthe functions x 7\u2192 dist(x, y) are continuous. Specifically, a subset U \u2286 Mis open\nif and only if, for every x \u2208 U, there exists a radius r > 0 such that the ball\n{y \u2208 M: dist(x, y) < r} is included in U.\nIn this section, we first state without proof that ifM is a Riemannian manifold\nthen its Riemannian metric induces a distance on (the connected components\nof) M, and that the topology of M as a manifold is equivalent to the topology\nof M as a metric space equipped with that distance. Intuitively, dist( x, y) is the\nlength of the shortest \u201creasonable\u201d curve on M joining x and y, or the infimum\nover the lengths of such curves. To make this precise, we first need to discuss\nvarious types of curves on manifolds [Lee18, pp33\u201334].\nDefinition 10.2. A curve segment on a manifold M is a continuous map\nc: [a, b] \u2192 M, where a \u2264 b are real. A curve segment c: [a, b] \u2192 Mis:\n\u2022 smooth if c can be extended to a smooth map \u02dcc: I \u2192 Mon a neighborhood I\nof [a, b], in which case c\u2032(a) and c\u2032(b) denote \u02dcc\u2032(a) and \u02dcc\u2032(b), respectively;\n\u2022 regular if it is smooth and c\u2032(t) \u0338= 0 for all t \u2208 [a, b];\n\u2022 piecewise smooth (resp., piecewise regular) if there exists a finite set of times\na = t0 < t1 < \u00b7 \u00b7\u00b7< tk\u22121 < tk = b such that the restrictions c|[ti\u22121,ti] are\nsmooth (resp., regular) curve segments for i = 1, . . . , k.\nIn particular, piecewise regular curves are piecewise smooth. We say a curve\nsegment c: [a, b] \u2192 Mconnects x to y if c(a) = x and c(b) = y.\nLet M be a Riemannian manifold. Given a piecewise smooth curve segment\nc: [a, b] \u2192 M, we define the length of c as the integral of its speed \u2225c\u2032(t)\u2225c(t):\nL(c) =\nZ b\na\n\u2225c\u2032(t)\u2225c(t) dt. (10.1)\nWhile c\u2032(ti) may be undefined, the speed of each curve segment c|[ti\u22121,ti] is\nsmooth: the integral is computed by summing over these intervals.\nThe notion of length of a curve leads to a natural notion of distance on M,\ncalled the Riemannian distance:\ndist(x, y) = inf\nc\nL(c), (10.2)\nwhere the infimum is taken over all piecewise regular curve segments onM which", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2664, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7070e3dc-f567-4ced-938a-01254dabc7d5": {"__data__": {"id_": "7070e3dc-f567-4ced-938a-01254dabc7d5", "embedding": null, "metadata": {"page_label": "262", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b52cf43b-8adc-4e65-92b5-66d57870a103", "node_type": "4", "metadata": {"page_label": "262", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "2d9c617d1bebb356367acd25ae91dcdd815b2b9ef2c1bf82176e5df4a4357c42", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n262 Additional tools\nconnect x to y. It is equivalent to take the infimum over all piecewise smooth\ncurve segments. We have the following important result [Lee18, Thm. 2.55].\nTheorem 10.3. If M is connected (meaning each pair of points is connected by\na curve segment), equation (10.2) defines a distance. Equipped with this distance,\nM is a metric space whose metric topology coincides with its atlas topology.\nIf M is not connected, we may consider this result on the connected compo-\nnents of M separately. Sometimes, it helps to extend the definition to accept\ndist(x, y) = \u221e when x, ybelong to distinct connected components.\nIf the infimum in (10.2) is attained 1 for some curve segment c, we call c a\nminimizing curve. Remarkably, up to parameterization, these are geodesics (Def-\ninition 5.38) [Lee18, Thm. 6.4]. In other words, two competing generalizations\nof the notion of straight line from linear spaces to manifolds turn out to be\nequivalent: one based on shortest paths, one based on zero acceleration.\nTheorem 10.4. Every minimizing curve admits a constant-speed parameteriza-\ntion such that it is a geodesic, called a minimizing geodesic.\nThis theorem admits a partial converse [Lee18, Thm. 6.15]. It could not have a\nfull converse since, for example, two nearby points on a sphere can be connected\nthrough both a short and a long geodesic.\nTheorem 10.5. Every geodesic \u03b3 on M is locally minimizing, that is, every t\nin the domain of \u03b3 has a neighborhood I in the domain of \u03b3 such that, if a, b\u2208 I\nsatisfy a < b, then the restriction \u03b3|[a,b] is a minimizing curve.\nEquipped with a distance, we define a first notion of completeness. Recall that\na sequence x0, x1, x2, . . .is Cauchy if for every \u03b5 >0 there exists an integer k\nsuch that, for all m, n > k, dist(xm, xn) < \u03b5.\nDefinition 10.6. A connected Riemannian manifold is metrically complete if\nit is complete as a metric space equipped with the Riemannian distance, that is,\nif every Cauchy sequence on the manifold converges on the manifold.\nThere exists another useful notion of completeness for manifolds.\nDefinition 10.7. A Riemannian manifold is geodesically complete if every\ngeodesic can be extended to a geodesic defined on the whole real line.\nThe following theorem is an important classical result: see [Lee18, Thm. 6.19,\nPb. 6-14] for a proof. It justifies omitting to specify whether we mean met-\nric or geodesic completeness. The last part of the statement is the Heine\u2013\nBorel property : recall Definition 8.26 for compact sets; a set S is bounded if\nsupx,y\u2208S dist(x, y) is finite.\n1 This is not always the case: think of M = R2\\{0} as a Riemannian submanifold of R2, and\nconnect x and \u2212x.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2931, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7a55b3b2-e00b-4b85-8c8a-09fd52b64178": {"__data__": {"id_": "7a55b3b2-e00b-4b85-8c8a-09fd52b64178", "embedding": null, "metadata": {"page_label": "263", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "956b801d-f4b6-4869-b17b-2f9fb6f04d0d", "node_type": "4", "metadata": {"page_label": "263", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "1294b644ab42ad1cfcfe6c2efb84abfc4552e9714a6840aef6a35c7cefc3b06a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n10.1 Distance, geodesics and completeness 263\nTheorem 10.8 (Hopf\u2013Rinow). A connected Riemannian manifold M is met-\nrically complete if and only if it is geodesically complete. Additionally, M is\ncomplete (in either sense) if and only if its compact subsets are exactly its closed\nand bounded subsets.\nFor disconnected manifolds, complete refers to geodesic completeness, which\nis equivalent to metric completeness of each connected component. For example,\nthe orthogonal group O( n), which has two connected components, is complete\nin this sense.\nOn a complete manifold, two points in the same connected component can\nalways be connected by a (not necessarily unique) geodesic which attains the\ninfimum in (10.2) [Lee18, Cor. 6.21].\nTheorem 10.9. If M is complete, then any two points x, yin the same con-\nnected component are connected by a minimizing geodesic segment c: [0, 1] \u2192 M\nsuch that c(0) = x, c(1) = y and dist(x, y) = L(c).\nThe converse of Theorem 10.9 does not hold: consider M = (0, 1) as a Rie-\nmannian submanifold of R.\nExample 10.10. Compact Riemannian manifolds are complete.\nExample 10.11. A finite-dimensional Euclidean space E is connected and com-\nplete. The unique minimizing geodesic from x to y is the line segment t 7\u2192\n(1 \u2212 t)x + ty on [0, 1], and the Riemannian distance dist(x, y) is equal to the\nEuclidean distance \u2225x \u2212 y\u2225.\nExercise 10.12. Let M be a Riemanian submanifold of N. Show that if N is\ncomplete and if M is a closed subset of N, then M is complete. In particular,\nRiemannian submanifolds of Euclidean spaces which are closed are complete.\nExercise 10.13. Show that the length of a piecewise smooth curve segment\nc: [a, b] \u2192 Mis independent of parameterization, in that L(c\u25e6h) = L(c) for any\nmonotone, piecewise regular h: [0, 1] \u2192 [a, b] such that h(0) = a and h(1) = b.\nFurther show that if c is a piecewise regular curve segment then h can be chosen\nsuch that c \u25e6 h is piecewise regular with speed (whenever it is defined) equal to\nthe constant L(c).\nExercise 10.14. Show that the distance on a Riemannian product manifold\nM = M1 \u00d7 \u00b7 \u00b7\u00b7 \u00d7 Mn is given by\ndist(x, y) =\nvuut\nnX\ni=1\ndist(xi, yi)2,\nwhere dist denotes Riemannian distance on M and M1, . . . ,Mn alike, as indi-\ncated by context. Hint: the equality can be established through a pair of matching\ninequalities. For one side, Jensen\u2019s inequality may be helpful. For the other side,\nit may be helpful to use the results of Exercise 10.13.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2694, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "10adc40f-f4d9-4ee9-8bec-1b477f3f803d": {"__data__": {"id_": "10adc40f-f4d9-4ee9-8bec-1b477f3f803d", "embedding": null, "metadata": {"page_label": "264", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b5c75f33-816b-4863-b28d-51cec4d3b529", "node_type": "4", "metadata": {"page_label": "264", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "04fc4af190f96144d4cb1b40cb0c06c4d1ba5e845ac06b620ba3b57592c23055", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n264 Additional tools\nExercise 10.15. Let M = M/ \u223c be a Riemannian quotient manifold with\nquotient map \u03c0 : M \u2192 M, as in Section 9.7. Let distM and distM denote the\nRiemannian distances on M and M, respectively. Show that for all [x], [y] \u2208 M\nthe distances satisfy\ndistM([x], [y]) \u2264 inf\nx\u2032\u223cx,y\u2032\u223cy\ndistM(x\u2032, y\u2032).\nMoreover, show that if M is connected and complete then the inequality holds\nwith equality. Hint: for the second part, use Proposition 9.61.\n10.2 Exponential and logarithmic maps\nUsing standard tools from the study of ordinary differential equations, one can\nshow that on a Riemannian manifold, for every ( x, v) \u2208 TM, there exists a\nunique maximal geodesic [Lee18, Cor. 4.28]\n\u03b3v : I \u2192 M, with \u03b3v(0) = x and \u03b3\u2032\nv(0) = v.\nHere, maximal refers to the fact that the interval I is as large as possible (this\nis not in contrast to the notion of minimizing geodesic we just defined in the\nprevious section). We use these geodesics to define a special map.\nDefinition 10.16. Consider the following subset of the tangent bundle:\nO = {(x, v) \u2208 TM : \u03b3v is defined on an interval containing [0, 1]}.\nThe exponential map Exp: O \u2192 Mis defined by\nExp(x, v) = Expx(v) = \u03b3v(1).\nThe restriction Expx is defined on Ox = {v \u2208 TxM : (x, v) \u2208 O}.\nFor example, in a Euclidean space, Exp x(v) = x + v. By Definition 10.7, a\nmanifold M is (geodesically) complete exactly if the domain of the exponential\nmap is the whole tangent bundle T M.\nGiven t \u2208 R, it holds that \u03b3tv(1) = \u03b3v(t) whenever either is defined. This\nallows us to express the exponential map as\nExpx(tv) = \u03b3v(t), (10.3)\nwhich is often more practical. In particular, the domain of Exp x is star-shaped\naround the origin in T xM, that is,\nv \u2208 Ox =\u21d2 tv \u2208 Ox for all t \u2208 [0, 1].\nConveniently, Exp is smooth [Lee18, Lem. 5.18, Prop. 5.19].\nProposition 10.17. The exponential map is smooth on its domain O, which is\nopen in TM.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2130, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4837b13e-1b1f-4272-9899-b00bd4ca680f": {"__data__": {"id_": "4837b13e-1b1f-4272-9899-b00bd4ca680f", "embedding": null, "metadata": {"page_label": "265", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3d390d59-9101-4607-ae69-71824268fa87", "node_type": "4", "metadata": {"page_label": "265", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "2983bf42f867df43c5bdd7bba40a656c99271a06d7df60b29b5d2865bc89ae60", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n10.2 Exponential and logarithmic maps 265\nThe domain O contains all tangent space origins. We say that O is a neigh-\nborhood of the zero section of the tangent bundle :\n{(x, 0) \u2208 TM : x \u2208 M} \u2282 O. (10.4)\nThe exponential map is a retraction on its domain. More precisely:\nProposition 10.18. The exponential map is a second-order retraction, with a\npossibly restricted domain O \u2286TM.\nProof. Proposition 10.17 claims smoothness of Exp. By definition, for all ( x, v)\nin TM the curve c(t) = Exp x(tv) = \u03b3v(t) satisfies c(0) = x and c\u2032(0) = v so\nthat Exp is a retraction. Finally, it is clear that this retraction is second order\n(Definition 5.42) since \u03b3\u2032\u2032\nv (t) is zero for all t, hence in particular c\u2032\u2032(0) = 0.\nGiven a point x and a (sufficiently short) tangent vector v, the exponential\nmap produces a new point y = Expx(v). One may reasonably wonder whether,\ngiven the two points x, y, one can recover the tangent vector v. In what follows,\nwe aim to understand to what extent the exponential map can be (smoothly)\ninverted.\nA first observation, rooted in the inverse function theorem for manifolds (see\nTheorem 4.16), is that any retraction R at a point x is locally a diffeomorphism\naround the origin in the tangent space at x, because R x is smooth and DR x(0)\nis the identity (hence invertible). This applies in particular to the exponential\nmap. For the latter, the injectivity radius quantifies how large the local domains\ncan be.\nWhen \u22c6we say a map F is a diffeomorphism on an open domain U, we mean\nthat F(U) is open and F is a diffeomorphism from U to F(U).\nDefinition 10.19. The injectivity radius of a Riemannian manifold M at a\npoint x, denoted by inj(x), is the supremum over radii r >0 such that Expx is\ndefined and is a diffeomorphism on the open ball\nB(x, r) = {v \u2208 TxM : \u2225v\u2225x < r}.\nBy the inverse function theorem, inj(x) > 0.\nConsider the ball U = B(x, inj(x)) in the tangent space at x. Its image\nU = Exp x(U) is a neighborhood of x in M. By definition, Exp x : U \u2192 Uis\na diffeomorphism: it has a well-defined smooth inverse Exp \u22121\nx : U \u2192U. With\nthese choices of domains, v = Exp\u22121\nx (y) is the unique shortest tangent vector at\nx such that Expx(v) = y. Indeed, if there existed another vector u \u2208 TxM such\nthat Expx(u) = y and \u2225u\u2225x \u2264 \u2225v\u2225x, then u would be included in U, which would\ncontradict invertibility. This motivates the following definition.2\n2 Note that Logx may be discontinuous (think of x and y close to each other on a sphere from\nwhich we remove the midpoint ( x + y)/\u2225x + y\u2225). Nevertheless, see below for smoothness\nproperties on restricted domains (Corollary 10.25 in particular), and see also [Lee18, \u00a7 10]\nfor a related discussion of cut locus and conjugate points.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2961, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "879c8edd-27d5-431f-8584-fc7696be59fb": {"__data__": {"id_": "879c8edd-27d5-431f-8584-fc7696be59fb", "embedding": null, "metadata": {"page_label": "266", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd9ad72c-0414-4e21-8086-23886873827a", "node_type": "4", "metadata": {"page_label": "266", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "a987a062201495d6fc1dcc90f53c633d7cfb02e62d62a0ac8a6cb3a2f7043cb7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n266 Additional tools\nDefinition 10.20. For x \u2208 M, let Logx denote the logarithmic map at x,\nLogx(y) = arg min\nv\u2208Ox\n\u2225v\u2225x subject to Expx(v) = y, (10.5)\nwith domain such that this is uniquely defined.\nFor example, in a Euclidean space, Log x(y) = y \u2212 x for all x, y. In particu-\nlar, with domains U = B(x, inj(x)) and U = Expx(U) as above, the inverse of\nExpx : U \u2192 Uis the (possibly restricted) map Log x : U \u2192U, which is then a\ndiffeomorphism. With different domain restrictions however, the inverse of Expx\nmay be different from Log x. This is illustrated in the following example.\nExample 10.21. On the sphere Sn\u22121, the exponential map is (Example 5.37)\nv 7\u2192 Expx(v) = cos(\u2225v\u2225)x + sin(\u2225v\u2225)\n\u2225v\u2225 v.\nThis is smooth over the whole tangent bundle, with the usual smooth extension\nsin(t)/t = 1 at t = 0. Given x, y\u2208 Sn\u22121, we seek an expression for Exp\u22121\nx (y).\nSince x\u22a4x = 1 and x\u22a4v = 0, considering y = Expx(v) as above, we deduce that\nx\u22a4y = cos(\u2225v\u2225). Thus, the following vector (which is readily computed given x\nand y) is parallel to v:\nu \u225c y \u2212 (x\u22a4y)x = Projx(y) = sin(\u2225v\u2225)\n\u2225v\u2225 v.\nIt has norm |sin(\u2225v\u2225)| and is parallel to v. Let us exclude the case u = 0 which\nis easily treated separately. Then, dividing u by its norm yields:\nu\n\u2225u\u2225 = sign(sin(\u2225v\u2225)) v\n\u2225v\u2225.\nIf we restrict the domain of Expx to contain exactly those tangent vectors v\nwhose norm is strictly less than \u03c0, then sign(sin(\u2225v\u2225)) = 1 . Furthermore, the\nequation x\u22a4y = cos( \u2225v\u2225) then admits the unique solution \u2225v\u2225 = arccos( x\u22a4y),\nwhere arccos: [ \u22121, 1] \u2192 [0, \u03c0] is the principal inverse of cos. Overall, this yields\nthe following expression:\ny 7\u2192 Exp\u22121\nx (y) = arccos(x\u22a4y) u\n\u2225u\u2225, (10.6)\nsmooth over Sn\u22121\\{\u2212x}. Since the chosen domain for Expx is B(x, \u03c0), the in-\nverse is the logarithm: Exp\u22121\nx = Logx. Also, dist(x, y) = arccos(x\u22a4y).\nThe crucial point is that, in deriving this expression, we made the (somewhat\narbitrary) choice of defining the domain of Expx in a specific way. This leads\nto a particular formula for the inverse, and a particular domain for Exp\u22121\nx . If\nwe choose the domain of Expx differently, we may very well obtain a different\nformula for Exp\u22121\nx (not equal to Logx) and a different domain for it as well.\nFor example, on the circle S1, we could decide that if y is ahead of x (counter-\nclockwise) by an angle less than \u03c0/2, then Exp\u22121\nx (y) returns a vector of length", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2612, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ed544c8f-c7ed-44bc-b9a4-46848d4d576e": {"__data__": {"id_": "ed544c8f-c7ed-44bc-b9a4-46848d4d576e", "embedding": null, "metadata": {"page_label": "267", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b240c279-dcae-4e0e-b619-149cf2442806", "node_type": "4", "metadata": {"page_label": "267", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "45c74e638b80bcbc7516bb0f99fed6feff3e5faf5c26244cf345797df11f0d92", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n10.2 Exponential and logarithmic maps 267\nless than \u03c0/2, and otherwise it returns a vector of length less than 3\u03c0/2, pointing\nin the clockwise direction. In short, domains matter.\nBefore moving on to broader smoothness concerns, we quote useful relations\nbetween Exp, Log and dist [Lee18, Prop. 6.11].\nProposition 10.22. If \u2225v\u2225x < inj(x), the geodesic c(t) = Exp x(tv) on the\ninterval [0, 1] is the minimizing curve connecting x to y = Expx(v), unique up\nto parameterization. In particular, dist(x, y) = \u2225v\u2225x, and Logx(y) = v.\nSo far, we have fixed the point x, allowing us to claim that, on some domains,\nthe map y 7\u2192 Exp\u22121\nx (y) is smooth in y. In order to discuss smoothness jointly\nin x and y, we need more work. We start with a general discussion valid for all\nretractions, and specialize to the exponential map later on.\nProposition 10.23. Let M be a manifold with retraction R defined on a neigh-\nborhood O of the zero section of TM. Consider the following map:\nE : O \u2192 M \u00d7 M: (x, v) 7\u2192 E(x, v) = (x, Rx(v)). (10.7)\nIf T \u2286 Ois open in TM such that, for all x, Rx is a diffeomorphism on Tx =\n{v \u2208 TxM : (x, v) \u2208 T }, then V = E(T ) is open in M \u00d7 Mand E : T \u2192 Vis\na diffeomorphism.\nProof. First, to see thatE : T \u2192 Vis invertible, consider any pairs (x, v), (y, w) \u2208\nT such that E(x, v) = E(y, w). In other words, we have (x, Rx(v)) = (y, Ry(w)),\nso that x = y and v, w\u2208 Tx. By assumption, R x is injective on Tx, hence we\ndeduce from Rx(v) = Rx(w) that v = w.\nSecond, to show that V is open and E : T \u2192 Vis a diffeomorphism, it re-\nmains to check that the differential of E is invertible everywhere in T (the result\nthen follows from applying the inverse function theorem at each point of T , see\nTheorem 4.16). To this end, consider any ( x, v) \u2208 T. Somewhat informally, the\ndifferential DE(x, v) is a block matrix of size two-by-two as displayed below. (To\nbe formal, we should give a more precise description of the tangent space to TM\nat (x, v). Here, it is identified with T xM \u00d7TxM.)\nDE(x, v) \u2243\n\u0014I 0\n\u2217 DRx(v)\n\u0015\nIndeed, the differential of the first entry of E(x, v) = ( x, Rx(v)) with respect\nto x is the identity, and it is zero with respect to v. The second entry has\nsome unspecified differential with respect to x, while its differential with respect\nto v is DRx(v). Crucially, since v is in Tx, we know by assumption that R x is a\ndiffeomorphism around v, hence DRx(v) is invertible. We conclude that DE(x, v)\nis invertible for all ( x, v) \u2208 T, as announced.\nIn particular, under the stated conditions, ( x, y) 7\u2192 (x, R\u22121\nx (y)) is a diffeo-\nmorphism from V to T , meaning the inverse retraction can be defined smoothly\njointly in x and y (with care when it comes to domains).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2952, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2b638283-572f-417a-8a7a-3b9534898d44": {"__data__": {"id_": "2b638283-572f-417a-8a7a-3b9534898d44", "embedding": null, "metadata": {"page_label": "268", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b2bb905c-3558-404b-9ed2-fdee69cb81ea", "node_type": "4", "metadata": {"page_label": "268", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "ce6b974c4189021595c1eb81f1d8619de4bf876d8dcfd4126a9d7a2fc05bb34a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n268 Additional tools\nIn this last proposition, the fact that T is open is crucial: this is what ties the\ndomains Tx together. Without this assumption, we can still have an inverse, but\nnot necessarily a smooth inverse. Furthermore, it is natural to want to include\nthe tangent space origins in T , that is, to make T a neighborhood of the zero\nsection in TM. It is convenient to make this happen using a continuous function\n\u2206: M \u2192(0, \u221e]:\nT =\n\b\n(x, v) \u2208 TM : \u2225v\u2225x < \u2206(x)\n\t\n. (10.8)\nIf Rx is defined and is a diffeomorphism on the open ball B(x, \u2206(x)) in T xM\nfor all x, then Proposition 10.23 applies, and V contains the diagonal {(x, x) :\nx \u2208 M}.\nConveniently, for the exponential map, we can take \u2206 to be as large as one\ncould possibly hope, namely: we can choose \u2206 to be the injectivity radius func-\ntion. (This holds even if M is not connected or complete, see Section 10.8.)\nProposition 10.24. On a Riemannian manifold M, the injectivity radius func-\ntion inj: M \u2192(0, \u221e] is continuous.\nCorollary 10.25. The map (x, v) 7\u2192 (x, Expx(v)) is a diffeomorphism from\nT =\n\b\n(x, v) \u2208 TM : \u2225v\u2225x < inj(x)\n\t\nto\nV =\n\b\n(x, y) \u2208 M \u00d7 M: dist(x, y) < inj(x)\n\t\n.\nIts inverse is (x, y) 7\u2192 (x, Logx(y)), smooth from V to T .\nUnder this corollary, we see that ( x, y) 7\u2192 Logx(y) is smooth jointly in x and\ny over some domain. This is apparent in Example 10.21 for the sphere, where\ninj(x) = \u03c0 for all x.\nMore generally, we show that for any retraction there exists a positive and\ncontinuous function \u2206 which can be used to argue existence of a smooth inverse.\nProposition 10.26. On a Riemannian manifold M, consider the following open\nsubsets of the tangent bundle TM:\nV\u03b4(x) = {(x\u2032, v\u2032) \u2208 TM : dist(x, x\u2032) < \u03b4and \u2225v\u2032\u2225x\u2032 < \u03b4}.\n(In particular, x and x\u2032 must be in the same connected component.) Notice that\n(x, 0) is in V\u03b4(x) for all \u03b4 >0. For any retraction R on M defined on a neigh-\nborhood O of the zero section in TM, define \u2206: M \u2192(0, \u221e] by:\n\u2206(x) = sup\n\b\n\u03b4 >0 : V\u03b4(x) \u2286 Oand E is a diffeomorphism on V\u03b4(x)\n\t\n,\nwhere E is as defined in (10.7). Then, \u2206 is positive and continuous, and Rx is\ndefined and is a diffeomorphism on B(x, \u2206(x)) for all x.\nProof. To see that \u2206(x) is positive at everyx, apply the inverse function theorem", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2470, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fd8919c8-3344-43e7-9a98-2a6ac70ddc63": {"__data__": {"id_": "fd8919c8-3344-43e7-9a98-2a6ac70ddc63", "embedding": null, "metadata": {"page_label": "269", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6550ff3c-0c39-488f-a6fb-ec7fdb79e127", "node_type": "4", "metadata": {"page_label": "269", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "b3ab2308e54b2448d1e3006cda17ecf9feae77d5feefb16cd762713e1245250e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n10.2 Exponential and logarithmic maps 269\nto the fact that the differential of E at (x, 0) is invertible, since it is of the form\nDE(x, 0) \u2243 [ I 0\n\u2217 I ] (same as in the proof of Proposition 10.23).\nIt is sufficient to reason on each connected component of M separately, hence\nwe may assumeM is connected. If \u2206(x) = \u221e for some x, then O = TM and E is\na diffeomorphism on that domain, so that \u2206(x) = \u221e for all x: this is compatible\nwith the claim. We can now assume \u2206( x) is finite for all x.\nTo see that \u2206 is continuous, we show that \u2206( x) \u2212 \u2206(x\u2032) \u2264 dist(x, x\u2032) for\nevery two points x, x\u2032 \u2208 M. Then, switching the roles of x and x\u2032, we find\n|\u2206(x) \u2212 \u2206(x\u2032)| \u2264dist(x, x\u2032), which shows \u2206 is continuous with respect to the\nRiemannian distance. This is equivalent to continuity with respect to the atlas\ntopology by Theorem 10.3.\nPick any two points x, x\u2032 \u2208 M. If dist( x, x\u2032) \u2265 \u2206(x), the claim is clear. So\nassume dist(x, x\u2032) < \u2206(x), and define \u03b4 = \u2206(x) \u2212 dist(x, x\u2032). We claim that\nV\u03b4(x\u2032) \u2282 V\u2206(x)(x).\nIndeed, pick any (x\u2032\u2032, v\u2032\u2032) \u2208 V\u03b4(x\u2032). Then,\n1. \u2225v\u2032\u2032\u2225x\u2032\u2032 < \u03b4\u2264 \u2206(x), and\n2. dist( x\u2032\u2032, x) \u2264 dist(x\u2032\u2032, x\u2032) + dist(x\u2032, x) < \u03b4+ dist(x\u2032, x) = \u2206(x).\nWe know E is a diffeomorphism on V\u2206(x)(x). Thus, E is also a diffeomorphism\non V\u03b4(x\u2032). By definition of \u2206( x\u2032), this implies \u2206( x\u2032) \u2265 \u03b4 = \u2206(x) \u2212 dist(x, x\u2032),\nwhich is what we needed to show.\nThe conclusion about R x follows from the fact that E is a diffeomorphism on\neach V\u2206(x)(x) (which covers B(x, \u2206(x))) and from the form of D E, as in the\nproof of Proposition 10.23.\nFor general retractions, we obtain the following corollary which notably means\nthat, over some domain of M \u00d7 Mwhich contains all pairs ( x, x), the map\n(x, y) 7\u2192 R\u22121\nx (y) can be defined smoothly jointly in x and y. There is no need\nto require that M be a Riemannian manifold because the existence of a Rie-\nmannian metric is guaranteed [Lee12, Prop. 13.3]: that is sufficient to apply\nProposition 10.26.\nCorollary 10.27. For any retraction R on a manifold M there exists a neigh-\nborhood T of the zero section of the tangent bundle TM on which\n(x, v) 7\u2192 (x, Rx(v))\nis a diffeomorphism; T can be taken of the form (10.8) (with respect to an arbi-\ntrary Riemannian metric) with \u2206: M \u2192(0, \u221e] continuous.\nWe close with the notion of injectivity radius of a whole manifold. It may be\nzero, positive or infinite. The set of manifolds with positive injectivity radius is\nstrictly included in the set of complete manifolds: Exercise 10.30. See [Lee18,\nLem. 6.16] for a proof of Proposition 10.29.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2758, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2942296e-debd-4d75-bcf3-9b390ed5b156": {"__data__": {"id_": "2942296e-debd-4d75-bcf3-9b390ed5b156", "embedding": null, "metadata": {"page_label": "270", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "230d3219-36f4-4bde-bb69-355cda70d5f6", "node_type": "4", "metadata": {"page_label": "270", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "f114a2fdeaad338cf28fe6776b24e50fc179b704bc1cf8a4edab751036719831", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n270 Additional tools\nDefinition 10.28. The injectivity radius inj(M) of a Riemannian manifold M\nis the infimum of inj(x) over x \u2208 M.\nProposition 10.29. For a compact Riemannian manifold, inj(M) \u2208 (0, \u221e).\nA Euclidean space has infinite injectivity radius. The unit sphere S n\u22121 has\ninjectivity radius \u03c0. Importantly, the manifold Rm\u00d7n\nr of matrices with fixed rank\nr embedded in Rm\u00d7n (Section 7.5) has zero injectivity radius. This is in part\ndue to the fact that there exist matrices in Rm\u00d7n\nr that are arbitrarily close to\nmatrices of rank strictly less than r, as measured in the embedding space (Rm\u00d7n\nr\nis not complete).\nExercise 10.30. Show that if inj(M) is positive then M is complete. The con-\nverse is not true: give an example of a complete, connected manifold whose in-\njectivity radius is zero.\nExercise 10.31. Let K be any subset of a Riemannian manifold M and let\nr: K \u2192R+ be continuous, with R+ = {t \u2208 R : t \u2265 0}. Show that\nT = {(x, s) \u2208 TM : x \u2208 Kand \u2225s\u2225x \u2264 r(x)}\nis compact in TM if and only if K is compact in M.\nExercise 10.32. Let M = M1 \u00d7 M2 be a Riemannian product manifold, as in\nExample 3.57. Let O1, O2 be the domains of the exponential maps on M1, M2,\nrespectively. Show that the domain of the exponential map on M is O = O1 \u00d7O2,\nwith the identification of tangent bundles on product manifolds as in (3.31). For\n(x1, v1) \u2208 O1 and (x2, v2) \u2208 O2, show that\nExpx(v) =\n\u0000\nExpx1 (v1), Expx2 (v2)\n\u0001\n,\nwhere x = (x1, x2) and v = (v1, v2), and Exp denotes the exponential map on\neach respective manifold. Hint: use Exercise 5.39.\n10.3 Parallel transport\nConsider a manifold M equipped with a connection \u2207, and a tangent vector\nu \u2208 TxM.3 In several situations, it is desirable to somehow transport u from x\nto another point y \u2208 M. In so doing, we would like for u and its transported\nversion to be related in some meaningful way.\nThe geometric tool of choice for this task is calledparallel transport (or parallel\ntranslation). Let c: I \u2192 Mbe a smooth curve such that\nc(0) = x and c(1) = y.\nConsider a smooth vector field Z \u2208 X(c) on this curve with Z(0) = u. If Z does\n3 In this section,M may or may not be Riemannian, and\u2207 may or may not be the Riemannian\nconnection.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2442, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "13df8067-684e-43a3-a787-f600f16df205": {"__data__": {"id_": "13df8067-684e-43a3-a787-f600f16df205", "embedding": null, "metadata": {"page_label": "271", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "efcb2069-1933-4eae-93bd-ab290586a3b2", "node_type": "4", "metadata": {"page_label": "271", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "63fcb5ad4cb14297c9092df67d4d8ef23d445b629d7712b689eca9bf2ba950e8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n10.3 Parallel transport 271\nc(t)\nZ(t)\nM\nFigure 10.1 Parallel transports \u2018move\u2019 tangent vectors from one tangent space to an-\nother, along a specified curve. They can be used to compare or combine tangent vectors\nat different points by transporting them to a common tangent space.\nnot \u2018vary\u2019 too much, it is tempting to consider Z(1) as a transport of u to y. One\nconvenient way to formalize this is to require that Z be parallel with respect to\nthe chosen connection \u2207. Explicitly, using the covariant derivative D\ndt induced\nby \u2207 (Theorem 5.29), we require\nZ \u2208 X(c), Z (0) = u, and D\ndtZ = 0. (10.9)\nUsing standard tools from linear ordinary differential equations, one can show\nthat such a vector field exists and is unique [Lee18, Thm. 4.32].\nDefinition 10.33. A vector field Z \u2208 X(c) such that D\ndt Z = 0 is parallel.\nTheorem 10.34. On a manifold M with a connection and induced covariant\nderivative D\ndt , for any smooth curve c: I \u2192 M, t0 \u2208 I and u \u2208 Tc(t0)M, there\nexists a unique parallel vector field Z \u2208 X(c) such that Z(t0) = u.\nThis justifies the following definition of parallel transport (Figure 10.1).\nDefinition 10.35. Given a smooth curve c on M, the parallel transport of\ntangent vectors at c(t0) to the tangent space at c(t1) along c is the map\nPTc\nt1\u2190t0 : Tc(t0)M \u2192Tc(t1)M\ndefined by PTc\nt1\u2190t0 (u) = Z(t1), where Z \u2208 X(c) is the unique parallel vector field\nsuch that Z(t0) = u.\nIn particular, t 7\u2192 PTc\nt\u2190t0 (u) is a parallel vector field along c. Also, if Z is\nparallel along c, then Z(t1) = PTc\nt1\u2190t0 (Z(t0)) for all t0, t1 in the domain of c.\nOn occasion, we may write PT c\ny\u2190x or even PTy\u2190x when the times t0, t1 and\nthe curve c such that x = c(t0) and y = c(t1) are clear from context, but beware:\neven if we use the Riemannian connection,\nParallel transport from x to y depends on the choice of curve connecting x and\ny.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2104, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "381d8c3c-760b-49be-a448-a88abd5a39c1": {"__data__": {"id_": "381d8c3c-760b-49be-a448-a88abd5a39c1", "embedding": null, "metadata": {"page_label": "272", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fbb23cff-c529-46fe-9fbe-92a9ea2b3750", "node_type": "4", "metadata": {"page_label": "272", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "d5b2a2a375c7df665c31def89a26ac7a8513158750b990f6ade55a87439e3645", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n272 Additional tools\nIndeed, think of a tangent vector at the equator pointing North. Transport it to\nthe North pole via the shortest path. Alternatively, transport the same vector\nby first moving along the equator for some distance before going to the North\npole: the results are different. This is, in fact, a crucial feature of Riemannian\ngeometry, intimately related to the notion of curvature [Lee18, Ch. 7].\nOn a Riemannian manifold, when the curve c is not specified, one often implic-\nitly means to move along the minimizing geodesic connecting x and y, assuming\nit exists and is unique.\nProposition 10.36. The parallel transport operator PTc\nt1\u2190t0 is linear. Also,\nPTc\nt2\u2190t1 \u25e6 PTc\nt1\u2190t0 = PT c\nt2\u2190t0 and PTc\nt\u2190t is the identity. In particular, the\ninverse of PTc\nt1\u2190t0 is PTc\nt0\u2190t1 . If M is Riemannian and \u2207 is compatible with\nthe Riemannian metric, then 4 parallel transport is an isometry, that is,\n\u2200u, v\u2208 Tc(t0)M, \u27e8u, v\u27e9c(t0) =\n\nPTc\nt1\u2190t0 (u), PTc\nt1\u2190t0 (v)\n\u000b\nc(t1) .\nStated differently, the adjoint and the inverse of PTc\nt1\u2190t0 coincide.\nProof. For linearity, consider u, v\u2208 Tc(t0)M and a, b\u2208 R, arbitrary. By The-\norem 10.34, there exist unique parallel vector fields Zu, Zv \u2208 X(c) such that\nZu(t0) = u and Zv(t0) = v. Since Z = aZu + bZv \u2208 X(c) is also parallel and\nZ(t0) = au + bv, we conclude that Z is the unique parallel vector field used in\nthe definition of\nPTc\nt1\u2190t0 (au + bv) = Z(t1) = aZu(t1) + bZv(t1)\n= aPTc\nt1\u2190t0 (u) + bPTc\nt1\u2190t0 (v),\nwhich shows linearity. The composition rule is clear, as is the fact that PT c\nt\u2190t is\nthe identity. Then, the inverse follows by setting t2 = t0 in the composition rule.\nTo verify isometry, notice that\nd\ndt\u27e8Zu(t), Zv(t)\u27e9c(t) =\n\u001cD\ndtZu(t), Zv(t)\n\u001d\nc(t)\n+\n\u001c\nZu(t), D\ndtZv(t)\n\u001d\nc(t)\n= 0,\nusing compatibility of the covariant derivative with the Riemannian metric and\nthe fact that Zu, Zv are parallel. Thus, the inner product is constant along c.\nOne convenient tool afforded to us by parallel transports is the notion of\nparallel frames along a curve c. Consider an arbitrary basis e1, . . . , ed for the\ntangent space at c(\u00aft), for an arbitrary \u00aft in the domain of definition ofc. Construct\nthe parallel vector fields\nEi(t) = PTc\nt\u2190\u00aft(ei), i = 1, . . . , d. (10.10)\nSince parallel transports are invertible, for all t, the vectors Ei(t) form a basis for\nthe tangent space at c(t). (Also, if the manifold is Riemannian and we transport\nvia a connection that is compatible with the metric, then orthonormality would\n4 The converse also holds: if parallel transport is an isometry, then \u2207 is compatible with the\nmetric [Lee18, Prop. 5.5].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2855, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9b2d9b4e-3618-4ecd-992d-3788fbc2855c": {"__data__": {"id_": "9b2d9b4e-3618-4ecd-992d-3788fbc2855c", "embedding": null, "metadata": {"page_label": "273", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "97f6bb7c-60ed-4be7-89cf-2a2d820277ed", "node_type": "4", "metadata": {"page_label": "273", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "a2c7d9584b1c61caf16cb926873a2cb769ce536a1040a80f6fb34b778369c0ec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n10.3 Parallel transport 273\nbe preserved.) As a result, for any Y \u2208 X(c) there exist unique, real functions \u03b1i\nsuch that\nY (t) =\ndX\ni=1\n\u03b1i(t)Ei(t). (10.11)\nThese functions are smooth since Y is smooth. Owing to linearity,\nPTc\nt1\u2190t0 (Y (t0)) =\ndX\ni=1\n\u03b1i(t0)Ei(t1).\nIn particular, we see that this is smooth in both t0 and t1.\nUsing parallel frames, we can show that covariant derivatives admit a con-\nvenient expression in terms of parallel transports: transport the vector field to\na common tangent space, differentiate in the usual way (in that fixed tangent\nspace), then transport back.\nProposition 10.37. Consider a smooth curve c: I \u2192 M. Given a vector field\nZ \u2208 X(c) and t0 \u2208 I, let z : I \u2192 Tc(t0)M be z(t) = PTc\nt0\u2190tZ(t). Then,\nD\ndtZ(t) = PTc\nt\u2190t0\n\u0012 d\ndtz(t)\n\u0013\n= lim\n\u03b4\u21920\nPTc\nt\u2190t+\u03b4Z(t + \u03b4) \u2212 Z(t)\n\u03b4 .\nProof. Transport any basis e1, . . . , ed of Tc(t0)M along c to form a frame Ei(t) =\nPTc\nt\u2190t0 ei. There exist unique, smooth, real functions\u03b11, . . . , \u03b1d such that Z(t) =Pd\ni=1 \u03b1i(t)Ei(t). Then, by the properties of covariant derivatives (Theorem 5.29)\nand D\ndt Ei = 0,\nD\ndtZ(t) =\ndX\ni=1\n\u03b1\u2032\ni(t)Ei(t)\n= PTc\nt\u2190t0\ndX\ni=1\n\u03b1\u2032\ni(t)ei\n= PTc\nt\u2190t0\nd\ndt\ndX\ni=1\n\u03b1i(t)ei = PTc\nt\u2190t0\nd\ndtz(t),\nas announced. The important point is that z is a map between (fixed) linear\nspaces, which is why we can take a classical derivative d\ndt .\nExercise 10.38. On the sphere Sn\u22121 as a Riemannian submanifold of Rn with\nthe usual metric and connection, parallel transport along the geodesic c(t) =\nExpx(tv) admits the following explicit expression [QGA10a]:\nPTc\nt\u21900(u) =\n\u0012\nIn + (cos(t\u2225v\u2225) \u2212 1) vv\u22a4\n\u2225v\u22252 \u2212 sin(t\u2225v\u2225)xv\u22a4\n\u2225v\u2225\n\u0013\nu.\nVerify this claim. (Recall Example 5.37 for the exponential.)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1927, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "313de4db-a1e7-4bd5-ac63-36afc713995e": {"__data__": {"id_": "313de4db-a1e7-4bd5-ac63-36afc713995e", "embedding": null, "metadata": {"page_label": "274", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f62c1df-e1f8-47a2-b79f-b79839306c0d", "node_type": "4", "metadata": {"page_label": "274", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "2bfb35cd6de0e7fcb0da8cc1b9f994938871e97f3afdfe429be640eedab07436", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n274 Additional tools\nExercise 10.39. Let c = (c1, c2) be a smooth curve on a Riemannian product\nmanifold M = M1 \u00d7M2. Verify that parallel transport along c is given in terms\nof parallel transports along c1 and c2 as\nPTc\ntb\u2190ta(v) =\n\u0000\nPTc1\ntb\u2190ta(v1), PTc2\ntb\u2190ta(v2)\n\u0001\n,\nwhere v = (v1, v2) is tangent to M at c(ta). Hint: recall Exercise 5.34.\n10.4 Lipschitz conditions and Taylor expansions\nOne of the most convenient regularity assumptions one can make regarding the\ncost function f of an optimization problem is that it or its derivatives be Lipschitz\ncontinuous. Indeed, in the Euclidean case, it is well known that such properties\nlead to global bounds on the discrepancy between f and its Taylor expansions\nof various orders. These, in turn, ease worst-case iteration complexity analyses.\nHere, we consider definitions of Lipschitz continuity on Riemannian manifolds,\nand we derive Taylor bounds analogous to their Euclidean counterparts. In so\ndoing, we are careful not to require the manifold to be complete.\nIn this section,\u22c6\n1. We implicitly assume all of our manifolds are Riemannian,\n2. We usually omit subscripts for inner products and norms (writing \u27e8\u00b7, \u00b7\u27e9 and\n\u2225 \u00b7 \u2225instead of \u27e8\u00b7, \u00b7\u27e9x and \u2225 \u00b7 \u2225x), and\n3. We state explicitly how many times we need maps to be (continuously) differ-\nentiable, thus not assuming smoothness (infinite differentiability) by default:\nrecall Remark 8.6.\nLet A, Bbe two metric spaces. A map F : A \u2192 B is L-Lipschitz continuous if\nL \u2265 0 is such that\n\u2200x, y\u2208 A, distB(F(x), F(y)) \u2264 L distA(x, y), (10.12)\nwhere distA, distB denote the distances on A and B. In particular:\nDefinition 10.40. A function f : M \u2192R on a connected manifold M is L-\nLipschitz continuous if\n\u2200x, y\u2208 M, |f(x) \u2212 f(y)| \u2264L dist(x, y), (10.13)\nwhere dist is the Riemannian distance on M. If M is disconnected, we require\nthe condition to hold on each connected component separately.\nThe definition above can be reformulated as we show below. This second formu-\nlation is more convenient to study iterates of optimization algorithms presented\nas xk+1 = Expxk (sk) for some sk \u2208 Txk M.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2344, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dd27ee08-683a-45c8-9d79-f090516961f0": {"__data__": {"id_": "dd27ee08-683a-45c8-9d79-f090516961f0", "embedding": null, "metadata": {"page_label": "275", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4a007f77-a9da-47a7-97ee-58ec53715948", "node_type": "4", "metadata": {"page_label": "275", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "e17174362fd515ffcb4619e3ece328b36410fe78b75d6435bb8906a6a5811a72", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n10.4 Lipschitz conditions and Taylor expansions 275\nc(t)\n\u03b3(t)\nM\nnot M\nFigure 10.2 In Lemma 10.42, the curve \u03b3 is made of a finite number of minimizing\ngeodesic segments, with endpoints on c.\nProposition 10.41. A function f : M \u2192R is L-Lipschitz continuous if and\nonly if\n\u2200(x, s) \u2208 O, |f(Expx(s)) \u2212 f(x)| \u2264L\u2225s\u2225, (10.14)\nwhere O \u2286TM is the domain of the exponential map (Definition 10.16).\nTo prove this, we first introduce a lemma which states that any continuous\ncurve c can be interpolated by a \u2018broken geodesic\u2019\u03b3. Also, if c is piecewise smooth\nit has a length and we have L(\u03b3) \u2264 L(c). See Figure 10.2.\nLemma 10.42. Given c: [0, 1] \u2192 Mcontinuous on a manifold M, there ex-\nist a finite number of times 0 = t0 < t1 < \u00b7 \u00b7\u00b7< tn\u22121 < tn = 1 such that\ndist(c(ti), c(ti+1)) < inj(c(ti)) for i = 0, . . . , n\u2212 1.\nThese times define a piecewise regular curve \u03b3 : [0, 1] \u2192 Msatisfying \u03b3(ti) =\nc(ti) and such that \u03b3|[ti,ti+1] is the minimizing geodesic connecting its endpoints.\nAs such, there exist tangent vectors s0, . . . , sn\u22121 such that \u03b3(ti+1) = Exp\u03b3(ti)(si)\nand Pn\u22121\ni=0 \u2225si\u2225 = L(\u03b3).\nProof. Consider the recursive routine construct with inputs a, bwhich proceeds\nas follows: if dist( c(a), c(b)) < inj(c(a)), return ( a, b); if not, return the results\nof construct(a, (a + b)/2) and construct((a + b)/2, b) merged. We claim that\nconstruct(0, 1) is an appropriate selection. Indeed, the routine terminates after\na finite number of steps because inj \u25e6c is continuous and positive on the compact\ndomain [0, 1] so that it is bounded away from zero, andc is continuous so that for\n\u03b5 >0 small enough we can have dist(c(t), c(t+\u03b5)) arbitrarily small. Furthermore,\nfor all selected ti, ti+1, we have dist(c(ti), c(ti+1)) < inj(c(ti)). Hence, there exists\na (unique) minimizing geodesic connecting c(ti) to c(ti+1), for all i, and \u2225si\u2225 =\nL(\u03b3|[ti,ti+1]). (We used Proposition 10.22.)\nProof of Proposition 10.41. Under condition (10.13), the claim is clear:\n\u2200(x, s) \u2208 O, |f(Expx(s)) \u2212 f(x)| \u2264L dist(Expx(s), x) \u2264 L\u2225s\u2225\nsince t 7\u2192 Expx(ts) is a smooth curve defined on [0 , 1] with length \u2225s\u2225.\nThe other way around, let us assume condition (10.14) holds. IfM is complete", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2414, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7e32ece6-2eb9-4a02-933c-dee36e2ec95c": {"__data__": {"id_": "7e32ece6-2eb9-4a02-933c-dee36e2ec95c", "embedding": null, "metadata": {"page_label": "276", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9bddfd4f-f097-4a4f-aaa6-7e0824ef3948", "node_type": "4", "metadata": {"page_label": "276", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "91b65b45baf1070f23b32e942e9b867152da55fed5d31216b01d4b975a093bc5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n276 Additional tools\n(that is, O = TM) the claim is also clear: by Theorem 10.9, for all x, y\u2208 M\nin the same connected component there exists s in TxM such that y = Expx(s)\nand \u2225s\u2225 = dist(x, y).\nIf M is not complete, we proceed as follows: by definition of distance (10.2),\nfor all x, y\u2208 Min the same connected component and for all \u03b5 > 0, there\nexists a piecewise regular curve c: [0, 1] \u2192 Mwith length L(c) \u2264 dist(x, y) + \u03b5\nsuch that c(0) = x and c(1) = y. Construct a broken geodesic \u03b3 as provided\nby Lemma 10.42: there exist times 0 = t0 < \u00b7 \u00b7\u00b7< tn = 1 and tangent vectors\ns0, . . . , sn\u22121 such that \u03b3(ti) = c(ti) and \u03b3(ti+1) = Exp \u03b3(ti)(si) for all i, andPn\u22121\ni=0 \u2225si\u2225 = L(\u03b3) \u2264 L(c). Then,\n|f(x) \u2212 f(y)| \u2264\nn\u22121X\ni=0\n|f(\u03b3(ti)) \u2212 f(\u03b3(ti+1))|\n(10.14)\n\u2264\nn\u22121X\ni=0\nL \u00b7 \u2225si\u2225 \u2264L \u00b7 L(c).\nThis reasoning holds for all \u03b5 >0, hence condition (10.13) follows.\nIf f has a continuous gradient, then f is Lipschitz continuous exactly if its\ngradient is bounded.\nProposition 10.43. If f : M \u2192R has a continuous gradient, then f is L-\nLipschitz continuous if and only if\n\u2200x \u2208 M, \u2225gradf(x)\u2225 \u2264L. (10.15)\nProof. For any (x, s) \u2208 O, consider c(t) = Expx(ts) for t \u2208 [0, 1]. Then,\nf(c(1)) \u2212 f(c(0)) =\nZ 1\n0\n(f \u25e6 c)\u2032(t)dt =\nZ 1\n0\n\u27e8gradf(c(t)), c\u2032(t)\u27e9dt.\nThus, if the gradient norm is bounded by L at all points along c,\n|f(Expx(s)) \u2212 f(x)| \u2264L\nZ 1\n0\n\u2225c\u2032(t)\u2225dt = L \u00b7 L(c) = L\u2225s\u2225.\nThis shows that (10.14) holds.\nThe other way around, for any x \u2208 M, assuming (10.14) holds and using that\nthe domain of Exp x is open around the origin, we have:\n\u2225gradf(x)\u2225 = max\ns\u2208TxM,\u2225s\u2225=1\n\u27e8gradf(x), s\u27e9\n= max\ns\u2208TxM,\u2225s\u2225=1\nDf(x)[s]\n= max\ns\u2208TxM,\u2225s\u2225=1\nlim\nt\u21920+\nf(Expx(ts)) \u2212 f(x)\nt \u2264 L,\nsince f(Expx(ts)) \u2212 f(x) \u2264 L\u2225ts\u2225 = L|t|.\nWe now turn to defining Lipschitz continuity for the Riemannian gradient of\na function f. Since gradf is a map from M to TM, to apply the general notion\nof Lipschitz continuity directly we would need to pick a distance on the tangent\nbundle. However, this would not lead to interesting notions for us. Indeed, the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2255, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cb50dfdc-9814-4ba4-8722-4b29dd122255": {"__data__": {"id_": "cb50dfdc-9814-4ba4-8722-4b29dd122255", "embedding": null, "metadata": {"page_label": "277", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f286dde6-5e05-4582-876a-f6e6b14c2966", "node_type": "4", "metadata": {"page_label": "277", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "ed9bbaf419b5ac9d947c19bbb309e0065ef17a1874301fd606972b9c654ab0ef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n10.4 Lipschitz conditions and Taylor expansions 277\ndistance between grad f(x) and grad f(y) would necessarily have to be positive\nif x \u0338= y since they would always be distinct points in T M. Contrast this to the\nEuclidean case f : E \u2192R, where it is natural to measure grad f(x) \u2212 gradf(y)\nin the Euclidean metric, disregarding the base points. With this in mind, it\nis reasonable to resort to parallel transport (Section 10.3) to compare tangent\nvectors at distinct points. Since parallel transport is dependent on paths, this\nleaves some leeway in the definition.\nThe following definition is fairly common. Notice how the restriction by the\ninjectivity radius allows us to choose a privileged path along which to transport\n(owing to Proposition 10.22). For our purpose, the notion below is particularly\nrelevant with V = grad f, in which case we would say f has an L-Lipschitz\ncontinuous gradient.\nDefinition 10.44. A vector field V on a connected manifold M is L-Lipschitz\ncontinuous if, for all x, y\u2208 Mwith dist(x, y) < inj(x),\n\u2225PT\u03b3\n0\u21901V (y) \u2212 V (x)\u2225 \u2264L dist(x, y), (10.16)\nwhere \u03b3 : [0, 1] \u2192 Mis the unique minimizing geodesic connecting x to y. If M\nis disconnected, we require the condition on each connected component.\nHere too, we provide an equivalent definition in terms of the exponential map:\nthis may be more convenient to analyze optimization algorithms, and has the\nadded benefit of allowing the comparison of points which are further apart than\nthe injectivity radius (but still connected by a geodesic).\nProposition 10.45. A vector field V on a manifold M is L-Lipschitz continuous\nif and only if\n\u2200(x, s) \u2208 O, \u2225P\u22121\ns V (Expx(s)) \u2212 V (x)\u2225 \u2264L\u2225s\u2225, (10.17)\nwhere O \u2286TM is the domain of Exp and Ps denotes parallel transport along\n\u03b3(t) = Expx(ts) from t = 0 to t = 1.\nProof. For any x, y\u2208 Msuch that dist( x, y) < inj(x), there exists a unique\ns \u2208 TxM such that y = Expx(s) and \u2225s\u2225 = dist(x, y). Thus, if condition (10.17)\nholds, then (10.16) holds.\nThe other way around, for any ( x, s) \u2208 O, consider the geodesic \u03b3(t) =\nExpx(ts) defined over [0, 1]. It may or may not be minimizing. In any case, owing\nto Lemma 10.42, the interval [0 , 1] can be partitioned by 0 = t0 < \u00b7 \u00b7\u00b7< tn = 1\nsuch that dist(\u03b3(ti), \u03b3(ti+1)) < inj(\u03b3(ti)). Since\nPT\u03b3\n0\u21901 = PT\u03b3\nt0\u2190tn\u22121 \u25e6 PT\u03b3\ntn\u22121\u2190tn", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2542, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eb554ecb-673a-4cf8-a9dd-4619ca1477f1": {"__data__": {"id_": "eb554ecb-673a-4cf8-a9dd-4619ca1477f1", "embedding": null, "metadata": {"page_label": "278", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6a233649-a7fb-4d92-96f8-cdd486554d17", "node_type": "4", "metadata": {"page_label": "278", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "ac0e1db64fa621b1263b1d773d56bc045f0ca3a9d7dc05f2a24cea03fa970ea5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n278 Additional tools\nand since parallel transport is an isometry, we find that\n\u2225PT\u03b3\nt0\u2190tnV (\u03b3(tn)) \u2212 V (x)\u2225 = \u2225PT\u03b3\ntn\u22121\u2190tnV (\u03b3(tn)) \u2212 PT\u03b3\ntn\u22121\u2190t0 V (x)\u2225\n\u2264 \u2225PT\u03b3\ntn\u22121\u2190tnV (\u03b3(tn)) \u2212 V (\u03b3(tn\u22121))\u2225\n+ \u2225PT\u03b3\ntn\u22121\u2190t0 V (x) \u2212 V (\u03b3(tn\u22121))\u2225\n\u2264 L dist(\u03b3(tn\u22121), \u03b3(tn))\n+ \u2225PT\u03b3\nt0\u2190tn\u22121 V (\u03b3(tn\u22121)) \u2212 V (x)\u2225,\nwhere in the last step we were able to use (10.16) since \u03b3|[tn\u22121,tn] is the unique\nminimizing geodesic connecting \u03b3(tn\u22121) and \u03b3(tn). Repeat this argument on the\nright-most term n \u2212 1 times to see that\n\u2225PT\u03b3\n0\u21901V (Expx(s)) \u2212 V (x)\u2225 \u2264L\nn\u22121X\ni=0\ndist(\u03b3(ti), \u03b3(ti+1)) = L \u00b7 L(\u03b3),\nusing that \u03b3 is a geodesic. To conclude, note that L(\u03b3) = \u2225s\u2225 and that PT\u03b3\n0\u21901 =\nP\u22121\ns , so that condition (10.17) holds.\nIf the vector field V is continuously differentiable, then Lipschitz continuity of\nV is equivalent to boundedness of its covariant derivative. In turn, this makes\nit possible to compare the values of V at points connected by curves other than\ngeodesics.\nProposition 10.46. If V is a continuously differentiable vector field on a man-\nifold M, then it is L-Lipschitz continuous if and only if\n\u2200(x, s) \u2208 TM, \u2225\u2207sV \u2225 \u2264L\u2225s\u2225, (10.18)\nwhere \u2207 is the Riemannian connection. In that case, for any smooth curve\nc: [0, 1] \u2192 Mconnecting any x to any y, it holds that\n\u2225PTc\n0\u21901V (y) \u2212 V (x)\u2225 \u2264L \u00b7 L(c). (10.19)\nProof. We first show that (10.18) implies (10.19). Since the latter itself im-\nplies (10.17), this also takes care of showing that (10.18) implies V is L-Lipschitz\ncontinuous. To this end, consider an orthonormal basis e1, . . . , ed \u2208 TxM and\ntheir parallel transports Ei(t) = PTc\nt\u21900(ei). Then, V (c(t)) = Pd\ni=1 vi(t)Ei(t) for\nsome continuously differentiable functions vi, and\ndX\ni=1\nv\u2032\ni(t)Ei(t) = D\ndt(V \u25e6 c)(t) = \u2207c\u2032(t)V.\nFurthermore,\nPTc\n0\u21901V (c(1)) \u2212 V (c(0)) =\ndX\ni=1\n(vi(1) \u2212 vi(0))ei\n=\ndX\ni=1\n\u0012Z 1\n0\nv\u2032\ni(t)dt\n\u0013\nei =\nZ 1\n0\nPTc\n0\u2190t\n\u0000\n\u2207c\u2032(t)V\n\u0001\ndt.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2100, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "db87863c-e864-493c-a267-5ce64cd7d207": {"__data__": {"id_": "db87863c-e864-493c-a267-5ce64cd7d207", "embedding": null, "metadata": {"page_label": "279", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "670f214d-2fee-4eba-ae05-e4af5389371a", "node_type": "4", "metadata": {"page_label": "279", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "1739f5c9aac38d42ebaebf3152f8c66e02ada8808685950e5ccd303de4c9b879", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n10.4 Lipschitz conditions and Taylor expansions 279\nConsequently, using that parallel transports are isometric,\n\u2225PTc\n0\u21901V (y) \u2212 V (x)\u2225 \u2264\nZ 1\n0\n\u2225\u2207c\u2032(t)V \u2225dt\n(10.18)\n\u2264 L\nZ 1\n0\n\u2225c\u2032(t)\u2225dt = L \u00b7 L(c).\nNow for the other direction: assumeV is L-Lipschitz continuous. For anyx \u2208 M,\nusing that the domain of Exp x is open around the origin, we know that for all\ns \u2208 TxM the smooth curve c(t) = Expx(ts) is defined around t = 0. Then, by\nProposition 10.37,\n\u2207sV = D\ndtV (c(t))\n\f\f\f\f\nt=0\n= lim\nt\u21920\nPTc\n0\u2190tV (c(t)) \u2212 V (c(0))\nt .\nBy (10.17), the norm of the numerator is bounded by L\u2225ts\u2225, which concludes\nthe proof.\nCorollary 10.47. If f : M \u2192R is twice continuously differentiable on a mani-\nfold M, then gradf is L-Lipschitz continuous if and only if Hessf(x) has operator\nnorm bounded by L for all x, that is, if for all x we have\n\u2225Hessf(x)\u2225 = max\ns\u2208TxM\n\u2225s\u2225=1\n\u2225Hessf(x)[s]\u2225 \u2264L.\nLet us summarize these findings.\nCorollary 10.48. For a vector field V on a manifold M, these are equivalent:\n1. V is L-Lipschitz continuous.\n2. For all x, yin the same component with dist(x, y) < inj(x), it holds that\n\u2225PT\u03b3\n0\u21901V (y) \u2212 V (x)\u2225 \u2264L dist(x, y) with \u03b3 the unique minimizing geodesic\nconnecting x to y.\n3. For all (x, s) in the domain of Exp, \u2225P\u22121\ns V (Expx(s)) \u2212V (x)\u2225 \u2264L\u2225s\u2225, where\nPs is parallel transport along c(t) = Expx(ts) from t = 0 to t = 1.\nIf V is continuously differentiable, the above are equivalent to the following:\n4. For all smooth c: [0, 1] \u2192 M, \u2225PTc\n0\u21901V (c(1)) \u2212 V (c(0))\u2225 \u2264L \u00b7 L(c).\n5. For all (x, s) \u2208 TM, \u2225\u2207sV \u2225 \u2264L\u2225s\u2225.\nParticularizing the above to V = grad f provides a good understanding of\nfunctions f with Lipschitz continuous gradients.\nGoing one degree higher, we now define and (begin to) discuss functions with\na Lipschitz continuous Hessian . The Hessian of f associates to each x a linear\nmap Hessf(x) from TxM to itself. The following definition applies.\nDefinition 10.49. For each x \u2208 M, let H(x): T xM \u2192TxM be linear. If M\nis connected, we say H is L-Lipschitz continuous if for all x, y\u2208 Msuch that\ndist(x, y) < inj(x) we have\n\u2225PT\u03b3\n0\u21901 \u25e6 H(y) \u25e6 PT\u03b3\n1\u21900 \u2212 H(x)\u2225 \u2264L dist(x, y), (10.20)\nwhere \u2225 \u00b7 \u2225denotes the operator norm with respect to the Riemannian metric,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2430, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "80a92745-daf2-496d-b18a-10226dafa0c3": {"__data__": {"id_": "80a92745-daf2-496d-b18a-10226dafa0c3", "embedding": null, "metadata": {"page_label": "280", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bafb9a70-4250-43d8-9cce-d99225bc788e", "node_type": "4", "metadata": {"page_label": "280", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "b62643e30af4c7dce51d5cb887289580ebcc757723d37e54d2da4e9bd1ce4da3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n280 Additional tools\nand \u03b3 : [0, 1] \u2192 Mis the unique minimizing geodesic connecting x to y. If M is\ndisconnected, we require the condition on each connected component.\nRecall the operator norm is \u2225H(x)\u2225 = maxs\u2208TxM,\u2225s\u2225=1 \u2225H(x)[s]\u2225. The proof\nof the following proposition is left as an exercise.\nProposition 10.50. For each x \u2208 M, let H(x): T xM \u2192TxM be linear. The\nmap H is L-Lipschitz continuous if and only if\n\u2200(x, s) \u2208 O, \u2225P\u22121\ns \u25e6 H(Expx(s)) \u25e6 Ps \u2212 H(x)\u2225 \u2264L\u2225s\u2225, (10.21)\nwhere O \u2286TM is the domain of Exp and Ps denotes parallel transport along\n\u03b3(t) = Expx(ts) from t = 0 to t = 1.\nIn Section 10.7, we define what it means for a map H as above to be differ-\nentiable, and we define its covariant derivative \u2207H. Then, as a particular case\nof Proposition 10.83 we get the following claim, analogous to Proposition 10.46\nabove.\nProposition 10.51. For each x \u2208 M, let H(x): T xM \u2192TxM be linear. If H\nis continuously differentiable, it is L-Lipschitz continuous if and only if\n\u2200(x, s) \u2208 TM, \u2225\u2207sH\u2225 \u2264L\u2225s\u2225, (10.22)\nwhere \u2207 is the Riemannian connection and \u2207sH : TxM \u2192TxM is linear. In\nthat case, for any smooth curve c: [0, 1] \u2192 Mconnecting x to y, we have\n\u2225PTc\n0\u21901 \u25e6 H(y) \u25e6 PTc\n1\u21900 \u2212 H(x)\u2225 \u2264L \u00b7 L(c). (10.23)\nCorollary 10.52. If f : M \u2192R is three times continuously differentiable on a\nmanifold M, then Hessf is L-Lipschitz continuous if and only if\n\u2200(x, s) \u2208 TM, \u2225\u2207sHessf\u2225 \u2264L\u2225s\u2225,\nwhere \u2207sHessf is a self-adjoint linear map on TxM defined by (10.49).\nA summary of the same kind as Corollary 10.48 holds here as well.\nIn Section 10.7, we show how to cast f, gradf and Hessf as tensor fields of\norder zero, one and two, respectively. We show how the Riemannian connection\ncan be used to differentiate tensor fields in general, and we discuss Lipschitz\ncontinuity at that level of generality. This provides for the missing details in our\nbrief discussion of Lipschitz continuous Hessians, and indicates how to deal with\nderivatives of arbitrary order.\nWe can now derive some of the most useful consequences of Lipschitz conti-\nnuity, namely, bounds on the difference between a function f (or its derivatives)\nand corresponding Taylor expansions.\nWe use the following notation often: given ( x, s) in the domain O of the ex-\nponential map, let \u03b3(t) = Exp x(ts) be the corresponding geodesic (defined in\nparticular on the interval [0, 1]); then, we let\nPts = PT\u03b3\nt\u21900 (10.24)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2620, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0d930596-c103-49c0-8a95-2e72011abe61": {"__data__": {"id_": "0d930596-c103-49c0-8a95-2e72011abe61", "embedding": null, "metadata": {"page_label": "281", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4ce3996c-b193-4014-a767-c4ef44f971ff", "node_type": "4", "metadata": {"page_label": "281", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "32a018f966b78a67a255e3c62488a3f701d7379a734d8c0b17c7b6bc8ca924bf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n10.4 Lipschitz conditions and Taylor expansions 281\ndenote parallel transport from x to Expx(ts) along \u03b3. Since \u03b3 is a geodesic, its\nvelocity vector field is parallel and we have\n\u03b3\u2032(t) = Pts\u03b3\u2032(0) = Ptss. (10.25)\nThis will be helpful a number of times.\nProposition 10.53. Let f : M \u2192R be continuously differentiable on a manifold\nM. Let \u03b3(t) = Expx(ts) be defined on [0, 1] and assume there exists L \u2265 0 such\nthat, for all t \u2208 [0, 1],\n\r\rP\u22121\nts gradf(\u03b3(t)) \u2212 gradf(x)\n\r\r \u2264 L\u2225ts\u2225.\nThen, the following inequality holds:\n|f(Expx(s)) \u2212 f(x) \u2212 \u27e8s, gradf(x)\u27e9| \u2264L\n2 \u2225s\u22252.\nProof. Consider the real function f \u25e6 \u03b3 on [0, 1]; we have:\nf(\u03b3(1)) = f(\u03b3(0)) +\nZ 1\n0\n(f \u25e6 \u03b3)\u2032(t)dt\n= f(x) +\nZ 1\n0\n\u27e8gradf(\u03b3(t)), \u03b3\u2032(t)\u27e9dt\n= f(x) +\nZ 1\n0\n\nP\u22121\nts gradf(\u03b3(t)), s\n\u000b\ndt,\nwhere on the last line we used \u03b3\u2032(t) = Ptss (10.25) and the fact that Pts is an\nisometry, so that its adjoint with respect to the Riemannian metric is equal to\nits inverse. Moving f(x) to the left-hand side and subtracting \u27e8gradf(x), s\u27e9 on\nboth sides, we get\nf(Expx(s)) \u2212 f(x) \u2212 \u27e8gradf(x), s\u27e9 =\nZ 1\n0\n\nP\u22121\nts gradf(\u03b3(t)) \u2212 gradf(x), s\n\u000b\ndt.\nUsing Cauchy\u2013Schwarz and our main assumption, it follows that\n|f(Expx(s)) \u2212 f(x) \u2212 \u27e8s, gradf(x)\u27e9| \u2264\nZ 1\n0\ntL\u2225s\u22252dt = L\n2 \u2225s\u22252,\nas announced.\nThe following corollary shows that the regularity assumptions A4.3 (p59)\nand A6.6 (p135) hold for the exponential retraction over its whole domain pro-\nvided gradf is L-Lipschitz (Definition 10.44). See also Exercise 10.58.\nCorollary 10.54. If f : M \u2192R has L-Lipschitz continuous gradient, then\n|f(Expx(s)) \u2212 f(x) \u2212 \u27e8s, gradf(x)\u27e9| \u2264L\n2 \u2225s\u22252\nfor all (x, s) in the domain of the exponential map.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1878, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b5ed8d35-6dbf-46db-8c2b-3ba8d25b60db": {"__data__": {"id_": "b5ed8d35-6dbf-46db-8c2b-3ba8d25b60db", "embedding": null, "metadata": {"page_label": "282", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "47530729-e34b-4a50-8d0f-fd3bd80a70b0", "node_type": "4", "metadata": {"page_label": "282", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "89e18da262c1e6ae77aba19befa637c2afc3b3d98ca85dcbc2747b1b70f78ccd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n282 Additional tools\nProposition 10.55. Let f : M \u2192R be twice continuously differentiable on a\nmanifold M. Let \u03b3(t) = Exp x(ts) be defined on [0, 1] and assume there exists\nL \u2265 0 such that, for all t \u2208 [0, 1],\n\r\rP\u22121\nts \u25e6 Hessf(\u03b3(t)) \u25e6 Pts \u2212 Hessf(x)\n\r\r \u2264 L\u2225ts\u2225.\nThen, the two following inequalities hold:\n\f\f\f\ff(Expx(s)) \u2212 f(x) \u2212 \u27e8s, gradf(x)\u27e9 \u22121\n2 \u27e8s, Hessf(x)[s]\u27e9\n\f\f\f\f \u2264 L\n6 \u2225s\u22253,\n\r\rP\u22121\ns gradf(Expx(s)) \u2212 gradf(x) \u2212 Hessf(x)[s]\n\r\r \u2264 L\n2 \u2225s\u22252.\nProof. The proof is in three steps.\nStep 1: a preliminary computation. Pick an arbitrary basis e1, . . . , ed for\nTxM and define the parallel vector fields Ei(t) = Ptsei along \u03b3(t). The vectors\nE1(t), . . . , Ed(t) form a basis for T \u03b3(t)M for each t \u2208 [0, 1]. As a result, we can\nexpress the gradient of f along \u03b3(t) in these bases,\ngradf(\u03b3(t)) =\ndX\ni=1\n\u03b1i(t)Ei(t), (10.26)\nwith \u03b11(t), . . . , \u03b1d(t) differentiable. Using the Riemannian connection \u2207 and\nassociated covariant derivative D\ndt , we find on the one hand that\nD\ndtgradf(\u03b3(t)) = \u2207\u03b3\u2032(t)gradf = Hessf(\u03b3(t))[\u03b3\u2032(t)],\nand on the other hand that\nD\ndt\ndX\ni=1\n\u03b1i(t)Ei(t) =\ndX\ni=1\n\u03b1\u2032\ni(t)Ei(t) = Pts\ndX\ni=1\n\u03b1\u2032\ni(t)ei.\nCombining with \u03b3\u2032(t) = Ptss (10.25), we deduce that\ndX\ni=1\n\u03b1\u2032\ni(t)ei =\n\u0000\nP\u22121\nts \u25e6 Hessf(\u03b3(t)) \u25e6 Pts\n\u0001\n[s].\nGoing back to (10.26), we also see that\nG(t) \u225c P\u22121\nts gradf(\u03b3(t)) =\ndX\ni=1\n\u03b1i(t)ei\nis a map from (a subset of) R to T xM\u2014two linear spaces\u2014so that we can\ndifferentiate it in the usual way:\nG\u2032(t) =\ndX\ni=1\n\u03b1\u2032\ni(t)ei.\nOverall, we conclude that\nG\u2032(t) = d\ndtP\u22121\nts gradf(\u03b3(t)) =\n\u0000\nP\u22121\nts \u25e6 Hessf(\u03b3(t)) \u25e6 Pts\n\u0001\n[s]. (10.27)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1798, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "01db5fe1-00c8-435d-a84b-271d0af75ddf": {"__data__": {"id_": "01db5fe1-00c8-435d-a84b-271d0af75ddf", "embedding": null, "metadata": {"page_label": "283", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "450b9095-9db1-4a7d-aca0-28221ac7ba21", "node_type": "4", "metadata": {"page_label": "283", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "53ee9eb5d2f64a6eceb10dc40e98ea3450782ac235c2626a42063cb6dc2c57c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n10.4 Lipschitz conditions and Taylor expansions 283\nThis comes in handy in the next step.\nStep 2: Taylor expansion of the gradient. Since G\u2032 is continuous,\nP\u22121\nts gradf(\u03b3(t)) = G(t) = G(0) +\nZ t\n0\nG\u2032(\u03c4)d\u03c4\n= gradf(x) +\nZ t\n0\n\u0000\nP\u22121\n\u03c4s \u25e6 Hessf(\u03b3(\u03c4)) \u25e6 P\u03c4s\n\u0001\n[s]d\u03c4.\nMoving grad f(x) to the left-hand side and subtracting Hess f(x)[ts] on both\nsides, we find\nP\u22121\nts gradf(\u03b3(t)) \u2212 gradf(x) \u2212 Hessf(x)[ts]\n=\nZ t\n0\n\u0000\nP\u22121\n\u03c4s \u25e6 Hessf(\u03b3(\u03c4)) \u25e6 P\u03c4s \u2212 Hessf(x)\n\u0001\n[s]d\u03c4.\nUsing the main assumption on Hess f along \u03b3, it follows that\n\r\rP\u22121\nts gradf(\u03b3(t)) \u2212 gradf(x) \u2212 Hessf(x)[ts]\n\r\r\n\u2264\nZ t\n0\n\u03c4L\u2225s\u22252d\u03c4 = L\n2 \u2225ts\u22252. (10.28)\nFor t = 1, this is one of the announced inequalities.\nStep 3: Taylor expansion of the function value. With the same start as\nin the proof of Proposition 10.53 and subtracting the term 1\n2 \u27e8s, Hessf(x)[s]\u27e9 on\nboth sides, we get\nf(Expx(s)) \u2212 f(x) \u2212 \u27e8gradf(x), s\u27e9 \u22121\n2 \u27e8s, Hessf(x)[s]\u27e9\n=\nZ 1\n0\n\nP\u22121\nts gradf(\u03b3(t)) \u2212 gradf(x) \u2212 Hessf(x)[ts], s\n\u000b\ndt.\nUsing (10.28) and Cauchy\u2013Schwarz, it follows that\n\f\f\f\ff(Expx(s)) \u2212 f(x) \u2212 \u27e8s, gradf(x)\u27e9 \u22121\n2 \u27e8s, Hessf(x)[s]\u27e9\n\f\f\f\f\n\u2264\nZ 1\n0\nt2 L\n2 \u2225s\u22253dt = L\n6 \u2225s\u22253,\nas announced.\nThe following corollary shows that the regularity assumption A6.7 (p135)\nholds for the exponential retraction over its whole domain provided Hess f is\nL-Lipschitz (Definition 10.49). See also Exercise 10.87.\nCorollary 10.56. If f : M \u2192R has L-Lipschitz continuous Hessian, then\n\f\f\f\ff(Expx(s)) \u2212 f(x) \u2212 \u27e8s, gradf(x)\u27e9 \u22121\n2 \u27e8s, Hessf(x)[s]\u27e9\n\f\f\f\f \u2264 L\n6 \u2225s\u22253,\nand\n\r\rP\u22121\ns gradf(Expx(s)) \u2212 gradf(x) \u2212 Hessf(x)[s]\n\r\r \u2264 L\n2 \u2225s\u22252,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1793, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "633ce579-ef01-4537-ba05-686fc669e45a": {"__data__": {"id_": "633ce579-ef01-4537-ba05-686fc669e45a", "embedding": null, "metadata": {"page_label": "284", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d703b4c9-83d1-4332-a78f-4e0e8b936d38", "node_type": "4", "metadata": {"page_label": "284", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "83760f3a3ddef84a1470528e02b436e3d6ee50393581e0b19dbc7ea1d62977e3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n284 Additional tools\nfor all (x, s) in the domain of the exponential map.\nTo close this section, the following statement from [LKB22b] provides Lipschitz-\ntype bounds for pullbacks through arbitrary retractions so long as we restrict our\nattention to a compact subset of the tangent bundle. The proof (omitted) merely\nuses the fact that if f \u25e6 R is sufficiently many times continuously differentiable\nthen its derivatives are bounded on any compact set. In turn, that can be used\nto bound truncation errors on Taylor expansions of f \u25e6 Rx in TxM uniformly\nin x over a compact set. This is convenient to verify typical regularity assump-\ntions such as A4.3, A6.6 and A6.7, though it should be noted that the constants\nL1, L2 below exist merely owing to the compactness-and-continuity argument:\nthis provides little insight (let alone control) over those constants.\nLemma 10.57. Consider a retraction R on M, a compact subset K \u2286 Mand\na continuous, nonnegative function r: K \u2192R. The set\nT = {(x, s) \u2208 TM : x \u2208 Kand \u2225s\u2225 \u2264r(x)}\nis compact in the tangent bundle TM (Exercise 10.31). Assume f : M \u2192R is\ntwice continuously differentiable. There exists a constant L1 such that, for all\n(x, s) \u2208 T, with \u02c6fx = f \u25e6 Rx, we have\n|f(Rx(s)) \u2212 f(x) \u2212 \u27e8s, gradf(x)\u27e9| \u2264L1\n2 \u2225s\u22252,\n\r\r\rgrad \u02c6fx(s) \u2212 grad \u02c6fx(0)\n\r\r\r \u2264 L1\u2225s\u2225,\nand \u2225Hess \u02c6fx(0)\u2225 \u2264L1 for all x \u2208 K. If additionally f is three times continuously\ndifferentiable, then there exists a constant L2 such that, for all (x, s) \u2208 T,\n\f\f\f\ff(Rx(s)) \u2212 f(x) \u2212 \u27e8s, gradf(x)\u27e9 \u22121\n2\u27e8s, Hess \u02c6fx(0)[s]\u27e9\n\f\f\f\f \u2264 L2\n6 \u2225s\u22253,\n\r\r\rgrad \u02c6fx(s) \u2212 grad \u02c6fx(0) \u2212 Hess \u02c6fx(0)[s]\n\r\r\r \u2264 L2\n2 \u2225s\u22252,\n\r\r\rHess \u02c6fx(s) \u2212 Hess \u02c6fx(0)\n\r\r\r \u2264 L2\u2225s\u2225.\n(Recall grad \u02c6fx(0) = gradf(x) and, if the retraction is second order, Hess \u02c6fx(0) =\nHessf(x).)\nThe exercise below has clear implications for the regularity assumptions A4.3\n(p59) and A6.6 (p135). See also Exercise 10.87.\nExercise 10.58. Let f : M \u2192R be twice continuously differentiable on a man-\nifold M equipped with a retraction R. Assume we have\n|f(Rx(s)) \u2212 f(x) \u2212 \u27e8s, gradf(x)\u27e9| \u2264L\n2 \u2225s\u22252 (10.29)\nfor all (x, s) in a neighborhood of the zero section in the tangent bundle. With\n\u02c6fx = f \u25e6Rx, show that \u2225Hess \u02c6fx(0)\u2225 \u2264L. Deduce that if R is second order then the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2473, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "15c77bf3-753c-4bea-a375-a79ec14f5ff1": {"__data__": {"id_": "15c77bf3-753c-4bea-a375-a79ec14f5ff1", "embedding": null, "metadata": {"page_label": "285", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eed11105-b8b4-4fec-8c7b-e937d9f7e54d", "node_type": "4", "metadata": {"page_label": "285", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "15df61b6c66f812d9045bba5b140865b78771b3ca7e23e62488ada71ce5c0dcd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n10.5 Transporters 285\ninequalities (10.29) hold only if gradf is L-Lipschitz continuous. With R = Exp\nin particular, verify that the three following claims are equivalent:\n1. Inequalities (10.29) hold in a neighborhood of the zero section in TM;\n2. gradf is L-Lipschitz continuous;\n3. Inequalities (10.29) hold over the whole domain of Exp.\nExercise 10.59. Give a proof of Proposition 10.50, for example by adapting\nthat of Proposition 10.45.\n10.5 Transporters\nThe strong properties of parallel transports (Section 10.3) make them great for\ntheoretical purposes, and in some cases they can even be computed via explicit\nexpressions. In general though, computing parallel transports involves numeri-\ncally solving ordinary differential equations, which is typically too expensive in\npractice. Furthermore, we may want to dispense with the need to choose a curve\nconnecting x and y explicitly to transport vectors from T xM to TyM, as this\nmay add to the computational burden (e.g., require computing Log x(y) if we\nmean to transport along minimizing geodesics).\nAs an alternative, we define a poor man\u2019s version of parallel transports called\ntransporters.5 There is no need for a Riemannian structure or connection. Infor-\nmally, for x and y close enough to one another, we aim to define linear maps of\nthe form\nTy\u2190x : TxM \u2192TyM,\nwith Tx\u2190x in particular being the identity map. If M is an embedded subman-\nifold of a Euclidean space, we present a simple transporter based on orthogonal\nprojections to tangent spaces in Proposition 10.66.\nIt is natural and convenient to ask that these maps vary smoothly with respect\nto x and y. One indirect way to make sense of this statement would be to\nrequire that the map ((x, u), y) 7\u2192 Ty\u2190xu be smooth from (an open submanifold\nof) T M \u00d7 Mto T M. However, it is more instructive (and eventually more\ncomfortable) to endow the set of linear maps between tangent spaces of two\nmanifolds with a smooth structure. (Here, the two manifolds are the same.) Once\nthis is done, we can formalize the notion of smoothness for a map (x, y) 7\u2192 Ty\u2190x.\nThis is in direct analogy with how we defined the tangent bundle TM as a disjoint\nunion of tangent spaces, associating a linear space T xM to each point x \u2208 M.\nHere, we associate to each pair ( x, y) \u2208 M \u00d7 Nthe linear space of linear maps\nfrom TxM to TyN. The proof is an exercise.\n5 This is different from the notion of vector transportas defined in [AMS08, \u00a7 8.1]: we connect\nboth concepts at the end of this section.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2752, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f6df8a8a-2ed5-499a-b155-f75a0b87cf23": {"__data__": {"id_": "f6df8a8a-2ed5-499a-b155-f75a0b87cf23", "embedding": null, "metadata": {"page_label": "286", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ddca60da-d5c3-4473-b5ec-10869091ec7f", "node_type": "4", "metadata": {"page_label": "286", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "1746da60da1632d8857b444decd3225ffb14cb97d8a5249d47534ffd2f97a2f4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n286 Additional tools\nProposition 10.60. For manifolds M and N of dimensions m and n, the\ndisjoint union of linear maps from the tangent spaces of M to those of N,\nL(TM, TN) = {(x, y,L) |x \u2208 M, y\u2208 Nand L: TxM \u2192TyN is linear},\nis itself a manifold, with charts as follows. For any pair of charts (U, \u03c6) and\n(V, \u03c8) of M and N, respectively, pick local frames on U and V as in Proposi-\ntion 8.51; then,\n\u03a6(x, y,L) = (\u03c6(x), \u03c8(y), Mat(L)) \u2208 Rm \u00d7 Rn \u00d7 Rn\u00d7m\nis a chart on \u03c0\u22121(U \u00d7V), where Mat(L) is the matrix that represents L with re-\nspect to the bases of TxM and TyN provided by the local frames, and\u03c0(x, y,L) =\n(x, y) is the projector from L(TM, TN) to M \u00d7 N.\nThe manifold L(TM, TN) is a vector bundle of M \u00d7 Nin that it (smoothly)\nattaches a linear space to each point of that manifold. Maps such as transporters\ndefined below have the property that they map ( x, y) to ( x, y,L) for some L:\nthese are called sections of the vector bundle. In the same way, vector fields are\ncalled sections of the tangent bundle.\nDefinition 10.61. Given a manifold M, let V be open in M \u00d7 Msuch that\n(x, x) \u2208 Vfor all x \u2208 M. A transporter on V is a smooth map\nT: V \u2192 L(TM, TM): ( x, y) 7\u2192 Ty\u2190x\nsuch that Ty\u2190x is linear from TxM to TyM and Tx\u2190x is the identity.\nIn this definition, smoothness of T is understood with V as an open submani-\nfold of the product manifoldM\u00d7M and L(TM, TM) equipped with the smooth\nstructure of Proposition 10.60. Formally, this means that for any pair (\u00afx, \u00afy) \u2208 V\nand local frames defined on neighborhoods U\u00afx and U\u00afy, the matrix that represents\nTy\u2190x with respect to these local frames varies smoothly with ( x, y) in U\u00afx \u00d7 U\u00afy.\nWe detail this in the proof of the next proposition, which shows that inverting\nthe linear maps of a transporter yields a transporter.\nProposition 10.62. For a transporter T on V, let V\u2032 be the set of pairs (x, y) \u2208\nV such that Tx\u2190y is invertible. Then, the maps\nT\u2032\ny\u2190x = (Tx\u2190y)\u22121 : TxM \u2192TyM\ndefine a transporter T\u2032 on V\u2032.\nProof. For all x \u2208 M, since T x\u2190x is the identity map, clearly ( x, x) \u2208 V\u2032 and\nT\u2032\nx\u2190x is itself the identity. Likewise, for all ( x, y) \u2208 V\u2032, it is clear that T \u2032\ny\u2190x is\nlinear. It remains to argue that V\u2032 is open in M\u00d7M and that T\u2032 is smooth from\nV\u2032 to L(TM, TM).\nTo this end, consider an arbitrary pair (\u00af x, \u00afy) \u2208 V\u2032 and let U1, . . . , Ud be a\nlocal frame on a neighborhood U\u00afx of \u00afx with d = dim M\u2014see Proposition 8.51.\nLikewise, let W1, . . . , Wd be a local frame on a neighborhood U\u00afy of \u00afy. If need be,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2732, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "96bde251-5198-438d-b02c-8d6625beb6e8": {"__data__": {"id_": "96bde251-5198-438d-b02c-8d6625beb6e8", "embedding": null, "metadata": {"page_label": "287", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "11c28e0f-bfcf-4f20-905b-d935c59c84aa", "node_type": "4", "metadata": {"page_label": "287", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "7d8e75fd42cebb3c4e0d2929ef92849c26946cab7716b0b00d8ddfdc0e2c04c2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n10.5 Transporters 287\nreduce U\u00afx and U\u00afy to smaller neighborhoods of \u00afx and \u00afy so that U\u00afx \u00d7U \u00afy \u2286 V. (We\ncan do this because V is open in M\u00d7M , hence it is a union of products of open\nsets in M. One of these products, say \u02dcU\u00d7 \u02c6U, contains (\u00afx, \u00afy), as otherwise it would\nnot be in V; thus: \u00afx \u2208 \u02dcU and \u00afy \u2208 \u02c6U. Replace U\u00afx by its intersection with \u02dcU, and\nsimilarly for U\u00afy: now U\u00afx \u00d7U\u00afy \u2286 Vis a neighborhood of (\u00afx, \u00afy) and the local frames\nare well defined.) Since T is smooth, the matrix G(x, y) in Rd\u00d7d that represents\nTx\u2190y with respect to the bases U1(x), . . . , Ud(x) and W1(y), . . . , Wd(y) varies\nsmoothly with (x, y) in U\u00afx \u00d7 U\u00afy. In particular, the function ( x, y) 7\u2192 det G(x, y)\nis smooth on this domain, so that the subset ofU\u00afx\u00d7U\u00afy over which detG(x, y) \u0338= 0\n(that is, over which T x\u2190y is invertible) is open, and it contains (\u00afx, \u00afy). In other\nwords: this subset is a neighborhood of (\u00afx, \u00afy) in V\u2032. Since each point in V\u2032 admits\nsuch a neighborhood, we find that V\u2032 is open. Furthermore, the matrix that\nrepresents T\u2032\ny\u2190x is simply G(x, y)\u22121. This is a smooth function of ( x, y) on the\nopen set where the inverse is well defined, confirming that T \u2032 is smooth from V\u2032\nto L(TM, TM).\nWith similar developments, we also get the following result once we equip the\nmanifold with a Riemannian metric.\nProposition 10.63. Let M be a Riemannian manifold and let T be a trans-\nporter for M on V. Then, T\u2032 defined by the maps\nT\u2032\ny\u2190x = (Tx\u2190y)\u2217 : TxM \u2192TyM\nis a transporter on V. (As always, the superscript \u2217 denotes the adjoint, here\nwith respect to the Riemannian metric at x and y.)\nProof sketch. Compared to Proposition 10.62 (and using the same notation), an\nextra step in the proof is to show that, using local frames, the Riemannian metric\ncan be represented as a smooth map from x to M(x): a symmetric, positive\ndefinite matrix of size d which allows us to write \u27e8u, v\u27e9x = \u00afu\u22a4M(x)\u00afv with \u00afu, \u00afv \u2208\nRd denoting the coordinate vectors of u, vin the same local frame. Upon doing\nso, it is straightforward to show that the matrix which represents (T x\u2190y)\u2217 is\nM(y)\u22121G(x, y)\u22a4M(x), which is indeed smooth in ( x, y).\nUpon choosing a smoothly varying collection of curves that uniquely connect\npairs of nearby points on M, it is easy to construct a transporter from parallel\ntransport along those curves (with respect to some connection). One way of\nchoosing such families of curves is via a retraction.\nConveniently, the differentials of a retraction also provide a transporter. This\nis because, with y = Rx(v), by perturbing v in TxM we perturb R x(v) away\nfrom y, thus producing a tangent vector in T yM. That is a good alternative\nwhen parallel transports are out of reach.\nProposition 10.64. For a retraction R on a manifold M, let T be a neighbor-\nhood of the zero section of TM such that E(x, v) = ( x, Rx(v)) is a diffeomor-\nphism from T to V = E(T )\u2014such neighborhoods exist by Corollary 10.27. For", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3171, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "34847f06-ae2e-462b-bf75-47863b86e34e": {"__data__": {"id_": "34847f06-ae2e-462b-bf75-47863b86e34e", "embedding": null, "metadata": {"page_label": "288", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "206a325b-2ba2-4b1a-a37f-3395c772c50b", "node_type": "4", "metadata": {"page_label": "288", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "7b56c9f119dc058fe5b2b1fa615f468dffa7f3f2e8323e19d9f3bbfd0b04a00c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n288 Additional tools\nour purpose, this means (x, y) 7\u2192 (x, R\u22121\nx (y)) is a diffeomorphism from V to T ,\nyielding a smooth choice of curves joining pairs (x, y).\n1. Assume Tx = {v \u2208 TxM : (x, v) \u2208 T }is star-shaped around the origin for\nall x. Parallel transport along retraction curves defines a transporter on V via\nTy\u2190x = PTc\n1\u21900, where c(t) = Rx(tv) and v = R\u22121\nx (y).\n2. The differentials of the retraction define a transporter on V via Ty\u2190x =\nDRx(v), where v = R\u22121\nx (y).\nProof sketch. The domain V \u2286 M \u00d7 Mis open and indeed contains all pairs\n(x, x) since E(x, 0) = (x, x). For both proposed transporters, it is clear that Tx\u2190x\nis the identity and that Ty\u2190x is a linear map from TxM to TyM. Smoothness for\nparallel transport can be argued with tools from ordinary differential equations\n(it takes some work). Smoothness for the retraction-based transport follows by\ncomposition of smooth maps since T y\u2190x = DRx(R\u22121\nx (y)).\nExample 10.65. Transporters can be used to transport linear maps between\ncertain tangent spaces to other tangent spaces. This is useful notably in defining\na Riemannian version of the famous BFGS algorithm. For example, if A is a\nlinear map from TxM to TxM, then we may transport it to a linear map from\nTyM to TyM in at least three ways using a transporter T:\nTy\u2190x \u25e6 A \u25e6Tx\u2190y, (Tx\u2190y)\u2217 \u25e6 A \u25e6Tx\u2190y, (Tx\u2190y)\u22121 \u25e6 A \u25e6Tx\u2190y.\nIf A is self-adjoint, then so is the second operator. If the transporter is obtained\nthrough parallel transport as in Proposition 10.64 and the curve connecting x\nto y is the same as the curve connecting y to x (for example, if we use unique\nminimizing geodesics), then all three operators are equal: see Proposition 10.36.\nFor manifolds embedded in Euclidean spaces, an especially convenient trans-\nporter is given by orthogonal projectors to the tangent spaces. In contrast to\nProposition 10.64, it does not involve retractions.\nProposition 10.66. Let M be an embedded submanifold of a Euclidean space\nE. For all x, y\u2208 M, exploiting the fact that both TxM and TyM are subspaces\nof E, define the linear maps\nTy\u2190x = Proj y\n\f\f\nTxM ,\nwhere Projy is the orthogonal projector from E to TyM, here restricted to TxM.\nThis is a transporter on all of M \u00d7 M.\nProof. By design, Tx\u2190x is the identity and T y\u2190x is linear from T xM to TyM.\nMoreover, T is smooth as can be deduced by an argument along the same lines\nas in Exercise 3.66.\nFor a quotient manifold M = M/ \u223c, in the same way that we discussed\nconditions for a retraction R on the total space M to induce a retraction R on\nthe quotient manifold M, it is tempting to derive a transporter T on M from a\ntransporter T on M. We show through an example how this can be done.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2920, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "58d6437f-0355-4229-8e67-6c34b1badf96": {"__data__": {"id_": "58d6437f-0355-4229-8e67-6c34b1badf96", "embedding": null, "metadata": {"page_label": "289", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "49df4625-0661-4a0c-a7e8-cc152a295e96", "node_type": "4", "metadata": {"page_label": "289", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "a4658a096d45f01c117712f02818bb48321eb718d641c3663126acfda07adaa6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n10.5 Transporters 289\nExample 10.67. Consider the Grassmann manifold Gr(n, p) = St( n, p)/ \u223c\n(Section 9.16). Equip the total space with the polar retraction (7.24), RX(V ) =\npfactor(X + V ), and with the projection transporter,\nTY \u2190X = ProjSt\nY |TXSt(n,p),\nthe orthogonal projector from Rn\u00d7p to TY St(n, p) restricted to TXSt(n, p). This\ntransporter is defined globally on St(n, p)\u00d7St(n, p). Our tentative transporter on\nGr(n, p) is:\nT[Y ]\u2190[X](\u03be) = D\u03c0(Y )\n\u0002\nTY \u2190X(liftX(\u03be))\n\u0003\n, (10.30)\nwhere X is an arbitrary representative of [X], and Y is a representative of [Y ]\nsuch that Y = RX(V ) for some V \u2208 HX, assuming one exists. When such a\nchoice of Y and V exists, it is unique. Indeed, consider the map\nE : TGr(n, p) \u2192 Gr(n, p) \u00d7 Gr(n, p)\n: ([X], \u03be) 7\u2192 E([X], \u03be) = ([X], [RX(V )]), (10.31)\nwhere V = liftX(\u03be). In Exercise 10.71, we find that E from TGr(n, p) to V =\nE(TGr(n, p)) is smoothly invertible. In other words: if [Y ] can be reached from\n[X] through retraction, it is so by a unique tangent vector \u03be; the latter has a\nspecific horizontal lift V once we choose a specific representativeX. Furthermore,\nsince E\u22121 is continuous, V is open. Finally, V contains all pairs of the form\n([X], [X]). This set V is meant to be the domain of T.\nNow restricting our discussion to V, we rewrite (10.30) equivalently as\nliftY\n\u0000\nT[Y ]\u2190[X](\u03be)\n\u0001\n= ProjH\nY\n\u0000\nTY \u2190X(liftX(\u03be))\n\u0001\n= ProjH\nY (liftX(\u03be)) , (10.32)\nwhere we used that ProjH\nY \u25e6 ProjSt\nY = ProjH\nY . We must check (a) that T[Y ]\u2190[X] is\nwell defined, and (b) that it defines a transporter, both on V.\nFor (a), we must check that the right-hand side of (10.30) does not depend\non our choice of representatives X and Y . To this end, consider (10.32). Recall\nfrom Example 9.26 that if we choose the representative XQ instead of X for [X]\nwith some arbitrary Q \u2208 O(p), then liftXQ(\u03be) = lift X(\u03be)Q. The representative\nY also changes as a result. Indeed, given V \u2208 HX such that RX(V ) = Y , we\nknow that V Q\u2208 HXQ is such that RXQ(V Q) = Y Q(this is specific to the polar\nretraction by (9.9)), and this is the only horizontal vector at XQ that maps to\n[Y ]. Since ProjH\nY = In \u2212 Y Y\u22a4 = ProjH\nY Q, we find that\nliftY Q\n\u0000\nT[Y ]\u2190[X](\u03be)\n\u0001\n= ProjH\nY Q(liftXQ(\u03be))\n= (In \u2212 Y Y\u22a4) liftX(\u03be)Q\n= liftY\n\u0000\nT[Y ]\u2190[X](\u03be)\n\u0001\nQ.\nThis confirms that the lifted vectors correspond to each other in the appropriate\nway, that is, the result T[Y ]\u2190[X](\u03be) does not depend on our choice of representa-\ntive X.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2682, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8d27ce5e-57a1-49bd-a1c9-a724bb8a5298": {"__data__": {"id_": "8d27ce5e-57a1-49bd-a1c9-a724bb8a5298", "embedding": null, "metadata": {"page_label": "290", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1858b1e7-41bd-4c4d-ac0b-727fc158ad60", "node_type": "4", "metadata": {"page_label": "290", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "5588e1c0f8a219fed4c8ba17e3299de921905f27f47290c8ec7491e2ca6ee3fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n290 Additional tools\nRegarding (b), it is clear that T[Y ]\u2190[X] is a linear map from T[X]Gr(n, p)\nto T[Y ]Gr(n, p), as a composition of linear maps. Likewise, T is smooth as a\ncomposition of smooth maps (this also follows from Exercise 10.71, which shows\nthat given ([X], [Y ]) \u2208 V, for any choice of representative X, there is a smooth\nchoice of Y and V (horizontal) such that Y = RX(V )). It is easy to see that\nT[X]\u2190[X] is the identity. Finally, we already checked that V is an appropriate\ndomain for a transporter.\nHow do we use this transporter in practice? If we are simply given two rep-\nresentatives X and Y and the lift U of \u03be at X, then before applying (10.32) we\nmust replace Y by Y Q, for the unique Q such that there exists V \u2208 HX with\nRX(V ) = Y Q. This can be done if and only if X\u22a4Y is invertible. Explicitly, one\ncan reason from Exercise 10.71 that Q is nothing but the polar factor of X\u22a4Y .\nThen, we can follow this procedure:\n1. Compute Q \u2208 O(p) via SVD, as Q = \u02dcU \u02dcV \u22a4 with \u02dcU \u02dc\u03a3 \u02dcV \u22a4 = X\u22a4Y ;\n2. By (10.32), liftY Q\n\u0000\nT[Y ]\u2190[X](\u03be)\n\u0001\n= ProjH\nY Q(liftX(\u03be)) = U \u2212 Y (Y \u22a4U);\n3. Finally, liftY\n\u0000\nT[Y ]\u2190[X](\u03be)\n\u0001\n= (U \u2212 Y (Y \u22a4U))Q\u22a4.\nOften times though, Y is a point that was generated by retraction of some hor-\nizontal vector from X. If that retraction is the polar retraction, then using this\ntransporter is straightforward: X\u22a4Y is symmetric and positive definite, hence its\npolar factor is Q = Ip, and it is sufficient to compute U \u2212 Y (Y \u22a4U).\nIn closing, we connect the notion of transporter (used in [HGA15, \u00a7 4.3]) to\nthat of vector transport (favored in [AMS08, Def. 8.1.1]).\nDefinition 10.68. A vector transport on a manifold M is a smooth map\n(x, u, v) 7\u2192 VT(x,u)(v)\nfrom the Whitney sum (which can be endowed with a smooth structure)\nTM \u2295TM = {(x, u, v) : x \u2208 Mand u, v\u2208 TxM}\nto TM, satisfying the following for some retraction R on M:\n1. VT(x,u) is a linear map from the tangent space at x to the tangent space at\nRx(u) for all (x, u) \u2208 TM; and\n2. VT(x,0) is the identity on TxM for all x \u2208 M.\nEquivalently, we can define a vector transport associated to a retraction R as\na smooth map VT: T M \u2192 L(TM, TM) such that VT(x,u) is a linear map from\nTxM to TRx(u)M and VT(x,0) is the identity on T xM. From this perspective,\nit is clear that a transporter T and a retraction R can be combined to define a\nvector transport through VT(x,u) = TRx(u)\u2190x. However, not all vector transports\nare of this form because in general we could have VT (x,u) \u0338= VT (x,w) even if\nRx(u) = R x(w), which the transporter construction does not allow. The other\nway around, a vector transport with associated retraction R can be used to define", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2898, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ea0b0dc0-1f8c-4769-b26d-9ed3fdf8c822": {"__data__": {"id_": "ea0b0dc0-1f8c-4769-b26d-9ed3fdf8c822", "embedding": null, "metadata": {"page_label": "291", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "98932691-9658-4b72-9f32-cca87b88640d", "node_type": "4", "metadata": {"page_label": "291", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "e3a8b15257d8217c381c429b9bafd2f1bbb8714856ae84ba4816cd45c59cae01", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n10.5 Transporters 291\na transporter if we first restrict the domain such that (x, u) 7\u2192 (x, Rx(u)) admits\na smooth inverse (see Corollary 10.27).\nExercise 10.69. Give a proof of Proposition 10.60.\nExercise 10.70. Give a proof of Proposition 10.63.\nExercise 10.71. With notation as in Example 10.67, show that E (10.31) is\ninvertible. Furthermore, show that given two arbitrary representatives X and Y\nof [X] and [RX(V )] (respectively), V is given by\nV = Y (X\u22a4Y )\u22121 \u2212 X, (10.33)\nand deduce that the inverse of E is smooth. From this formula, it is also apparent\nthat [Y ] can be reached from [X] if and only if X\u22a4Y is invertible. Compare with\nExercise 7.2.\nExercise 10.72. For the orthogonal group (M = O(n)) or the group of rotations\n(M = SO( n)) as a Riemannian submanifold of Rn\u00d7n (see Section 7.4), it is\nnatural to consider the following transporter:\nTY \u2190X(U) = Y X\u22a4U, (10.34)\nwhere X, Y \u2208 Mare orthogonal matrices of size n and U \u2208 TXM is such\nthat X\u22a4U is skew-symmetric. Show that this is indeed a transporter and that it\nis isometric. Then, show that this is not parallel transport along geodesics (see\nExercise 7.3). If we represent tangent vectors U = X\u2126 simply as their skew-\nsymmetric part \u2126, then this transporter requires no computations.\nExercise 10.73. Let R be a retraction on a Riemannian manifold M. The\ndifferentiated retraction plays a special role as a link between the Riemannian\ngradient and Hessian of f : M \u2192R and the (classical) gradients and Hessians\nof the pullbacks \u02c6f = f \u25e6 Rx : TxM \u2192R.\nProve the following identities [ABBC20, \u00a7 6]: if f is differentiable, then\ngrad \u02c6f(s) = T\u2217\ns gradf(Rx(s)), (10.35)\nwhere Ts = DRx(s) is a linear map from TxM to TRx(s)M, and T\u2217\ns is its adjoint.\nIf f is twice differentiable, then\nHess \u02c6f(s) = T\u2217\ns \u25e6 Hessf(Rx(s)) \u25e6 Ts + Ws, (10.36)\nwith Ws a self-adjoint linear map on TxM defined by\n\u27e8\u02d9s, Ws( \u02d9s)\u27e9x = \u27e8gradf(Rx(s)), c\u2032\u2032(0)\u27e9Rx(s) , (10.37)\nwhere c\u2032\u2032(0) = D\ndt c\u2032(0) is the initial intrinsic acceleration of the smooth curve\nc(t) = Rx(s + t \u02d9s). Argue that Ws is indeed linear and self-adjoint.\nCheck that these formulas generalize Propositions 8.59 and 8.71 (as well as\ntheir embedded counter-parts, Propositions 3.59 and 5.45).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2453, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1d57b5c2-b2c7-4e24-b772-e6347289ff6e": {"__data__": {"id_": "1d57b5c2-b2c7-4e24-b772-e6347289ff6e", "embedding": null, "metadata": {"page_label": "292", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d5b3c507-d5b4-446d-bc96-be2669b748cb", "node_type": "4", "metadata": {"page_label": "292", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "814824b2988f978a2c2f4d96707e81c3872c3f8102b9c56a44846fb389b81d57", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n292 Additional tools\nAs a comment: For all u, v\u2208 TxM, we can use (10.37) to compute \u27e8u, Ws(v)\u27e9x\nowing to the polarization identity\n\u27e8u, Ws(v)\u27e9x = 1\n4\n\u0000\n\u27e8u + v, Ws(u + v)\u27e9x \u2212 \u27e8u \u2212 v, Ws(u \u2212 v)\u27e9x\n\u0001\n.\nThis is why (10.37) fully determines Ws.\n10.6 Finite difference approximation of the Hessian\nIn order to minimize a smooth function f : M \u2192R on a Riemannian manifold,\nseveral optimization algorithms (notably those in Chapter 6) require computa-\ntion of the Riemannian Hessian applied to a vector: Hessf(x)[u]. Since obtaining\nan explicit expression for the Hessian may be tedious, 6 it is natural to explore\navenues to approximate it numerically. To this end, we consider finite difference\napproximations.\nFor any smooth curve c: I \u2192 Msuch that c(0) = x and c\u2032(0) = u, it holds\nthat\nHessf(x)[u] = \u2207ugradf = D\ndt(gradf \u25e6 c)(0). (10.38)\nUsing Proposition 10.37, we can further rewrite the right-hand side in terms of\nparallel transport along c:\nHessf(x)[u] = lim\nt\u21920\nPTc\n0\u2190t(gradf(c(t))) \u2212 gradf(x)\nt . (10.39)\nThis suggests the approximation\nHessf(x)[u] \u2248 PTc\n0\u2190\u00aft(gradf(c(\u00aft))) \u2212 gradf(x)\n\u00aft (10.40)\nfor some well-chosen \u00aft > 0: small enough to be close to the limit (see Corol-\nlary 10.56 to quantify this error), large enough to avoid numerical issues. Of\ncourse, we could also use higher-order finite differences.\nIn light of Section 10.5, we may ask: is it legitimate to replace the parallel\ntransport in (10.40) with a transporter? We already verified this for a special\ncase in Example 5.32, where we considered a Riemannian submanifold M of a\nEuclidean space E with the transporter obtained by orthogonal projection to\ntangent spaces. In this section, we consider the general setting.\nWith a transporter T on a Riemannian manifold M, we contemplate the\nfollowing candidate approximation for the Hessian:\nHessf(x)[u] \u2248 Tx\u2190c(\u00aft)(gradf(c(\u00aft))) \u2212 gradf(x)\n\u00aft . (10.41)\nImplementing this formula takes little effort compared to the hassle of deriv-\ning formulas for the Hessian by hand. For example, in the Manopt toolbox,\n6 See also Section 4.7 for a word regarding automatic differentiation.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2350, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a27a8566-5435-4a67-a0a3-213347bb755f": {"__data__": {"id_": "a27a8566-5435-4a67-a0a3-213347bb755f", "embedding": null, "metadata": {"page_label": "293", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5406a7e8-6a98-42a4-a0e3-0e0e6c15835d", "node_type": "4", "metadata": {"page_label": "293", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "73a6e2b5667e2afd0bb451103d58f300a2cea8929f803839157f00f42b9040a0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n10.6 Finite difference approximation of the Hessian 293\nthe default behavior when the Hessian is needed but unavailable is to fall back\non (10.41) with c(\u00aft) = Rx(\u00aftu) and \u00aft >0 set such that \u2225\u00aftu\u2225x = 2\u221214. This costs\none retraction, one gradient evaluation (assuming gradf(x) is available), and one\ncall to a transporter. Moreover, this satisfies radial linearity as required in A6.1\n(p133). To justify (10.41), we generalize Proposition 10.37.\nProposition 10.74. Let c: I \u2192 Mbe a smooth curve on a Riemannian mani-\nfold equipped with a transporter T. For a fixed t0 \u2208 I, let v1, . . . , vd form a basis\nof Tc(t0)M and define the vector fields\nVi(t) =\n\u0000\nTc(t0)\u2190c(t)\n\u0001\u22121\n(vi). (10.42)\nGiven a vector field Z \u2208 X(c), it holds that\nD\ndtZ(t0) = lim\n\u03b4\u21920\nTc(t0)\u2190c(t0+\u03b4)Z(t0 + \u03b4) \u2212 Z(t0)\n\u03b4 +\ndX\ni=1\n\u03b1i(t0) D\ndtVi(t0),\nwhere \u03b11(t0), . . . , \u03b1d(t0) are the coefficients of Z(t0) in the basis v1, . . . , vd.\nProof. The vector fields V1, . . . , Vd play a role similar to parallel frames. By\nProposition 10.62, these vector fields depend smoothly on t in a neighborhood I0\nof t0. Furthermore, I0 can be chosen small enough so that V1(t), . . . , Vd(t) form\na basis of T c(t)M for each t \u2208 I0. Hence, there exists a unique set of smooth\nfunctions \u03b1i : I0 \u2192 R such that\nZ(t) =\ndX\ni=1\n\u03b1i(t)Vi(t).\nOn the one hand, using properties of covariant derivatives, we see that\nD\ndtZ(t) =\ndX\ni=1\n\u03b1\u2032\ni(t)Vi(t) + \u03b1i(t) D\ndtVi(t).\nOn the other hand, defining G as\nG(t) = Tc(t0)\u2190c(t)(Z(t)) =\ndX\ni=1\n\u03b1i(t)vi,\nwe find that\nG\u2032(t) =\ndX\ni=1\n\u03b1\u2032\ni(t)vi.\nCombining both findings at t0 using Vi(t0) = vi, it follows that\nD\ndtZ(t0) = G\u2032(t0) +\ndX\ni=1\n\u03b1i(t0) D\ndtVi(t0).\nSince G is a map between (open subsets of) linear spaces, we can write G\u2032(t0)\nas a limit in the usual way.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1998, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "57d40cf2-50cc-4343-bf93-ff8856b49744": {"__data__": {"id_": "57d40cf2-50cc-4343-bf93-ff8856b49744", "embedding": null, "metadata": {"page_label": "294", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f7cb44b3-d4a6-40f6-b0e7-dd790c594e57", "node_type": "4", "metadata": {"page_label": "294", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "d241d125bbe76d4a754ee95a5873d5e179115d160cbad7bc2dd189092d0eb058", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n294 Additional tools\nApplying this to (10.38) yields a corollary relevant to formula (10.41).\nCorollary 10.75. For any smooth curve c on a Riemannian manifold M such\nthat c(0) = x and c\u2032(0) = u, and for any transporter T, orthonormal basis\nv1, . . . , vd of TxM and associated vector fields V1, . . . , Vd defined by\nVi(t) =\n\u0000\nTx\u2190c(t)\n\u0001\u22121\n(vi), (10.43)\nit holds that\nHessf(x)[u] = lim\nt\u21920\nTx\u2190c(t)(gradf(c(t))) \u2212 gradf(x)\nt\n+\ndX\ni=1\n\u27e8gradf(x), vi\u27e9x\nD\ndtVi(0). (10.44)\nThus we see that the approximation (10.41) is justified at or near a critical\npoint. This is typically sufficient to obtain good performance with second-order\noptimization algorithms such as RTR.\nThe approximation is also justified at a general point x if the vectors D\ndt Vi(0)\nvanish. This is of course the case if we use parallel transport, recovering (10.39).\nLikewise, circling back to Example 5.32 for the case where M is a Riemannian\nsubmanifold of a Euclidean space E and the transporter is taken to be simply\northogonal projection to tangent spaces (Proposition 10.66), we also get the\nfavorable simplification. As a reminder, this yields the particularly convenient\nformula\nHessf(x)[u] = lim\nt\u21920\nProjx(gradf(c(t))) \u2212 gradf(x)\nt , (10.45)\nwhere Projx is the orthogonal projector from E to TxM, c(t) satisfies c(0) = x\nand c\u2032(0) = u, and gradf(c(t)) is interpreted as a vector in E.\n10.7 Tensor fields and their covariant differentiation\nGiven a Riemannian manifold M, we can think of a smooth vector field U \u2208\nX(M) as a map from X(M) to the set of smooth real-valued functions F(M) as\nfollows:\nV 7\u2192 U(V ) = \u27e8U, V\u27e9.\nThis map is F(M)-linear in its argument, meaning\nU(fV + gW ) = fU (V ) + gU(W)\nfor all V, W\u2208 X(M) and f, g\u2208 F(M). Likewise, we can think of the Riemannian\nmetric itself as a map from X(M) \u00d7 X(M) to F(M):\n(U, V) 7\u2192 \u27e8U, V\u27e9.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2070, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8445e8e4-f07d-4a5f-ad3b-4cc0de8c1e9a": {"__data__": {"id_": "8445e8e4-f07d-4a5f-ad3b-4cc0de8c1e9a", "embedding": null, "metadata": {"page_label": "295", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2470caf3-782b-4802-ae14-e833646fc961", "node_type": "4", "metadata": {"page_label": "295", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "2d82341cc914def274ed255142de0ed0e7155dbfa23195a2d55cdc53d42e3e9c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n10.7 Tensor fields and their covariant differentiation 295\nThis mapping is F(M)-linear in each of its arguments. These two maps are\nexamples of tensor fields of order one and two, respectively.\nDefinition 10.76. A smooth tensor field T of order k on a manifold M is a\nmap\nT : X(M) \u00d7 \u00b7 \u00b7 \u00b7 \u00d7X(M) \u2192 F(M)\nwhich is F(M)-linear in each one of its k inputs. The set of such objects is\ndenoted by Xk(M). If the ordering of the inputs is irrelevant, we say T is a\nsymmetric (smooth) tensor field . (See also Remark 10.84.)\nIn our examples above, vector fields are identified with tensor fields of order\none, while the Riemannian metric is a symmetric tensor field of order two. As a\nnon-example, notice that the Riemannian connection \u2207, conceived of as a map\n\u2207: X(M) \u00d7 X(M) \u00d7 X(M) \u2192 F(M): ( U, V, W) 7\u2192 \u27e8\u2207U V , W\u27e9,\nis not a tensor field, because it is only R-linear in V , not F(M)-linear. Indeed,\n\u2207U (fV ) = f\u2207U V + (Uf )V for f \u2208 F(M).\nImportantly, tensor fields are pointwise objects, in that they associate to each\npoint x \u2208 Ma well-defined multilinear map (i.e., a tensor) on the tangent space\nTxM\u2014hence the name tensor field. In order to see this, consider a tensor field T\nof order k and a local frame W1, . . . , Wd on a neighborhood U of x (Section 3.9).\nThen, the input vector fields U1, . . . , Uk \u2208 X(M) can each be expanded in the\nlocal frame as\nUi|U = fi,1W1 + \u00b7 \u00b7\u00b7+ fi,dWd =\ndX\nj=1\nfi,jWj,\nwhere the fi,j are smooth functions on U. Working only on the domain U and\nusing the linearity properties of tensor fields, we find 7\nT(U1, . . . , Uk) =\ndX\nj1=1\n\u00b7 \u00b7\u00b7\ndX\njk=1\nf1,j1 \u00b7 \u00b7\u00b7fk,jk T(Wj1 , . . . , Wjk ).\nEvaluating this function atx\u2032 \u2208 U, the result depends onU1, . . . , Uk only through\nthe values of the fi,j at x\u2032, that is, T(U1, . . . , Uk)(x\u2032) depends on the vector fields\nonly through U1(x\u2032), . . . , Uk(x\u2032). Moreover, the dependence is linear in each one.\nThis offers a useful perspective on tensor fields of order k: they associate to\neach point x of M a k-linear function on the tangent space at that point, namely,\nT(x): T xM \u00d7 \u00b7 \u00b7 \u00b7 \u00d7TxM \u2192R.\nThis function is defined by\nT(x)(u1, . . . , uk) = T(U1, . . . , Uk)(x),\n7 Via bump functions as in Section 5.6, we can extend the vector fields from U to all of M,\nwhich is necessary to apply T. To be formal, we should then also argue why the conclusions\nwe reach based on these special vector fields generalize\u2014see [Lee12, Lem. 12.24].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2655, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d8c22092-6ccc-4aa0-8a50-cd6e9b889fcf": {"__data__": {"id_": "d8c22092-6ccc-4aa0-8a50-cd6e9b889fcf", "embedding": null, "metadata": {"page_label": "296", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "27c4933c-9cf8-4176-8bab-cab08935e61a", "node_type": "4", "metadata": {"page_label": "296", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "53add4a2e9a4ea8b82077605db8b2221a9467ee7318fc2093c99e5e28780b5fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n296 Additional tools\nwhere the Ui \u2208 X(M) are arbitrary so long as Ui(x) = ui.\nContinuing with our examples, for a vector field U \u2208 X(M), the notation\nu = U(x) normally refers to a tangent vector at x, while if we think of U as a\ntensor field, then U(x) denotes the linear function v 7\u2192 \u27e8u, v\u27e9x on the tangent\nspace at x. The Riemannian metric is a tensor field of order two; let us call it G.\nThen, G(x) is a bilinear function on TxM\u00d7TxM such that G(x)(u, v) = \u27e8u, v\u27e9x.\nThe map x 7\u2192 T(x) is smooth, in a sense we now make precise. Similarly to\nProposition 10.60, we can define a tensor bundle of order k over M as:\nTkTM =\n\b\n(x, L) : x \u2208 Mand L \u2208 TkTxM\n\t\n, where (10.46)\nTkTxM =\n\b\nk-linear functions from (T xM)k to R\n\t\n.\nEach tensor bundle can be endowed with a natural smooth manifold structure\nsuch that \u03c0 : TkTM \u2192 Mdefined by \u03c0(x, L) = x is smooth. This is identical\nto how we equipped the tangent bundle T M with such a smooth structure.\nThen, any section of T kTM, that is, any map T from M to TkTM such that\n\u03c0(T(x)) = x is called a tensor field of order k. This is commonly taken as the\ndefinition of a tensor field. Smooth tensor fields as we defined them above are\nexactly the smooth sections as defined here [Lee12, Prop. 12.19, Lem. 12.24].\nBy convention, T0TxM = R, so that T 0TM = M \u00d7R. Notice that T 1TxM\ncan be identified with T xM, and that T 2TxM can be identified with the set\nof linear maps from T xM into itself. Thus, T 1TM can be identified with T M\nitself, and T2TM can be identified as:\nT2TM \u2261 {(x, L) : x \u2208 Mand L: TxM \u2192TxM is linear}. (10.47)\nNow that we think of smooth tensor fields as smooth maps on manifolds, it is\nnatural to ask what happens if we differentiate them. In Chapter 5, we introduced\nthe notion of connection \u2207 on the tangent bundle T M. One can formalize the\nidea that \u2207 induces a connection on any tensor bundle, unique once we require\ncertain natural properties [Lee18, Prop. 4.15]. This gives meaning to the notation\n\u2207V T for V \u2208 X(M). Omitting quite a few details, we give an opportunistic\nconstruction of this object.\nRecall how V fis the derivative of a real function f against a vector field V .\nSince T(U1, . . . , Uk) is a smooth function on M, we can differentiate it against\nany smooth vector field V to obtain V T(U1, . . . , Uk), also a smooth function\non M. The definition below is crafted to secure a natural chain rule for this\ndifferentiation.\nDefinition 10.77. Given a smooth tensor field T of order k on a manifold M\nwith a connection \u2207, the total covariant derivative \u2207T of T is a smooth tensor", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2816, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0390de83-b6b8-452a-b65f-e3f179f27420": {"__data__": {"id_": "0390de83-b6b8-452a-b65f-e3f179f27420", "embedding": null, "metadata": {"page_label": "297", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7ffc6755-54d2-4d01-a257-2d7a5191a139", "node_type": "4", "metadata": {"page_label": "297", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "fe076498d360d1ae8bf9d63d5112dfa06cbffcf0be3ee2c914b16c3efb417386", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n10.7 Tensor fields and their covariant differentiation 297\nfield of order k + 1 on M, defined for all U1, . . . , Uk, V\u2208 X(M) by\n\u2207T(U1, . . . , Uk, V) = V T(U1, . . . , Uk)\n\u2212\nkX\ni=1\nT(U1, . . . , Ui\u22121, \u2207V Ui, Ui+1, . . . , Uk). (10.48)\n(If \u2207T = 0, we call T parallel.) We also let \u2207V T be a smooth tensor field of\norder k (symmetric if T is symmetric), defined by\n(\u2207V T)(U1, . . . , Uk) = \u2207T(U1, . . . , Uk, V).\n(It is an exercise to check that these are indeed tensor fields.)\nAs an example, it is instructive to see how gradients and Hessians of scalar\nfields fit into the framework of covariant differentiation of tensor fields.\nExample 10.78. Let M be a Riemannian manifold with its Riemannian con-\nnection \u2207. A smooth function f : M \u2192R is a tensor field of order zero. Differ-\nentiating f as a tensor field using Definition 10.77, we find that \u2207f is a tensor\nfield of order one defined by\n\u2207f(U) = Uf = Df(U) = \u27e8gradf, U\u27e9.\nIn other words, \u2207f is the differential Df, which we identify with the gradient\nvector field through the Riemannian metric. We now differentiate \u2207f to produce\n\u2207(\u2207f) = \u22072f: a tensor field of order two defined by\n\u22072f(U, V) = V \u2207f(U) \u2212 \u2207f(\u2207V U)\n= V \u27e8gradf, U\u27e9 \u2212 \u27e8gradf, \u2207V U\u27e9\n= \u27e8\u2207V gradf, U\u27e9\n= \u27e8Hessf(V ), U\u27e9.\nIn other words, \u22072f and the Riemannian Hessian Hessf are identified through\nthe Riemannian metric. This also shows that \u22072f is symmetric. Going one step\nfurther, we differentiate \u22072f to produce \u22073f = \u2207(\u22072f): a tensor field of order\nthree defined by\n\u22073f(U, V, W) = W\u22072f(U, V)\n\u2212 \u22072f(\u2207W U, V) \u2212 \u22072f(U, \u2207W V )\n= W\u27e8Hessf(V ), U\u27e9\n\u2212 \u27e8Hessf(V ), \u2207W U\u27e9 \u2212 \u27e8Hessf(\u2207W V ), U\u27e9\n= \u27e8\u2207W (Hessf(V )) \u2212 Hessf(\u2207W V ), U\u27e9.\nNotice that \u22073f is symmetric in its first two inputs, but not necessarily in its\nthird input (see also Exercise 10.87). Based on the above, for a given W \u2208 X(M)\nit is useful to introduce \u2207W Hessf : X(M) \u2192 X(M): an operator of the same type", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2132, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "95d47cc4-2a4e-49e7-9cfe-c0f3d9795cb3": {"__data__": {"id_": "95d47cc4-2a4e-49e7-9cfe-c0f3d9795cb3", "embedding": null, "metadata": {"page_label": "298", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "543b0ae1-2f13-48cf-a344-cb67045ef7bb", "node_type": "4", "metadata": {"page_label": "298", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "4cbd48e919fa7b256148ea93fd025e279cc8a47ea64f169c3e1b9923ba261417", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n298 Additional tools\nas the Riemannian Hessian itself given by\n\u2207W Hessf(V ) = \u2207W (Hessf(V )) \u2212 Hessf(\u2207W V ). (10.49)\nThe smooth tensor field (U, V) 7\u2192 \u27e8\u2207W Hessf(V ), U\u27e9 = \u22073f(U, V, W) is sym-\nmetric in U and V .\nSince tensor fields are pointwise objects, we can make sense of the notation\n\u2207vT for v \u2208 TxM as follows: \u2207T is a (k +1)-tensor field on M, so that (\u2207T)(x)\nis a (k + 1)-linear map on TxM; fixing the last input to be v, we are left with\n(\u2207vT)(u1, . . . , uk) =\n\u0000\n(\u2207T)(x)\n\u0001\n(u1, . . . uk, v). (10.50)\nThus, \u2207vT is a k-linear map on TxM; if T is symmetric, so is \u2207vT. In particular,\n\u2207vHessf is self-adjoint on TxM for v \u2208 TxM.\nGiven a curvec on M, we defined the covariant derivative of a vector field along\nc in Section 5.7. This extends to tensors, in direct analogy with Theorem 5.29.\nDefinition 10.79. Let c: I \u2192 Mbe a smooth curve on a manifold M. A\nsmooth tensor field Z of order k along c is a map\nZ : X(c) \u00d7 \u00b7 \u00b7\u00b7 \u00d7X(c) \u2192 F(I)\nthat is F(I)-linear in each of its k inputs. We denote the set of such fields Xk(c).\nHere too, we can reason that tensor fields are pointwise objects, in that Z(t)\nis a k-linear map from (T c(t)M)k to R: see [Lee18, Thm. 4.24, Prop. 5.15].\nTheorem 10.80. Let c: I \u2192 Mbe a smooth curve on a manifold with a con-\nnection \u2207. There exists a unique operator D\ndt : Xk(c) \u2192 Xk(c) satisfying these\nproperties for all Y, Z\u2208 Xk(c), T \u2208 Xk(M), g\u2208 F(I) and a, b\u2208 R:\n1. R-linearity: D\ndt (aY + bZ) = a D\ndt Y + b D\ndt Z;\n2. Leibniz rule: D\ndt (gZ) = g\u2032Z + g D\ndt Z;\n3. Chain rule:\n\u0000D\ndt (T \u25e6 c)\n\u0001\n(t) = \u2207c\u2032(t)T for all t \u2208 I.\nThis operator is called the induced covariant derivative.\nIn the statement above, we understand \u2207c\u2032(t)T through (10.50).\nAs a result of Definition 10.77, we also have the following chain rule: given", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2012, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3b1b5764-703b-4ce6-8b18-41ffaa464fef": {"__data__": {"id_": "3b1b5764-703b-4ce6-8b18-41ffaa464fef", "embedding": null, "metadata": {"page_label": "299", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52ca2beb-b4ec-4cbe-95dd-9bb2a35460bb", "node_type": "4", "metadata": {"page_label": "299", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "4ebf1043ee448a4d831ccbebba2fd85ecc87a7aaab716263d00e7672f4447978", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n10.7 Tensor fields and their covariant differentiation 299\nZ \u2208 Xk(c) and U1, . . . , Uk \u2208 X(c):\nd\ndt\n\u0000\nZ(U1, . . . , Uk)\n\u0001\n= d\ndt\n\u0000\nZ(t)(U1(t), . . . , Uk(t))\n\u0001\n=\n\u0012D\ndtZ(t)\n\u0013\n(U1(t), . . . , Uk(t))\n+ Z(t)\n\u0012D\ndtU1(t), U2(t), . . . , Uk(t)\n\u0013\n+ \u00b7 \u00b7\u00b7\n+ Z(t)\n\u0012\nU1(t), . . . , Uk\u22121(t), D\ndtUk(t)\n\u0013\n=\n\u0012D\ndtZ\n\u0013\n(U1, . . . , Uk)\n+ Z\n\u0012D\ndtU1, U2, . . . , Uk\n\u0013\n+ \u00b7 \u00b7\u00b7+ Z\n\u0012\nU1, . . . , Uk\u22121, D\ndtUk\n\u0013\n.\n(10.51)\nExample 10.81. If c: I \u2192 Mis a geodesic on the Riemannian manifold M\nwith Riemannian connection \u2207 and f : M \u2192R is smooth, we have:\n(f \u25e6 c)\u2032 = \u2207c\u2032f = (\u2207f \u25e6 c)(c\u2032),\n(f \u25e6 c)\u2032\u2032 = (\u2207c\u2032\u2207f)(c\u2032) + (\u2207f \u25e6 c)(c\u2032\u2032) = (\u22072f \u25e6 c)(c\u2032, c\u2032),\n(f \u25e6 c)\u2032\u2032\u2032 = (\u2207c\u2032\u22072f)(c\u2032, c\u2032) + (\u22072f \u25e6 c)(c\u2032\u2032, c\u2032) + (\u22072f \u25e6 c)(c\u2032, c\u2032\u2032)\n= (\u22073f \u25e6 c)(c\u2032, c\u2032, c\u2032).\nIn particular, if c(0) = x and c\u2032(0) = u, then at t = 0 it follows that:\n(f \u25e6 c)\u2032(0) = \u2207f(x)(u) = \u27e8gradf(x), u\u27e9x ,\n(f \u25e6 c)\u2032\u2032(0) = \u22072f(x)(u, u) = \u27e8Hessf(x)[u], u\u27e9x ,\n(f \u25e6 c)\u2032\u2032\u2032(0) = \u22073f(x)(u, u, u) = \u27e8(\u2207uHessf)[u], u\u27e9x ,\nwith \u2207uHessf as defined through (10.49) and (10.50).\nCircling back to Section 10.4, let us now discuss Lipschitz continuity of tensor\nfields. As afforded by Remark 8.6, here we do not require tensor fields to be\nsmooth (that is, infinitely differentiable). We understand the differentiability\nproperties of tensor fields as maps between manifolds, as outlined around (10.46).\nWe start with a definition.\nDefinition 10.82. A tensor field T of order k on a Riemannian manifold M\nwith its Riemannian connection is L-Lipschitz continuous if for all (x, s) in the\ndomain of the exponential map and for all u1, . . . , uk \u2208 TxM we have\n|T(Expx(s))(Psu1, . . . , Psuk) \u2212 T(x)(u1, . . . , uk)| \u2264L\u2225s\u2225x\u2225u1\u2225x \u00b7 \u00b7\u00b7 \u2225uk\u2225x,\nwhere Ps is parallel transport along \u03b3(t) = Expx(ts) from t = 0 to t = 1.\nIt is an exercise to show that this definition is compatible with the ones we\nintroduced earlier, for example for f, gradf and Hessf.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2115, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "91f07f65-bed8-4b2e-9209-2884625942b9": {"__data__": {"id_": "91f07f65-bed8-4b2e-9209-2884625942b9", "embedding": null, "metadata": {"page_label": "300", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d207774e-acd5-408d-871a-c87c3a53fe93", "node_type": "4", "metadata": {"page_label": "300", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "4c6d5d8e8986d0b596aa3ad51b6da30ca87d9aa9d3a91c7937c01c9466bf5094", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n300 Additional tools\nAssume T is differentiable. Let c be a smooth curve on M satisfying c(0) = x\nand c\u2032(0) = s. Parallel transport u1, . . . , uk \u2208 TxM along c to form Ui(t) =\nPTc\nt\u21900ui for i = 1, . . . , k. Then, owing to D\ndt Ui = 0 for all i we have (using (10.51)\nand the chain rule in Theorem 10.80):\nd\ndt\n\u0010\n(T \u25e6 c)(U1, . . . , Uk)\n\u0011\n=\n\u0012D\ndt(T \u25e6 c)\n\u0013\n(U1, . . . , Uk)\n= (\u2207c\u2032T)(U1, . . . , Uk)\n= (\u2207T \u25e6 c)(U1, . . . , Uk, c\u2032). (10.52)\nIn particular, at t = 0 this says:\n\u2207T(x)(u1, . . . , uk, s) = (\u2207sT)(u1, . . . , uk) = d\ndt(T \u25e6 c)(U1, . . . , Uk)\n\f\f\f\f\nt=0\n.\n(10.53)\nIn light of these identities, we have the following simple observations.\nProposition 10.83. Let T be a tensor field of order k on M (Riemannian).\n1. If T is differentiable and L-Lipschitz continuous, then \u2207T is bounded by L,\nthat is, for all x \u2208 Mand for all u1, . . . uk, s\u2208 TxM,\n|\u2207T(x)(u1, . . . , uk, s)| = |(\u2207sT)(u1, . . . , uk)| \u2264 L\u2225s\u2225x\u2225u1\u2225x \u00b7 \u00b7\u00b7 \u2225uk\u2225x.\n2. If T is continuously differentiable and \u2207T is bounded by L, then T is L-\nLipschitz continuous.\nProof. For the first claim, use (10.53), continuity of the absolute value function\nand Definition 10.82 with c(t) = Expx(ts) to see that\n|\u2207T(x)(u1, . . . , uk, s)| = |(\u2207sT)(u1, . . . , uk)|\n= lim\nt\u21920\n|T(Expx(ts))(Ptsu1, . . . , Ptsuk) \u2212 T(x)(u1, . . . , uk)|\n|t|\n\u2264 lim\nt\u21920\n1\n|t|L\u2225ts\u2225x\u2225u1\u2225x \u00b7 \u00b7\u00b7 \u2225uk\u2225x = L\u2225s\u2225x\u2225u1\u2225x \u00b7 \u00b7\u00b7 \u2225uk\u2225x,\nas announced.\nFor the second claim, a bit of standard calculus and (10.52) (including notation\nthere) yield\nT(c(1))(U1(1), . . . , Uk(1)) \u2212 T(x)(u1, . . . , uk)\n=\nZ 1\n0\nd\ndt\n\u0010\nT(c(t))(U1(t), . . . , Uk(t))\n\u0011\ndt\n=\nZ 1\n0\n(\u2207T \u25e6 c)(U1, . . . , Uk, c\u2032)(t)dt.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1866, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "26011f24-7f32-492d-b756-2af370ee2832": {"__data__": {"id_": "26011f24-7f32-492d-b756-2af370ee2832", "embedding": null, "metadata": {"page_label": "301", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "02d625f2-bb2a-43d9-a4ae-aae00c226437", "node_type": "4", "metadata": {"page_label": "301", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "c186307dd1c2d95c2a534bda57d16ed8aa917a153c9e33cf5c1dab8eb68163d2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n10.7 Tensor fields and their covariant differentiation 301\nTherefore, using that \u2207T is bounded by L,\n|T(c(1))(U1(1), . . . , Uk(1)) \u2212 T(x)(u1, . . . , uk)|\n\u2264\nZ 1\n0\nL\u2225U1(t)\u2225c(t) \u00b7 \u00b7\u00b7 \u2225Uk(t)\u2225c(t)\u2225c\u2032(t)\u2225c(t)dt\n= L\u2225u1\u2225x \u00b7 \u00b7\u00b7 \u2225uk\u2225x \u00b7 L(c),\nwhere L(c) is the length of the curve c over [0, 1]. The claim follows in particular\nby letting c(t) = Expx(ts), in which case L(c) = \u2225s\u2225x.\nRemark 10.84. The Riemannian metric establishes a one-to-one correspon-\ndence between the tangent vector u \u2208 TxM and the linear map v 7\u2192 \u27e8u, v\u27e9x from\nTxM to R. In the absence of a metric, we distinguish between these two types\nof objects, respectively called vectors and covectors. When doing so, it is useful\nto define tensor fields as maps that transform vector fields and/or covector fields\ninto scalar fields, leading to the notions of covariant, contravariant and mixed\ntensor fields. These terms are likely to come up in discussions of tensor fields on\nRiemannian manifolds as well, because they are often familiar to readers from\nnon-Riemannian smooth geometry. See [Lee12, Ch. 12] and [Lee18, Ch. 4, Ch. 5]\nfor details.\nExercise 10.85. Check that \u2207T as provided by Definition 10.77 is indeed a\ntensor field of order k + 1, and that if T is symmetric, then \u2207V T is a symmetric\ntensor field of order k.\nExercise 10.86. Let M be a Riemannian manifold and let G be the smooth\ntensor field of order two defined by G(U, V) = \u27e8U, V\u27e9. Check that a connection \u2207\non M is compatible with the metric if and only if G is parallel. (See also [Lee18,\nProp. 5.5] for further characterizations of compatibility between the metric and\nthe connection.)\nExercise 10.87. Let f : M \u2192R be three times continuously differentiable on a\nRiemannian manifold M equipped with a retraction R. Assume\n\f\f\f\ff(Rx(s)) \u2212 f(x) \u2212 \u27e8s, gradf(x)\u27e9x \u2212 1\n2 \u27e8s, Hessf(x)[s]\u27e9x\n\f\f\f\f \u2264 L\n6 \u2225s\u22253\nx (10.54)\nfor all (x, s) in a neighborhood of the zero section in the tangent bundle. (Com-\npare with regularity assumption A6.7 on p135; see also Exercise 10.58.) Further\nassume R is a third-order retraction, which we define to be a second-order re-\ntraction for which all curves c(t) = R x(ts) with (x, s) \u2208 TM obey c\u2032\u2032\u2032(0) = 0 ,\nwhere c\u2032\u2032\u2032 = D\ndt c\u2032\u2032. In particular, the exponential map is a third-order retraction\n(see also Exercise 10.88). Show for all (x, s) \u2208 TM that\n|\u27e8(\u2207sHessf)[s], s\u27e9x| = |\u22073f(x)(s, s, s)| \u2264L\u2225s\u22253\nx. (10.55)\nIn other words: the symmetric part of \u22073f(x) is bounded by L.\nConversely, show that if (10.55) holds for all (x, s) \u2208 TM, then (10.54) holds\nwith R = Exp for all (x, s) in the domain of Exp.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2812, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7edff24c-a136-4660-b1c0-2a12cb23919d": {"__data__": {"id_": "7edff24c-a136-4660-b1c0-2a12cb23919d", "embedding": null, "metadata": {"page_label": "302", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "621e5e1d-65fd-4868-9e3d-575f78528742", "node_type": "4", "metadata": {"page_label": "302", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "34a35015bfd6f2865a98a1ffe33d2fac81a1176694520a88973abc2562debe90", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n302 Additional tools\nWe state here without details that \u22073f(x) may not be symmetric when x is\nnot a critical point of f and also when M is not flat at x (this follows from the\nRicci identity applied to \u2207f). Therefore, \u22073f(x) may not be bounded by L even\nif its symmetric part is. Consequently, the developments here do not allow us\nto conclude as to the Lipschitz properties of Hessf (which would have otherwise\nfollowed from Corollary 10.52 or Proposition 10.83).\nExercise 10.88. Consider a Riemannian submanifold M of a Euclidean space\nE. Given (x, v) \u2208 TM, let c: I \u2192 Mbe a smooth curve on M such that c(0) = x\nand x + tv \u2212 c(t) is orthogonal to Tc(t)M for all t: this is the case if c(t) is a\ncurve obtained through metric projection retraction (see Section 5.12). Show that\nc\u2032\u2032\u2032(0) = \u22122W\n\u0000\nv, d\ndt c\u2032(0)\n\u0001\n, where W is the Weingarten map (5.38). In general,\nthis is nonzero. Thus, we do not expect the metric projection retraction to be\nthird order in general. Hint: use (5.18) to express c\u2032\u2032\u2032(0) in terms of extrinsic\nderivatives of c, and simplify that expression by computing one more derivative\nof g in the proof of Proposition 5.55.\nExercise 10.89. Given a vector field V on M (Riemannian), let T = \u27e8V, \u00b7\u27e9 be\nthe associated tensor field of order one. Show that T is L-Lipschitz continuous\nin the sense of Definition 10.82 if and only if V is L-Lipschitz continuous in\nthe sense of Definition 10.44. Likewise, for each x \u2208 M, let H(x) denote a\nlinear map from TxM into itself, and let T denote the associated tensor field\nof order two defined through T(x)(u, v) = \u27e8H(x)(u), v\u27e9x. Show that T is L-\nLipschitz continuous in the sense of Definition 10.82 if and only if H is L-\nLipschitz continuous in the sense of Definition 10.49.\n10.8 Notes and references\nConsidering geodesics as length-minimizing curves, it is possible to generalize\nthe concept of geodesic to arbitrary metric spaces, specifically, without the need\nfor a smooth or Riemannian structure. See for example the monograph by Bac\u00b4 ak\n[Bac14] for an introduction to geodesic metric spaces, and applications in convex\nanalysis and optimization on Hadamard spaces.\nFor a connected manifold, there always exists a Riemannian metric which\nmakes it complete [NO61].\nPropositions 10.23 and 10.26 (and their proofs and corollaries) were designed\nwith Eitan Levin and also discussed with Stephen McKeown. They are inspired\nby the proof of the Tubular Neighborhood Theorem in [Lee18, Thm. 5.25] and\n[Pet06, Prop. 5.18]. They notably imply that the injectivity radius function\ninj: M \u2192R is lower-bounded by a positive, continuous function, with (one could\nargue) fewer technicalities than are required to prove inj itself is continuous.\nMany Riemannian geometry textbooks restrict their discussion of the injectiv-\nity radius to connected and complete manifolds. For disconnected manifolds, we\ndefined complete to mean geodesically complete, i.e., each connected component", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3191, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7777ca6a-9d67-4eb6-a6d2-5ca06d0aa0bc": {"__data__": {"id_": "7777ca6a-9d67-4eb6-a6d2-5ca06d0aa0bc", "embedding": null, "metadata": {"page_label": "303", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e857529b-17d6-40b7-a5d3-dd246e1731f9", "node_type": "4", "metadata": {"page_label": "303", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "ced605d56997c3a8466051e79c6df64b9fee11b3d26e0e0880252ade8c6e45d4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n10.8 Notes and references 303\nis metrically complete. A function is continuous on a disconnected set if it is\ncontinuous on each connected component. Thus, continuity of inj on connected\nand complete manifolds implies continuity on complete manifolds. It is easy to\nfind a published proof that the function inj: M \u2192R is continuous if M is (con-\nnected and) complete (see for example [Lee18, Prop. 10.37]), but it has proven\ndifficult to locate one that applies to incomplete manifolds as well. The claim\nappears without proof in [Cha06, Thm. III.2.3]. Following a discussion, Stephen\nMcKeown provided8 a proof that inj is lower-semicontinuous, then John M. Lee\nadded a proof that inj is upper-semicontinuous, together confirming that it is\ncontinuous: see Lemmas 10.90 and 10.91 below. Both proofs rely on continuity\nin the complete case. They are rearranged to highlight commonalities.\nOur main motivation to study continuity of inj is to reach Corollary 10.25,\nstating that the map ( x, y) 7\u2192 Logx(y) is smooth over the specified domain.\nO\u2019Neill makes a similar statement: pick an open set S \u2286 M; that set is deemed\nconvex (by [O\u2019N83, Def. 5.5]) if, for all x \u2208 S, the map Expx is a diffeomorphism\nfrom some neighborhood of the origin in TxM to S. Then, (x, v) 7\u2192 (x, Expx(v))\nis a diffeomorphism from the appropriate set in T S to S \u00d7 S [O\u2019N83, Lem. 5.9].\nThis shows the map L: S \u00d7 S \u2192 TS such that L(x, y) \u2208 TxM is the initial\nvelocity of the (unique) geodesic \u03b3 : [0, 1] \u2192 S connecting x to y is smooth: L\nis also a kind of inverse for the exponential (though not necessarily the same as\nLog).\nThe tool of choice to differentiate the exponential map (and the logarithmic\nmap) is Jacobi fields [Lee18, Prop. 10.10]. Some examples of this are worked out\nin the context of optimization on manifolds in [CB22a] and [LC20].\nParallel transporting a tangent vector u at x to all the points in a normal\nneighborhood [Lee18, p131] of x along geodesics through x contained in that\nneighborhood results in a smooth vector field. This is a well-known fact; details\nof the argument appear notably in [LB20, Lem. A.1].\nThe Riemannian notion of Lipschitz continuous gradient (and also Lipschitz\ncontinuous vector field) appears in [dCN95, Def. 3.1, p79] and [dCNdLO98,\nDef. 4.1], with a definition equivalent to the characterization we give in Proposi-\ntion 10.45. This may be their first occurrence in an optimization context. There\ntoo, the motivation is to derive inequalities such as the ones in Proposition 10.53.\nSuch inequalities appear often in optimization papers, see also [AMS08, \u00a7 7.4],\n[SFF19, App. A] and [ABBC20], among many others. Lipschitz continuous Hes-\nsians appear in an optimization context in [FS02, Def. 2.2], in line with our\ncharacterization in Proposition 10.50. A general definition of Lipschitz continu-\nous maps in tangent bundles of any order (covering tensors fields of any order)\nappears in [RW12], in the preliminaries on geometry. A general notion of \u2018fun-\ndamental theorem of calculus\u2019 for tensor fields on Riemannian manifolds, based\non parallel transport, is spelled out in [ABM08, eq. (2.3)].\nAs an alternative to Definition 10.44, one could also endow the tangent bundle\n8 mathoverflow.net/questions/335032", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3495, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d2415395-05d9-4e08-a8dd-3ebe2c06eeb3": {"__data__": {"id_": "d2415395-05d9-4e08-a8dd-3ebe2c06eeb3", "embedding": null, "metadata": {"page_label": "304", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "652e63cd-50e9-47c5-b5c0-05069ec9a45e", "node_type": "4", "metadata": {"page_label": "304", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "e8f0c586f0775cf62bf731f7e9baf53bbd869582b31a6873d9d299341393ea4a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n304 Additional tools\nwith a metric space structure, so that we can then apply the standard notion of\nLipschitz continuity to vector fields as maps between two metric spaces. A canon-\nical choice would follow from the Sasaki metric [GHL04, \u00a7 2.B.6]. See [dOF20]\nfor a comparison of the two concepts.\nThe notion of vector transport appears in [AMS08, \u00a7 8]. The related notion\nof transporter is introduced in [QGA10a] with reference to a linear structure\nspace, and further developed in [HGA15] for general manifolds. The constructions\nof transporters from other transporters via inversions and adjoints are natural\nextensions.\nCorollary 10.75 handles finite differences of the Riemannian Hessian using an\narbitrary transporter (Definition 10.61). An analogous result for vector trans-\nports (Definition 10.68) appears in [AMS08, Lem. 8.2.2].\nIn Exercise 10.73, we contemplate the role of the initial acceleration of curves\nof the form c(t) = Rx(s+t \u02d9s). Consider the special case where R is the exponential\nmap. If s = 0, then c is a geodesic so that c\u2032\u2032(0) = 0; but for s \u0338= 0 we expect\nc\u2032\u2032(0) \u0338= 0 in general. The tangent vector c\u2032\u2032(0) and its norm are tightly related\nto curvature of the manifold. See [CB22a, LC20] for a discussion of that vector\nand its effects on the Lipschitzness of pullbacks and their derivatives.\nIn Section 10.7, we follow do Carmo [dC92, \u00a7 4.5], in that we rely on the Rie-\nmannian metric to avoid the need to distinguish between vectors and covectors,\nand we build up the differentiation of tensor fields by quoting the desired chain\nrule directly, bypassing many technical steps. This simplifies the discussion with-\nout loss of generality.\nWe close this section with the proofs of continuity of the injectivity radius,\nas stated in Proposition 10.24. In these proofs, we do not need to worry about\ninfinite values. Indeed, if inj( x) = \u221e at some point x, then Exp x is defined on\nall of TxM. Thus, the connected component of x is complete [Lee18, Cor. 6.20],\nand it follows by [Lee18, Prop. 10.37] that inj is continuous on that component.\n(More specifically: inj is infinite at all points in that component.)\nLemma 10.90. The injectivity radius function inj: M \u2192(0, \u221e] is lower-semi-\ncontinuous.\nProof by Stephen McKeown. For contradiction, assume inj is not lower-semi-\ncontinuous at some point x \u2208 M. Then, there exists a sequence of points\nx0, x1, x2, . . .on M such that\nlim\nk\u2192\u221e\nxk = x and \u2200k, inj(xk) \u2264 r < R= inj(x).\nDefine \u03b5 = R\u2212r\n3 and r\u2032 = r+\u03b5, r\u2032\u2032 = r+2\u03b5 so that r < r\u2032 < r\u2032\u2032 < R. By definition\nof R, the exponential map Expx induces a diffeomorphism \u03c6: B(x, R) \u2192 Bd(R):\nfrom the open geodesic ball on M centered at x with radius R to the open\nEuclidean ball in Rd centered at the origin with radius R, where d = dim M. (\u03d5\nis constructed from the inverse of Expx followed by a linear isometry from TxM\nto Rd.) Let g denote the Riemannian metric on M, let \u02dcg denote the pushforward", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3179, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3457e16e-6058-4483-af57-b1966b724f1b": {"__data__": {"id_": "3457e16e-6058-4483-af57-b1966b724f1b", "embedding": null, "metadata": {"page_label": "305", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7377f3b8-0bd3-4cb8-b3bd-337711129bcb", "node_type": "4", "metadata": {"page_label": "305", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "8260be8eb55c42f505c5035df4eb2242174314fd90882206646c3232bf56d440", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n10.8 Notes and references 305\nof g|B(x,R) to Bd(R) (through \u03c6), and let g0 denote the Euclidean metric on Rd.\nConsider a smooth bump function \u03c7: Rd \u2192 [0, 1] whose value is 1 on the closed\nball \u00afBd(r\u2032\u2032) and with support in \u00afBd(R) [Lee12, Prop. 2.25]. Then,\n\u02c6g = \u03c7\u02dcg + (1 \u2212 \u03c7)g0\nis a Riemannian metric on Rd such that \u02c6M = (Rd, \u02c6g) is a (connected) complete\nRiemannian manifold [Lee18, Pb. 6-10].\nConsequently, the injectivity radius function \u02c6inj: \u02c6M \u2192(0, \u221e] is continu-\nous [Lee18, Prop. 10.37]. Furthermore, since the metrics g and \u02c6g agree (through\n\u03c6) on \u00afB(x, r\u2032\u2032) and \u00afBd(r\u2032\u2032), we deduce that for all y \u2208 Mand \u03c1 >0 it holds that\nB(y, \u03c1) \u2286 B(x, r\u2032\u2032) = \u21d2\n( \u02c6inj(\u02c6y) = inj(y) if inj( y) < \u03c1,\n\u02c6inj(\u02c6y) \u2265 \u03c1 otherwise,\nwhere \u02c6y \u2208 Rd is the image of y through \u03c6.\nWe use this fact in two ways:\n1. B(x, r\u2032\u2032) \u2286 B(x, r\u2032\u2032) and inj(x) = R > r\u2032\u2032, hence \u02c6inj(\u02c6x) \u2265 r\u2032\u2032, and\n2. There exists k0 large enough such that, for all k \u2265 k0, dist(xk, x) < \u03b5, so that\nB(xk, r\u2032) \u2282 B(x, r\u2032\u2032). Moreover, inj(xk) \u2264 r < r\u2032, so that \u02c6inj(\u02c6xk) = inj(xk) \u2264 r\nfor all k \u2265 k0.\nTogether with the fact that \u02c6inj is continuous, these yield:\nr < r\u2032\u2032 \u2264 \u02c6inj(\u02c6x) = lim\nk\u2192\u221e\n\u02c6inj(\u02c6xk) \u2264 r,\na contradiction.\nLemma 10.91. The injectivity radius function inj: M \u2192 (0, \u221e] is upper-\nsemicontinuous.\nProof by John M. Lee. The proof parallels that of lower-semicontinuity. For con-\ntradiction, assume inj is not upper-semicontinuous at some point x \u2208 M. Then,\nthere exists a sequence of points x0, x1, x2, . . .on M such that\nlim\nk\u2192\u221e\nxk = x and \u2200k, inj(xk) \u2265 R > r= inj(x).\nDefine \u03b5 = R\u2212r\n3 and r\u2032 = r + \u03b5, r\u2032\u2032 = r + 2\u03b5 so that r < r\u2032 < r\u2032\u2032 < R. Because\nthe injectivity radius at x is smaller than at the points in the sequence, it is not\nenough to consider Exp x to setup a diffeomorphism with a ball in Rd. Instead,\nwe pick a special point in the sequence to act as a center. Let k0 be large enough\nso that dist( xk, x) < \u03b5/2 < \u03b5 for all k \u2265 k0. By triangular inequality, we also\nhave dist(xk, xk0 ) < \u03b5for all k \u2265 k0. Now, use Expxk0\nto setup a diffeomorphism\n\u03c6: B(xk0 , R) \u2192 Bd(R): we can do this since inj( xk0 ) \u2265 R. Let g denote the\nRiemannian metric on M, let \u02dcg denote the pushforward of g|B(xk0 ,R) to Bd(R)\n(through \u03c6), and let g0 denote the Euclidean metric on Rd. Consider a smooth", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2506, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5932ad7d-368d-4cc2-8818-1fcbb82eee36": {"__data__": {"id_": "5932ad7d-368d-4cc2-8818-1fcbb82eee36", "embedding": null, "metadata": {"page_label": "306", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7f4af078-8631-4a7e-9b92-c9eca377e19e", "node_type": "4", "metadata": {"page_label": "306", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "3269c9721ec351b59b53a7f9ce10d789f77c6a861d8b6721b2c2d379a91792fd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n306 Additional tools\nbump function \u03c7: Rd \u2192 [0, 1] whose value is 1 on the closed ball \u00afBd(r\u2032\u2032) and\nwith support in \u00afBd(R). Then,\n\u02c6g = \u03c7\u02dcg + (1 \u2212 \u03c7)g0\nis a Riemannian metric on Rd such that \u02c6M = (Rd, \u02c6g) is a (connected) complete\nRiemannian manifold.\nConsequently, the injectivity radius function \u02c6inj: \u02c6M \u2192(0, \u221e] is continuous.\nFurthermore, since the metrics g and \u02c6g agree (through \u03c6) on \u00afB(xk0 , r\u2032\u2032) and\n\u00afBd(r\u2032\u2032), we deduce that for all y \u2208 Mand \u03c1 >0 it holds that\nB(y, \u03c1) \u2286 B(xk0 , r\u2032\u2032) = \u21d2\n( \u02c6inj(\u02c6y) = inj(y) if inj( y) < \u03c1,\n\u02c6inj(\u02c6y) \u2265 \u03c1 otherwise,\nwhere \u02c6y \u2208 Rd is the image of y through \u03c6.\nWe use this fact in two ways:\n1. B(x, r\u2032) \u2282 B(xk0 , r\u2032\u2032) and inj(x) = r < r\u2032, hence \u02c6inj(\u02c6x) = inj(x) = r, and\n2. For all k \u2265 k0, B(xk, r\u2032) \u2282 B(xk0 , r\u2032\u2032) and inj(xk) \u2265 R > r\u2032, thus \u02c6inj(\u02c6xk) \u2265 r\u2032.\nTogether with the fact that \u02c6inj is continuous, these yield:\nr = inj(x) = \u02c6inj(\u02c6x) = lim\nk\u2192\u221e\n\u02c6inj(\u02c6xk) \u2265 r\u2032 > r,\na contradiction.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1183, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "72e1c06c-abdf-421c-84e4-d7a0d073ed78": {"__data__": {"id_": "72e1c06c-abdf-421c-84e4-d7a0d073ed78", "embedding": null, "metadata": {"page_label": "307", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ae3c8357-e8f3-4ce7-a39b-8976e9687996", "node_type": "4", "metadata": {"page_label": "307", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "ddeb3b2066b1f2229fe388ec5986aff64e70d902c062e9484e7d47499029e6e1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n11 Geodesic convexity\nIn this chapter, we discuss elementary notions of convexity for optimization on\nmanifolds. In so doing, we resort to notions of Riemannian distance, geodesics\nand completeness as covered in Section 10.1. At times, we also use the exponential\nmap introduced in Section 10.2 and the concept of Lipschitz continuity from\nSection 10.4.\nThe study of convexity on Riemannian manifolds, called geodesic convexity,\npredates optimization on manifolds. In the context of optimization, it attracted\na lot of attention as soon as the 70s. Excellent reference books on this topic\ninclude one by Udri\u00b8 ste [Udr94] and another by Rapcs\u00b4 ak [Rap97].\nThere is some variation in how geodesically convex sets are defined by different\nauthors. This is partly because the needs for convexity may differ depending on\nusage. We favor the permissive definition of Rapcs\u00b4 ak [Rap97,\u00a7 6] and relate it to\ntwo other popular definitions in Section 11.3.\nAll three definitions turn out to be equivalent for complete, simply connected\nRiemannian manifolds with nonpositive curvature. Those are called Cartan\u2013\nHadamard manifolds. They provide the most favorable playground for geodesic\nconvexity. They include Euclidean spaces, hyperbolic spaces (Section 7.6), the\npositive orthant Rn\n+ (Section 11.6) and the set of positive definite matrices\nSym(n)+ (Section 11.7), all with the appropriate Riemannian metrics.\nApplications of geodesically convex optimization notably include covariance\nmatrix estimation [Wie12, NSAY+19], Gaussian mixture modeling [HS15, HS19],\nmatrix square root computation [Sra16], metric learning [ZHS16], statistics and\naveraging on manifolds [Moa03, Moa05, Fle13], a whole class of optimization\nproblems called geometric programming [BKVH07], operator scaling in relation\nto the Brascamp\u2013Lieb constant [Vis18, AZGL+18], integrative PCA and matrix\nnormal models [TA21, FORW21] and Tyler-M estimation [FM20].\nZhang and Sra analyze a collection of algorithms specifically designed for\ngeodesically convex optimization [ZS16], providing worst-case iteration complex-\nity results. We discuss gradient descent in Section 11.5. See also Section 11.8 for\nreferences to literature about the possibility of accelerating gradient descent on\nmanifolds.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2515, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "45150d4b-45b4-4834-b1bc-96c457b5043b": {"__data__": {"id_": "45150d4b-45b4-4834-b1bc-96c457b5043b", "embedding": null, "metadata": {"page_label": "308", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "58f6aa3f-f410-41ce-b9bb-d2c4fb818eb0", "node_type": "4", "metadata": {"page_label": "308", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "01fb7a1c301cc97ab77f581f4524240b9336c797f7ffd34b9966415dd46e82f1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n308 Geodesic convexity\n11.1 Convex sets and functions in linear spaces\nRecall that a subset S of a linear space E is a convex set if for all x, yin S the\nline segment t 7\u2192 (1 \u2212 t)x + ty for t \u2208 [0, 1] is in S. Furthermore,1 f : S \u2192 R is a\nconvex function if S is convex and for all x, y\u2208 S we have:\n\u2200t \u2208 [0, 1], f ((1 \u2212 t)x + ty) \u2264 (1 \u2212 t)f(x) + tf(y).\nLikewise, f is strictly convex if for x \u0338= y we have\n\u2200t \u2208 (0, 1), f ((1 \u2212 t)x + ty) < (1 \u2212 t)f(x) + tf(y).\nIf E is a Euclidean space with norm \u2225 \u00b7 \u2225, we say f is \u00b5-strongly convex for some\n\u00b5 > 0 if x 7\u2192 f(x) \u2212 \u00b5\n2 \u2225x\u22252 is convex, or equivalently, if for all x, y\u2208 S and\nt \u2208 [0, 1] it holds that:\nf((1 \u2212 t)x + ty) \u2264 (1 \u2212 t)f(x) + tf(y) \u2212 t(1 \u2212 t)\u00b5\n2 \u2225x \u2212 y\u22252. (11.1)\nIn optimization, our main reason to care about convex functions is that their\nlocal minimizers (if any exist) are global minimizers.\nAn equivalent way of defining convex functions is to define convexity for one-\ndimensional functions first. Then, f : S \u2192 R is convex if and only if f is convex\nwhen restricted to all line segments in the convex set S, that is, for all x, y\ndistinct in S, the composition f \u25e6c: [0, 1] \u2192 R is convex with c(t) = (1\u2212t)x+ty.\nA similar statement holds for strict and strong convexity.\nWe adopt the latter perspective in the next section to generalize beyond linear\nspaces. To that end, a few basic facts about one-dimensional convex functions\ncome in handy.\nLemma 11.1. Let g : I \u2192 R be defined on a connected set I \u2286 R.\n1. If g is convex, then g is continuous in the interior of I, denoted intI.\n2. If g is differentiable on I:2\n(a) g is convex if and only if g(y) \u2265 g(x) + (y \u2212 x)g\u2032(x) for all x, y\u2208 I.\n(b) g is strictly convex if and only if g(y) > g(x)+( y\u2212x)g\u2032(x) for all x, y\u2208 I\ndistinct.\n(c) g is \u00b5-strongly convex if and only if g(y) \u2265 g(x)+( y\u2212x)g\u2032(x)+ \u00b5\n2 (y\u2212x)2\nfor all x, y\u2208 I.\n3. If g is continuously differentiable on I and twice differentiable on intI:\n(a) g is convex if and only if g\u2032\u2032(x) \u2265 0 for all x \u2208 intI.\n(b) g is strictly convex if (but not only if) g\u2032\u2032(x) > 0 for all x \u2208 intI.\n(c) g is \u00b5-strongly convex if and only if g\u2032\u2032(x) \u2265 \u00b5 for all x \u2208 intI.\nProof. For the first point, see [HUL01, p15]. A proof of the second and third\npoints follows for convenience. Note that it is allowed for I not to be open.\n1 With some care, we may allow f to take on infinite values [Roc70, HUL01].\n2 If I is not open, we mean differentiable in the sense that there exists an extension \u00af g of g\ndifferentiable on a neighborhood of I. Then, g\u2032(x) \u225c \u00afg\u2032(x) for all x \u2208 I.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2782, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "070b9fee-43ca-4fb0-92b9-6af843183dd4": {"__data__": {"id_": "070b9fee-43ca-4fb0-92b9-6af843183dd4", "embedding": null, "metadata": {"page_label": "309", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a5383756-ad7d-473b-8748-8e5ca3db9873", "node_type": "4", "metadata": {"page_label": "309", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "b6c48b09ab2650d4ce3e46c4a83575a3b939ec13e70d4d91d53e431f625abe15", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n11.1 Convex sets and functions in linear spaces 309\n2. (a) Assume the inequalities hold. Then, for x, y\u2208 I and t \u2208 [0, 1] arbitrary,\ndefine z = (1 \u2212 t)x + ty. Both of the following inequalities hold:\ng(x) \u2265 g(z) + (x \u2212 z)g\u2032(z), g (y) \u2265 g(z) + (y \u2212 z)g\u2032(z).\nAdd them up with weights 1 \u2212 t and t, respectively:\n(1 \u2212 t)g(x) + tg(y) \u2265 g(z) +\n\u0000\n(1 \u2212 t)(x \u2212 z) + t(y \u2212 z)\n\u0001\ng\u2032(z)\n= g(z)\n= g((1 \u2212 t)x + ty).\nThis shows g is convex. The other way around, if g is convex, then for all\nx, y\u2208 I and t \u2208 (0, 1] we have\ng(x + t(y \u2212 x)) = g((1 \u2212 t)x + ty)\n\u2264 (1 \u2212 t)g(x) + tg(y) = g(x) + t(g(y) \u2212 g(x)).\nMove g(x) to the left-hand side and divide by t to find:\ng(y) \u2265 g(x) + g(x + t(y \u2212 x)) \u2212 g(x)\nt .\nSince this holds for all x, y, tas prescribed and since g is differentiable at x,\nwe can take the limit for t \u2192 0 and conclude that the sought inequalities\nhold.\n(b) Assume the strict inequalities hold. Then, for all x, y\u2208 I distinct and\nfor all t \u2208 (0, 1), define z = (1 \u2212 t)x + ty; we have:\ng(x) > g(z) + (x \u2212 z)g\u2032(z), g (y) > g(z) + (y \u2212 z)g\u2032(z).\nMultiply by 1 \u2212 t and t, respectively, and add them up:\n(1 \u2212 t)g(x) + tg(y) > g(z) = g((1 \u2212 t)x + ty).\nThis shows g is strictly convex. The other way around, assume g is strictly\nconvex: it lies strictly below its chords, that is, for all x, ydistinct in I,\n\u2200t \u2208 (0, 1), g ((1 \u2212 t)x + ty) < (1 \u2212 t)g(x) + tg(y).\nSince g is convex, it also lies above its first-order approximations:\n\u2200t \u2208 [0, 1], g (x + t(y \u2212 x)) \u2265 g(x) + t(y \u2212 x)g\u2032(x).\nThe left-hand sides coincide, so that combining we find:\n\u2200t \u2208 (0, 1), (1 \u2212 t)g(x) + tg(y) > g(x) + t(y \u2212 x)g\u2032(x).\nSubtract g(x) on both sides and divide by t to conclude.\n(c) By definition, g is \u00b5-strongly convex if and only if h(x) = g(x) \u2212 \u00b5\n2 x2\nis convex, and we just showed that the latter is convex if and only if\nh(y) \u2265 h(x)+( y \u2212x)h\u2032(x) for all x, y\u2208 I, which is equivalent to the claim.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2126, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eb93d18d-b068-4afa-8230-62cf3ccd2af8": {"__data__": {"id_": "eb93d18d-b068-4afa-8230-62cf3ccd2af8", "embedding": null, "metadata": {"page_label": "310", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "32057854-cb08-4bfc-9a7d-5a8390ab5503", "node_type": "4", "metadata": {"page_label": "310", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "41b77584de8f1074691272d119003554fc570c25a35301784a51ae9d81d8f7e7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n310 Geodesic convexity\n3. Taylor\u2019s theorem applies to g: for all x, ydistinct in I, there exists z strictly\nbetween x and y such that\ng(y) = g(x) + (y \u2212 x)g\u2032(x) + 1\n2(y \u2212 x)2g\u2032\u2032(z). (11.2)\n(a) If g\u2032\u2032(z) \u2265 0 for all z \u2208 intI, then g(y) \u2265 g(x) + (y \u2212 x)g\u2032(x) for all\nx, y\u2208 I by (11.2), hence g is convex. The other way around, if g is convex,\nthen for all x, yin I we have:\ng(y) \u2265 g(x) + (y \u2212 x)g\u2032(x) and g(x) \u2265 g(y) + (x \u2212 y)g\u2032(y).\nRearrange and combine to find\n(y \u2212 x)g\u2032(y) \u2265 g(y) \u2212 g(x) \u2265 (y \u2212 x)g\u2032(x).\nWe deduce that y \u2265 x implies g\u2032(y) \u2265 g\u2032(x), that is, g\u2032 is nondecreasing on\nI. For all x \u2208 intI, consider the following limit, where y goes to x while\nremaining in I:\n0 \u2264 lim\ny\u2192x\ng\u2032(y) \u2212 g\u2032(x)\ny \u2212 x = g\u2032\u2032(x).\nThis shows g\u2032\u2032(x) \u2265 0 for all x in int I. (The same argument also shows\nthat if g is twice differentiable on I and I has any boundary points, then\ng\u2032\u2032 is also nonnegative on those points.)\n(b) If g\u2032\u2032(z) > 0 for all z \u2208 intI, then g(y) > g(x) + (y \u2212 x)g\u2032(x) for all\nx, y\u2208 I distinct by (11.2), hence g is strictly convex. The converse is not\ntrue: g(x) = x4 is smooth and strictly convex on R, yet g\u2032\u2032(0) = 0.\n(c) By definition, g is \u00b5-strongly convex if and only if h(x) = g(x) \u2212 \u00b5\n2 x2 is\nconvex, and we just showed that the latter is convex if and only ifh\u2032\u2032(x) \u2265 0\nfor all x \u2208 intI, which is equivalent to the claim.\n11.2 Geodesically convex sets and functions\nIn this section, we present a classical generalization of convexity to Riemannian\nmanifolds. The main idea is to use geodesic segments instead of line segments.\nThere are, however, a number of subtly different ways one can do this, due to the\nfact that, in contrast to line segments in Euclidean spaces, geodesics connecting\npairs of points may not exist, may not be unique, and may not be minimizing.\nSee Section 11.3 for a discussion of popular alternative definitions.\nDefinition 11.2. A subset S of a Riemannian manifold M is geodesically con-\nvex if, for every x, y\u2208 S, there exists a geodesic segment c: [0, 1] \u2192 Msuch\nthat c(0) = x, c(1) = y and c(t) is in S for all t \u2208 [0, 1].\nIn this definition,c is a geodesic forM, not necessarily forS (which may or may\nnot be a manifold). In particular, singletons and the empty set are geodesically\nconvex.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2489, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e057538f-3cad-4d04-a9d9-68b8d1218ab8": {"__data__": {"id_": "e057538f-3cad-4d04-a9d9-68b8d1218ab8", "embedding": null, "metadata": {"page_label": "311", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dd7371e0-94b6-4347-9e8f-e40fa9bb20b7", "node_type": "4", "metadata": {"page_label": "311", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "6b06366a249f60ff6ec449e890927c90c8129459b2b75b0d8f847a274b296601", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n11.2 Geodesically convex sets and functions 311\nIf M is a Euclidean space, then a subset is convex in the usual sense if and\nonly if it is geodesically convex, because the only geodesic connecting x to y (up\nto reparameterization) is c(t) = (1 \u2212 t)x + ty.\nBy Theorem 10.9, a connected and complete Riemannian manifold is geodesi-\ncally convex. This includes spheres, the Stiefel manifold St( n, p) for p < n, and\nthe group of rotations SO( n)\u2014but we will soon see that such compact mani-\nfolds are not interesting for convexity unless we restrict our attention to subsets.\nMore interestingly, any hemisphere of Sn\u22121, open or closed, is a geodesically con-\nvex subset of S n\u22121. The hyperbolic space discussed in Section 7.6 is connected\nand complete; hence geodesically convex. Likewise, the manifold of positive real\nnumbers, R+ = {x >0}, equipped with the metric \u27e8u, v\u27e9x = uv\nx2 , is connected\nand complete; hence geodesically convex. We consider two generalizations of the\nlatter in Sections 11.6 and 11.7 to handle Rn\n+ and Sym(n)+, that is, entrywise\npositive vectors and positive definite matrices, respectively.\nFor a subset S of a manifold M, we say a curve c on M connects x to y in S\nif it is continuous, c(0) = x, c(1) = y and c(t) is in S for all t \u2208 [0, 1].\nIn a geodesically convex set S, any two points are connected in S by at least\none geodesic segment c. Composing a function f : S \u2192 R with c yields a real\nfunction on [0 , 1]. If all of these compositions are convex in the usual sense, we\nsay f is convex in a geometric sense. Note that we do not require f to be smooth\nor even continuous.\nDefinition 11.3. A function f : S \u2192 R is geodesically (strictly) convex if S\nis geodesically convex and f \u25e6 c: [0, 1] \u2192 R is (strictly) convex for each geodesic\nsegment c: [0, 1] \u2192 Mwhose image is in S (with c(0) \u0338= c(1)).\nIn the above definition, we are tacitly referring to the Riemannian structure\non M for which S \u2286 Mis geodesically convex and for which the curves c are\ngeodesics. Here too, if M is a Euclidean space, we recover the standard notion\nof (strictly) convex function.\nIn other words, forS a geodesically convex set, we sayf : S \u2192 R is geodesically\nconvex if for all x, y\u2208 S and all geodesics c connecting x to y in S the function\nf \u25e6 c: [0, 1] \u2192 R is convex, that is,\n\u2200t \u2208 [0, 1], f (c(t)) \u2264 (1 \u2212 t)f(x) + tf(y). (11.3)\nIf additionally whenever x \u0338= y we have\n\u2200t \u2208 (0, 1), f (c(t)) < (1 \u2212 t)f(x) + tf(y), (11.4)\nthen we say f is geodesically strictly convex.\nDefinition 11.4. We say f : S \u2192 R is geodesically (strictly) concave if \u2212f is\ngeodesically (strictly) convex, and f is geodesically linear if it is both geodesically\nconvex and concave.\nWe also extend the notion of strong convexity, in analogy with (11.1). Recall\nthat the length of a curve segment was defined in Section 10.1.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3070, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ebbceecc-e1c2-4157-89c6-20c84dedd341": {"__data__": {"id_": "ebbceecc-e1c2-4157-89c6-20c84dedd341", "embedding": null, "metadata": {"page_label": "312", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b2f05688-c05e-4d48-97aa-dba15fa156d1", "node_type": "4", "metadata": {"page_label": "312", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "d4b45aca86a8a1a81766b50e58c8187080baeddbf32287207be00823d1b39ab3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n312 Geodesic convexity\nDefinition 11.5. A function f : S \u2192 R is geodesically \u00b5-strongly convex for\nsome \u00b5 > 0 if the set S is geodesically convex and for each geodesic segment\nc: [0, 1] \u2192 Mwhose image is in S we have\nf(c(t)) \u2264 (1 \u2212 t)f(c(0)) + tf(c(1)) \u2212 t(1 \u2212 t)\u00b5\n2 L(c)2,\nwhere L(c) = \u2225c\u2032(0)\u2225c(0) is the length of the geodesic segment. This is equivalent\nto the requirement that f \u25e6 c: [0, 1] \u2192 R be \u00b5L(c)2-strongly convex in the usual\nsense.\nClearly, geodesic strong convexity implies geodesic strict convexity.\nSame as for standard convexity in linear spaces, geodesic convexity ensures\nthat local minimizers, if they exist, are global minimizers.\nTheorem 11.6. If f : S \u2192 R is geodesically convex, then any local minimizer\nis a global minimizer.\nProof. For contradiction, assume x \u2208 S is a local minimizer that is not a global\nminimizer. Then, there exists y \u2208 S such that f(y) < f(x). There also exists a\ngeodesic c connecting c(0) = x to c(1) = y in S such that, for all t \u2208 (0, 1],\nf(c(t)) \u2264 (1 \u2212 t)f(x) + tf(y) = f(x) + t(f(y) \u2212 f(x)) < f(x),\nwhich contradicts the claim that x is a local minimizer.\nStrict convexity yields uniqueness of minimizers, when they exist.\nTheorem 11.7. If f : S \u2192 R is geodesically strictly convex, then it admits at\nmost one local minimizer, which is necessarily the global minimizer.\nProof. From Theorem 11.6, we know that any local minimizer is a global mini-\nmizer. Assume for contradiction that there exist two distinct global minimizers,\nx and y, so that f(x) = f(y) = f\u22c6. There exists a geodesic c connecting them in\nS such that, for t \u2208 (0, 1),\nf(c(t)) < (1 \u2212 t)f(x) + tf(y) = f\u22c6,\nwhich contradicts global optimality of x and y.\nThe sublevel sets of geodesically convex functions are geodesically convex. 3\nMoreover, the intersection of such sublevel sets is also geodesically convex. How-\never, the intersection of arbitrary geodesically convex sets is not necessarily\ngeodesically convex: see Section 11.3.\nProposition 11.8. Let i \u2208 I index an arbitrary collection of geodesically convex\nfunctions fi : S \u2192 R and scalars \u03b1i \u2208 R. Define the sublevel sets\nSi = {x \u2208 S : fi(x) \u2264 \u03b1i}.\n3 The converse does not hold: a function on a geodesically convex set is called geodesically\nquasiconvex if all of its sublevel sets are geodesically convex [Rap97, Lem. 13.1.1].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2564, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7a5c971c-1131-4e5f-a82c-5f0d271f5219": {"__data__": {"id_": "7a5c971c-1131-4e5f-a82c-5f0d271f5219", "embedding": null, "metadata": {"page_label": "313", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e7a6c66-f944-40a2-8a74-ad7a15bf7e0c", "node_type": "4", "metadata": {"page_label": "313", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "dd497616e85ed0ff18059bc360185ed6c6834c9a3d165a83413c7fc2fc85dcb0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n11.2 Geodesically convex sets and functions 313\nTheir intersection S\u2032 = \u2229i\u2208ISi is geodesically convex. In particular, the sublevel\nsets of one geodesically convex function f are geodesically convex sets, and the\nset of global minimizers of f is geodesically convex.\nProof. The claim is clear if S\u2032 is empty. Assume it is not. Pick an arbitrary pair\nof points x, y\u2208 S\u2032, and an arbitrary geodesic c: [0, 1] \u2192 Mconnecting x to y in\nS: there exists at least one because x, yare in S and S is geodesically convex.\nFor every i \u2208 I and for all t \u2208 [0, 1], it holds that\nfi(c(t)) \u2264 (1 \u2212 t)fi(x) + tfi(y) \u2264 (1 \u2212 t)\u03b1i + t\u03b1i = \u03b1i,\nwhere we used the fact that x and y belong to Si. We conclude that c(t) belongs\nto each Si for all t \u2208 [0, 1], so that c is in fact a geodesic connecting x and y in\nS\u2032. Thus, S\u2032 is geodesically convex.\nHere is a take-away from Proposition 11.8. Let S be a geodesically convex\nset on M. If f, f1, . . . , fm are geodesically convex functions on S and g1, . . . , gp\nare geodesically linear functions on S (Definition 11.4), then for arbitrary reals\n\u03b11, . . . , \u03b1m and \u03b21, . . . , \u03b2p we call\nmin\nx\u2208S\nf(x) subject to fi(x) \u2264 \u03b1i for i = 1, . . . , m,\ngj(x) = \u03b2j for j = 1, . . . , p (11.5)\na geodesically convex program. Since the constraint gj(x) = \u03b2j is equivalent to\nthe two constraints gj(x) \u2264 \u03b2j and \u2212gj(x) \u2264 \u2212\u03b2j, and since both gj and \u2212gj\nare geodesically convex, it follows that the set S\u2032 of points which satisfy all\nconstraints in (11.5) is geodesically convex. Therefore, any local minimizer of\nf|S\u2032 is a global minimizer of f|S\u2032.\nA connected, complete Riemannian manifold is a geodesically convex set. Its\ninterior4 is the whole manifold itself. Consider that observation with the follow-\ning fact [Rap97, Thm. 6.1.8], [Udr94, Thm. 3.6].\nProposition 11.9. If f : S \u2192 R is geodesically convex, then f is continuous on\nthe interior of S.\nCompact manifolds are complete, and continuous functions on compact sets\nattain their maximum. Consider geodesics through the maximizer to conclude:\nCorollary 11.10. If M is a connected, compact Riemannian manifold and\nf : M \u2192R is geodesically convex, then f is constant.\nThe take-away is that on compact manifolds geodesic convexity is only inter-\nesting on subsets of a connected component. We proceed with a generalization.\nWhen a geodesically convex function admits a maximizer (it may admit none,\none or many), this maximizer typically occurs on the boundary of the geodesi-\ncally convex domain. Indeed, a maximizer occurs \u2018inside\u2019 the domain only in\n4 The interior of a subset S of a manifold M is the union of all subsets of S open in M.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2865, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "923a8673-c5b2-460a-b726-824d420550bd": {"__data__": {"id_": "923a8673-c5b2-460a-b726-824d420550bd", "embedding": null, "metadata": {"page_label": "314", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8f8aab45-0db5-45c5-87da-2ddbec6fac83", "node_type": "4", "metadata": {"page_label": "314", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "1d80e6728bad95f869b8db9720fa3e8805ee32c4755682f3800cae4089c496a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n314 Geodesic convexity\nuninteresting situations. We formalize this below, with a definition and a propo-\nsition. The definition is an extension from the classical case [Roc70, Thm. 6.4].\nIt is helpful to picture a two-dimensional triangle or disk in R3.\nDefinition 11.11. Let S be a geodesically convex set on a Riemannian manifold\nM. The relative interior of S, denoted by relintS, is the set of points x \u2208 S with\nthe following property: for all y \u2208 S, all geodesics c: [0, 1] \u2192 Mconnecting\nx = c(0) to y = c(1) in S can be extended to the domain [\u2212\u03b5, 1] for some \u03b5 >0,\nand still be geodesics of M with image in S.\nProposition 11.12. Let f : S \u2192 R be geodesically convex. If f attains its max-\nimum at a point x in the relative interior of S, then f is constant on S.\nProof. Pick an arbitrary point y \u2208 S. Our goal is to show f(y) = f(x). Consider\nany geodesic c: [0, 1] \u2192 Mconnecting x = c(0) to y = c(1) in S. Since x is in the\nrelative interior of S, we can extend the domain of c to [\u2212\u03b5, 1] for some \u03b5 >0,\nand it is still a geodesic in S. Let z = c(\u2212\u03b5). Since f is geodesically convex on\nS, f \u25e6 c is convex and we deduce:\nf(x) \u2264 1\n1 + \u03b5f(z) + \u03b5\n1 + \u03b5f(y).\nMultiply by 1 + \u03b5; since x is a maximizer, f(z) \u2264 f(x) and we find:\n\u03b5f(x) \u2264 \u03b5f(y).\nSince \u03b5 is positive, we deduce that f(x) \u2264 f(y). But x is a maximizer and hence\nf(x) \u2265 f(y). It follows that f(x) = f(y), as announced.\nExercise 11.13. Assume f and g are geodesically convex on the set S. Show\nthat x 7\u2192 max(f(x), g(x)) is geodesically convex on S. Further show that x 7\u2192\n\u03b1f(x) + \u03b2g(x) is geodesically convex on S for all \u03b1, \u03b2\u2265 0.\nExercise 11.14. Let f : S \u2192 R be geodesically convex. Show that if h: R \u2192 R\nis nondecreasing and convex, then h \u25e6 f is geodesically convex on S.\nExercise 11.15. Let S1 be a geodesically convex set on a Riemannian manifold\nM1, and similarly for S2 on M2. Verify that S1 \u00d7 S2 is geodesically convex on\nthe Riemannian product manifold M1 \u00d7 M2.\n11.3 Alternative definitions of geodesically convex sets*\nDefinition 11.2 for a geodesically convex set S is the one preferred by Rapcs\u00b4 ak,\nwell suited for optimization purposes [Rap91, Def. 6.1.1]. It is rather permissive:\nit merely requires that every pair of points x, y\u2208 S be connected by some\ngeodesic segment in the set. It does not require all geodesic segments connecting\nx and y to stay in S, nor does it require uniqueness of such a segment, nor that\nthere exist a minimizing geodesic segment connecting x and y and that this one\nstay in S\u2014all properties we have in Euclidean spaces.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2784, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "846c5a10-6bbf-4656-8b74-4780c29815b7": {"__data__": {"id_": "846c5a10-6bbf-4656-8b74-4780c29815b7", "embedding": null, "metadata": {"page_label": "315", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8b79e3c1-a36c-456c-9af4-78184a34c259", "node_type": "4", "metadata": {"page_label": "315", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "7013244c037acd5191cacd0f917bc878ba5b84e59622780a247db4254cee813e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n11.3 Alternative definitions of geodesically convex sets* 315\nThis permissive definition still allows us to establish most optimization re-\nsults we may desire, but it does have some undesirable effects. For example, the\nintersection of two geodesically convex sets may fail to be geodesically convex\n(notwithstanding Proposition 11.8). Indeed, let M = {x \u2208 R2 : x2\n1 + x2\n2 = 1} be\nthe unit circle as a Riemannian submanifold of R2, and consider S1 = {x \u2208 M:\nx1 \u2265 0} and S2 = {x \u2208 M: x1 \u2264 0}. Clearly, S1 and S2 are geodesically convex\nbut their intersection S1 \u2229 S2 = {(0, 1), (0, \u22121)} is not.\nA common way to restrict Definition 11.2 is to require all geodesic segments\nconnecting points in S to stay in S. Udri\u00b8 ste [Udr94, Def. 1.3] and Sakai [Sak96,\nDef. IV.5.1] call this total convexity, up to the following minor points: Udri\u00b8 ste\ntacitly requires M to be complete (instead, we here require existence of at least\none geodesic segment connecting each pair of points in S), and Sakai requires S\nto be non-empty (we do not).\nDefinition 11.16. A subset S of a Riemannian manifold M is geodesically\ntotally convex if, for every x, y\u2208 S, there is at least one geodesic segment\nc: [0, 1] \u2192 Msuch that c(0) = x and c(1) = y, and, for all such segments,\nc(t) is in S for all t \u2208 [0, 1].\nAnother way to restrict Definition 11.2 is to require each pair of points in\nS to be connected by a unique minimizing geodesic segment, and for that seg-\nment to stay in S. (Recall Theorem 10.4 for minimizing geodesics.) Lee calls\nsuch sets geodesically convex [Lee18, p166], whereas Sakai calls them strongly\nconvex [Sak96, Def. IV.5.1]. We use the latter name.\nDefinition 11.17. A subset S of a Riemannian manifold M is geodesically\nstrongly convex if, for every x, y\u2208 S, there exists a unique minimizing geodesic\nsegment c: [0, 1] \u2192 Msuch that c(0) = x and c(1) = y; and c(t) is in S for all\nt \u2208 [0, 1].\nMore verbosely, Definition 11.17 requires the following: given x, y\u2208 S ar-\nbitrary, consider all geodesic segments c: [0, 1] \u2192 Mconnecting c(0) = x to\nc(1) = y in M; we must have that exactly one of those segments is minimizing,\nand moreover that this minimizing geodesic segment lies entirely in S.\nIf a set S is geodesically totally convex or geodesically strongly convex, then\nit is also geodesically convex. It is an exercise to show that (a) neither converse\nis true, and (b) total convexity does not imply strong convexity, nor the other\nway around.\nNotwithstanding, for special manifolds all three notions of convexity are equiv-\nalent. The following result applies to Cartan\u2013Hadamard manifolds.\nTheorem 11.18. Assume M is a complete Riemannian manifold such that\neach pair of points x, y\u2208 Mis connected by a unique geodesic segment. Then,\nthe notions of geodesic convexity, geodesic total convexity and geodesic strong\nconvexity are equivalent and can be stated as: S \u2286 Mis geodesically convex if\nfor all x, yin S the geodesic segment connecting them stays in S.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3222, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d05e87ea-7e8d-48d1-886a-bad0a45e158c": {"__data__": {"id_": "d05e87ea-7e8d-48d1-886a-bad0a45e158c", "embedding": null, "metadata": {"page_label": "316", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "22385d06-cfcf-4cf8-a4ad-c8782f270e7c", "node_type": "4", "metadata": {"page_label": "316", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "6bace5e2344942cc111ab1553d44a7791d86cf2ab2cb87b7d8b9363a3c55aecd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n316 Geodesic convexity\nProof. By assumption, M is connected and complete. Theorem 10.9 provides\nthat each pair of points x, yis connected by a minimizing geodesic segment. Still\nby assumption, no other geodesic segment connects x and y. Thus, the geodesic\nsegment is minimizing.\nFor each point x on a Riemannian manifold, there exists a positive r0 > 0 such\nthat every geodesic ball of radius r \u2264 r0 centered at x is geodesically strongly\nconvex [Lee18, Thm. 6.17].\nThe notion of geodesically convex function f : S \u2192 R as in Definition 11.3\nextends verbatim with S a geodesically totally or strongly convex set, and we\nstill call these functions geodesically convex.\nExercise 11.19. Let M be the unit sphere as a Riemannian submanifold of Rn.\nShow that if S \u2286 Mis geodesically totally convex, then either S = \u2205 or S = M.\nFurther argue that the spherical cap S\u03b1 = {x \u2208 M: x1 \u2265 \u03b1} is\n1. geodesically convex for all \u03b1 \u2208 R,\n2. geodesically totally convex if and only if \u03b1 /\u2208 (\u22121, 1], and\n3. geodesically strongly convex if and only if \u03b1 >0.\nDeduce that, while geodesic total convexity and geodesic strong convexity both\nimply geodesic convexity, no other implications hold among these three notions\nin general.\nExercise 11.20. Show that (unlike geodesic convexity) the properties of geodesic\ntotal convexity and geodesic strong convexity are closed under intersection.\n11.4 Differentiable geodesically convex functions\nFor functions which have a gradient or Hessian, geodesic convexity can be char-\nacterized in practical ways through inequalities involving derivatives at a base\npoint. (Recall Remark 8.6 defining maps which are k times differentiable, as\nopposed to smooth.)\nWe start with a statement using gradients. On a technical note, recall from\nSection 10.2 that a geodesic segment c: [0, 1] \u2192 Madmits a unique extension\nto a maximally large open interval containing [0 , 1]: this is how we make sense\nof c\u2032(0) and c\u2032(1).\nTheorem 11.21. Let S be a geodesically convex set on a Riemannian manifold\nM and let f : M \u2192R be differentiable in a neighborhood ofS. Then, f|S : S \u2192 R\nis geodesically convex if and only if for all geodesic segments c: [0, 1] \u2192 M\ncontained in S we have (letting x = c(0)):\n\u2200t \u2208 [0, 1], f (c(t)) \u2265 f(x) + t \u27e8gradf(x), c\u2032(0)\u27e9x . (11.6)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2518, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8786e5bc-66a4-4a93-a884-94534e1936fc": {"__data__": {"id_": "8786e5bc-66a4-4a93-a884-94534e1936fc", "embedding": null, "metadata": {"page_label": "317", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb6a9b07-3ea9-4e54-9ca6-374759e2ab12", "node_type": "4", "metadata": {"page_label": "317", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "5739719edf92e20784ece8b634686a7c80841e3f11b1c2e9c93effdaf4335a41", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n11.4 Differentiable geodesically convex functions 317\nMoreover, f|S is geodesically \u00b5-strongly convex for some \u00b5 >0 if and only if\n\u2200t \u2208 [0, 1], f (c(t)) \u2265 f(x) + t \u27e8gradf(x), c\u2032(0)\u27e9x + t2 \u00b5\n2 L(c)2. (11.7)\nFinally, f|S is geodesically strictly convex if and only if, whenever c\u2032(0) \u0338= 0,\n\u2200t \u2208 (0, 1], f (c(t)) > f(x) + t \u27e8gradf(x), c\u2032(0)\u27e9x . (11.8)\nProof. By definition, f|S is geodesically (strictly) convex if and only if, for all\nx, y\u2208 S and all geodesics c connecting x to y in S, the composition f \u25e6 c is\n(strictly) convex from [0 , 1] to R. By extending the domain of c somewhat, we\nsee that f \u25e6c is differentiable on an open interval which contains [0, 1]: this allows\nus to call upon Lemma 11.1.\nFirst, f \u25e6 c is convex if and only if for all s, t\u2208 [0, 1]:\nf(c(t)) \u2265 f(c(s)) + (t \u2212 s)(f \u25e6 c)\u2032(s).\nSince f is differentiable in a neighborhood of S, we have\n(f \u25e6 c)\u2032(s) = Df(c(s))[c\u2032(s)] = \u27e8gradf(c(s)), c\u2032(s)\u27e9c(s) .\nCombine and set s = 0 to conclude that if f|S is geodesically convex then the\ninequalities (11.6) hold. The other way around, if the inequalities (11.6) hold,\nthen (by reparameterization of c) we conclude that f \u25e6 c is convex for all c as\nprescribed, hence f|S is geodesically convex. The proof for strong convexity is\nsimilar.\nSecond, assuming c\u2032(0) \u0338= 0, we have that f \u25e6 c is strictly convex if and only if\nfor all s, tdistinct in [0, 1]:\nf(c(t)) > f(c(s)) + (t \u2212 s)(f \u25e6 c)\u2032(s).\nAgain, using differentiability of f and setting s = 0, it follows that f \u25e6 c is\nstrictly convex if and only if inequality (11.8) holds. Conclude similarly to the\nfirst part.\nIn Section 11.5, we use the inequalities provided by geodesic strong convexity\ntogether with inequalities that hold if the gradient of f is Lipschitz continuous\nto analyze Riemannian gradient descent.\nThe following corollary is of particular importance to optimization. Note that\nwe need the geodesically convex domain to be open. Indeed, it is possible for a\nglobal minimizer to have nonzero gradient if it lies on the boundary of S. (See\nalso Exercise 11.26.)\nCorollary 11.22. If f is differentiable and geodesically convex on an open geo-\ndesically convex set, then x is a global minimizer of f if and only if gradf(x) = 0.\nProof. If grad f(x) = 0, then Theorem 11.21 shows f(x) \u2264 f(y) for all y in\nthe domain of f (this does not require the domain to be open). The other way\naround, since the domain of f is open, it is in particular an open submanifold of\nM and we can apply Proposition 4.5 to conclude that if x is a global minimizer,\nthen gradf(x) = 0.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2794, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "93ce2744-83ac-47f5-aecf-18ea79d02ef8": {"__data__": {"id_": "93ce2744-83ac-47f5-aecf-18ea79d02ef8", "embedding": null, "metadata": {"page_label": "318", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b88b751-f134-4b4f-86a5-f267d9404d34", "node_type": "4", "metadata": {"page_label": "318", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "d07b29f4adc777938727e680939f6be5d2d7ad7e44606bf12372bd1dc754c82e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n318 Geodesic convexity\nThe next theorem provides a characterization of convexity based on second-\norder derivatives, also with the requirement that the domain be open.\nTheorem 11.23. Let f : S \u2192 R be twice differentiable on an open geodesically\nconvex set S. The function f is\n1. Geodesically convex if and only if Hessf(x) \u2ab0 0;\n2. Geodesically \u00b5-strongly convex if and only if Hessf(x) \u2ab0 \u00b5 Id;\n3. Geodesically strictly convex if (but not only if) Hessf(x) \u227b 0,\nall understood to hold for all x \u2208 S.\nProof. Similarly to the proof of Theorem 11.21, we start with the fact that f\nis geodesically convex if and only if f \u25e6 c: [0, 1] \u2192 R is convex for all geodesic\nsegments c: [0, 1] \u2192 Mwhose image lies in S. Calling upon Lemma 11.1, we\nfind that this is the case if and only if, for all such geodesics, it holds that\n\u2200t \u2208 (0, 1), (f \u25e6 c)\u2032\u2032(t) \u2265 0.\nSince f is twice differentiable everywhere in S, we get that\n(f \u25e6 c)\u2032\u2032(t) = d\ndt\u27e8gradf(c(t)), c\u2032(t)\u27e9c(t) = \u27e8Hessf(c(t))[c\u2032(t)], c\u2032(t)\u27e9c(t) ,\nwhere we also used that c\u2032\u2032(t) = 0 since c is a geodesic.\nIf Hessf(x) is positive semidefinite for all x in S, then ( f \u25e6 c)\u2032\u2032(t) \u2265 0 for all\nc as prescribed and t \u2208 (0, 1), so that f is geodesically convex. The other way\naround, if f is geodesically convex, it follows that\n\u27e8Hessf(c(0))[c\u2032(0)], c\u2032(0)\u27e9c(0) \u2265 0\nfor all admissible c (where we particularized to t = 0). For all x \u2208 S and suffi-\nciently small v \u2208 TxM, the geodesic c with c(0) = x and c\u2032(0) = v remains in\nS for t \u2208 [0, 1] since S is open in M. Thus, for all such x and v, we deduce that\n\u27e8Hessf(x)[v], v\u27e9x \u2265 0, which confirms Hessf(x) is positive semidefinite, and this\nholds at all points x \u2208 S.\nThe same proof applies for strong convexity, either using or showing that\n(f \u25e6 c)\u2032\u2032(t) \u2265 \u00b5L(c)2 for all admissible c and t \u2208 [0, 1], and recalling that L(c) =\n\u2225c\u2032(t)\u2225c(t) since c is a geodesic defined over [0 , 1].\nIf Hess f(x) is positive definite at all x \u2208 S, then ( f \u25e6 c)\u2032\u2032(t) > 0 whenever\nc\u2032(0) \u0338= 0, which confirms f is geodesically strictly convex. The converse is not\ntrue because it also does not hold in the Euclidean case: consider f(x) = x4 on\nS = (\u22121, 1) \u2282 R.\nExample 11.24. Let M be a compact Riemannian manifold. If f : M \u2192R has\npositive semidefinite Hessian at all points, then f is geodesically convex on each\nconnected component of M by Theorem 11.23. Since M is compact, it follows\nfrom Corollary 11.10 that f is constant on each connected component. Therefore,\nthe Hessian of f is in fact zero everywhere.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2732, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3c5fff63-83cd-45b2-ae3f-e500e8d914d0": {"__data__": {"id_": "3c5fff63-83cd-45b2-ae3f-e500e8d914d0", "embedding": null, "metadata": {"page_label": "319", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6fac8246-f819-492c-8c3c-13db2df74f81", "node_type": "4", "metadata": {"page_label": "319", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "b168121be28079ca5fb90f836446761c0b9ef5530b2ea0924d7549ed96c761cf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n11.5 Geodesic strong convexity and Lipschitz continuous gradients 319\nExample 11.25. Given a differentiable function f : M \u2192R on a manifold M,\nit is natural to wonder whether there exists a Riemannian metric for M such\nthat f is geodesically convex. If such a metric exists, then (a) the domain of f\nis geodesically convex with that metric, and (b) the critical points of f are its\nglobal minimizers (by Corollary 11.22 since M is open regardless of the metric).\nSince the notions of criticality (Definition 4.4) and global optimality are inde-\npendent of the Riemannian metric, it follows that no Riemannian metric makes\nf geodesically convex if f has a suboptimal critical point. A similar reasoning\nholds if f (not necessarily differentiable) has a suboptimal local minimizer (by\nTheorem 11.6) [Vis18]. Of course, it may still be possible to choose a metric such\nthat f is geodesically convex on a submanifold of M.\nExercise 11.26. Let S be a geodesically convex set in M, not necessarily open.\nDefine the cone of feasible directions Kx of S at x to be the set of vectors c\u2032(0)\nfor all possible geodesic segments c in S satisfying c(0) = x. Let f : M \u2192R be\ndifferentiable in a neighborhood of S and geodesically convex on S. Show that\nx\u22c6 \u2208 S is a global minimizer of f|S if and only if\n\u27e8gradf(x\u22c6), v\u27e9x\u22c6 \u2265 0\nfor all v in Kx\u22c6. (The closure of Kx is called the tangent cone to S at x.)\n11.5 Geodesic strong convexity and Lipschitz continuous gradients\nRecall Definition 10.16 for the exponential map Exp: T M \u2192 M. If S is geodesi-\ncally convex and x, yare two points in S, then there exists a geodesic segment\nin S connecting x to y. In terms of Exp, this can be stated as: there exists a\ntangent vector v \u2208 TxM such that the curve c(t) = Expx(tv) stays in S for all\nt \u2208 [0, 1] with c(0) = x and c(1) = y. Notice that the length of that geodesic\nsegment satisfies dist(x, y) \u2264 L(c) = \u2225v\u2225x (Section 10.1).\nThus, Theorem 11.21 provides that, if f : M \u2192R is differentiable in a neigh-\nborhood of S and if f|S is geodesically convex, then given x \u2208 S and v \u2208 TxM\nsuch that c(t) = Expx(tv) is in S for all t \u2208 [0, 1], we have\n\u2200t \u2208 [0, 1], f (Expx(tv)) \u2265 f(x) + t \u27e8gradf(x), v\u27e9x , (11.9)\nand the inequality is strict for t \u2208 (0, 1] if f|S is geodesically strictly convex.\nFurthermore, if f|S is geodesically \u00b5-strongly convex, then\n\u2200t \u2208 [0, 1], f (Expx(tv)) \u2265 f(x) + t \u27e8gradf(x), v\u27e9x + t2 \u00b5\n2 \u2225v\u22252\nx. (11.10)\nThese convenient inequalities should be compared with the corresponding one\nwe have if the gradient of f is L-Lipschitz continuous (Proposition 10.53):\n\u2200t \u2208 [0, 1], f (Expx(tv)) \u2264 f(x) + t \u27e8gradf(x), v\u27e9x + t2 L\n2 \u2225v\u22252\nx. (11.11)\nWhen both of the latter inequalities hold, we can obtain strong guarantees for", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2968, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5bd4f037-2568-440b-a903-bb44557f612b": {"__data__": {"id_": "5bd4f037-2568-440b-a903-bb44557f612b", "embedding": null, "metadata": {"page_label": "320", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0936e118-139e-4968-92aa-c2b13fdf6a56", "node_type": "4", "metadata": {"page_label": "320", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "33cf5955b9edd104efbd3f9b045670e3d176064c42836ebb9538acb6398966ed", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n320 Geodesic convexity\noptimization algorithms. To illustrate this, we first work through a couple of\nfacts about geodesically strongly convex functions. For starters, strong convexity\nensures existence and uniqueness of a minimizer. (Recall Theorem 10.8 regarding\ncomplete manifolds.)\nLemma 11.27. Let S be a non-empty, closed and geodesically convex set in a\ncomplete manifold M. Assume f : M \u2192R is differentiable in a neighborhood of\nS. If f|S is geodesically \u00b5-strongly convex with \u00b5 >0, then the sublevel sets of\nf|S are compact and f|S has exactly one global minimizer.\nProof. Let x0 \u2208 S be arbitrary. We first argue that the sublevel set S0 = {x \u2208\nS : f(x) \u2264 f(x0)} is compact. Since f is continuous around S and S is closed,\nS0 is closed. Since M is complete, it remains to show that S0 is bounded in M.\nFor contradiction, assume that this is not the case. Then, there exists a sequence\nx1, x2, x3, . . .in S0 such that limk\u2192\u221e dist(x0, xk) = \u221e. Each xk is in S and S is\ngeodesically convex hence there exists vk \u2208 Tx0 M such that c(t) = Expx0 (tvk)\nremains in S for t \u2208 [0, 1] and c(1) = xk. Then, we have by (11.10) that\nf(xk) \u2265 f(x0) + \u27e8gradf(x0), vk\u27e9x0 + \u00b5\n2 \u2225vk\u22252\nx0 .\nSince dist(x0, xk) goes to infinity and dist(x0, xk) \u2264 L(c) = \u2225vk\u2225x0 , we have that\n\u2225vk\u2225x0 goes to infinity. Moreover, for all k,\nf(xk) \u2265 f(x0) \u2212 \u2225gradf(x0)\u2225x0 \u2225vk\u2225x0 + \u00b5\n2 \u2225vk\u22252\nx0 .\nThe right-hand side goes to infinity with k \u2192 \u221e, hence so does f(xk). This is\nincompatible with f(xk) \u2264 f(x0) for all k, hence S0 is compact.\nSince f|S0 is continuous, it attains its minimum at some point x\u22c6 in S0. Thus,\nfor all x \u2208 S, we either have x /\u2208 S0 in which case f(x) > f(x0) \u2265 f(x\u22c6), or we\nhave x \u2208 S0 in which case f(x) \u2265 f(x\u22c6). Therefore, x\u22c6 is also a minimizer for\nf|S.\nSince geodesic strong convexity implies geodesic strict convexity, it follows\nfrom Theorem 11.7 that x\u22c6 is the only minimizer of f|S.\nIn the same setting as the previous lemma, we find that the norm of the\ngradient of a geodesically strongly convex function at some point x provides\ncrisp information about the optimality gap at x.\nLemma 11.28. Let S be a non-empty, closed and geodesically convex set in a\ncomplete manifold M. Assume f : M \u2192R is differentiable in a neighborhood of\nS. If f|S is geodesically \u00b5-strongly convex with \u00b5 >0, then it satisfies a Polyak\u2013\n Lojasiewicz inequality:\n\u2200x \u2208 S, f (x) \u2212 f(x\u22c6) \u2264 1\n2\u00b5\u2225gradf(x)\u22252\nx, (11.12)\nwhere x\u22c6 is the minimizer of f|S.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2682, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d76d16f4-a7f7-4abe-b1cd-fdba9a4ab019": {"__data__": {"id_": "d76d16f4-a7f7-4abe-b1cd-fdba9a4ab019", "embedding": null, "metadata": {"page_label": "321", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36440607-c683-4dff-9733-34b32fde5961", "node_type": "4", "metadata": {"page_label": "321", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "65f60da710e153b56c4686928213ad67026acd53387d6066e00f711e90ee8d15", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n11.5 Geodesic strong convexity and Lipschitz continuous gradients 321\nProof. The minimizer x\u22c6 of f|S exists and is unique by Lemma 11.27. Fix x \u2208 S\narbitrary. Both x and x\u22c6 are in S which is geodesically convex, hence there exists\nvx \u2208 TxM such that x\u22c6 = Expx(vx) and t 7\u2192 Expx(tvx) remains in S for all\nt \u2208 [0, 1]. Therefore, geodesic \u00b5-strong convexity (11.10) provides:\nf(x\u22c6) = f(Expx(vx)) \u2265 f(x) + \u27e8gradf(x), vx\u27e9x + \u00b5\n2 \u2225vx\u22252\nx\n\u2265 inf\nv\u2208TxM\nf(x) + \u27e8gradf(x), v\u27e9x + \u00b5\n2 \u2225v\u22252\nx\n= f(x) \u2212 1\n2\u00b5\u2225gradf(x)\u22252\nx,\nwhere the infimum is attained by v = \u22121\n\u00b5 gradf(x) (the critical point of the\nquadratic in v). Rearrange to conclude.\nThe two last results provide sufficient context to study a simple version of\nRiemannian gradient descent applied to a function which is geodesically strongly\nconvex and has a Lipschitz continuous gradient. See Section 11.8 for further\nreferences.\nTheorem 11.29. Let f : M \u2192R be differentiable and geodesically convex on a\ncomplete manifold M. Given x0 \u2208 M, consider the sublevel set S0 = {x \u2208 M:\nf(x) \u2264 f(x0)}. Assume f has L-Lipschitz continuous gradient on a neighborhood\nof S0 and f|S0 is geodesically \u00b5-strongly convex with \u00b5 >0. Consider gradient\ndescent with exponential retraction and constant step-size 1/L initialized at x0,\nnamely,\nxk+1 = Expxk\n\u0012\n\u2212 1\nLgradf(xk)\n\u0013\n, k = 0, 1, 2, . . .\nThe function f has a unique minimizer x\u22c6 and the iterates converge to it at least\nlinearly. More precisely, with \u03ba = L/\u00b5 \u2265 1 (the condition number of f|S0 ), the\nwhole sequence stays in S0 and we have\nf(xk) \u2212 f(x\u22c6) \u2264\n\u0012\n1 \u2212 1\n\u03ba\n\u0013k\n(f(x0) \u2212 f(x\u22c6)) and (11.13)\ndist(xk, x\u22c6) \u2264\nr\n1 \u2212 1\n\u03ba\nk\n\u221a\u03ba dist(x0, x\u22c6) (11.14)\nfor all k \u2265 0. (Note that\nq\n1 \u2212 1\n\u03ba \u2264 1 \u2212 1\n2\u03ba .)\nProof. By construction, S0 is non-empty. It is also closed since f is continuous,\nand geodesically convex since f is geodesically convex. Thus, Lemma 11.27 pro-\nvides that f|S0 has a unique minimizer x\u22c6 \u2208 S0, and it is clear that x\u22c6 is also\nthe unique minimizer of f on M since x /\u2208 S0 =\u21d2 f(x) > f(x0) \u2265 f(x\u22c6).\nNext, we argue by induction that allxk are in S0. Of course,x0 is in S0. Assume\nxk is in S0. We know grad f is L-Lipschitz continuous on a neighborhood U of\nS0. Consider the curve c(t) = Expxk (\u2212tgradf(xk)). Notice that c(0) is in U. Let\nI denote the largest interval around t = 0 such that c(t) is in U for all t \u2208 I.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2566, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5157dd00-551c-40c6-94c8-d6ae20ed1faa": {"__data__": {"id_": "5157dd00-551c-40c6-94c8-d6ae20ed1faa", "embedding": null, "metadata": {"page_label": "322", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0395d880-e48b-4cbd-9c60-b3423d610b38", "node_type": "4", "metadata": {"page_label": "322", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "9f181d2aad7db39756496c53728785c3e10bc458627030fca50c0905cbe01963", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n322 Geodesic convexity\nThis interval is open since c\u22121(U) is open. Then, L-Lipschitz continuity of the\ngradient provides:\n\u2200t \u2208 I, f (c(t)) \u2264 f(xk) \u2212 t\n\u0012\n1 \u2212 tL\n2\n\u0013\n\u2225gradf(xk)\u22252\nxk .\nWe want to show that I contains [0 , 2/L). To this end, let \u00aft = sup I be the\nfirst (positive) time such that c(\u00aft) leaves U. If \u00aft is infinite, we have nothing\nto do. Assume \u00aft is finite. The above inequality holds for all 0 \u2264 t < \u00aft. By\ncontinuity, it must also hold for t = \u00aft. For contradiction, assume \u00aft <2/L. Then,\nf(c(\u00aft)) \u2264 f(xk) \u2264 f(x0) because \u00aft\n\u0000\n1 \u2212 \u00aftL\n2\n\u0001\n> 0 and xk \u2208 S0. This implies that\nc(\u00aft) is in S0, a contradiction. Hence,I contains the interval [0, 2/L). In particular,\nit contains 1/L. Since xk+1 = c(1/L), we deduce that\nf(xk+1) \u2264 f(xk) \u2212 1\n2L\u2225gradf(xk)\u22252\nxk , (11.15)\nconfirming that the whole sequence remains in S0.\nSubtract f(x\u22c6) on both sides of (11.15) to find\nf(xk+1) \u2212 f(x\u22c6) \u2264 f(xk) \u2212 f(x\u22c6) \u2212 1\n2L\u2225gradf(xk)\u22252\nxk (11.16)\nfor all k. Lemma 11.28 bounds the gradient norm at xk \u2208 S0 as:\n\u2225gradf(xk)\u22252\nxk \u2265 2\u00b5(f(xk) \u2212 f(x\u22c6)). (11.17)\nCombining the latter two inequalities, it follows that\nf(xk+1) \u2212 f(x\u22c6) \u2264\n\u0010\n1 \u2212 \u00b5\nL\n\u0011\n(f(xk) \u2212 f(x\u22c6)) (11.18)\nfor all k. It is clear when comparing (11.10) and (11.11) that L \u2265 \u00b5, hence\n\u03ba = L\n\u00b5 \u2265 1 and we can conclude for the sequence ( f(xk))k=0,1,2....\nSince x\u22c6 and each xk are in S0 which is geodesically convex, there exists vk \u2208\nTx\u22c6M such that the curve c(t) = Exp x\u22c6(tvk) connects c(0) = x\u22c6 to c(1) = xk\nwhile remaining in S0 for all t \u2208 [0, 1]. Then, geodesic strong convexity (11.10)\nprovides\nf(xk) \u2265 f(x\u22c6) + \u27e8gradf(x\u22c6), vk\u27e9x\u22c6 + \u00b5\n2 \u2225vk\u22252\nx\u22c6.\nSince x\u22c6 is the minimizer of f on M, we know that grad f(x\u22c6) = 0. Moreover,\ndist(xk, x\u22c6) \u2264 L(c) = \u2225vk\u2225x\u22c6. Thus,\ndist(xk, x\u22c6)2 \u2264 2\n\u00b5(f(xk) \u2212 f(x\u22c6)) (11.19)\nfor all k. Combine with the bound on f(xk) \u2212 f(x\u22c6) to deduce:\ndist(xk, x\u22c6) \u2264\ns\n2(f(x0) \u2212 f(x\u22c6))\n\u00b5\nr\n1 \u2212 1\n\u03ba\nk\n. (11.20)\nNow consider x0 and x\u22c6. They are in the same connected component of M since\nall iterates xk are in the same connected component (any two consecutive iterates", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2286, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c7701a9c-f8f4-4606-9bd1-267259ec85aa": {"__data__": {"id_": "c7701a9c-f8f4-4606-9bd1-267259ec85aa", "embedding": null, "metadata": {"page_label": "323", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "87547b46-efa1-4f0c-b5bf-b4d3e868f54b", "node_type": "4", "metadata": {"page_label": "323", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "b72524c8f274376bf7cce0715cb8ec9449fa1f4eb0c217a1566621e8d956bbe1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n11.6 Example: positive reals and geometric programming 323\nare connected by a geodesic segment) and the sequence converges to x\u22c6. Hence,\nx0 and x\u22c6 are connected by a minimizing geodesic \u03b3 of M such that \u03b3(0) = x\u22c6,\n\u03b3(1) = x0 and L(\u03b3) = dist( x0, x\u22c6) since M is complete (Theorem 10.9). It is\neasy to see that \u03b3(t) is in S0 for all t \u2208 [0, 1] because f is geodesically convex on\nall of M hence\n\u2200t \u2208 [0, 1], f (\u03b3(t)) \u2264 (1 \u2212 t)f(\u03b3(0)) + tf(\u03b3(1))\n= f(x0) \u2212 (1 \u2212 t)(f(x0) \u2212 f(x\u22c6)) \u2264 f(x0).\nTherefore, Lipschitz continuity of the gradient (11.11) provides:\nf(x0) \u2264 f(x\u22c6) + L\n2 dist(x0, x\u22c6)2, (11.21)\nwhere we used gradf(x\u22c6) = 0. Plug this into (11.20) to conclude.\n11.6 Example: positive reals and geometric programming\nAs usual, let Rn denote the Euclidean space with metric \u27e8u, v\u27e9 = u\u22a4v. The\npositive orthant\nRn\n+ = {x \u2208 Rn : x1, . . . , xn > 0} (11.22)\nis a convex subset of Rn, in the usual sense. Being an open set, it is also an open\nsubmanifold of Rn. Its tangent spaces are all identified with Rn.\nWe can make Rn\n+ into a Riemannian submanifold of Rn using the Euclidean\nmetric. Geodesic convexity on that manifold is equivalent to convexity in the\nusual sense: this is not particularly interesting. Furthermore, this manifold is\nnot complete\u2014its geodesics are the straight lines of Rn: they cease to exist when\nthey leave Rn\n+.\nWe can endow Rn\n+ with a different Riemannian metric so as to make it com-\nplete. This leads to a different notion of geodesic convexity on Rn\n+. The key is to\nestablish a diffeomorphism between Rn and Rn\n+, and to pullback the Riemannian\ngeometry of Rn to Rn\n+ through that diffeomorphism.\nTo this end, consider the map \u03c6: Rn\n+ \u2192 Rn:\n\u03c6(x) = log(x) = (log(x1), . . . ,log(xn))\u22a4. (11.23)\nThis is a diffeomorphism between the manifolds Rn\n+ and Rn because it is smooth\nand its inverse \u03c6\u22121 : Rn \u2192 Rn\n+ is smooth too:\n\u03c6\u22121(y) = exp(y) = (ey1 , . . . , eyn)\u22a4. (11.24)\nNote also the following expressions for the differential of \u03c6 at x \u2208 Rn\n+ and its\ninverse (both are maps from Rn to Rn):\nD\u03c6(x)[u] =\n\u0012u1\nx1\n, . . . ,un\nxn\n\u0013\u22a4\n, (D\u03c6(x))\u22121 [z] = (x1z1, . . . , xnzn)\u22a4.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2347, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ec04ceea-3dbb-4b91-a03b-ad04e45b9103": {"__data__": {"id_": "ec04ceea-3dbb-4b91-a03b-ad04e45b9103", "embedding": null, "metadata": {"page_label": "324", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d3248e59-364f-453b-93c4-3810ea964e4e", "node_type": "4", "metadata": {"page_label": "324", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "a19c06e08d8e9944ea6c1384e84ab687edeadafff2bccddd0078f3f87ac40404", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n324 Geodesic convexity\nThey will come in handy.\nEquipped with this diffeomorphism, we can define a Riemannian metric \u27e8\u00b7, \u00b7\u27e9+\non Rn\n+ as follows: D\u03c6(x) is an invertible linear map from TxRn\n+ to T\u03c6(x)Rn, and\nwe define the inner product on T xRn\n+ so as to make this map an isometry, that\nis,5\n\u27e8u, v\u27e9+\nx \u225c \u27e8D\u03c6(x)[u], D\u03c6(x)[v]\u27e9 =\nnX\ni=1\nuivi\nx2\ni\n. (11.25)\n(Notice how the metric at x is given by the Euclidean Hessian of the log-barrier\nfunction x 7\u2192 \u2212Pn\ni=1 log(xi).)\nSince Rn\n+ now has two distinct Riemannian geometries, we let\nM = (Rn\n+, \u27e8\u00b7, \u00b7\u27e9+) (11.26)\ndenote the Riemannian manifold obtained with the pullback metric, to avoid\nambiguity. This is implemented in Manopt as positivefactory.\nIt is an exercise to show that the geodesics of M are exactly the images of\ngeodesics of Rn through \u03c6\u22121, that is, all geodesics of M are of the form\nc(t) = \u03c6\u22121(y + tz) = exp(y + tz) =\n\u0000\ney1+tz1 , . . . , eyn+tzn\n\u0001\n, (11.27)\nfor some y, z\u2208 Rn. These are defined for all t, hence M is complete. (Intuitively,\nas we near the missing boundary of Rn\n+, that is, as some xi nears zero, the\nmetric\u2019s 1/x2\ni scaling distorts lengths, making the boundary seem infinitely far\naway.) Moreover, for any two points x, x\u2032 \u2208 M, there exists a unique geodesic\nc: [0, 1] \u2192 M(necessarily minimizing) connecting them:\nc(t) = exp(log(x) + t(log(x\u2032) \u2212 log(x))). (11.28)\nWe are now in a good position to study geodesic convexity on M.\nProposition 11.30. A set S \u2286 Rn\n+ is geodesically convex on M if and only if\nC = log(S) is convex in Rn.\nProof. Assume S is geodesically convex. For any two points y, y\u2032 \u2208 C, let x =\n\u03c6\u22121(y) and x\u2032 = \u03c6\u22121(y\u2032) be the corresponding points inS. Since S is geodesically\nconvex, the geodesic (11.28) is included in S for t \u2208 [0, 1]. Hence, C contains\n\u03c6(c(t)) = log(x) +t(log(x\u2032) \u2212 log(x)) = y + t(y\u2032 \u2212 y) for t \u2208 [0, 1]: this is the line\nsegment connecting y to y\u2032, hence C is convex. The proof is similar in the other\ndirection.\nProposition 11.31. Let S be geodesically convex on M. Then, f : S \u2192 R is\ngeodesically (strictly) convex on M if and only if the function\ng : log( S) \u2192 R: y 7\u2192 g(y) = f(exp(y))\nis (strictly) convex in Rn.\n5 Compare this with the metric we imposed on the relative interior of the simplex in Exer-\ncise 3.65, namely, \u27e8u, v\u27e9x = Pn\ni=1\nuivi\nxi\n. That one is a pullback from the usual metric on the\npositive orthant of the unit sphere (up to scaling); it is not complete.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2652, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "641f8d16-fda8-4f97-a374-3f386be13462": {"__data__": {"id_": "641f8d16-fda8-4f97-a374-3f386be13462", "embedding": null, "metadata": {"page_label": "325", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2137841b-687f-44b0-900a-9f46763c4e98", "node_type": "4", "metadata": {"page_label": "325", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "10705889e2b00ee81b12992b7f129c95d61c390874a0b6c254ed8b22c79aeec6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n11.6 Example: positive reals and geometric programming 325\nProof. By definition, f is geodesically convex if and only if for all x, x\u2032 \u2208 S and\nt \u2208 [0, 1] it holds that\nf(c(t)) \u2264 (1 \u2212 t)f(x) + tf(x\u2032) = (1 \u2212 t)g(y) + tg(y\u2032),\nwhere x = exp( y), x\u2032 = exp( y\u2032), and c(t) is the geodesic uniquely specified\nby (11.28). Conclude with the observation that\nf(c(t)) = f(exp(log(x) + t(log(x\u2032) \u2212 log(x)))) = g((1 \u2212 t)y + ty\u2032).\n(The argument is the same for geodesic strict convexity.)\nLet us consider an example. The function f : Rn\n+ \u2192 R defined by\nf(x) = xa1\n1 \u00b7 \u00b7\u00b7xan\nn (11.29)\nwith some a \u2208 Rn is (usually) not convex on Rn\n+, but it is geodesically convex on\nM. Indeed, S = Rn\n+ is geodesically convex (since M is connected and complete),\nand\ng(y) = f(exp(y)) = (ey1 )a1\n\u00b7 \u00b7\u00b7(eyn)an\n= ea\u22a4y\nis convex on all of Rn because it is the composition of a linear (hence convex)\nfunction of y with a convex, nondecreasing function (see also Exercise 11.14).\nWith this example, we can identify a whole class of geodesically convex func-\ntions on M, based on the observation that nonnegative linear combinations of\ngeodesically convex functions are geodesically convex (see Exercise 11.13).\nDefinition 11.32. A posynomial is a function f : Rn\n+ \u2192 R of the form\nf(x) =\nKX\nk=1\nckxa1k\n1 \u00b7 \u00b7\u00b7xank\nn ,\nwhere c1, . . . , cK are nonnegative and the exponents aik are arbitrary. All posyn-\nomials are geodesically convex on M. If K = 1, f is called a monomial.\nBy Proposition 11.8, this implies that sets of the form\n{x \u2208 Rn\n+ : f(x) \u2264 \u03b1}\nare geodesically convex in M for any posynomial f and \u03b1 \u2208 R.\nWe can say even more about monomials. Givenf(x) = cxa1\n1 \u00b7 \u00b7\u00b7xan\nn with c >0,\nthe function log f is well defined on Rn\n+. Moreover, log f is geodesically linear on\nM (Definition 11.4). Indeed,\nlog(f(exp(y))) = log(c) + a\u22a4y\nis affine. By Proposition 11.31, this implies both logf and \u2212log f are geodesically\nconvex, as announced. Consequently, sets of the form\n{x \u2208 Rn\n+ : log f(x) = log \u03b2} = {x \u2208 Rn\n+ : f(x) = \u03b2}\nare geodesically convex in M for any monomial f and \u03b2 >0.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2304, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b07f7d64-1fed-4ff5-b063-da2d40b54f0a": {"__data__": {"id_": "b07f7d64-1fed-4ff5-b063-da2d40b54f0a", "embedding": null, "metadata": {"page_label": "326", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "afd89625-11d5-441b-84e9-a7a2e4372c6d", "node_type": "4", "metadata": {"page_label": "326", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "07e1556bfa17968bb1fa2dc859fbd5ee6c7e4f53b29dcb8be0881222b6de6228", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n326 Geodesic convexity\nOverall, we reach the conclusion that problems of the form\nmin\nx\u2208Rn\n+\nf(x) subject to fi(x) \u2264 1, i = 1, . . . , m,\ngj(x) = 1, j = 1, . . . , p,(11.30)\nare geodesically convex whenever f, f1, . . . , fm are posynomials and g1, . . . , gp\nare monomials. These optimization problems are known as geometric programs:\nsee the tutorials by Peterson [Pet76] and by Boyd, Kim, Vandenberghe and Has-\nsibi [BKVH07] for the more standard construction of this class of problems and\na list of applications. This is also discussed under the lens of geodesic convexity\nin [Rap97, Ch. 10].\nBy construction, M and Rn are not only diffeomorphic but also isometric:\nessentially, they are the same Riemannian manifold. Thus, the notion of geodesic\nconvexity on M is not meaningfully different from classical Euclidean convexity\nin Rn (though it is different from classical convexity in Rn\n+). The next section\npresents a more interesting example.\nExercise 11.33. Let M and \u02dcM be two Riemannian manifolds with \u03c6: M \u2192\u02dcM\na diffeomorphism such that D\u03c6(x) is an isometry for all x \u2208 M: \u27e8u, v\u27e9x =\n\u27e8D\u03c6(x)[u], D\u03c6(x)[v]\u27e9\u03c6(x) (that is, \u03c6 is a Riemannian isometry). In the context\nof this section, \u02dcM is the Euclidean space Rn, M is Rn\n+ with the metric (11.25)\nand \u03c6 is given by (11.23).\nLet \u2207 and \u02dc\u2207 denote the Riemannian connections on M and \u02dcM, respectively.\nShow that they are related by\n\u2207uV = D\u03c6(x)\u22121\nh\n\u02dc\u2207D\u03c6(x)[u] \u02dcV\ni\nfor all u \u2208 TxM and V \u2208 X(M) with \u02dcV \u25e6 \u03c6 = D \u03c6 \u25e6 V . From there, deduce\nan expression for D\ndt on M in terms of the covariant derivative \u02dcD\ndt on \u02dcM, and\nconclude that c is a geodesic on M if and only if \u03c6 \u25e6 c is a geodesic on \u02dcM.\nExplicitly, with Exp and \u02dcExp the exponential maps on M and \u02dcM, respectively,\nestablish the formula\nExpx(u) = \u03c6\u22121\n\u0010\n\u02dcExp\u03c6(x)(D\u03c6(x)[u])\n\u0011\nfor all (x, u) \u2208 TM. Use this to verify (11.28) as well as the fact that S \u2286 Mis\ngeodesically convex if and only if \u03c6(S) \u2286 \u02dcM is geodesically convex, and likewise\nfor geodesic convexity of f : S \u2192 R and f \u25e6 \u03c6\u22121 : \u03c6(S) \u2192 R.\n11.7 Example: positive definite matrices\nConsider the set of symmetric, positive definite matrices of size n:\nSym(n)+ = {X \u2208 Sym(n) : X \u227b 0}. (11.31)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2415, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8b8b7998-0be7-402a-9296-4e7075781390": {"__data__": {"id_": "8b8b7998-0be7-402a-9296-4e7075781390", "embedding": null, "metadata": {"page_label": "327", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4387c6f1-280d-46a5-a45b-06b5ecdfd831", "node_type": "4", "metadata": {"page_label": "327", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "4091c84a6675ff5f8e8cdd086b7593a410eb195772e31e6d57eb45fc2dc5acd1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n11.7 Example: positive definite matrices 327\nThis is a convex set in the Euclidean space Sym( n) of symmetric matrices of\nsize n, with the inner product \u27e8U, V\u27e9 = Tr( U\u22a4V ) = Tr( UV ). It is an open\nsubmanifold; its tangent spaces are identified with Sym( n).\nIn analogy with Rn\n+, we aim to endow Sym( n)+ with a Riemannian struc-\nture, ideally one that makes it complete. There are at least two ways of doing\nthis. In both cases, for n = 1 we recover the same Riemannian geometry as we\nconstructed for R1\n+ in the previous section.\nOne way is to construct a diffeomorphism between Sym( n)+ and a complete\nmanifold, just like log provided a diffeomorphism from Rn\n+ to Rn. Here, we can\ndefine \u03c6: Sym(n)+ \u2192 Sym(n) to be the principal matrix logarithm, 6\n\u03c6(X) = log(X). (11.32)\nIts inverse is the matrix exponential \u03c6\u22121(Y ) = exp(Y ). Both are smooth on the\nspecified domains, hence \u03c6 is indeed a diffeomorphism. Based on this observation,\nwe can pullback the Euclidean metric from Sym(n) to Sym(n)+ in order to define\nthe following inner product on T XSym(n)+ = Sym(n):\n\u27e8U, V\u27e9log\nX \u225c \u27e8Dlog(X)[U], Dlog(X)[V ]\u27e9. (11.33)\nThis is theLog-Euclidean metric studied in detail by Arsigny et al. [AFPA07]. For\nthe same reasons as in the previous section, we can easily describe its geodesics\nand geodesic convexity (Exercise 11.33):\n\u2022 The unique (and minimizing) geodesic connecting X, X\u2032 \u2208 Sym(n)+ with\nrespect to the Log-Euclidean metric is\nc(t) = exp(log(X) + t(log(X\u2032) \u2212 log(X))). (11.34)\n\u2022 A set S \u2286 Sym(n)+ is geodesically convex in that metric if and only if log( S)\nis convex in Sym(n).\n\u2022 Given such a geodesically convex set S, a function f : S \u2192 R is geodesically\n(strictly) convex if and only if f \u25e6 exp is (strictly) convex on Sym( n).\nAnother\u2014and by some measures, more common\u2014metric on Sym( n)+ is the\nso-called affine invariant metric. On the tangent space TXSym(n)+, it is defined\nas follows:\n\u27e8U, V\u27e9aff\nX =\nD\nX\u22121/2UX \u22121/2, X\u22121/2V X\u22121/2\nE\n= Tr(X\u22121UX \u22121V ). (11.35)\nThe central expression ensures that the inputs to \u27e8\u00b7, \u00b7\u27e9 are symmetric matrices.\nThe metric at X matches the Hessian of the log-barrier X 7\u2192 \u2212log(det(X)).\nThis is implemented in Manopt as sympositivedefinitefactory.\nThis metric is named after the following property: for allM \u2208 Rn\u00d7n invertible,\nit holds that MXM \u22a4 is positive definite, and:\n\nMUM \u22a4, MV M\u22a4\u000baff\nMXM \u22a4 = \u27e8U, V\u27e9aff\nX . (11.36)\n6 See Section 4.7 for questions related to the computation of matrix functions and their\ndifferentials.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2721, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ea847dca-62b0-41e9-99f7-33075d1042d2": {"__data__": {"id_": "ea847dca-62b0-41e9-99f7-33075d1042d2", "embedding": null, "metadata": {"page_label": "328", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c844f8f7-2cad-4320-959b-0a2c86c1745f", "node_type": "4", "metadata": {"page_label": "328", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "9f957d6ccbcf72404d5891d99ac1236cddefce01c2995c63130c52f475d0e403", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n328 Geodesic convexity\nOne concrete consequence is that if c: [0, 1] \u2192 Sym(n)+ is a smooth curve, then\nthe length of c is equal to the length of the other curve t 7\u2192 Mc(t)M\u22a4 because\ntheir speeds are equal for all t. Likewise, the length of the curve t 7\u2192 c(t)\u22121\nis equal to that of c. One can show that the geodesic such that c(0) = X and\nc\u2032(0) = V is given by [Bha07, Thm. 6.1.6], [Vis18, Ex. 4.9]:\nExpX(tV ) = c(t) = X1/2 exp\n\u0010\ntX\u22121/2V X\u22121/2\n\u0011\nX1/2. (11.37)\nThis is defined for all t, thus the manifold is complete. Moreover, the manifold\nis Cartan\u2013Hadamard which makes it well suited for applications of geodesically\nconvex optimization. In order to ensure c(1) = X\u2032 (another positive definite\nmatrix), set V to be\nLogX(X\u2032) = X1/2 log(X\u22121/2X\u2032X\u22121/2)X1/2. (11.38)\nThis provides the initial velocity at X of the unique geodesic segment connecting\nX and X\u2032. It follows that\ndist(X, X\u2032)2 = \u27e8LogX(X\u2032), LogX(X\u2032)\u27e9aff\nX = \u2225log(X\u22121/2X\u2032X\u22121/2)\u22252\nF, (11.39)\nwhere \u2225\u00b7\u2225F denotes the Frobenius norm. With some care, it is possible to ex-\npress Exp, Log and dist without any matrix square roots, but matrix inverses,\nexponentials and logarithms are still necessary.\nTo solve optimization problems over Sym( n)+ it is helpful to compute gra-\ndients and Hessians. Let \u00aff : Sym(n) \u2192 R be a function over the space of sym-\nmetric matrices with the usual metric from Rn\u00d7n. Assume f is smooth on the\nopen set Sym( n)+. Further let f = \u00aff|Sym(n)+ formally denote the restriction of\n\u00aff to the manifold of positive definite matrices equipped with the affine invari-\nant metric. The gradients and Hessians of f and \u00aff are related as follows for all\n(X, V) \u2208 TSym(n)+:\ngradf(X) = Xgrad \u00aff(X)X, (11.40)\nHessf(X)[V ] = XHess \u00aff(X)[V ]X + V grad \u00aff(X)X + Xgrad \u00aff(X)V\n2 .\nIf \u00aff is defined over all of Rn\u00d7n, then it is necessary to replace grad \u00aff(X) and\nHess \u00aff(X)[V ] by their symmetric parts. These formulas are derived from [SH15,\n\u00a7 3] where expressions also appear for the Riemannian connection and parallel\ntransport on Sym(n)+.\nExample 11.34. Let \u00aff : Sym(n) \u2192 R be defined by \u00aff(X) = log(det( X)), and\nlet f = \u00aff|Sym(n)+ be its restriction to positive definite matrices with the affine\ninvariant metric. From Example 4.28 we know that grad \u00aff(X) = X\u22121 and hence\nalso that Hess \u00aff(X)[V ] = \u2212X\u22121V X\u22121. It thus follows from (11.40) that\ngradf(X) = X and Hessf(X)[V ] = 0\nfor all (X, V) \u2208 TSym(n)+. In particular, Hessf(X) is both positive and negative", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2685, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d06f85b0-44b8-4673-abf8-ccf145c2bda1": {"__data__": {"id_": "d06f85b0-44b8-4673-abf8-ccf145c2bda1", "embedding": null, "metadata": {"page_label": "329", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8bac2634-4280-4e50-af81-46bf0ffd6d18", "node_type": "4", "metadata": {"page_label": "329", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "5989d0572ebb41cedd1f92afa6b1393e74767674a2b9fb6cf274fa76954d56fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n11.8 Notes and references 329\nsemidefinite for all X. It follows from Theorem 11.23 and Definition 11.4 that\nf is geodesically linear on Sym(n)+.\nBhatia [Bha07, Ch. 6] and Moakher [Moa05] (among others) provide a dis-\ncussion of the affine invariant geometry of positive definite matrices. Moakher\nas well as Sra and Hosseini [SH15] discuss geodesic convexity on Sym( n)+ en-\ndowed with the affine invariant geometry, with applications. See [NSAY+19] for\nan overview of reasons to use the affine invariant metric when positive definite\nmatrices represent zero-mean Gaussian distributions (in comparison with other\npossible structures), and for an application of geodesic convexity to robust dis-\ntribution estimation.\n11.8 Notes and references\nUdri\u00b8 ste and Rapcs\u00b4 ak wrote a large number of papers on the subject of Rie-\nmannian convexity through the late 70s, 80s and 90s: see the many references\nin [Rap91, Rap97] and [Udr94]. Several of the results discussed in this chap-\nter (and more) can be found in those works. Other useful resources include\n[dCNdLO98], [Moa05], [SH15], [ZS16] to name a few.\nWhen discussing convexity of a function in Rn, one usually allows f to take\non infinite values. It is also habitual to allow f to be nondifferentiable, in which\ncase one resorts to subgradients instead of gradients. This can be generalized to\nRiemannian manifolds; see for example [FO98, ZS16, GH16, BFM17]. Another\nclassical tool in the study of convex functions is the Fenchel dual: see [BHSL+21]\nfor a discussion of that notion on Riemannian manifolds.\nPropositions 11.9 and 11.12 are akin to [Roc70, Thm. 10.1, Thm. 32.1] in\nRn. Euclidean versions of Theorems 11.21 and 11.23 are classical, see for exam-\nple [HUL01, Thm. B.4.1.1, p110, Thm. B.4.3.1, p115].\nSome references for Exercise 11.33 regarding Riemannian isometries are [Lee18,\nLem. 4.37, Prop. 4.38, Prop. 5.13] and [FLP20, \u00a7 4].\nOn a Cartan\u2013Hadamard manifold, given any point y, the function f(x) =\n1\n2 dist(x, y)2 is geodesically 1-strongly convex on the whole manifold [Lee18,\nLem. 12.15]. In particular, any geodesic ball centered at y is geodesically con-\nvex since it is a sublevel set of f. More detailed information about the Hessian\nof the distance and the squared distance functions on complete manifolds with\nbounded curvature can be found in [Sak96, pp153\u2013154].\nOne could also consider a notion of retraction convexity [Hua13, Def. 4.3.1].\nGiven a retraction R on a manifold M, a set S \u2286 Mis retraction convex if for all\nx, y\u2208 S there exists v \u2208 TxM such that c(t) = Rx(tv) satisfies c(0) = x, c(1) = y\nand c([0, 1]) \u2286 S. A function f : S \u2192 R is retraction convex if f composed with\nall retraction curves in S is convex. For the exponential retraction, this reduces\nto the notion of geodesic convexity defined in this chapter. Retraction convexity\nis referenced notably in [TFBJ18] and [KSM18].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3131, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "68d138bb-c2d8-4192-8fa7-937fad5d3817": {"__data__": {"id_": "68d138bb-c2d8-4192-8fa7-937fad5d3817", "embedding": null, "metadata": {"page_label": "330", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f22c021b-18a4-4c68-b7db-f919355c79f1", "node_type": "4", "metadata": {"page_label": "330", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "2d0789647aef39fb1c0ed660ab2c413750eca1deb84b571ae1ad146ac71e36c2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n330 Geodesic convexity\nThere is a link between geodesic convexity and barrier functions for interior\npoint methods. Quiroz and Oliveira [QO04] for example study Rn\n+ with a general\nfamily of diagonal Riemannian metrics, and show applications to the design and\nanalysis of interior point methods for linear programming.\nSee [Tak11, MMP18] and references therein for discussions of Riemannian ge-\nometries on Sym( n)+ related to the Wasserstein distance between probability\ndistributions, particularized to Gaussian distributions with positive definite co-\nvariance matrices.\nSection 11.5 provides some simple results regarding Riemannian gradient de-\nscent applied to a geodesically strongly convex function with Lipschitz contin-\nuous gradient, based on discussions with Chris Criscitiello. The main claim is\nTheorem 11.29. The proof relies on a Polyak\u2013 Lojasiewicz inequality built in\nLemma 11.28. This is a direct extension from the Euclidean case [Pol63, KNS16]\nto the Riemannian case. Such extensions also appear in various forms and for\nvariations of the setting here in [ZS16], [CMRS20, Thm. 4] and [CB22b]. Instead\nof assuming convexity, one can also assume the conclusions of Lemma 11.28\ndirectly and obtain a more general result.\nPerhaps the most famous algorithm for convex optimization in Rn is the ac-\ncelerated gradient method (also known as the fast gradient method or Nesterov\u2019s\ngradient method), for which a version of Theorem 11.29 holds in Rn with \u03ba re-\nplaced by \u221a\u03ba. There is interest in determining whether that algorithm has a\nsensible analog on Riemannian manifolds. Recent work on this topic includes\na discussion of the difficulties of the task [ZS18], with positive takes involv-\ning continuous-time perspectives [AOBL20a], methods based on estimate se-\nquences [AS20, AOBL20b] and methods based on geodesic maps [MR20], but\nalso negative takes (impossibility results) in [HM21, CB22b], all applying to\nsubtly different settings.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2212, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "77df77e6-5e86-44ed-b8c1-0da6fcdf6e8a": {"__data__": {"id_": "77df77e6-5e86-44ed-b8c1-0da6fcdf6e8a", "embedding": null, "metadata": {"page_label": "331", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ba329b1f-4c71-4067-8634-8fb764e0809a", "node_type": "4", "metadata": {"page_label": "331", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "1117073eaadfbafb6b379dd75855bca5d57a5b20316d9bd582906b1d8757b737", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\nReferences\n[Abb84] E. Abbott. Flatland: A Romance of Many Dimensions . Seeley &\nCo., 1884. [see p2]\n[ABBC20] N. Agarwal, N. Boumal, B. Bullins, and C. Cartis. Adaptive reg-\nularization with cubics on manifolds. Mathematical Programming,\n188(1):85\u2013134, 2020. [see p153, 291, 303]\n[ABG07] P.-A. Absil, C. G. Baker, and K. A. Gallivan. Trust-region methods\non Riemannian manifolds. Foundations of Computational Mathe-\nmatics, 7(3):303\u2013330, 2007. [see p145, 147, 151]\n[ABM08] F. Alvarez, J. Bolte, and J. Munier. A unifying local convergence\nresult for Newton\u2019s method in Riemannian manifolds. Foundations\nof Computational Mathematics , 8(2):197\u2013226, Apr 2008. [see p303]\n[ADM+02] R.L. Adler, J.P. Dedieu, J.Y. Margulies, M. Martens, and M. Shub.\nNewton\u2019s method on Riemannian manifolds and a geometric\nmodel for the human spine. IMA Journal of Numerical Analysis ,\n22(3):359\u2013390, 2002. [see p151]\n[AFPA07] V. Arsigny, P. Fillard, X. Pennec, and N. Ayache. Geometric means\nin a novel vector space structure on symmetric positive-definite\nmatrices. SIAM J. Matrix Anal. Appl. , 29(1):328\u2013347, 2007. [see\np327]\n[AK06] P.-A. Absil and K. Kurdyka. On the stable equilibrium points of\ngradient systems. Systems & Control Letters , 55(7):573\u2013577, July\n2006. [see p81]\n[AM12] P.-A. Absil and J. Malick. Projection-like retractions on matrix\nmanifolds. SIAM Journal on Optimization , 22(1):135\u2013158, 2012.\n[see p115, 118, 161]\n[AMH09] A. Al-Mohy and N. Higham. Computing the fr\u00b4 echet derivative of\nthe matrix exponential, with an application to condition number\nestimation. SIAM Journal on Matrix Analysis and Applications ,\n30(4):1639\u20131657, 2009. [see p75]\n[AMS08] P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization Algo-\nrithms on Matrix Manifolds . Princeton University Press, Prince-\nton, NJ, 2008. [see pxi, 50, 68, 75, 80, 81, 82, 117, 141, 147, 151, 152, 160, 162,\n180, 210, 211, 233, 254, 255, 285, 290, 303, 304]", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2171, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "db654770-d2e3-4b41-a241-16039dba6935": {"__data__": {"id_": "db654770-d2e3-4b41-a241-16039dba6935", "embedding": null, "metadata": {"page_label": "332", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2db6666a-2f77-4fe6-835c-6b4923f29798", "node_type": "4", "metadata": {"page_label": "332", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "adad8ff1bea7c9a0820f37f3f285545f4324c142521f1721722617ddfdd3f5f9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n332 References\n[AMT13] P.-A. Absil, R. Mahony, and J. Trumpf. An extrinsic look at the\nRiemannian Hessian. In Frank Nielsen and Fr\u00b4 ed\u00b4 eric Barbaresco,\neditors, Geometric Science of Information , volume 8085 of Lec-\nture Notes in Computer Science , pages 361\u2013368. Springer Berlin\nHeidelberg, 2013. [see p118, 253]\n[AO15] P.-A. Absil and I.V. Oseledets. Low-rank retractions: a survey\nand new results. Computational Optimization and Applications ,\n62(1):5\u201329, 2015. [see p169]\n[AOBL20a] F. Alimisis, A. Orvieto, G. B\u00b4 ecigneul, and A. Lucchi. A continuous-\ntime perspective for modeling acceleration in Riemannian opti-\nmization. In S. Chiappa and R. Calandra, editors, Proceedings of\nthe Twenty Third International Conference on Artificial Intelli-\ngence and Statistics, volume 108 of Proceedings of Machine Learn-\ning Research, pages 1297\u20131307. PMLR, 2020. [see p330]\n[AOBL20b] F. Alimisis, A. Orvieto, G. B\u00b4 ecigneul, and A. Lucchi. Prac-\ntical accelerated optimization on Riemannian manifolds. arXiv\n2002.04144, 2020. [see p330]\n[AS20] K. Ahn and S. Sra. From Nesterov\u2019s estimate sequence to Rie-\nmannian acceleration. In Conference on Learning Theory , pages\n84\u2013118. PMLR, 2020. [see p330]\n[ASS+09] S. Agarwal, N. Snavely, I. Simon, S.M. Seitz, and R. Szeliski. Build-\ning Rome in a day. In Computer Vision, 2009 IEEE 12th Interna-\ntional Conference on, pages 72\u201379. IEEE, 2009. [see p11]\n[AZGL+18] Z. Allen-Zhu, A. Garg, Y. Li, R. Oliveira, and A. Wigderson. Oper-\nator scaling via geodesically convex optimization, invariant theory\nand polynomial identity testing. In Proceedings of the 50th An-\nnual ACM SIGACT Symposium on Theory of Computing (STOC),\npages 172\u2013181, 2018. [see p307]\n[Bac14] M. Bac\u00b4 ak.Convex analysis and optimization in Hadamard spaces ,\nvolume 22 of De Gruyter Series in Nonlinear Analysis and Appli-\ncations. Walter de Gruyter GmbH & Co KG, 2014. [see p302]\n[BAC18] N. Boumal, P.-A. Absil, and C. Cartis. Global rates of conver-\ngence for nonconvex optimization on manifolds. IMA Journal of\nNumerical Analysis, 39(1):1\u201333, February 2018. [see p81, 152]\n[BAJN20] G.O. Berger, P.-A. Absil, R.M. Jungers, and Y. Nesterov. On\nthe quality of first-order approximation of functions with H\u00a8 older\ncontinuous gradient. Journal of Optimization Theory and Appli-\ncations, 185(1):17\u201333, 2020. [see p81]\n[Bar95] A.I. Barvinok. Problems of distance geometry and convex prop-\nerties of quadratic maps. Discrete & Computational Geometry ,\n13(1):189\u2013202, 1995. [see p15]\n[BBJN18] S. Bhojanapalli, N. Boumal, P. Jain, and P. Netrapalli. Smoothed\nanalysis for low-rank solutions to semidefinite programs in\nquadratic penalty form. In S. Bubeck, V. Perchet, and P. Rigollet,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2942, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "11f66497-d53e-458a-a96d-b4088b14633d": {"__data__": {"id_": "11f66497-d53e-458a-a96d-b4088b14633d", "embedding": null, "metadata": {"page_label": "333", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6162e212-cdf6-4d23-8336-f548aea9e5d4", "node_type": "4", "metadata": {"page_label": "333", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "53d6b8886a0ac5208c77d37a504ec20237210beb786ff8b540598d2fefe1a854", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\nReferences 333\neditors, Proceedings of the 31st Conference On Learning Theory ,\nvolume 75 of Proceedings of Machine Learning Research , pages\n3243\u20133270. PMLR, 06\u201309 Jul 2018. [see p16]\n[BBV16] A.S. Bandeira, N. Boumal, and V. Voroninski. On the low-rank\napproach for semidefinite programs arising in synchronization and\ncommunity detection. In Proceedings of The 29th Conference on\nLearning Theory, COLT 2016, New York, NY, June 23\u201326 , 2016.\n[see p16]\n[BC70] F. Brickell and R.S. Clark. Differentiable manifolds: an introduc-\ntion. Van Nostrand Reinhold, 1970. [see pxii, 2, 182, 189, 190, 191, 210,\n254]\n[Ber22] R. Bergmann. Manopt.jl: Optimization on manifolds in Julia.\nJournal of Open Source Software , 7(70):3866, 2022. [see p154]\n[Bes87] A.L. Besse. Einstein Manifolds. Springer Berlin Heidelberg, 1987.\n[see p257]\n[BFM17] G.C. Bento, O.P. Ferreira, and J.G. Melo. Iteration-complexity\nof gradient, subgradient and proximal point methods on Rieman-\nnian manifolds. Journal of Optimization Theory and Applications ,\n173(2):548\u2013562, 2017. [see p81, 329]\n[BH15] B.G. Bodmann and J. Haas. Frame potentials and the geometry of\nframes. Journal of Fourier Analysis and Applications , 21(6):1344\u2013\n1383, May 2015. [see p81, 253]\n[BH19] R. Bergmann and R. Herzog. Intrinsic formulation of KKT con-\nditions and constraint qualifications on smooth manifolds. SIAM\nJournal on Optimization , 29(4):2423\u20132444, 2019. [see p81, 151]\n[Bha07] R. Bhatia. Positive definite matrices . Princeton University Press,\n2007. [see p328, 329]\n[BHSL+21] R. Bergmann, R. Herzog, M. Silva Louzeiro, D. Tenbrinck, and\nJ. Vidal-N\u00b4 u\u02dc nez. Fenchel duality theory and a primal-dual algo-\nrithm on Riemannian manifolds. Foundations of Computational\nMathematics, 21(6):1465\u20131504, 2021. [see p329]\n[BKVH07] S. Boyd, S.-J. Kim, L. Vandenberghe, and A. Hassibi. A tutorial on\ngeometric programming. Optimization and Engineering , 8(1):67\u2013\n127, 2007. [see p307, 326]\n[BM03] S. Burer and R.D.C. Monteiro. A nonlinear programming algo-\nrithm for solving semidefinite programs via low-rank factorization.\nMathematical Programming, 95(2):329\u2013357, 2003. [see p16]\n[BM05] S. Burer and R.D.C. Monteiro. Local minima and convergence in\nlow-rank semidefinite programming. Mathematical Programming,\n103(3):427\u2013444, 2005. [see p15, 16]\n[BM06] I. Brace and J.H. Manton. An improved BFGS-on-manifold al-\ngorithm for computing weighted low rank approximations. In", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2672, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2ced5460-0d85-4c52-bb57-a1475b5eae8e": {"__data__": {"id_": "2ced5460-0d85-4c52-bb57-a1475b5eae8e", "embedding": null, "metadata": {"page_label": "334", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "188ed599-0bef-4e2f-a6a8-ed897a3a33ea", "node_type": "4", "metadata": {"page_label": "334", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "7df1a89d977562924a7ff16c3907922c163b4f71f52e0fd64763e2f58413f05b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n334 References\nProceedings of the 17th International Symposium on Mathemati-\ncal Theory of Networks and Systems , pages 1735\u20131738, 2006. [see\np81]\n[BMAS14] N. Boumal, B. Mishra, P.-A. Absil, and R. Sepulchre. Manopt, a\nMatlab toolbox for optimization on manifolds. Journal of Machine\nLearning Research, 15(42):1455\u20131459, 2014. [see p154]\n[Bon13] S. Bonnabel. Stochastic gradient descent on Riemannian mani-\nfolds. Automatic Control, IEEE Transactions on, 58(9):2217\u20132229,\n2013. [see p81]\n[BV21] P. Breiding and N. Vannieuwenhoven. The condition number of\nRiemannian approximation problems. SIAM Journal on Optimiza-\ntion, 31(1):1049\u20131077, 2021. [see p118]\n[BVB16] N. Boumal, V. Voroninski, and A.S. Bandeira. The non-convex\nBurer\u2013Monteiro approach works on smooth semidefinite programs.\nIn D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Gar-\nnett, editors, Advances in Neural Information Processing Systems\n29, pages 2757\u20132765. Curran Associates, Inc., 2016. [see p16]\n[BVB19] N. Boumal, V. Voroninski, and A.S. Bandeira. Deterministic guar-\nantees for Burer-Monteiro factorizations of smooth semidefinite\nprograms. Communications on Pure and Applied Mathematics ,\n73(3):581\u2013608, 2019. [see p15, 16]\n[BZ05] J. Borwein and Q. Zhu. Techniques of Variational Analysis. CMS\nBooks in Mathematics. Springer-Verlag, 2005. [see p52]\n[BZA20] T. Bendokat, R. Zimmermann, and P.-A. Absil. A Grassmann\nmanifold handbook: Basic geometry and computational aspects.\narXiv preprint arXiv:2011.13699 , 2020. [see p75, 253, 254]\n[CB22a] C. Criscitiello and N. Boumal. An accelerated first-order method\nfor non-convex optimization on manifolds. Journal of Foundations\nof Computational Mathematics , 2022. [see p303, 304]\n[CB22b] C. Criscitiello and N. Boumal. Negative curvature obstructs ac-\nceleration for strongly geodesically convex optimization, even with\nexact first-order oracles. In P.-L. Loh and M. Raginsky, editors,\nProceedings of Thirty Fifth Conference on Learning Theory , vol-\nume 178 of Proceedings of Machine Learning Research, pages 496\u2013\n542. PMLR, 02\u201305 Jul 2022. [see p330]\n[CGT00] A.R. Conn, N.I.M. Gould, and P.L. Toint. Trust-region methods.\nMPS-SIAM Series on Optimization. Society for Industrial and Ap-\nplied Mathematics, 2000. [see p144, 145, 146, 147, 151]\n[CGT11a] C. Cartis, N.I.M. Gould, and P. Toint. Adaptive cubic regularisa-\ntion methods for unconstrained optimization. Part II: worst-case\nfunction- and derivative-evaluation complexity.Mathematical Pro-\ngramming, 130:295\u2013319, 2011. [see p153]\n[CGT11b] C. Cartis, N.I.M. Gould, and P.L. Toint. Adaptive cubic regularisa-\ntion methods for unconstrained optimization. Part I: motivation,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2919, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6d57c268-cf21-4fa0-9ea4-76efbe03a3d4": {"__data__": {"id_": "6d57c268-cf21-4fa0-9ea4-76efbe03a3d4", "embedding": null, "metadata": {"page_label": "335", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7066ee75-cd40-4dee-a9c3-12e566896e8a", "node_type": "4", "metadata": {"page_label": "335", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "9d3636bf339da85578de3ec88a4177e0a0494b857a4c09279257dcc6774f9afd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\nReferences 335\nconvergence and numerical results. Mathematical Programming,\n127(2):245\u2013295, 2011. [see p153]\n[CGT12] C. Cartis, N.I.M. Gould, and P.L. Toint. Complexity bounds for\nsecond-order optimality in unconstrained optimization. Journal of\nComplexity, 28(1):93\u2013108, 2012. [see p152]\n[Cha06] I. Chavel. Riemannian geometry: a modern introduction , volume\n108 of Cambridge Tracts in Mathematics . Cambridge University\nPress, 2006. [see p303]\n[Cif21] D. Cifuentes. On the burer\u2013monteiro method for general semidef-\ninite programs. Optimization Letters , 15(6):2299\u20132309, January\n2021. [see p16]\n[CLR18] F.E. Curtis, Z. Lubberts, and D.P. Robinson. Concise com-\nplexity analyses for trust region methods. Optimization Letters,\n12(8):1713\u20131724, June 2018. [see p152]\n[CM19] D. Cifuentes and A. Moitra. Polynomial time guarantees for the\nBurer-Monteiro method. arXiv 1912.01745, 2019. [see p16]\n[CMRS20] S. Chewi, T. Maunu, P. Rigollet, and A.J. Stromme. Gradient\ndescent algorithms for Bures\u2013Wasserstein barycenters. In Confer-\nence on Learning Theory (COLT), pages 1276\u20131304. PMLR, 2020.\n[see p330]\n[CMSZ20] S. Chen, S. Ma, A.M.C. So, and T. Zhang. Proximal gradi-\nent method for nonsmooth optimization over the Stiefel manifold.\nSIAM Journal on Optimization , 30(1):210\u2013239, 2020. [see p81]\n[dBEG08] A. d\u2019Aspremont, F. Bach, and L. El Ghaoui. Optimal solutions\nfor sparse principal component analysis. The Journal of Machine\nLearning Research, 9:1269\u20131294, 2008. [see p10]\n[dC92] M.P. do Carmo. Riemannian geometry. Mathematics: Theory &\nApplications. Birkh\u00a8 auser Boston Inc., Boston, MA, 1992. Trans-\nlated from the second Portuguese edition by Francis Flaherty. [see\np254, 255, 304]\n[dCN95] J.X. da Cruz Neto. M\u00b4 etodos Geod\u00b4 esicos na Programa\u00b8 cao\nMatem\u00b4 atica. PhD thesis, Ph.D. Thesis, COPPE/UFRJ, Rio de\nJaneiro, Brazil, 1995. [see p303]\n[dCNdLO98] J.X. da Cruz Neto, L.L. de Lima, and P.R. Oliveira. Geodesic\nalgorithms in Riemannian geometry. Balkan Journal of Geometry\nand Its Applications , 3(2):89\u2013100, 1998. [see p81, 303, 329]\n[DE99] L. Dieci and T. Eirola. On smooth decompositions of matrices.\nSIAM Journal on Matrix Analysis and Applications , 20(3):800\u2013\n819, 1999. [see p75]\n[Deh95] J. Dehaene. Continuous-time matrix algorithms, systolic algo-\nrithms and adaptive neural networks . PhD thesis, Katholieke uni-\nversiteit Leuven, 1995. [see p75]\n[DH94] E. Dudek and K. Holly. Nonlinear orthogonal projection. Annales\nPolonici Mathematici, 59(1):1\u201331, 1994. [see p118]", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2746, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8165b7bb-31c6-4159-8e08-220ce0e2f2cb": {"__data__": {"id_": "8165b7bb-31c6-4159-8e08-220ce0e2f2cb", "embedding": null, "metadata": {"page_label": "336", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "87870eaf-c124-48e4-90c7-fb047d51fe8a", "node_type": "4", "metadata": {"page_label": "336", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "7d8ce7aef806f638154ffd49aa4c931c98e22f942f31dd553f95554d48ec7f80", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n336 References\n[DH19] A. Douik and B. Hassibi. Manifold optimization over the set of dou-\nbly stochastic matrices: A second-order geometry. IEEE Transac-\ntions on Signal Processing, 67(22):5761\u20135774, November 2019. [see\np180]\n[DMV99] J. Dehaene, M. Moonen, and J. Vandewalle. Analysis of a class of\ncontinuous-time algorithms for principal component analysis and\nsubspace tracking. IEEE Transactions on Circuits and Systems I:\nFundamental Theory and Applications , 46(3):364\u2013372, 1999. [see\np75]\n[dOF20] F.R. de Oliveira and O.P. Ferreira. Newton method for finding a\nsingularity of a special class of locally Lipschitz continuous vector\nfields on Riemannian manifolds. Journal of Optimization Theory\nand Applications, 185(2):522\u2013539, 2020. [see p304]\n[EAS98] A. Edelman, T.A. Arias, and S.T. Smith. The geometry of algo-\nrithms with orthogonality constraints. SIAM journal on Matrix\nAnalysis and Applications , 20(2):303\u2013353, 1998. [see p2, 81, 162, 180,\n250, 253]\n[FCPJ04] P.T. Fletcher, Lu; C., S.M. Pizer, and S. Joshi. Principal geodesic\nanalysis for the study of nonlinear statistics of shape. IEEE Trans-\nactions on Medical Imaging, 23(8):995\u20131005, August 2004. [see p180]\n[Fep17] F. Feppon. Riemannian geometry of matrix manifolds for La-\ngrangian uncertainty quantification of stochastic fluid flows. Mas-\nter\u2019s thesis, Massachusetts Institute of Technology, 2017. [see p75]\n[FL19] F. Feppon and P.F.J. Lermusiaux. The extrinsic geometry of\ndynamical systems tracking nonlinear matrix projections. SIAM\nJournal on Matrix Analysis and Applications, 40(2):814\u2013844, 2019.\n[see p75]\n[Fle13] T.P. Fletcher. Geodesic regression and the theory of least squares\non Riemannian manifolds. International Journal of Computer Vi-\nsion, 105(2):171\u2013185, Nov 2013. [see p307]\n[FLP20] O.P. Ferreira, M.S. Louzeiro, and L.F. Prudente. Iteration-\ncomplexity and asymptotic analysis of steepest descent method\nfor multiobjective optimization on Riemannian manifolds. Journal\nof Optimization Theory and Applications , 184:507\u2013533, December\n2020. [see p329]\n[FM20] C. Franks and A. Moitra. Rigorous guarantees for Tyler\u2019s M-\nestimator via quantum expansion. In J. Abernethy and S. Agar-\nwal, editors, Proceedings of Thirty Third Conference on Learning\nTheory (COLT), volume 125 of Proceedings of Machine Learning\nResearch, pages 1601\u20131632. PMLR, 09\u201312 Jul 2020. [see p307]\n[FO98] O.P. Ferreira and P.R. Oliveira. Subgradient algorithm on Rieman-\nnian manifolds. Journal of Optimization Theory and Applications ,\n97(1):93\u2013104, Apr 1998. [see p329]", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2791, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e0b6e48b-8e90-44e0-917e-cc41596557a0": {"__data__": {"id_": "e0b6e48b-8e90-44e0-917e-cc41596557a0", "embedding": null, "metadata": {"page_label": "337", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "256b7554-b675-4320-b4b0-63d1c495fe40", "node_type": "4", "metadata": {"page_label": "337", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "9abd848d184685512d006c96f3d594b5177ca1964e5c34807314430c62d1abf5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\nReferences 337\n[FORW21] C. Franks, R. Oliveira, A. Ramachandran, and M. Walter. Near\noptimal sample complexity for matrix and tensor normal models\nvia geodesic convexity. preprint arXiv:2110.07583, 2021. [see p307]\n[FS02] O.P. Ferreira and B.F. Svaiter. Kantorovich\u2019s theorem on New-\nton\u2019s method in Riemannian manifolds. Journal of Complexity ,\n18(1):304\u2013329, 2002. [see p303]\n[Gab82] D. Gabay. Minimizing a differentiable function over a differen-\ntial manifold. Journal of Optimization Theory and Applications ,\n37(2):177\u2013219, 1982. [see p2, 80, 81]\n[GH16] P. Grohs and S. Hosseini. \u03b5-subgradient algorithms for locally\nLipschitz functions on Riemannian manifolds. Advances in Com-\nputational Mathematics, 42(2):333\u2013360, 2016. [see p329]\n[GHL04] S. Gallot, D. Hulin, and J. LaFontaine. Riemannian geometry .\nSpringer Verlag, 2004. [see p248, 254, 257, 304]\n[GQ20] J. Gallier and J. Quaintance. Differential Geometry and Lie\nGroups. Springer International Publishing, 2020. [see p254]\n[Gri81] A. Griewank. The modification of Newton\u2019s method for uncon-\nstrained optimization by bounding cubic terms. Technical Report\nTechnical report NA/12, Department of Applied Mathematics and\nTheoretical Physics, University of Cambridge, 1981. [see p152]\n[GW08] A. Griewank and A. Walther. Evaluating Derivatives: Principles\nand Techniques of Algorithmic Differentiation . Society for Indus-\ntrial and Applied Mathematics, 2 edition, 2008. [see p78]\n[GZAL14] J. Goes, T. Zhang, R. Arora, and G. Lerman. Robust stochastic\nprincipal component analysis. In Artificial Intelligence and Statis-\ntics, pages 266\u2013274, 2014. [see p10]\n[HAG16] W. Huang, P.-A. Absil, and K.A. Gallivan. A Riemannian BFGS\nMethod for Nonconvex Optimization Problems , pages 627\u2013634.\nSpringer International Publishing, Cham, 2016. [see p81]\n[HGA15] W. Huang, K.A. Gallivan, and P.-A. Absil. A Broyden class of\nquasi-Newton methods for Riemannian optimization. SIAM Jour-\nnal on Optimization , 25(3):1660\u20131685, 2015. [see p81, 290, 304]\n[Hig08] N. Higham. Functions of Matrices . Society for Industrial and\nApplied Mathematics, Philadelphia, PA, USA, 2008. [see p74, 75, 78]\n[HLV18] P. Hand, C. Lee, and V. Voroninski. ShapeFit: Exact location\nrecovery from corrupted pairwise directions. Communications on\nPure and Applied Mathematics , 71(1):3\u201350, 2018. [see p5]\n[HM96] U. Helmke and J.B. Moore. Optimization and Dynamical Systems.\nSpringer, 2nd edition, 1996. [see p2, 81]\n[HM21] L. Hamilton and A. Moitra. A no-go theorem for acceleration in the\nhyperbolic plane. In Advances in Neural Information Processing\nSystems (NeurIPS), 2021. [see p330]\n[HS15] R. Hosseini and S. Sra. Matrix manifold optimization for Gaussian\nmixtures. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3002, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c38b2fb0-3955-4d68-9f22-b664e21540b5": {"__data__": {"id_": "c38b2fb0-3955-4d68-9f22-b664e21540b5", "embedding": null, "metadata": {"page_label": "338", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6949dab2-4c32-43a2-a6f2-e97b28262b91", "node_type": "4", "metadata": {"page_label": "338", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "0cf5957564e1395d8f94b6567caaa0582d0444454d45f7c3123ab9b931986b4a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n338 References\nand R. Garnett, editors, Advances in Neural Information Process-\ning Systems 28 , pages 910\u2013918. Curran Associates, Inc., 2015. [see\np14, 307]\n[HS18] G. Heidel and V. Schulz. A Riemannian trust-region method for\nlow-rank tensor completion. Numerical Linear Algebra with Appli-\ncations, 25(6):e2175, 2018. [see p180]\n[HS19] R. Hosseini and S. Sra. An alternative to EM for Gaussian mixture\nmodels: Batch and stochastic Riemannian optimization. Mathe-\nmatical Programming, pages 1\u201337, 2019. [see p307]\n[HS20] R. Hosseini and S. Sra. Recent Advances in Stochastic Riemannian\nOptimization, pages 527\u2013554. Springer International Publishing,\n2020. [see p81]\n[HU17] S. Hosseini and A. Uschmajew. A Riemannian gradient sampling\nalgorithm for nonsmooth optimization on manifolds. SIAM Jour-\nnal on Optimization , 27(1):173\u2013189, 2017. [see p81]\n[Hua13] W. Huang. Optimization algorithms on Riemannian manifolds\nwith applications. PhD thesis, Florida State University, 2013. [see\np329]\n[HUL01] J.-B. Hiriart-Urruty and C. Lemar\u00b4 echal. Fundamentals of convex\nanalysis. Grundlehren Text Editions. Springer-Verlag Berlin Hei-\ndelberg, 1 edition, 2001. [see p308, 329]\n[JBAS10] M. Journ\u00b4 ee, F. Bach, P.-A. Absil, and R. Sepulchre. Low-rank\noptimization on the cone of positive semidefinite matrices. SIAM\nJournal on Optimization , 20(5):2327\u20132351, 2010. [see p16, 180]\n[JD15] B. Jiang and Y.-H. Dai. A framework of constraint preserving\nupdate schemes for optimization on Stiefel manifold. Mathematical\nProgramming, 153(2):535\u2013575, 2015. [see p161]\n[JMM19] P. Jawanpuria, M. Meghwanshi, and B. Mishra. Low-rank approx-\nimations of hyperbolic embeddings. 2019 IEEE 58th Conference\non Decision and Control (CDC) , Dec 2019. [see p180]\n[JNRS10] M. Journ\u00b4 ee, Y. Nesterov, P. Richt\u00b4 arik, and R. Sepulchre. Gener-\nalized power method for sparse principal component analysis. The\nJournal of Machine Learning Research , 11:517\u2013553, 2010. [see p10]\n[KGB16] A. Kovnatsky, K. Glashoff, and M.M. Bronstein. MADMM: A\nGeneric Algorithm for Non-smooth Optimization on Manifolds ,\npages 680\u2013696. Springer International Publishing, Cham, 2016.\n[see p81]\n[KMU+20] V. Khrulkov, L. Mirvakhabova, E. Ustinova, I. Oseledets, and\nV. Lempitsky. Hyperbolic image embeddings. In IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition (CVPR) ,\n2020. [see p180]\n[KN63] S. Kobayashi and K. Nomizu. Foundations of differential geometry,\nVol. I. John Wiley and Sons, 1963. [see p258]", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2724, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5847cb31-b6d9-4678-9c60-261959ec6c43": {"__data__": {"id_": "5847cb31-b6d9-4678-9c60-261959ec6c43", "embedding": null, "metadata": {"page_label": "339", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "233713ff-9f4a-4d52-bab8-e89bae908ac0", "node_type": "4", "metadata": {"page_label": "339", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "f6398e4055adf4bb3c6b6853d2fd66a68fa422fa347494852195daefb0ed328e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\nReferences 339\n[KNS16] H. Karimi, J. Nutini, and M. Schmidt. Linear convergence of gradi-\nent and proximal-gradient methods under the Polyak\u2013 Lojasiewicz\ncondition. In Machine Learning and Knowledge Discovery in\nDatabases (ECML PKDD), pages 795\u2013811. Springer International\nPublishing, 2016. [see p330]\n[KSM18] H. Kasai, H. Sato, and B. Mishra. Riemannian stochastic recursive\ngradient algorithm. In J. Dy and A. Krause, editors,Proceedings of\nthe 35th International Conference on Machine Learning, volume 80\nof Proceedings of Machine Learning Research , pages 2516\u20132524,\nStockholmsmassan, Stockholm Sweden, 10\u201315 Jul 2018. PMLR.\n[see p81, 329]\n[KSV14] D. Kressner, M. Steinlechner, and B. Vandereycken. Low-rank\ntensor completion by Riemannian optimization. BIT Numerical\nMathematics, 54(2):447\u2013468, Jun 2014. [see p180]\n[Lag07] C. Lageman. Pointwise convergence of gradient-like systems.Math-\nematische Nachrichten, 280(13\u201314):1543\u20131558, October 2007. [see\np81]\n[LB20] C. Liu and N. Boumal. Simple algorithms for optimization on\nRiemannian manifolds with constraints. Applied Mathematics and\nOptimization, 82(3):949\u2013981, 2020. [see p181, 303]\n[LC20] M. Lezcano-Casado. Curvature-dependant global convergence\nrates for optimization on manifolds of bounded geometry. arXiv\npreprint arXiv:2008.02517, 2020. [see p303, 304]\n[Lee12] J.M. Lee. Introduction to Smooth Manifolds, volume 218 of Gradu-\nate Texts in Mathematics. Springer-Verlag New York, 2nd edition,\n2012. [see pxii, 21, 27, 50, 51, 52, 66, 98, 99, 117, 118, 189, 190, 191, 198, 207,\n208, 209, 210, 211, 217, 218, 221, 222, 223, 254, 255, 256, 258, 259, 269, 295, 296,\n301, 305]\n[Lee18] J.M. Lee. Introduction to Riemannian Manifolds , volume 176 of\nGraduate Texts in Mathematics . Springer, 2nd edition, 2018. [see\npxii, 50, 51, 117, 118, 174, 208, 210, 211, 254, 260, 261, 262, 263, 264, 265, 267,\n269, 271, 272, 296, 298, 301, 302, 303, 304, 305, 315, 316, 329]\n[Lev20] E. Levin. Towards optimization on varieties. Undergraduate senior\nthesis, Princeton University, 2020. [see p180]\n[Lic79] A. Lichnewsky. Une m\u00b4 ethode de gradient conjugu\u00b4 e sur des vari\u00b4 et\u00b4 es:\nApplication ` a certains probl` emes de valeurs propres non lin\u00b4 eaires.\nNumerical Functional Analysis and Optimization , 1(5):515\u2013560,\n1979. [see p2, 81]\n[LKB22a] E. Levin, J. Kileel, and N. Boumal. The effect of smooth\nparametrizations on nonconvex optimization landscapes. arXiv\n2207.03512, 2022. [see p259]\n[LKB22b] E. Levin, J. Kileel, and N. Boumal. Finding stationary points on\nbounded-rank matrices: A geometric hurdle and a smooth remedy.\nMathematical Programming, 2022. [see p13, 152, 180, 284]", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2889, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "73f6fb7e-7cc1-4a7a-8c28-e4def73b0d76": {"__data__": {"id_": "73f6fb7e-7cc1-4a7a-8c28-e4def73b0d76", "embedding": null, "metadata": {"page_label": "340", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dbb25a45-cab6-4951-9892-f16aefb15e8f", "node_type": "4", "metadata": {"page_label": "340", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "3fbeb01c2b36713183c80f823819db8400c0c7f0ffda36e0c3456a56d725e675", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n340 References\n[LLY20] Z. Lai, L.-H. Lim, and K. Ye. Simpler Grassmannian optimization.\narXiv preprint arXiv:2009.13502 , 2020. [see p253]\n[ Loj65] S.  Lojasiewicz. Ensembles semi-analytiques. Lecture Notes IHES\n(Bures-sur-Yvette), 1965. [see p82]\n[LTW22] S. Li, G. Tang, and M.B. Wakin. Landscape correspondence of\nempirical and population risks in the eigendecomposition problem.\nIEEE Transactions on Signal Processing , 70:2985\u20132999, 2022. [see\np258]\n[Lue72] D.G. Luenberger. The gradient projection method along geodesics.\nManagement Science, 18(11):620\u2013631, 1972. [see p2, 80, 81, 179]\n[MA20] E. Massart and P.-A. Absil. Quotient geometry with simple\ngeodesics for the manifold of fixed-rank positive-semidefinite ma-\ntrices. SIAM Journal on Matrix Analysis and Applications ,\n41(1):171\u2013198, 2020. [see p180, 249]\n[Man02] J.H. Manton. Optimization algorithms exploiting unitary con-\nstraints. IEEE Transactions on Signal Processing , 50(3):635\u2013650,\nMarch 2002. [see p151]\n[Mat96] R. Mathias. A chain rule for matrix functions and applications.\nSIAM Journal on Matrix Analysis and Applications , 17(3):610\u2013\n620, 1996. [see p74]\n[Mey11] G. Meyer. Geometric optimization algorithms for linear regression\non fixed-rank matrices. PhD thesis, Universit\u00b4 e de Li` ege, Belgique,\n2011. [see p13]\n[Mic08] P.W. Michor. Topics in differential geometry, volume 93. American\nMathematical Society, 2008. [see p258]\n[Mis14] B. Mishra. A Riemannian approach to large-scale constrained least-\nsquares with symmetries . PhD thesis, Universit\u00b4 e de Li` ege, Bel-\ngique, 2014. [see p13]\n[MMP18] L. Malag` o, L. Montrucchio, and G. Pistone. Wasserstein Rieman-\nnian geometry of positive definite matrices. arXiv 1801.09269 ,\n2018. [see p330]\n[Moa03] M. Moakher. Means and averaging in the group of rotations. SIAM\nJournal on Matrix Analysis and Applications, 24(1):1\u201316, 2003. [see\np307]\n[Moa05] M. Moakher. A differential geometric approach to the geometric\nmean of symmetric positive-definite matrices. SIAM J. Matrix\nAnal. Appl., 26(3):735\u2013747, March 2005. [see p307, 329]\n[MR20] D. Mart\u00b4 \u0131nez-Rubio. Global Riemannian acceleration in hyperbolic\nand spherical spaces. arXiv preprint arXiv:2012.03618 , 2020. [see\np330]\n[MS85] A. Machado and I. Salavessa. Grassman manifolds as subsets of\nEuclidean spaces. In Cordero, editor, Differential Geometry, Proc.\n5th Int. Colloq. , volume 131 of Research Notes in Mathematics ,\npages 85\u2013102, Boston, MA, 1985. Pitman. [see p253]", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2715, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1dc34870-0f21-4cb1-8c72-21b328f3fee4": {"__data__": {"id_": "1dc34870-0f21-4cb1-8c72-21b328f3fee4", "embedding": null, "metadata": {"page_label": "341", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "edaed966-2e8e-4488-8778-c7be3d46cd79", "node_type": "4", "metadata": {"page_label": "341", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "a08c62623b8b9dd2f250b9d68fd466feea36c3d9051b98f230b7de2d8855eb5b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\nReferences 341\n[MS16] B. Mishra and R. Sepulchre. Riemannian preconditioning. SIAM\nJournal on Optimization , 26(1):635\u2013660, 2016. [see p146]\n[MS20] S. Marsland and S. Sommer. Riemannian geometry on shapes\nand diffeomorphisms: Statistics via actions of the diffeomorphism\ngroup. In X. Pennec, S. Sommer, and T. Fletcher, editors, Rie-\nmannian Geometric Statistics in Medical Image Analysis , pages\n135\u2013167. Academic Press, 2020. [see p180]\n[MT11] M. McCoy and J.A. Tropp. Two proposals for robust PCA us-\ning semidefinite programming. Electronic Journal of Statistics ,\n5:1123\u20131160, 2011. [see p10]\n[MV13] B. Mishra and B. Vandereycken. A Riemannian approach to low-\nrank algebraic Riccati equations. arXiv preprint arXiv:1312.4883,\n2013. [see p13]\n[MZL19] T. Maunu, T. Zhang, and G. Lerman. A well-tempered land-\nscape for non-convex robust subspace recovery.Journal of Machine\nLearning Research, 20(37):1\u201359, 2019. [see p10]\n[Nes18] Y. Nesterov. Lectures on Convex Optimization , volume 137 of\nOptimization and Its Applications . Springer, 2 edition, 2018. [see\np15]\n[NK17] M. Nickel and D. Kiela. Poincar\u00b4 e embeddings for learning hierar-\nchical representations. In I. Guyon, U. V. Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, edi-\ntors, Advances in Neural Information Processing Systems 30, pages\n6338\u20136347. Curran Associates, Inc., 2017. [see p180]\n[NNSS20] S. Neumayer, M. Nimmer, S. Setzer, and G. Steidl. On the ro-\ntational invariant L1-norm PCA. Linear Algebra and its Applica-\ntions, 587:243\u2013270, February 2020. [see p10]\n[NO61] K. Nomizu and H. Ozeki. The existence of complete Rieman-\nnian metrics. Proceedings of the American Mathematical Society ,\n12(6):889\u2013891, 1961. [see p302]\n[Nof17] V. Noferini. A formula for the Fr\u00b4 echet derivative of a generalized\nmatrix function. SIAM Journal on Matrix Analysis and Applica-\ntions, 38(2):434\u2013457, 2017. [see p74]\n[NP06] Y. Nesterov and B.T. Polyak. Cubic regularization of Newton\nmethod and its global performance. Mathematical Programming,\n108(1):177\u2013205, 2006. [see p152]\n[NSAY+19] V.A. Nguyen, S. Shafieezadeh-Abadeh, M.-C. Yue, D. Kuhn,\nand W. Wiesemann. Calculating optimistic likelihoods using\n(geodesically) convex optimization. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d ' Alch\u00b4 e-Buc, E. Fox, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems , volume 32.\nCurran Associates, Inc., 2019. [see p307, 329]", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2692, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3228b3c3-e854-4a53-8c46-90b420024efa": {"__data__": {"id_": "3228b3c3-e854-4a53-8c46-90b420024efa", "embedding": null, "metadata": {"page_label": "342", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "769eb413-31c1-496b-a30d-c6fd5a77cc53", "node_type": "4", "metadata": {"page_label": "342", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "b7288b5b93f3ddd0ea86350246177702128d2d6c5bb2f71551987079c00dab23", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n342 References\n[NW06] J. Nocedal and S. Wright. Numerical optimization. Springer Se-\nries in Operations Research and Financial Engineering. Springer\nScience & Business Media, 2 edition, 2006. [see p64, 121, 145, 151]\n[O\u2019N83] B. O\u2019Neill. Semi-Riemannian geometry: with applications to rela-\ntivity, volume 103. Academic Press, 1983. [see pxii, 50, 51, 52, 117, 118,\n175, 176, 210, 233, 254, 303]\n[Pat98] G. Pataki. On the rank of extreme matrices in semidefinite pro-\ngrams and the multiplicity of optimal eigenvalues. Mathematics of\noperations research, 23(2):339\u2013358, 1998. [see p15]\n[Pea94] B.A. Pearlmutter. Fast exact multiplication by the Hessian.Neural\nComputation, 6:147\u2013160, 1994. [see p78]\n[Pet76] E.L. Peterson. Geometric programming. SIAM Review, 18(1):1\u2013\n51, January 1976. [see p326]\n[Pet06] P. Petersen. Riemannian geometry, volume 171 of Graduate Texts\nin Mathematics. Springer, 2 edition, 2006. [see p302]\n[PJB18] T. Pumir, S. Jelassi, and N. Boumal. Smoothed analysis of the\nlow-rank approach for smooth semidefinite programs. In S. Ben-\ngio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information Processing\nSystems 31 , pages 2283\u20132292. Curran Associates, Inc., 2018. [see\np16]\n[Pol63] B.T. Polyak. Gradient methods for the minimisation of function-\nals. USSR Computational Mathematics and Mathematical Physics,\n3(4):864\u2013878, January 1963. [see p330]\n[QGA10a] C. Qi, K.A. Gallivan, and P.-A. Absil. An efficient BFGS algo-\nrithm for Riemannian optimization. In Proceedings of the 19th\nInternational Symposium on Mathematical Theory of Network and\nSystems (MTNS 2010), volume 1, pages 2221\u20132227, 2010. [see p273,\n304]\n[QGA10b] C. Qi, K.A. Gallivan, and P.A. Absil. Riemannian BFGS algo-\nrithm with applications. Recent Advances in Optimization and its\nApplications in Engineering, pages 183\u2013192, 2010. [see p81]\n[Qi11] C. Qi. Numerical optimization methods on Riemannian manifolds .\nPhD thesis, Florida State University, Tallahassee, FL, 2011. [see\np153]\n[QO04] E.A. Quiroz and P.R. Oliveira. New results on linear optimization\nthrough diagonal metrics and Riemannian geometry tools. Tech-\nnical report, Technical Report ES-654/04, PESC COPPE, Federal\nUniversity of Rio de Janeiro, 2004. [see p330]\n[Rap91] T. Rapcs\u00b4 ak. Geodesic convexity in nonlinear optimization. Jour-\nnal of Optimization Theory and Applications , 69(1):169\u2013183, Apr\n1991. [see p314, 329]\n[Rap97] T. Rapcs\u00b4 ak.Smooth Nonlinear Optimization in Rn, volume 19 of", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2762, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fa897d6e-3699-4280-9a2b-18153e0cd29d": {"__data__": {"id_": "fa897d6e-3699-4280-9a2b-18153e0cd29d", "embedding": null, "metadata": {"page_label": "343", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ec8a90ef-0811-47a7-986f-9b493c5416ca", "node_type": "4", "metadata": {"page_label": "343", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "aa261747b9f48bfd87f252aabb9424c6121b1375fbfad667c4329302e9e4e53e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\nReferences 343\nNonconvex Optimization and Its Applications . Springer, 1997. [see\np2, 81, 307, 312, 313, 326, 329]\n[RDTEL21] D.M. Rosen, K.J. Doherty, A. Ter\u00b4 an Espinoza, and J.J. Leonard.\nAdvances in inference and representation for simultaneous local-\nization and mapping. Annual Review of Control, Robotics, and\nAutonomous Systems, 4:215\u2013242, 2021. [see p12]\n[Roc70] R.T. Rockafellar. Convex analysis . Princeton University Press,\nPrinceton, NJ, 1970. [see p308, 314, 329]\n[RW12] W. Ring and B. Wirth. Optimization methods on Riemannian\nmanifolds and their application to shape space. SIAM Journal on\nOptimization, 22(2):596\u2013627, 2012. [see p81, 303]\n[Sak96] T. Sakai. Riemannian geometry, volume 149. American Mathe-\nmatical Society, 1996. [see p315, 329]\n[Sat16] H. Sato. A Dai\u2013Yuan-type Riemannian conjugate gradient method\nwith the weak Wolfe conditions. Computational Optimization and\nApplications, 64(1):101\u2013118, 2016. [see p81]\n[Sat21] H. Sato. Riemannian Optimization and Its Applications . Springer\nInternational Publishing, 2021. [see p81]\n[Sch66] P.H. Sch\u00a8 onemann. A generalized solution of the orthogonal Pro-\ncrustes problem. Psychometrika, 31(1):1\u201310, mar 1966. [see p161]\n[SFF19] Y. Sun, N. Flammarion, and M. Fazel. Escaping from saddle\npoints on Riemannian manifolds. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d\u2019Alch\u00b4 e Buc, E. Fox, and R. Garnett, edi-\ntors, Advances in Neural Information Processing Systems 32, pages\n7276\u20137286. Curran Associates, Inc., 2019. [see p303]\n[SH15] S. Sra and R. Hosseini. Conic geometric optimization on the man-\nifold of positive definite matrices. SIAM Journal on Optimization ,\n25(1):713\u2013739, 2015. [see p328, 329]\n[SI14] H. Sato and T. Iwai. Optimization algorithms on the Grass-\nmann manifold with application to matrix eigenvalue problems.\nJapan Journal of Industrial and Applied Mathematics , 31(2):355\u2013\n400, April 2014. [see p253, 258]\n[SI15] H. Sato and T. Iwai. A new, globally convergent Riemannian con-\njugate gradient method. Optimization, 64(4):1011\u20131031, 2015. [see\np81]\n[SKM19] H. Sato, H. Kasai, and B. Mishra. Riemannian stochastic variance\nreduced gradient algorithm with retraction and vector transport.\nSIAM Journal on Optimization , 29(2):1444\u20131472, 2019. [see p81]\n[Smi94] S.T. Smith. Optimization techniques on Riemannian manifolds.\nFields Institute Communications, 3(3):113\u2013135, 1994. [see p2, 80, 81]\n[SN22] S. Sun and J. Nocedal. A trust region method for the optimization\nof noisy functions. arXiv:2201.00973, 2022. [see p144]\n[SQW17] J. Sun, Q. Qu, and J. Wright. Complete dictionary recovery over\nthe sphere II: Recovery by Riemannian trust-region method. IEEE", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2909, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "47841a92-e73f-4ce9-b2ac-3a4f27bc52cd": {"__data__": {"id_": "47841a92-e73f-4ce9-b2ac-3a4f27bc52cd", "embedding": null, "metadata": {"page_label": "344", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "39839b0c-3cc8-4b2b-88f6-6759d9c35c5c", "node_type": "4", "metadata": {"page_label": "344", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "ace781034ea53df160ebd24aae28e26d664e85f4be88c0035462434e740ffef3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n344 References\nTransactions on Information Theory, 63(2):885\u2013914, Feb 2017. [see\np8]\n[Sra16] S. Sra. On the matrix square root via geometric optimization.\nElectronic Journal of Linear Algebra, 31(1):433\u2013443, Jul 2016. [see\np307]\n[Ste83] T. Steihaug. The conjugate gradient method and trust regions in\nlarge scale optimization. SIAM Journal on Numerical Analysis ,\n20(3):626\u2013637, 1983. [see p152]\n[TA21] T.M. Tang and G.I. Allen. Integrated principal components anal-\nysis. Journal of Machine Learning Research , 22(198):1\u201371, 2021.\n[see p307]\n[Tak11] A. Takatsu. Wasserstein geometry of Gaussian measures. Osaka\nJournal of Mathematics , 48(4):1005\u20131026, 2011. [see p330]\n[TB97] L.N. Trefethen and D. Bau. Numerical linear algebra. Society for\nIndustrial and Applied Mathematics, 1997. [see px, 125, 129, 151]\n[TD14] R. Tron and K. Daniilidis. On the quotient representation for the\nessential manifold. In 2014 IEEE Conference on Computer Vision\nand Pattern Recognition. IEEE, June 2014. [see p180]\n[TFBJ18] N. Tripuraneni, N. Flammarion, F. Bach, and M.I. Jordan. Av-\neraging stochastic gradient descent on Riemannian manifolds. In\nProceedings of The 31st Conference on Learning Theory, COLT ,\n2018. [see p329]\n[TG21] N. Trendafilov and M. Gallo. Multivariate Data Analysis on Matrix\nManifolds (with Manopt). Springer International Publishing, 2021.\n[see p11]\n[TKW16] J. Townsend, N. Koep, and S. Weichwald. PyManopt: a Python\ntoolbox for optimization on manifolds using automatic differenti-\nation. Journal of Machine Learning Research , 17(137):1\u20135, 2016.\n[see p154]\n[Toi81] P. Toint. Towards an efficient sparsity exploiting Newton method\nfor minimization. In I. S. Duff, editor, Sparse Matrices and Their\nUses, pages 57\u201388. Academic Press, 1981. [see p152]\n[Udr94] C. Udri\u00b8 ste. Convex functions and optimization methods on Rie-\nmannian manifolds , volume 297 of Mathematics and its applica-\ntions. Kluwer Academic Publishers, 1994. [see p2, 81, 307, 313, 315,\n329]\n[UV13] A. Uschmajew and B. Vandereycken. The geometry of algorithms\nusing hierarchical tensors. Linear Algebra and its Applications ,\n439(1):133\u2013166, July 2013. [see p180]\n[UV20] A. Uschmajew and B. Vandereycken. Geometric methods on low-\nrank matrix and tensor manifolds. In Handbook of Variational\nMethods for Nonlinear Geometric Data , pages 261\u2013313. Springer\nInternational Publishing, 2020. [see p180]", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2633, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b5e66dc3-f231-4e25-9092-5229d46dbe0d": {"__data__": {"id_": "b5e66dc3-f231-4e25-9092-5229d46dbe0d", "embedding": null, "metadata": {"page_label": "345", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "011ed87b-4187-4f74-94fc-c87a96a166b1", "node_type": "4", "metadata": {"page_label": "345", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "e8ab1a397755e3e49ec7ee507a050c858150a5fe9bcf20fce5c938d16bf69b91", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\nReferences 345\n[Van10] B. Vandereycken. Riemannian and multilevel optimization for\nrank-constrained matrix problems. Faculty of Engineering,\nKatholieke Universiteit Leuven , 2010. [see p13]\n[Van13] B. Vandereycken. Low-rank matrix completion by Riemannian\noptimization. SIAM Journal on Optimization , 23(2):1214\u20131236,\n2013. [see p13, 180]\n[Vav91] S.A. Vavasis. Nonlinear optimization: complexity issues . Oxford\nUniversity Press, Inc., 1991. [see p146]\n[VAV09] B. Vandereycken, P.-A. Absil, and S. Vandewalle. Embedded ge-\nometry of the set of symmetric positive semidefinite matrices of\nfixed rank. In SSP09, pages 389\u2013392, 2009. [see p180, 249]\n[Vis18] N.K. Vishnoi. Geodesic convex optimization: Differentiation on\nmanifolds, geodesics, and convexity. arXiv 1806.06373, 2018. [see\np307, 319, 328]\n[Wie12] A. Wiesel. Geodesic convexity and covariance estimation. IEEE\nTransactions on Signal Processing , 60(12):6182\u20136189, Dec 2012.\n[see p307]\n[WW20] I. Waldspurger and A. Waters. Rank optimality for the\nBurer\u2013Monteiro factorization. SIAM Journal on Optimization ,\n30(3):2577\u20132602, 2020. [see p16]\n[WY13] Z. Wen and W. Yin. A feasible method for optimization with\northogonality constraints. Mathematical Programming , 142(1\u2013\n2):397\u2013434, 2013. [see p161]\n[YZS14] W.H. Yang, L.-H. Zhang, and R. Song. Optimality conditions for\nthe nonlinear programming problems on Riemannian manifolds.\nPacific Journal of Optimization , 10(2):415\u2013434, 2014. [see p81, 151]\n[ZHS16] P.H. Zadeh, R. Hosseini, and S. Sra. Geometric mean metric learn-\ning. In Proceedings of the 33rd International Conference on In-\nternational Conference on Machine Learning , ICML, pages 2464\u2013\n2471. JMLR.org, 2016. [see p307]\n[ZRS16] H. Zhang, S.J. Reddi, and S. Sra. Riemannian SVRG: Fast\nstochastic optimization on Riemannian manifolds. In D. D. Lee,\nM. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, edi-\ntors, Advances in Neural Information Processing Systems 29, pages\n4592\u20134600. Curran Associates, Inc., 2016. [see p81]\n[ZS16] H. Zhang and S. Sra. First-order methods for geodesically convex\noptimization. In Conference on Learning Theory, pages 1617\u20131638,\n2016. [see p81, 307, 329, 330]\n[ZS18] H. Zhang and S. Sra. An estimate sequence for geodesically convex\noptimization. In S. Bubeck, V. Perchet, and P. Rigollet, editors,\nProceedings of the 31st Conference On Learning Theory, volume 75\nof Proceedings of Machine Learning Research , pages 1703\u20131723.\nPMLR, 06\u201309 Jul 2018. [see p330]", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2720, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb3cd242-89f9-480b-ade2-db8cfb11d920": {"__data__": {"id_": "bb3cd242-89f9-480b-ade2-db8cfb11d920", "embedding": null, "metadata": {"page_label": "346", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b9e152ee-53fa-4018-902a-7c51d497294f", "node_type": "4", "metadata": {"page_label": "346", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "38beec92290301a1e5ffde9e0921691de5a60b2b2ef6fdebfa6bad1d8c96c978", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n346 References\n[ZZ18] J. Zhang and S. Zhang. A cubic regularized Newton\u2019s method over\nRiemannian manifolds. arXiv preprint arXiv:1805.05565, 2018. [see\np153]", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 415, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d1f64769-24dc-4070-8618-978cbbe8bec1": {"__data__": {"id_": "d1f64769-24dc-4070-8618-978cbbe8bec1", "embedding": null, "metadata": {"page_label": "347", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e0b257d8-3d67-4f8a-bc49-0bd2a65d4b21", "node_type": "4", "metadata": {"page_label": "347", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "bcaadf9bd925fef78ce6a5dd2cf28eab93b6825a524ecf67a322099af36ab3ad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\nIndex\n[\u00b7], see equivalence class\nF(M)-linear, 48, 97\nII, see second fundamental form\nW, see Weingarten map\n\u2207, see connection\n\u2299, see Hadamard product\nR-linear, 97\n\u223c, see equivalence relation\n\u227b, see positive definite\n\u2ab0, see positive semidefinite\nA, see assumption\nacceleration, 105, 205\naccumulation point, 54\naction (of a vector field), 89\nadaptive regularization with cubics, 152\nadjoint (of a linear map), 23\naffine connection, see connecion\naffine invariant metric, 327\nalgorithms\nbacktracking, see line search\nconjugate gradients, 125\nRiemannian gradient descent, 57\nRiemannian Newton\u2019s method, 122\nRiemannian trust regions, 131\ntruncated conjugate gradients, 144\nambient space, 207\nARC, see adaptive regularization with cubics\nArmijo, see line search\nassumption\nA4.1 (lower-bounded f), 57\nA4.2 (sufficient decrease), 58\nA4.3 (Lipschitz-type first order), 59\nA6.1 (Hk first order), 133\nA6.2 (Hk second order), 133\nA6.3 (Cauchy step), 134\nA6.4 (eigenstep), 134\nA6.5 (lower-bounded f), 135\nA6.6 (Lipschitz-type first order), 135\nA6.7 (Lipschitz-type second order), 135\nA6.8 (Lipschitz-type gradient norm), 141\nA6.9 (retraction distortion), 147\natlas, 184\nmaximal, 184\ntopology, 189\nautomatic differentiation, 78\nbacktracking, see line search\nbase (of a tangent vector), 37\nbasis (for a topology), 189\nbasis (of a linear space), 21\nBeltrami\u2013Klein model, 174\nbounded, metric space, 262\nBrockett cost function, 11\nbump function, 98\nbundle\ntangent, 37, 197\ntensor, 296\nvector, 286\nBures\u2013Wasserstein metric, 249\ncanonical inner product, see Frobenius inner\nproduct\ncanonical projection, 216\nCartan\u2013Hadamard manifold, 307\nCauchy point, 134\nCG, see conjugate gradients\nchart, 182\ncompatible, 184\ncheap gradient principle, 78\nCk, 185\nClairaut\u2013Schwarz theorem, 24\nclosed manifold, 211\nclosed set, see topology\ncommutator, 89\ncompact manifold, 191\ncompact set, 191\ncompatible charts, 184\ncomplete\nEhresmann, see Ehresmann complete\nmetric space, see metrically complete\nRiemannian manifold, see geodesically\ncomplete\ncomplex circle, phases, 158\ncomplex differentiable, 73\ncondition number, 129, 321\nconjugate gradients, 125\nconnected components, 260\nconnected components (O(n)), 163", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2430, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "48295fdd-e8a5-44bc-bfce-f020fb85f4eb": {"__data__": {"id_": "48295fdd-e8a5-44bc-bfce-f020fb85f4eb", "embedding": null, "metadata": {"page_label": "348", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6ddb4227-96d3-4160-99b4-dcc7527c59be", "node_type": "4", "metadata": {"page_label": "348", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "7650d32cc58d40d03c630a16afb5a598cdc802d59614e4f1041d6afa65434798", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n348 Index\nconnected, topological space, 260\nconnection, 86, 97, 203\nEuclidean, 89\nRiemannian, 89\nsymmetric, 89\ntorsion free, 89\nconstrained optimization, 2, 179, 180\ncontinuous map, 188\ncontraction mapping theorem, 66\ncontravariant, 301\nconvergent sequence, 54\nconvex function, 308\ngeodesically, see geodesically convex\nfunction\nstrictly, 308\nstrongly, 308\nconvex set, 308\ngeodesically, see geodesically convex set\ncoordinate frame, 47\ncoordinate representative, 183, 185\ncoordinate vector field, 198, 200\ncoordinates, 182\ncost function, 53\ncotangent vector field, 49\ncovariant, 301\ncovariant derivative, 86, 97, 204\ncovariant derivative, induced, see induced\ncovariant derivative\ncovariant derivative, total, 296\ncovector, 301\ncritical point\nfirst order, 55\nsecond order, 120\nsecond order, strict, 120\ncurve segment, 261\nlength, 261\nminimizing, 262\npiecewise, 261\nregular, 261\nsmooth, 261\ndefining function, see local defining function\nderivation, 89, 94, 117, 194\ndescend (quotient), 223\ndiffeomorphism, 27\ndifferentiable structure, see smooth structure\ndifferential\ncomplex, 73\nembedded submanifold, 35\nlinear space, 24\nmatrix factorizations, 75\nmatrix function, 74\ndimension\nlinear space, 21\nmanifold, 30, 185\ndisjoint union, 37\ndistance, 261\ndistance, Riemannian, 261\nEhresmann complete, 258\neigenstep, 135\nembedded submanifold, see submanifold,\nembedded\nembedding space, 207\nentrywise product, see Hadamard product\nequivalence class, 10\nequivalence relation, 10\nessential manifold (camera pose), 180\nEuclidean connection, 89\nEuclidean gradient, 24\nEuclidean Hessian, 24\nEuclidean inner product, 21\nEuclidean metric, 41, see Euclidean inner\nproduct\nEuclidean norm, 22\nEuclidean space, 22\nexponential map, 264\ninverse, 268\non a sphere, 40\nextension (smooth map), 33\nextension lemma, 99\nextrinsic acceleration, 105, 114\nextrinsic curvature, 114\nfiber, 217\nfield\nscalar, 34\ntensor, 295\ntensor, on a curve, 298\nvector, 38, 198\nfield, tensor, 295\nfinite differences, 104, 292\nfinite subcover, 191\nfirst-order critical point, see critical point\nfirst-order stationary point, see critical point\nFisher\u2013Rao metric, 45\nfixed point, 67\nfixed-rank matrices, 165\nfoot, see base (of a tangent vector)\nFr\u00b4 echet derivative, 74\nFrobenius inner product, 22\nFrobenius norm, 22\nfunction, see map\ngeneral linear group, 220\ngeodesically complete, 262, 263\ngeodesically concave function, 311\ngeodesically convex function, 311\nstrictly, 311\nstrongly, 312\ngeodesically convex program, 313\ngeodesically convex set, 310\nstrongly, 315\ntotally, 315\ngeodesically linear function, 311\ngeodesically quasiconvex function, 312", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2857, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e674765f-8b58-490d-90ea-a460e2749993": {"__data__": {"id_": "e674765f-8b58-490d-90ea-a460e2749993", "embedding": null, "metadata": {"page_label": "349", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8282f81f-04fa-4306-b0e2-158ac8560fc9", "node_type": "4", "metadata": {"page_label": "349", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "df3242652ccdeb3c7dd3318fa6a5ca5da320d63a59435e6ea13f66370d74970b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\nIndex 349\ngeodesics, 106, 205\nhorizontal, 248\nmaximal, 264\nminimizing, 262\non a sphere, 40, 106\ngeometric program, 326\nglobal convergence, 64\nglobal frame, 47\ngradient\nEuclidean, 24\nRiemannian, 42, 201\nGrassmann manifold\nembedded geometry, 253\ngroup, 219\nrigid motion, 180\nspecial Euclidean, 180\ntranslations, 220\ngroup action, 220\nfree, 221\nisometric, 233\nproper, 221\nsmooth, 220\ngroup, Lie, 220\nH-conjugate, 126\nHadamard manifold, see Cartan\u2013Hadamard\nmanifold\nHadamard product, 22\nhairy ball theorem, 47\nHausdorff, see topology\nHeine\u2013Borel property, 262\nHessian\nEuclidean, 24\nRiemannian, 95, 204\nsignature, 110\nsymmetric, see Clairaut\u2013Schwarz theorem\nhomeomorphism, 189\nHopf\u2013Rinow theorem, 263\nhorizontal curve, 246\nhorizontal distribution, 255\nhorizontal geodesic, 248\nhorizontal lift, see lift, horizontal\nhorizontal space, 224\nhorizontal vector field, 226\nhyperbolic space, 174\nhyperboloid model, 174\nIFT, see inverse function theorem\nimage (of a linear map), 21\ninclusion map, 206\ninduced covariant derivative, 101, 205, 298\ninduced metric, 41\ninjectivity radius, 265\ninner product, 21\ninner product (pseudo), 174\ninterior, relative, 314\ninterior, set, 313\nintrinsic, 3\nintrinsic acceleration, see acceleration\ninvariance, 213\ninverse function theorem, 27\nJacobian, 124\nkernel, 21\nKKT conditions, 179\nKoszul formula, 93\nKrylov subspace, 127\nKurdyka\u2013 Lojasiewicz inequality, 82\nLagrange multipliers, 179\nlength, curve segment, 261\nLevi-Civita, see Riemannian connection\nLICQ, 179\nLie bracket, 89\nLie group, 220\nlift (of a map), 223\nlift, horizontal\nof a vector, 225\nof a vector field, 226\nof a vector field on a curve, 246\nlimit point, 54\nline search, 57, 62\nlinear manifold, 26\nlinear map, 21\nlinear operator, see linear map\nlinear space, 21\nLipschitz continuous\nEuclidean gradient, 60, 81\non a metric space, 274\nRiemannian gradient, 277, 281\nRiemannian Hessian, 279, 283\ntensor field, 299\ntensor field of order 2, 279\nvector field, 277\nLipschitz-type assumption, 61, 284, 301\nlocal convergence, 65\nat least linear, 65\nat least quadratic, 66\nR-linear, Q-linear, 65\nsuperlinear, 65\nlocal defining function, 26, 207\nlocal frame, 46\northonormal, 49\nlocal section, 223\nlog-barrier function, 324\nLog-Euclidean metric, 327\nlogarithmic map, 266, 268\nmanifold, 190\nmanifold*, 185\nManopt\ncheckgradient, 79\u201380\ncheckhessian, 150\ndexpm, dlogm, dsqrtm, 74\negrad2rgrad, 43, 44, 158, 245\nehess2rhess, 110\u2013112, 158, 245", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2667, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "75673f48-d3d0-47e3-9b32-8b3955087ae2": {"__data__": {"id_": "75673f48-d3d0-47e3-9b32-8b3955087ae2", "embedding": null, "metadata": {"page_label": "350", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3493933d-f315-4006-9040-419979d2e68b", "node_type": "4", "metadata": {"page_label": "350", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "9d442e534f6b20633ab710a8d1c4c254ac8c4df7fb003f9708fcc57aae92c88b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n350 Index\ntangent2ambient, 173\nmanifolds (factories), 155, 324, 327\nmap\ncontinuous, see continuous map\ndiffeomorphic, see diffeomorphism\nhomeomorphic, see homeomorphism\nlinear, see linear map\nopen, see open map\nproper, see proper map\nquotient, see quotient map\nsmooth, see smooth map\nmatrices of fixed rank, 165\nmatrix function, 74\nmatrix-free solver, 124\nmaximal atlas, 184\nmaximal geodesic, 264\nmetric, 201\naffine invariant, 327\nBures\u2013Wasserstein, 249\nEuclidean, see Euclidean metric\nFisher\u2013Rao, 45\ninduced, see induced metric\nLog-Euclidean, 327\nproduct, see product metric\nRiemannian, see Riemannian metric\nSasaki, 304\nmetric (on a manifold), 41\nmetric projection, 40, 114\nmetric space, 261\nmetric topology, 261\nmetrically complete, 262\nminimizer\nglobal, 53\nlocal, 53\nlocal, strict, 53\nminimizing curve segment, 262\nminimizing geodesic, 262\nminimum, see minimizer\nMinkowski pseudo-inner product, 174\nM\u00a8 obius band, 51\nmusical isomorphism, 46, 48\nnatural projection, see canonical projection\nneighborhood, 21, 32\nNewton equations, 122\nNewton step, 122\nNewton\u2019s method, 122\nnorm\nEuclidean, see Euclidean norm\noperator, see operator norm\nnormal space, 111, 161\nnull space, see kernel\nobjective function, 53\noblique manifold, 8, 158\none-form, 48\nopen cover, 191\nopen map, 218\nopen set, see topology\nopen submanifold, 26, 32\noperator, see linear map\noperator norm, 23\noptimality conditions\nfirst-order, necessary, 55\nsecond-order, necessary, 120\nsecond-order, sufficient, 121\noptimization algorithms, see algorithms\noptimizer, see minimizer\norbit, 221\norbit space, 221\norientable manifold, 51\northogonal group, 163, 220\northogonal projector\nto a manifold, 114\nto a tangent space, 43\northogonal vectors, 21\northonormal basis, 22\northonormal local frame, 49\northonormal matrices, 159\northonormal vectors, 22\nparallel frame, 272\nparallel tensor field, 297\nparallel translation, see parallel transport\nparallel transport, 271\nparallel vector field on a curve, 271\nparallelizable manifold, 47\nparameterization (local), 182\npartition of unity, 51, 191\nPCA, see principal component analysis\nphases, 158\nPoincar\u00b4 e ball model, 174\nPoincar\u00b4 e half-space model, 174\npolar retraction, 161\nPolyak\u2013 Lojasiewicz inequality, 82, 320\npositive definite, 23\npositive semidefinite, 23\nposynomial, 325\nprincipal component analysis, 6, 8\nproduct connection, 88\nproduct manifold\nconnection, 88\nconnection, Riemannian, 94\ndifferentials, 37\ndistance, 263\nembedded submanifolds, 32\nexponential map, 270\ngeneral manifolds, 191\ngeodesically convex sets, 314\ngeodesics, 106\ngradient, 46\nHessian, 96\ninduced covariant derivative, 105\nmetric, 42\nparallel transport, 274", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2895, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "413aaf1d-5361-4689-bcf5-a3b21ca31b8e": {"__data__": {"id_": "413aaf1d-5361-4689-bcf5-a3b21ca31b8e", "embedding": null, "metadata": {"page_label": "351", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "779388a3-56a8-46d5-9e95-71fb414b8c25", "node_type": "4", "metadata": {"page_label": "351", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "806b552f449119aa2b87afda2a7ea797b589e20909fe6c5aa27340d4993ce72f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\nIndex 351\nproduct of spheres, see oblique manifold\nretraction, 40\ntable, summary, 156\ntangent bundle, 39\nproduct metric, 42\nprojector, see orthogonal projector\nproper embedding, 51, 208\nproper map, 221\npullback, 55\nQ-factor, 160\nquotient manifold, 10, 216\nquotient map, 216\nquotient set, 10, 216\nquotient space, 216\nquotient topology, 216\nrange, see image (of a linear map)\nRayleigh quotient, 6, 9, 36\nglobal minimizers, 96, 245\ngradient, 44\nHessian, 96, 244\nreal projective space, 186\nregular submanifold, see submanifold,\nembedded\nrestriction, 24, 33\nretraction, 39, 199\ndifferential, 287, 291\ninverse, 269\nmetric projection, 40, 115\npolar factor, 161\nprojective, 118\nQ-factor, 160\nsecond order, 108, 115, 206\nthird order, 301, 302\ntopological, 208\ntruncated SVD, 168\nRGD, see Riemannian Gradient Descent\nRicci identity, 302\nRiemannian\nconnection, 89, 204\ndistance, 261\ngradient, 42, 201\ngradient descent, 57\nHessian, 95, 204\nisometry, 326\nmanifold, 41, 201\nmetric, 41, 201\nNewton\u2019s method, 122\nproduct manifold, 42\nquotient manifold, 233\nsubmanifold, 41\nsubmersion, 233\ntrust region method, 131\nrigid motion group, 180\nroot, see base (of a tangent vector)\nrotation group, see special orthogonal group\nRTR, see Riemannian trust regions\nSasaki metric, 304\nscalar field, 34\nSDP, see semidefinite program\nsecond fundamental form, 112, 210\nsecond-countable, see topology\nsecond-order critical point, see critical point\nsecond-order stationary point, see critical\npoint\nsection, local, 223\nsection, of a vector bundle, 286\nsection, zero, see zero section\nsectional curvature, 174\nself-adjoint linear map, 23\nsemidefinite program, 14\nshape space, 180\nsimplex, 14, 32, 45\nsingular value, 24\nsmooth at a point (map), 24, 33, 185\nsmooth manifold, see manifold\nsmooth map\nk-times differentiable, 185\nembedded submanifold, 33\nextension, 33\nlinear space, 24\nmanifold, 185\nsmooth structure, 185\nspan, 21\nspecial Euclidean group, 180, 220\nspecial orthogonal group, 164, 220\nspeed, 108, 261\nsphere\nproduct of spheres, see oblique manifold\nstar shaped, 116, 264\nstationary point, see critical point\nStiefel manifold, 159\nsublevel set, 61\nsubmanifold\nembedded in a linear space, 26\nembedded in a manifold, 37, 207\nimmersed, 51, 207\nlinear, 26\nopen, 26, 32, 190, 207\nproperly embedded, 51\nregular, see embedded\nRiemannian, 41, 209\nsubmersion, 218\nsubmersion, Riemannian, 233\nsubsequence, 54\nsubspace, see linear space\nsubspace topology, 32\nsuccessful step, 138\nsufficient decrease, 58\nsymmetric linear map, see self-adjoint linear\nmap\ntangent bundle, 37, 197\ntangent covector, 301", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2822, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9b2cc135-d7f5-4c0c-b191-0be9af6e2295": {"__data__": {"id_": "9b2cc135-d7f5-4c0c-b191-0be9af6e2295", "embedding": null, "metadata": {"page_label": "352", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dbe2a69f-e4ba-4656-8ab8-169e83be275c", "node_type": "4", "metadata": {"page_label": "352", "file_name": "IntroOptimManifolds_Boumal_2023.pdf", "file_path": "C:\\Users\\faiya\\AI-Fellow\\course_materials\\IntroOptimManifolds_Boumal_2023.pdf", "file_type": "application/pdf", "file_size": 5725959, "creation_date": "2025-11-20", "last_modified_date": "2025-11-20"}, "hash": "734f0e1ec8f2bd8c72d95fb32213ea0f08b51c68702da64692c282eaa628e997", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Material published by Cambridge University Press, https://cambridge.org/9781009166157. This pre-publication version is free for personal use only.\nSections, theorems, equations, etc. are numbered identically to the published version. Page numbering differs.\n352 Index\ntangent space, 30, 194\ntangent vector, 30, 194\ntangent\u2013cotangent isomorphism, see musical\nisomorphism\nTaylor expansion\norder 1, 55\norder 2, 107\norder 3, 299\ntCG, see truncated conjugate gradients\ntensor\nfixed rank, 180\ntensor bundle, 296\ntensor field, 295, 296\ntensor field on a curve, 298\ntopological manifold, 210\ntopological space, 188\ntopology, 21\natlas, 189\nHausdorff, 54, 189\nmanifold, 188\nmetric, 261\nquotient, 216\nsecond countable, 190\nsubmanifold, 32\nsubspace, 32, 189\ntotal space, 216\ntrace (of a matrix), 22\ntrace inner product, see Frobenius inner\nproduct\ntransport\nparallel, 271\ntransporter, 286\nvector, 290\ntruncated conjugate gradients, 144\ntrust region, 131\ntrust-region method, 131\ntrust-region subproblem, 131\ntubular neighborhood, 51, 118, 208\nunsuccessful step, 138\nvector bundle, 286\nvector field, 38, 198\nhorizontal, 226\nvector field on a curve, 101, 205\nvector space, see linear space\nvector transport, 290\nvelocity, 105, 199\nvertical space, 224\nWeingarten map, 112, 210\nzero section, 265", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1279, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}}